{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 100.0,
  "eval_steps": 500,
  "global_step": 10700,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009431181844974948,
      "grad_norm": 0.7654640078544617,
      "learning_rate": 0.0,
      "loss": 1.7455,
      "step": 1
    },
    {
      "epoch": 0.018862363689949896,
      "grad_norm": 0.5938794016838074,
      "learning_rate": 6.230529595015577e-08,
      "loss": 1.649,
      "step": 2
    },
    {
      "epoch": 0.028293545534924844,
      "grad_norm": 0.721000075340271,
      "learning_rate": 1.2461059190031153e-07,
      "loss": 1.686,
      "step": 3
    },
    {
      "epoch": 0.03772472737989979,
      "grad_norm": 0.7538696527481079,
      "learning_rate": 1.8691588785046729e-07,
      "loss": 1.7527,
      "step": 4
    },
    {
      "epoch": 0.04715590922487474,
      "grad_norm": 0.6481522917747498,
      "learning_rate": 2.4922118380062307e-07,
      "loss": 1.6976,
      "step": 5
    },
    {
      "epoch": 0.05658709106984969,
      "grad_norm": 0.668575644493103,
      "learning_rate": 3.1152647975077885e-07,
      "loss": 1.7083,
      "step": 6
    },
    {
      "epoch": 0.06601827291482464,
      "grad_norm": 0.6996336579322815,
      "learning_rate": 3.7383177570093457e-07,
      "loss": 1.6664,
      "step": 7
    },
    {
      "epoch": 0.07544945475979958,
      "grad_norm": 0.706834077835083,
      "learning_rate": 4.3613707165109035e-07,
      "loss": 1.6676,
      "step": 8
    },
    {
      "epoch": 0.08488063660477453,
      "grad_norm": 0.6753709316253662,
      "learning_rate": 4.984423676012461e-07,
      "loss": 1.6673,
      "step": 9
    },
    {
      "epoch": 0.09431181844974948,
      "grad_norm": 0.6355971097946167,
      "learning_rate": 5.607476635514019e-07,
      "loss": 1.6638,
      "step": 10
    },
    {
      "epoch": 0.10374300029472443,
      "grad_norm": 0.6809191107749939,
      "learning_rate": 6.230529595015577e-07,
      "loss": 1.6821,
      "step": 11
    },
    {
      "epoch": 0.11317418213969938,
      "grad_norm": 0.7577161192893982,
      "learning_rate": 6.853582554517134e-07,
      "loss": 1.7305,
      "step": 12
    },
    {
      "epoch": 0.12260536398467432,
      "grad_norm": 0.7492413520812988,
      "learning_rate": 7.476635514018691e-07,
      "loss": 1.7741,
      "step": 13
    },
    {
      "epoch": 0.13203654582964927,
      "grad_norm": 0.6763694882392883,
      "learning_rate": 8.099688473520249e-07,
      "loss": 1.7001,
      "step": 14
    },
    {
      "epoch": 0.14146772767462423,
      "grad_norm": 0.7911478877067566,
      "learning_rate": 8.722741433021807e-07,
      "loss": 1.7458,
      "step": 15
    },
    {
      "epoch": 0.15089890951959917,
      "grad_norm": 0.7049124240875244,
      "learning_rate": 9.345794392523365e-07,
      "loss": 1.6909,
      "step": 16
    },
    {
      "epoch": 0.16033009136457413,
      "grad_norm": 0.6701962947845459,
      "learning_rate": 9.968847352024923e-07,
      "loss": 1.7157,
      "step": 17
    },
    {
      "epoch": 0.16976127320954906,
      "grad_norm": 0.7013756632804871,
      "learning_rate": 1.059190031152648e-06,
      "loss": 1.7502,
      "step": 18
    },
    {
      "epoch": 0.17919245505452402,
      "grad_norm": 0.8361197113990784,
      "learning_rate": 1.1214953271028038e-06,
      "loss": 1.7582,
      "step": 19
    },
    {
      "epoch": 0.18862363689949896,
      "grad_norm": 0.715390682220459,
      "learning_rate": 1.1838006230529595e-06,
      "loss": 1.7362,
      "step": 20
    },
    {
      "epoch": 0.19805481874447392,
      "grad_norm": 0.6374483704566956,
      "learning_rate": 1.2461059190031154e-06,
      "loss": 1.6743,
      "step": 21
    },
    {
      "epoch": 0.20748600058944885,
      "grad_norm": 0.782860279083252,
      "learning_rate": 1.308411214953271e-06,
      "loss": 1.7556,
      "step": 22
    },
    {
      "epoch": 0.21691718243442382,
      "grad_norm": 0.7323156595230103,
      "learning_rate": 1.3707165109034267e-06,
      "loss": 1.7108,
      "step": 23
    },
    {
      "epoch": 0.22634836427939875,
      "grad_norm": 0.6277539134025574,
      "learning_rate": 1.4330218068535826e-06,
      "loss": 1.6674,
      "step": 24
    },
    {
      "epoch": 0.2357795461243737,
      "grad_norm": 0.6519919633865356,
      "learning_rate": 1.4953271028037383e-06,
      "loss": 1.666,
      "step": 25
    },
    {
      "epoch": 0.24521072796934865,
      "grad_norm": 0.654103696346283,
      "learning_rate": 1.5576323987538942e-06,
      "loss": 1.6085,
      "step": 26
    },
    {
      "epoch": 0.2546419098143236,
      "grad_norm": 0.8641164898872375,
      "learning_rate": 1.6199376947040499e-06,
      "loss": 1.7316,
      "step": 27
    },
    {
      "epoch": 0.26407309165929854,
      "grad_norm": 0.6328187584877014,
      "learning_rate": 1.6822429906542057e-06,
      "loss": 1.7029,
      "step": 28
    },
    {
      "epoch": 0.27350427350427353,
      "grad_norm": 0.7159401178359985,
      "learning_rate": 1.7445482866043614e-06,
      "loss": 1.6908,
      "step": 29
    },
    {
      "epoch": 0.28293545534924847,
      "grad_norm": 0.6565799117088318,
      "learning_rate": 1.8068535825545173e-06,
      "loss": 1.7075,
      "step": 30
    },
    {
      "epoch": 0.2923666371942234,
      "grad_norm": 0.6749705076217651,
      "learning_rate": 1.869158878504673e-06,
      "loss": 1.6907,
      "step": 31
    },
    {
      "epoch": 0.30179781903919833,
      "grad_norm": 0.675059974193573,
      "learning_rate": 1.9314641744548286e-06,
      "loss": 1.696,
      "step": 32
    },
    {
      "epoch": 0.3112290008841733,
      "grad_norm": 0.6839864253997803,
      "learning_rate": 1.9937694704049845e-06,
      "loss": 1.7052,
      "step": 33
    },
    {
      "epoch": 0.32066018272914826,
      "grad_norm": 0.6475088000297546,
      "learning_rate": 2.0560747663551404e-06,
      "loss": 1.6745,
      "step": 34
    },
    {
      "epoch": 0.3300913645741232,
      "grad_norm": 0.6926602721214294,
      "learning_rate": 2.118380062305296e-06,
      "loss": 1.7123,
      "step": 35
    },
    {
      "epoch": 0.3395225464190981,
      "grad_norm": 0.676397442817688,
      "learning_rate": 2.1806853582554518e-06,
      "loss": 1.6978,
      "step": 36
    },
    {
      "epoch": 0.3489537282640731,
      "grad_norm": 0.7528534531593323,
      "learning_rate": 2.2429906542056077e-06,
      "loss": 1.7102,
      "step": 37
    },
    {
      "epoch": 0.35838491010904805,
      "grad_norm": 0.7317143678665161,
      "learning_rate": 2.3052959501557635e-06,
      "loss": 1.6999,
      "step": 38
    },
    {
      "epoch": 0.367816091954023,
      "grad_norm": 0.7488920092582703,
      "learning_rate": 2.367601246105919e-06,
      "loss": 1.6877,
      "step": 39
    },
    {
      "epoch": 0.3772472737989979,
      "grad_norm": 0.6754742860794067,
      "learning_rate": 2.429906542056075e-06,
      "loss": 1.7072,
      "step": 40
    },
    {
      "epoch": 0.3866784556439729,
      "grad_norm": 0.6849754452705383,
      "learning_rate": 2.4922118380062308e-06,
      "loss": 1.6955,
      "step": 41
    },
    {
      "epoch": 0.39610963748894784,
      "grad_norm": 0.720705509185791,
      "learning_rate": 2.5545171339563862e-06,
      "loss": 1.7166,
      "step": 42
    },
    {
      "epoch": 0.4055408193339228,
      "grad_norm": 0.7476597428321838,
      "learning_rate": 2.616822429906542e-06,
      "loss": 1.6967,
      "step": 43
    },
    {
      "epoch": 0.4149720011788977,
      "grad_norm": 0.8653843402862549,
      "learning_rate": 2.6791277258566976e-06,
      "loss": 1.7947,
      "step": 44
    },
    {
      "epoch": 0.4244031830238727,
      "grad_norm": 0.7264969348907471,
      "learning_rate": 2.7414330218068535e-06,
      "loss": 1.707,
      "step": 45
    },
    {
      "epoch": 0.43383436486884763,
      "grad_norm": 0.7451070547103882,
      "learning_rate": 2.8037383177570094e-06,
      "loss": 1.702,
      "step": 46
    },
    {
      "epoch": 0.44326554671382257,
      "grad_norm": 0.7570288777351379,
      "learning_rate": 2.8660436137071652e-06,
      "loss": 1.7242,
      "step": 47
    },
    {
      "epoch": 0.4526967285587975,
      "grad_norm": 0.9144569635391235,
      "learning_rate": 2.9283489096573207e-06,
      "loss": 1.7471,
      "step": 48
    },
    {
      "epoch": 0.4621279104037725,
      "grad_norm": 0.7882859110832214,
      "learning_rate": 2.9906542056074766e-06,
      "loss": 1.681,
      "step": 49
    },
    {
      "epoch": 0.4715590922487474,
      "grad_norm": 0.7711055278778076,
      "learning_rate": 3.0529595015576325e-06,
      "loss": 1.6864,
      "step": 50
    },
    {
      "epoch": 0.48099027409372236,
      "grad_norm": 0.6980492472648621,
      "learning_rate": 3.1152647975077884e-06,
      "loss": 1.6534,
      "step": 51
    },
    {
      "epoch": 0.4904214559386973,
      "grad_norm": 0.8615327477455139,
      "learning_rate": 3.177570093457944e-06,
      "loss": 1.7075,
      "step": 52
    },
    {
      "epoch": 0.4998526377836723,
      "grad_norm": 0.7812010049819946,
      "learning_rate": 3.2398753894080997e-06,
      "loss": 1.6915,
      "step": 53
    },
    {
      "epoch": 0.5092838196286472,
      "grad_norm": 0.8518781661987305,
      "learning_rate": 3.3021806853582556e-06,
      "loss": 1.7739,
      "step": 54
    },
    {
      "epoch": 0.5187150014736222,
      "grad_norm": 0.7487072348594666,
      "learning_rate": 3.3644859813084115e-06,
      "loss": 1.6971,
      "step": 55
    },
    {
      "epoch": 0.5281461833185971,
      "grad_norm": 0.7855203151702881,
      "learning_rate": 3.426791277258567e-06,
      "loss": 1.6937,
      "step": 56
    },
    {
      "epoch": 0.537577365163572,
      "grad_norm": 0.7537444233894348,
      "learning_rate": 3.489096573208723e-06,
      "loss": 1.7052,
      "step": 57
    },
    {
      "epoch": 0.5470085470085471,
      "grad_norm": 0.6973413825035095,
      "learning_rate": 3.5514018691588787e-06,
      "loss": 1.6638,
      "step": 58
    },
    {
      "epoch": 0.556439728853522,
      "grad_norm": 0.8053691983222961,
      "learning_rate": 3.6137071651090346e-06,
      "loss": 1.715,
      "step": 59
    },
    {
      "epoch": 0.5658709106984969,
      "grad_norm": 0.9291551113128662,
      "learning_rate": 3.67601246105919e-06,
      "loss": 1.7786,
      "step": 60
    },
    {
      "epoch": 0.5753020925434719,
      "grad_norm": 0.7046278715133667,
      "learning_rate": 3.738317757009346e-06,
      "loss": 1.6813,
      "step": 61
    },
    {
      "epoch": 0.5847332743884468,
      "grad_norm": 0.7660468816757202,
      "learning_rate": 3.800623052959502e-06,
      "loss": 1.6924,
      "step": 62
    },
    {
      "epoch": 0.5941644562334217,
      "grad_norm": 0.8008422255516052,
      "learning_rate": 3.862928348909657e-06,
      "loss": 1.7348,
      "step": 63
    },
    {
      "epoch": 0.6035956380783967,
      "grad_norm": 0.7867982983589172,
      "learning_rate": 3.925233644859814e-06,
      "loss": 1.7205,
      "step": 64
    },
    {
      "epoch": 0.6130268199233716,
      "grad_norm": 0.72528076171875,
      "learning_rate": 3.987538940809969e-06,
      "loss": 1.6331,
      "step": 65
    },
    {
      "epoch": 0.6224580017683466,
      "grad_norm": 0.8032469749450684,
      "learning_rate": 4.0498442367601245e-06,
      "loss": 1.6546,
      "step": 66
    },
    {
      "epoch": 0.6318891836133216,
      "grad_norm": 0.8850103616714478,
      "learning_rate": 4.112149532710281e-06,
      "loss": 1.7381,
      "step": 67
    },
    {
      "epoch": 0.6413203654582965,
      "grad_norm": 0.8036268353462219,
      "learning_rate": 4.174454828660436e-06,
      "loss": 1.6973,
      "step": 68
    },
    {
      "epoch": 0.6507515473032714,
      "grad_norm": 0.8213297128677368,
      "learning_rate": 4.236760124610592e-06,
      "loss": 1.7211,
      "step": 69
    },
    {
      "epoch": 0.6601827291482464,
      "grad_norm": 0.6686810255050659,
      "learning_rate": 4.299065420560748e-06,
      "loss": 1.6153,
      "step": 70
    },
    {
      "epoch": 0.6696139109932213,
      "grad_norm": 0.8684684038162231,
      "learning_rate": 4.3613707165109035e-06,
      "loss": 1.7735,
      "step": 71
    },
    {
      "epoch": 0.6790450928381963,
      "grad_norm": 0.8903486132621765,
      "learning_rate": 4.42367601246106e-06,
      "loss": 1.7498,
      "step": 72
    },
    {
      "epoch": 0.6884762746831712,
      "grad_norm": 0.8338139057159424,
      "learning_rate": 4.485981308411215e-06,
      "loss": 1.6622,
      "step": 73
    },
    {
      "epoch": 0.6979074565281462,
      "grad_norm": 0.8331015706062317,
      "learning_rate": 4.548286604361371e-06,
      "loss": 1.7039,
      "step": 74
    },
    {
      "epoch": 0.7073386383731212,
      "grad_norm": 0.8496482372283936,
      "learning_rate": 4.610591900311527e-06,
      "loss": 1.695,
      "step": 75
    },
    {
      "epoch": 0.7167698202180961,
      "grad_norm": 0.8414210081100464,
      "learning_rate": 4.6728971962616825e-06,
      "loss": 1.6899,
      "step": 76
    },
    {
      "epoch": 0.726201002063071,
      "grad_norm": 0.9512236714363098,
      "learning_rate": 4.735202492211838e-06,
      "loss": 1.773,
      "step": 77
    },
    {
      "epoch": 0.735632183908046,
      "grad_norm": 0.7720208168029785,
      "learning_rate": 4.797507788161994e-06,
      "loss": 1.6277,
      "step": 78
    },
    {
      "epoch": 0.7450633657530209,
      "grad_norm": 0.7981156706809998,
      "learning_rate": 4.85981308411215e-06,
      "loss": 1.6045,
      "step": 79
    },
    {
      "epoch": 0.7544945475979958,
      "grad_norm": 0.9061678647994995,
      "learning_rate": 4.922118380062306e-06,
      "loss": 1.6868,
      "step": 80
    },
    {
      "epoch": 0.7639257294429708,
      "grad_norm": 0.8052390813827515,
      "learning_rate": 4.9844236760124615e-06,
      "loss": 1.6515,
      "step": 81
    },
    {
      "epoch": 0.7733569112879458,
      "grad_norm": 0.8688295483589172,
      "learning_rate": 5.046728971962617e-06,
      "loss": 1.7181,
      "step": 82
    },
    {
      "epoch": 0.7827880931329207,
      "grad_norm": 0.8621992468833923,
      "learning_rate": 5.1090342679127725e-06,
      "loss": 1.6528,
      "step": 83
    },
    {
      "epoch": 0.7922192749778957,
      "grad_norm": 0.8612547516822815,
      "learning_rate": 5.171339563862928e-06,
      "loss": 1.6771,
      "step": 84
    },
    {
      "epoch": 0.8016504568228706,
      "grad_norm": 0.8498473167419434,
      "learning_rate": 5.233644859813084e-06,
      "loss": 1.7042,
      "step": 85
    },
    {
      "epoch": 0.8110816386678456,
      "grad_norm": 0.8406471610069275,
      "learning_rate": 5.29595015576324e-06,
      "loss": 1.6212,
      "step": 86
    },
    {
      "epoch": 0.8205128205128205,
      "grad_norm": 0.860084056854248,
      "learning_rate": 5.358255451713395e-06,
      "loss": 1.6603,
      "step": 87
    },
    {
      "epoch": 0.8299440023577954,
      "grad_norm": 0.7796592712402344,
      "learning_rate": 5.4205607476635515e-06,
      "loss": 1.6289,
      "step": 88
    },
    {
      "epoch": 0.8393751842027705,
      "grad_norm": 0.8942676186561584,
      "learning_rate": 5.482866043613707e-06,
      "loss": 1.6387,
      "step": 89
    },
    {
      "epoch": 0.8488063660477454,
      "grad_norm": 0.8789440989494324,
      "learning_rate": 5.545171339563863e-06,
      "loss": 1.6271,
      "step": 90
    },
    {
      "epoch": 0.8582375478927203,
      "grad_norm": 0.9169065356254578,
      "learning_rate": 5.607476635514019e-06,
      "loss": 1.6105,
      "step": 91
    },
    {
      "epoch": 0.8676687297376953,
      "grad_norm": 0.8961508870124817,
      "learning_rate": 5.669781931464174e-06,
      "loss": 1.6203,
      "step": 92
    },
    {
      "epoch": 0.8770999115826702,
      "grad_norm": 0.8272176384925842,
      "learning_rate": 5.7320872274143305e-06,
      "loss": 1.6296,
      "step": 93
    },
    {
      "epoch": 0.8865310934276451,
      "grad_norm": 0.8919678330421448,
      "learning_rate": 5.794392523364486e-06,
      "loss": 1.6428,
      "step": 94
    },
    {
      "epoch": 0.8959622752726201,
      "grad_norm": 0.9409650564193726,
      "learning_rate": 5.856697819314641e-06,
      "loss": 1.6342,
      "step": 95
    },
    {
      "epoch": 0.905393457117595,
      "grad_norm": 0.8326494097709656,
      "learning_rate": 5.919003115264798e-06,
      "loss": 1.599,
      "step": 96
    },
    {
      "epoch": 0.91482463896257,
      "grad_norm": 0.8184787631034851,
      "learning_rate": 5.981308411214953e-06,
      "loss": 1.5901,
      "step": 97
    },
    {
      "epoch": 0.924255820807545,
      "grad_norm": 0.8790477514266968,
      "learning_rate": 6.0436137071651095e-06,
      "loss": 1.6161,
      "step": 98
    },
    {
      "epoch": 0.9336870026525199,
      "grad_norm": 0.9342159628868103,
      "learning_rate": 6.105919003115265e-06,
      "loss": 1.6365,
      "step": 99
    },
    {
      "epoch": 0.9431181844974948,
      "grad_norm": 0.8461617231369019,
      "learning_rate": 6.16822429906542e-06,
      "loss": 1.5613,
      "step": 100
    },
    {
      "epoch": 0.9525493663424698,
      "grad_norm": 0.8610730767250061,
      "learning_rate": 6.230529595015577e-06,
      "loss": 1.6064,
      "step": 101
    },
    {
      "epoch": 0.9619805481874447,
      "grad_norm": 0.8880638480186462,
      "learning_rate": 6.292834890965732e-06,
      "loss": 1.639,
      "step": 102
    },
    {
      "epoch": 0.9714117300324197,
      "grad_norm": 0.9025282859802246,
      "learning_rate": 6.355140186915888e-06,
      "loss": 1.598,
      "step": 103
    },
    {
      "epoch": 0.9808429118773946,
      "grad_norm": 0.8820400834083557,
      "learning_rate": 6.417445482866044e-06,
      "loss": 1.5737,
      "step": 104
    },
    {
      "epoch": 0.9902740937223696,
      "grad_norm": 0.942744791507721,
      "learning_rate": 6.479750778816199e-06,
      "loss": 1.6157,
      "step": 105
    },
    {
      "epoch": 0.9997052755673446,
      "grad_norm": 0.8801680207252502,
      "learning_rate": 6.542056074766355e-06,
      "loss": 1.6127,
      "step": 106
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.616720676422119,
      "learning_rate": 6.604361370716511e-06,
      "loss": 2.5473,
      "step": 107
    },
    {
      "epoch": 1.009431181844975,
      "grad_norm": 0.8855125904083252,
      "learning_rate": 6.666666666666667e-06,
      "loss": 1.5798,
      "step": 108
    },
    {
      "epoch": 1.0188623636899499,
      "grad_norm": 0.9711706638336182,
      "learning_rate": 6.728971962616823e-06,
      "loss": 1.5874,
      "step": 109
    },
    {
      "epoch": 1.028293545534925,
      "grad_norm": 1.026625394821167,
      "learning_rate": 6.791277258566978e-06,
      "loss": 1.6269,
      "step": 110
    },
    {
      "epoch": 1.0377247273798997,
      "grad_norm": 0.953822672367096,
      "learning_rate": 6.853582554517134e-06,
      "loss": 1.5626,
      "step": 111
    },
    {
      "epoch": 1.0471559092248748,
      "grad_norm": 0.9215110540390015,
      "learning_rate": 6.91588785046729e-06,
      "loss": 1.5129,
      "step": 112
    },
    {
      "epoch": 1.0565870910698496,
      "grad_norm": 1.0554875135421753,
      "learning_rate": 6.978193146417446e-06,
      "loss": 1.5752,
      "step": 113
    },
    {
      "epoch": 1.0660182729148246,
      "grad_norm": 0.9737895131111145,
      "learning_rate": 7.040498442367601e-06,
      "loss": 1.5853,
      "step": 114
    },
    {
      "epoch": 1.0754494547597995,
      "grad_norm": 0.9268094301223755,
      "learning_rate": 7.1028037383177574e-06,
      "loss": 1.5285,
      "step": 115
    },
    {
      "epoch": 1.0848806366047745,
      "grad_norm": 0.9738226532936096,
      "learning_rate": 7.165109034267913e-06,
      "loss": 1.5692,
      "step": 116
    },
    {
      "epoch": 1.0943118184497496,
      "grad_norm": 1.0181024074554443,
      "learning_rate": 7.227414330218069e-06,
      "loss": 1.5437,
      "step": 117
    },
    {
      "epoch": 1.1037430002947244,
      "grad_norm": 1.0745354890823364,
      "learning_rate": 7.289719626168225e-06,
      "loss": 1.5395,
      "step": 118
    },
    {
      "epoch": 1.1131741821396994,
      "grad_norm": 0.8795785903930664,
      "learning_rate": 7.35202492211838e-06,
      "loss": 1.5108,
      "step": 119
    },
    {
      "epoch": 1.1226053639846743,
      "grad_norm": 0.9081806540489197,
      "learning_rate": 7.4143302180685364e-06,
      "loss": 1.5647,
      "step": 120
    },
    {
      "epoch": 1.1320365458296493,
      "grad_norm": 0.9944812059402466,
      "learning_rate": 7.476635514018692e-06,
      "loss": 1.5025,
      "step": 121
    },
    {
      "epoch": 1.1414677276746241,
      "grad_norm": 0.9528874158859253,
      "learning_rate": 7.538940809968847e-06,
      "loss": 1.5274,
      "step": 122
    },
    {
      "epoch": 1.1508989095195992,
      "grad_norm": 0.973410964012146,
      "learning_rate": 7.601246105919004e-06,
      "loss": 1.5357,
      "step": 123
    },
    {
      "epoch": 1.1603300913645742,
      "grad_norm": 0.9602660536766052,
      "learning_rate": 7.663551401869159e-06,
      "loss": 1.5247,
      "step": 124
    },
    {
      "epoch": 1.169761273209549,
      "grad_norm": 0.9598841667175293,
      "learning_rate": 7.725856697819315e-06,
      "loss": 1.5226,
      "step": 125
    },
    {
      "epoch": 1.179192455054524,
      "grad_norm": 1.043658971786499,
      "learning_rate": 7.78816199376947e-06,
      "loss": 1.5151,
      "step": 126
    },
    {
      "epoch": 1.188623636899499,
      "grad_norm": 0.9893640875816345,
      "learning_rate": 7.850467289719627e-06,
      "loss": 1.4919,
      "step": 127
    },
    {
      "epoch": 1.198054818744474,
      "grad_norm": 0.9767170548439026,
      "learning_rate": 7.912772585669783e-06,
      "loss": 1.5091,
      "step": 128
    },
    {
      "epoch": 1.2074860005894488,
      "grad_norm": 0.995823323726654,
      "learning_rate": 7.975077881619938e-06,
      "loss": 1.4396,
      "step": 129
    },
    {
      "epoch": 1.2169171824344238,
      "grad_norm": 0.9689407348632812,
      "learning_rate": 8.037383177570094e-06,
      "loss": 1.4932,
      "step": 130
    },
    {
      "epoch": 1.2263483642793989,
      "grad_norm": 0.890282154083252,
      "learning_rate": 8.099688473520249e-06,
      "loss": 1.4625,
      "step": 131
    },
    {
      "epoch": 1.2357795461243737,
      "grad_norm": 0.920741617679596,
      "learning_rate": 8.161993769470406e-06,
      "loss": 1.4507,
      "step": 132
    },
    {
      "epoch": 1.2452107279693487,
      "grad_norm": 0.9492821097373962,
      "learning_rate": 8.224299065420562e-06,
      "loss": 1.467,
      "step": 133
    },
    {
      "epoch": 1.2546419098143236,
      "grad_norm": 0.9451605677604675,
      "learning_rate": 8.286604361370717e-06,
      "loss": 1.4324,
      "step": 134
    },
    {
      "epoch": 1.2640730916592986,
      "grad_norm": 0.8752357959747314,
      "learning_rate": 8.348909657320873e-06,
      "loss": 1.4912,
      "step": 135
    },
    {
      "epoch": 1.2735042735042734,
      "grad_norm": 0.8841747045516968,
      "learning_rate": 8.411214953271028e-06,
      "loss": 1.4527,
      "step": 136
    },
    {
      "epoch": 1.2829354553492485,
      "grad_norm": 0.8200156092643738,
      "learning_rate": 8.473520249221184e-06,
      "loss": 1.4482,
      "step": 137
    },
    {
      "epoch": 1.2923666371942235,
      "grad_norm": 0.9405101537704468,
      "learning_rate": 8.53582554517134e-06,
      "loss": 1.4282,
      "step": 138
    },
    {
      "epoch": 1.3017978190391983,
      "grad_norm": 0.8714200854301453,
      "learning_rate": 8.598130841121496e-06,
      "loss": 1.4231,
      "step": 139
    },
    {
      "epoch": 1.3112290008841734,
      "grad_norm": 0.8698478937149048,
      "learning_rate": 8.660436137071652e-06,
      "loss": 1.4087,
      "step": 140
    },
    {
      "epoch": 1.3206601827291482,
      "grad_norm": 0.9562171697616577,
      "learning_rate": 8.722741433021807e-06,
      "loss": 1.4629,
      "step": 141
    },
    {
      "epoch": 1.3300913645741232,
      "grad_norm": 0.872170627117157,
      "learning_rate": 8.785046728971963e-06,
      "loss": 1.4154,
      "step": 142
    },
    {
      "epoch": 1.339522546419098,
      "grad_norm": 0.8987860679626465,
      "learning_rate": 8.84735202492212e-06,
      "loss": 1.4338,
      "step": 143
    },
    {
      "epoch": 1.3489537282640731,
      "grad_norm": 0.8258122205734253,
      "learning_rate": 8.909657320872275e-06,
      "loss": 1.4442,
      "step": 144
    },
    {
      "epoch": 1.3583849101090482,
      "grad_norm": 0.805101215839386,
      "learning_rate": 8.97196261682243e-06,
      "loss": 1.3866,
      "step": 145
    },
    {
      "epoch": 1.367816091954023,
      "grad_norm": 0.8931204676628113,
      "learning_rate": 9.034267912772586e-06,
      "loss": 1.4473,
      "step": 146
    },
    {
      "epoch": 1.3772472737989978,
      "grad_norm": 0.7943611145019531,
      "learning_rate": 9.096573208722742e-06,
      "loss": 1.4251,
      "step": 147
    },
    {
      "epoch": 1.3866784556439729,
      "grad_norm": 0.777804434299469,
      "learning_rate": 9.158878504672899e-06,
      "loss": 1.3611,
      "step": 148
    },
    {
      "epoch": 1.396109637488948,
      "grad_norm": 0.8196223378181458,
      "learning_rate": 9.221183800623054e-06,
      "loss": 1.4002,
      "step": 149
    },
    {
      "epoch": 1.4055408193339227,
      "grad_norm": 0.8181077241897583,
      "learning_rate": 9.28348909657321e-06,
      "loss": 1.3875,
      "step": 150
    },
    {
      "epoch": 1.4149720011788978,
      "grad_norm": 0.8931801915168762,
      "learning_rate": 9.345794392523365e-06,
      "loss": 1.3681,
      "step": 151
    },
    {
      "epoch": 1.4244031830238728,
      "grad_norm": 0.8811408281326294,
      "learning_rate": 9.40809968847352e-06,
      "loss": 1.3719,
      "step": 152
    },
    {
      "epoch": 1.4338343648688476,
      "grad_norm": 0.8991786241531372,
      "learning_rate": 9.470404984423676e-06,
      "loss": 1.3947,
      "step": 153
    },
    {
      "epoch": 1.4432655467138225,
      "grad_norm": 0.9217286705970764,
      "learning_rate": 9.532710280373833e-06,
      "loss": 1.3693,
      "step": 154
    },
    {
      "epoch": 1.4526967285587975,
      "grad_norm": 0.90018630027771,
      "learning_rate": 9.595015576323989e-06,
      "loss": 1.4054,
      "step": 155
    },
    {
      "epoch": 1.4621279104037725,
      "grad_norm": 0.9666885137557983,
      "learning_rate": 9.657320872274144e-06,
      "loss": 1.3642,
      "step": 156
    },
    {
      "epoch": 1.4715590922487474,
      "grad_norm": 0.9692031741142273,
      "learning_rate": 9.7196261682243e-06,
      "loss": 1.3544,
      "step": 157
    },
    {
      "epoch": 1.4809902740937224,
      "grad_norm": 0.9654529690742493,
      "learning_rate": 9.781931464174455e-06,
      "loss": 1.3607,
      "step": 158
    },
    {
      "epoch": 1.4904214559386972,
      "grad_norm": 0.9256429076194763,
      "learning_rate": 9.844236760124612e-06,
      "loss": 1.3788,
      "step": 159
    },
    {
      "epoch": 1.4998526377836723,
      "grad_norm": 0.8808382153511047,
      "learning_rate": 9.906542056074768e-06,
      "loss": 1.305,
      "step": 160
    },
    {
      "epoch": 1.509283819628647,
      "grad_norm": 0.8711930513381958,
      "learning_rate": 9.968847352024923e-06,
      "loss": 1.3346,
      "step": 161
    },
    {
      "epoch": 1.5187150014736222,
      "grad_norm": 0.8950181603431702,
      "learning_rate": 1.0031152647975077e-05,
      "loss": 1.3079,
      "step": 162
    },
    {
      "epoch": 1.5281461833185972,
      "grad_norm": 0.8773860931396484,
      "learning_rate": 1.0093457943925234e-05,
      "loss": 1.338,
      "step": 163
    },
    {
      "epoch": 1.537577365163572,
      "grad_norm": 1.0735057592391968,
      "learning_rate": 1.015576323987539e-05,
      "loss": 1.2796,
      "step": 164
    },
    {
      "epoch": 1.547008547008547,
      "grad_norm": 1.0379823446273804,
      "learning_rate": 1.0218068535825545e-05,
      "loss": 1.2956,
      "step": 165
    },
    {
      "epoch": 1.556439728853522,
      "grad_norm": 0.8855969309806824,
      "learning_rate": 1.02803738317757e-05,
      "loss": 1.29,
      "step": 166
    },
    {
      "epoch": 1.565870910698497,
      "grad_norm": 1.0399141311645508,
      "learning_rate": 1.0342679127725856e-05,
      "loss": 1.3083,
      "step": 167
    },
    {
      "epoch": 1.5753020925434718,
      "grad_norm": 1.0241589546203613,
      "learning_rate": 1.0404984423676013e-05,
      "loss": 1.2859,
      "step": 168
    },
    {
      "epoch": 1.5847332743884468,
      "grad_norm": 0.9300718307495117,
      "learning_rate": 1.0467289719626168e-05,
      "loss": 1.2786,
      "step": 169
    },
    {
      "epoch": 1.5941644562334218,
      "grad_norm": 1.0085563659667969,
      "learning_rate": 1.0529595015576324e-05,
      "loss": 1.241,
      "step": 170
    },
    {
      "epoch": 1.6035956380783967,
      "grad_norm": 1.0078132152557373,
      "learning_rate": 1.059190031152648e-05,
      "loss": 1.3182,
      "step": 171
    },
    {
      "epoch": 1.6130268199233715,
      "grad_norm": 0.936501145362854,
      "learning_rate": 1.0654205607476635e-05,
      "loss": 1.2625,
      "step": 172
    },
    {
      "epoch": 1.6224580017683468,
      "grad_norm": 0.8827139735221863,
      "learning_rate": 1.071651090342679e-05,
      "loss": 1.2896,
      "step": 173
    },
    {
      "epoch": 1.6318891836133216,
      "grad_norm": 0.8099584579467773,
      "learning_rate": 1.0778816199376947e-05,
      "loss": 1.2955,
      "step": 174
    },
    {
      "epoch": 1.6413203654582964,
      "grad_norm": 0.9125014543533325,
      "learning_rate": 1.0841121495327103e-05,
      "loss": 1.2159,
      "step": 175
    },
    {
      "epoch": 1.6507515473032714,
      "grad_norm": 0.7758941650390625,
      "learning_rate": 1.0903426791277258e-05,
      "loss": 1.2704,
      "step": 176
    },
    {
      "epoch": 1.6601827291482465,
      "grad_norm": 0.7539327144622803,
      "learning_rate": 1.0965732087227414e-05,
      "loss": 1.2352,
      "step": 177
    },
    {
      "epoch": 1.6696139109932213,
      "grad_norm": 0.7294136881828308,
      "learning_rate": 1.102803738317757e-05,
      "loss": 1.243,
      "step": 178
    },
    {
      "epoch": 1.6790450928381961,
      "grad_norm": 0.6932767033576965,
      "learning_rate": 1.1090342679127726e-05,
      "loss": 1.2566,
      "step": 179
    },
    {
      "epoch": 1.6884762746831712,
      "grad_norm": 0.5864138603210449,
      "learning_rate": 1.1152647975077882e-05,
      "loss": 1.2158,
      "step": 180
    },
    {
      "epoch": 1.6979074565281462,
      "grad_norm": 0.5644065737724304,
      "learning_rate": 1.1214953271028037e-05,
      "loss": 1.2483,
      "step": 181
    },
    {
      "epoch": 1.707338638373121,
      "grad_norm": 0.5518339276313782,
      "learning_rate": 1.1277258566978193e-05,
      "loss": 1.2387,
      "step": 182
    },
    {
      "epoch": 1.716769820218096,
      "grad_norm": 0.5405808091163635,
      "learning_rate": 1.1339563862928348e-05,
      "loss": 1.1829,
      "step": 183
    },
    {
      "epoch": 1.7262010020630711,
      "grad_norm": 0.5193924307823181,
      "learning_rate": 1.1401869158878505e-05,
      "loss": 1.2844,
      "step": 184
    },
    {
      "epoch": 1.735632183908046,
      "grad_norm": 0.5300827622413635,
      "learning_rate": 1.1464174454828661e-05,
      "loss": 1.1991,
      "step": 185
    },
    {
      "epoch": 1.7450633657530208,
      "grad_norm": 0.49045681953430176,
      "learning_rate": 1.1526479750778816e-05,
      "loss": 1.232,
      "step": 186
    },
    {
      "epoch": 1.7544945475979958,
      "grad_norm": 0.4855671525001526,
      "learning_rate": 1.1588785046728972e-05,
      "loss": 1.2448,
      "step": 187
    },
    {
      "epoch": 1.7639257294429709,
      "grad_norm": 0.4772137403488159,
      "learning_rate": 1.1651090342679127e-05,
      "loss": 1.3013,
      "step": 188
    },
    {
      "epoch": 1.7733569112879457,
      "grad_norm": 0.4564795196056366,
      "learning_rate": 1.1713395638629283e-05,
      "loss": 1.2605,
      "step": 189
    },
    {
      "epoch": 1.7827880931329207,
      "grad_norm": 0.45045459270477295,
      "learning_rate": 1.177570093457944e-05,
      "loss": 1.2387,
      "step": 190
    },
    {
      "epoch": 1.7922192749778958,
      "grad_norm": 0.4491231143474579,
      "learning_rate": 1.1838006230529595e-05,
      "loss": 1.2811,
      "step": 191
    },
    {
      "epoch": 1.8016504568228706,
      "grad_norm": 0.4139431118965149,
      "learning_rate": 1.1900311526479751e-05,
      "loss": 1.2279,
      "step": 192
    },
    {
      "epoch": 1.8110816386678454,
      "grad_norm": 0.45780256390571594,
      "learning_rate": 1.1962616822429906e-05,
      "loss": 1.2158,
      "step": 193
    },
    {
      "epoch": 1.8205128205128205,
      "grad_norm": 0.4273248612880707,
      "learning_rate": 1.2024922118380062e-05,
      "loss": 1.2365,
      "step": 194
    },
    {
      "epoch": 1.8299440023577955,
      "grad_norm": 0.41679641604423523,
      "learning_rate": 1.2087227414330219e-05,
      "loss": 1.1953,
      "step": 195
    },
    {
      "epoch": 1.8393751842027704,
      "grad_norm": 0.3914400041103363,
      "learning_rate": 1.2149532710280374e-05,
      "loss": 1.2156,
      "step": 196
    },
    {
      "epoch": 1.8488063660477454,
      "grad_norm": 0.4099777936935425,
      "learning_rate": 1.221183800623053e-05,
      "loss": 1.2073,
      "step": 197
    },
    {
      "epoch": 1.8582375478927204,
      "grad_norm": 0.41393348574638367,
      "learning_rate": 1.2274143302180685e-05,
      "loss": 1.1794,
      "step": 198
    },
    {
      "epoch": 1.8676687297376953,
      "grad_norm": 0.3880572021007538,
      "learning_rate": 1.233644859813084e-05,
      "loss": 1.2497,
      "step": 199
    },
    {
      "epoch": 1.87709991158267,
      "grad_norm": 0.37813130021095276,
      "learning_rate": 1.2398753894080996e-05,
      "loss": 1.2296,
      "step": 200
    },
    {
      "epoch": 1.8865310934276451,
      "grad_norm": 0.3657616078853607,
      "learning_rate": 1.2461059190031153e-05,
      "loss": 1.1927,
      "step": 201
    },
    {
      "epoch": 1.8959622752726202,
      "grad_norm": 0.35973885655403137,
      "learning_rate": 1.2523364485981309e-05,
      "loss": 1.2219,
      "step": 202
    },
    {
      "epoch": 1.905393457117595,
      "grad_norm": 0.4011574685573578,
      "learning_rate": 1.2585669781931464e-05,
      "loss": 1.2148,
      "step": 203
    },
    {
      "epoch": 1.91482463896257,
      "grad_norm": 0.3607649803161621,
      "learning_rate": 1.264797507788162e-05,
      "loss": 1.1925,
      "step": 204
    },
    {
      "epoch": 1.924255820807545,
      "grad_norm": 0.33912643790245056,
      "learning_rate": 1.2710280373831775e-05,
      "loss": 1.2512,
      "step": 205
    },
    {
      "epoch": 1.93368700265252,
      "grad_norm": 0.3535759449005127,
      "learning_rate": 1.2772585669781932e-05,
      "loss": 1.2026,
      "step": 206
    },
    {
      "epoch": 1.9431181844974947,
      "grad_norm": 0.3547133505344391,
      "learning_rate": 1.2834890965732088e-05,
      "loss": 1.1969,
      "step": 207
    },
    {
      "epoch": 1.9525493663424698,
      "grad_norm": 0.3510802984237671,
      "learning_rate": 1.2897196261682243e-05,
      "loss": 1.2193,
      "step": 208
    },
    {
      "epoch": 1.9619805481874448,
      "grad_norm": 0.34051981568336487,
      "learning_rate": 1.2959501557632399e-05,
      "loss": 1.1939,
      "step": 209
    },
    {
      "epoch": 1.9714117300324197,
      "grad_norm": 0.32934707403182983,
      "learning_rate": 1.3021806853582554e-05,
      "loss": 1.2293,
      "step": 210
    },
    {
      "epoch": 1.9808429118773945,
      "grad_norm": 0.3296586573123932,
      "learning_rate": 1.308411214953271e-05,
      "loss": 1.229,
      "step": 211
    },
    {
      "epoch": 1.9902740937223697,
      "grad_norm": 0.3347749710083008,
      "learning_rate": 1.3146417445482867e-05,
      "loss": 1.206,
      "step": 212
    },
    {
      "epoch": 1.9997052755673446,
      "grad_norm": 0.33265650272369385,
      "learning_rate": 1.3208722741433022e-05,
      "loss": 1.2252,
      "step": 213
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.10310959815979,
      "learning_rate": 1.3271028037383178e-05,
      "loss": 1.0118,
      "step": 214
    },
    {
      "epoch": 2.009431181844975,
      "grad_norm": 0.3213876187801361,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 1.1823,
      "step": 215
    },
    {
      "epoch": 2.01886236368995,
      "grad_norm": 0.3094445765018463,
      "learning_rate": 1.3395638629283489e-05,
      "loss": 1.21,
      "step": 216
    },
    {
      "epoch": 2.028293545534925,
      "grad_norm": 0.3207859396934509,
      "learning_rate": 1.3457943925233646e-05,
      "loss": 1.1949,
      "step": 217
    },
    {
      "epoch": 2.0377247273798997,
      "grad_norm": 0.32482102513313293,
      "learning_rate": 1.3520249221183801e-05,
      "loss": 1.1956,
      "step": 218
    },
    {
      "epoch": 2.0471559092248746,
      "grad_norm": 0.33430010080337524,
      "learning_rate": 1.3582554517133957e-05,
      "loss": 1.1844,
      "step": 219
    },
    {
      "epoch": 2.05658709106985,
      "grad_norm": 0.3097274601459503,
      "learning_rate": 1.3644859813084112e-05,
      "loss": 1.1978,
      "step": 220
    },
    {
      "epoch": 2.0660182729148246,
      "grad_norm": 0.3213635981082916,
      "learning_rate": 1.3707165109034268e-05,
      "loss": 1.1931,
      "step": 221
    },
    {
      "epoch": 2.0754494547597995,
      "grad_norm": 0.3222464919090271,
      "learning_rate": 1.3769470404984425e-05,
      "loss": 1.2607,
      "step": 222
    },
    {
      "epoch": 2.0848806366047747,
      "grad_norm": 0.2950114607810974,
      "learning_rate": 1.383177570093458e-05,
      "loss": 1.1823,
      "step": 223
    },
    {
      "epoch": 2.0943118184497496,
      "grad_norm": 0.3165590763092041,
      "learning_rate": 1.3894080996884736e-05,
      "loss": 1.2286,
      "step": 224
    },
    {
      "epoch": 2.1037430002947244,
      "grad_norm": 0.2993481159210205,
      "learning_rate": 1.3956386292834891e-05,
      "loss": 1.1896,
      "step": 225
    },
    {
      "epoch": 2.113174182139699,
      "grad_norm": 0.3096025288105011,
      "learning_rate": 1.4018691588785047e-05,
      "loss": 1.1972,
      "step": 226
    },
    {
      "epoch": 2.1226053639846745,
      "grad_norm": 0.29699182510375977,
      "learning_rate": 1.4080996884735202e-05,
      "loss": 1.1711,
      "step": 227
    },
    {
      "epoch": 2.1320365458296493,
      "grad_norm": 0.2965775430202484,
      "learning_rate": 1.414330218068536e-05,
      "loss": 1.1249,
      "step": 228
    },
    {
      "epoch": 2.141467727674624,
      "grad_norm": 0.3035668432712555,
      "learning_rate": 1.4205607476635515e-05,
      "loss": 1.2058,
      "step": 229
    },
    {
      "epoch": 2.150898909519599,
      "grad_norm": 0.2923632264137268,
      "learning_rate": 1.426791277258567e-05,
      "loss": 1.208,
      "step": 230
    },
    {
      "epoch": 2.160330091364574,
      "grad_norm": 0.29909881949424744,
      "learning_rate": 1.4330218068535826e-05,
      "loss": 1.1805,
      "step": 231
    },
    {
      "epoch": 2.169761273209549,
      "grad_norm": 0.30544957518577576,
      "learning_rate": 1.4392523364485981e-05,
      "loss": 1.2357,
      "step": 232
    },
    {
      "epoch": 2.179192455054524,
      "grad_norm": 0.2810581922531128,
      "learning_rate": 1.4454828660436138e-05,
      "loss": 1.2303,
      "step": 233
    },
    {
      "epoch": 2.188623636899499,
      "grad_norm": 0.2890477776527405,
      "learning_rate": 1.4517133956386294e-05,
      "loss": 1.1779,
      "step": 234
    },
    {
      "epoch": 2.198054818744474,
      "grad_norm": 0.2779369652271271,
      "learning_rate": 1.457943925233645e-05,
      "loss": 1.26,
      "step": 235
    },
    {
      "epoch": 2.2074860005894488,
      "grad_norm": 0.2821201980113983,
      "learning_rate": 1.4641744548286605e-05,
      "loss": 1.1636,
      "step": 236
    },
    {
      "epoch": 2.2169171824344236,
      "grad_norm": 0.3149077892303467,
      "learning_rate": 1.470404984423676e-05,
      "loss": 1.1963,
      "step": 237
    },
    {
      "epoch": 2.226348364279399,
      "grad_norm": 0.28350338339805603,
      "learning_rate": 1.4766355140186916e-05,
      "loss": 1.1941,
      "step": 238
    },
    {
      "epoch": 2.2357795461243737,
      "grad_norm": 0.26094555854797363,
      "learning_rate": 1.4828660436137073e-05,
      "loss": 1.1363,
      "step": 239
    },
    {
      "epoch": 2.2452107279693485,
      "grad_norm": 0.2984012961387634,
      "learning_rate": 1.4890965732087228e-05,
      "loss": 1.2136,
      "step": 240
    },
    {
      "epoch": 2.2546419098143238,
      "grad_norm": 0.2824409604072571,
      "learning_rate": 1.4953271028037384e-05,
      "loss": 1.2112,
      "step": 241
    },
    {
      "epoch": 2.2640730916592986,
      "grad_norm": 0.258469820022583,
      "learning_rate": 1.501557632398754e-05,
      "loss": 1.1656,
      "step": 242
    },
    {
      "epoch": 2.2735042735042734,
      "grad_norm": 0.272185355424881,
      "learning_rate": 1.5077881619937695e-05,
      "loss": 1.1205,
      "step": 243
    },
    {
      "epoch": 2.2829354553492482,
      "grad_norm": 0.2777515649795532,
      "learning_rate": 1.5140186915887852e-05,
      "loss": 1.1827,
      "step": 244
    },
    {
      "epoch": 2.2923666371942235,
      "grad_norm": 0.2644251585006714,
      "learning_rate": 1.5202492211838007e-05,
      "loss": 1.1793,
      "step": 245
    },
    {
      "epoch": 2.3017978190391983,
      "grad_norm": 0.2787662148475647,
      "learning_rate": 1.5264797507788163e-05,
      "loss": 1.1855,
      "step": 246
    },
    {
      "epoch": 2.311229000884173,
      "grad_norm": 0.2636808753013611,
      "learning_rate": 1.5327102803738318e-05,
      "loss": 1.1711,
      "step": 247
    },
    {
      "epoch": 2.3206601827291484,
      "grad_norm": 0.2757667303085327,
      "learning_rate": 1.5389408099688474e-05,
      "loss": 1.1561,
      "step": 248
    },
    {
      "epoch": 2.3300913645741232,
      "grad_norm": 0.2665824294090271,
      "learning_rate": 1.545171339563863e-05,
      "loss": 1.1762,
      "step": 249
    },
    {
      "epoch": 2.339522546419098,
      "grad_norm": 0.256639301776886,
      "learning_rate": 1.5514018691588785e-05,
      "loss": 1.1788,
      "step": 250
    },
    {
      "epoch": 2.348953728264073,
      "grad_norm": 0.2728768587112427,
      "learning_rate": 1.557632398753894e-05,
      "loss": 1.1787,
      "step": 251
    },
    {
      "epoch": 2.358384910109048,
      "grad_norm": 0.26378053426742554,
      "learning_rate": 1.56386292834891e-05,
      "loss": 1.1676,
      "step": 252
    },
    {
      "epoch": 2.367816091954023,
      "grad_norm": 0.257930725812912,
      "learning_rate": 1.5700934579439254e-05,
      "loss": 1.1595,
      "step": 253
    },
    {
      "epoch": 2.377247273798998,
      "grad_norm": 0.26173749566078186,
      "learning_rate": 1.576323987538941e-05,
      "loss": 1.1764,
      "step": 254
    },
    {
      "epoch": 2.386678455643973,
      "grad_norm": 0.27103298902511597,
      "learning_rate": 1.5825545171339565e-05,
      "loss": 1.2282,
      "step": 255
    },
    {
      "epoch": 2.396109637488948,
      "grad_norm": 0.26379624009132385,
      "learning_rate": 1.588785046728972e-05,
      "loss": 1.2291,
      "step": 256
    },
    {
      "epoch": 2.4055408193339227,
      "grad_norm": 0.25321507453918457,
      "learning_rate": 1.5950155763239876e-05,
      "loss": 1.2109,
      "step": 257
    },
    {
      "epoch": 2.4149720011788975,
      "grad_norm": 0.2447948455810547,
      "learning_rate": 1.6012461059190032e-05,
      "loss": 1.1962,
      "step": 258
    },
    {
      "epoch": 2.424403183023873,
      "grad_norm": 0.2751834988594055,
      "learning_rate": 1.6074766355140187e-05,
      "loss": 1.1944,
      "step": 259
    },
    {
      "epoch": 2.4338343648688476,
      "grad_norm": 0.2605203688144684,
      "learning_rate": 1.6137071651090343e-05,
      "loss": 1.2088,
      "step": 260
    },
    {
      "epoch": 2.4432655467138225,
      "grad_norm": 0.24951331317424774,
      "learning_rate": 1.6199376947040498e-05,
      "loss": 1.1384,
      "step": 261
    },
    {
      "epoch": 2.4526967285587977,
      "grad_norm": 0.26953786611557007,
      "learning_rate": 1.6261682242990654e-05,
      "loss": 1.219,
      "step": 262
    },
    {
      "epoch": 2.4621279104037725,
      "grad_norm": 0.25067955255508423,
      "learning_rate": 1.6323987538940812e-05,
      "loss": 1.1965,
      "step": 263
    },
    {
      "epoch": 2.4715590922487474,
      "grad_norm": 0.22910158336162567,
      "learning_rate": 1.6386292834890968e-05,
      "loss": 1.1864,
      "step": 264
    },
    {
      "epoch": 2.480990274093722,
      "grad_norm": 0.26765310764312744,
      "learning_rate": 1.6448598130841123e-05,
      "loss": 1.1847,
      "step": 265
    },
    {
      "epoch": 2.4904214559386975,
      "grad_norm": 0.26936429738998413,
      "learning_rate": 1.651090342679128e-05,
      "loss": 1.1429,
      "step": 266
    },
    {
      "epoch": 2.4998526377836723,
      "grad_norm": 0.2599602937698364,
      "learning_rate": 1.6573208722741434e-05,
      "loss": 1.1519,
      "step": 267
    },
    {
      "epoch": 2.509283819628647,
      "grad_norm": 0.26585182547569275,
      "learning_rate": 1.663551401869159e-05,
      "loss": 1.1657,
      "step": 268
    },
    {
      "epoch": 2.5187150014736224,
      "grad_norm": 0.26591765880584717,
      "learning_rate": 1.6697819314641745e-05,
      "loss": 1.1746,
      "step": 269
    },
    {
      "epoch": 2.528146183318597,
      "grad_norm": 0.2618074417114258,
      "learning_rate": 1.67601246105919e-05,
      "loss": 1.1665,
      "step": 270
    },
    {
      "epoch": 2.537577365163572,
      "grad_norm": 0.24214406311511993,
      "learning_rate": 1.6822429906542056e-05,
      "loss": 1.2117,
      "step": 271
    },
    {
      "epoch": 2.547008547008547,
      "grad_norm": 0.2358642965555191,
      "learning_rate": 1.688473520249221e-05,
      "loss": 1.2116,
      "step": 272
    },
    {
      "epoch": 2.556439728853522,
      "grad_norm": 0.24009354412555695,
      "learning_rate": 1.6947040498442367e-05,
      "loss": 1.1875,
      "step": 273
    },
    {
      "epoch": 2.565870910698497,
      "grad_norm": 0.24896292388439178,
      "learning_rate": 1.7009345794392526e-05,
      "loss": 1.1669,
      "step": 274
    },
    {
      "epoch": 2.5753020925434718,
      "grad_norm": 0.2631851136684418,
      "learning_rate": 1.707165109034268e-05,
      "loss": 1.1858,
      "step": 275
    },
    {
      "epoch": 2.584733274388447,
      "grad_norm": 0.2572803795337677,
      "learning_rate": 1.7133956386292837e-05,
      "loss": 1.1704,
      "step": 276
    },
    {
      "epoch": 2.594164456233422,
      "grad_norm": 0.25289386510849,
      "learning_rate": 1.7196261682242992e-05,
      "loss": 1.1595,
      "step": 277
    },
    {
      "epoch": 2.6035956380783967,
      "grad_norm": 0.25388914346694946,
      "learning_rate": 1.7258566978193148e-05,
      "loss": 1.1465,
      "step": 278
    },
    {
      "epoch": 2.6130268199233715,
      "grad_norm": 0.2605651915073395,
      "learning_rate": 1.7320872274143303e-05,
      "loss": 1.2112,
      "step": 279
    },
    {
      "epoch": 2.6224580017683468,
      "grad_norm": 0.26017460227012634,
      "learning_rate": 1.738317757009346e-05,
      "loss": 1.1993,
      "step": 280
    },
    {
      "epoch": 2.6318891836133216,
      "grad_norm": 0.25563138723373413,
      "learning_rate": 1.7445482866043614e-05,
      "loss": 1.2149,
      "step": 281
    },
    {
      "epoch": 2.6413203654582964,
      "grad_norm": 0.24523092806339264,
      "learning_rate": 1.750778816199377e-05,
      "loss": 1.192,
      "step": 282
    },
    {
      "epoch": 2.6507515473032717,
      "grad_norm": 0.25830042362213135,
      "learning_rate": 1.7570093457943925e-05,
      "loss": 1.1661,
      "step": 283
    },
    {
      "epoch": 2.6601827291482465,
      "grad_norm": 0.23624952137470245,
      "learning_rate": 1.763239875389408e-05,
      "loss": 1.1683,
      "step": 284
    },
    {
      "epoch": 2.6696139109932213,
      "grad_norm": 0.253349632024765,
      "learning_rate": 1.769470404984424e-05,
      "loss": 1.17,
      "step": 285
    },
    {
      "epoch": 2.679045092838196,
      "grad_norm": 0.24778057634830475,
      "learning_rate": 1.7757009345794395e-05,
      "loss": 1.2138,
      "step": 286
    },
    {
      "epoch": 2.688476274683171,
      "grad_norm": 0.24613575637340546,
      "learning_rate": 1.781931464174455e-05,
      "loss": 1.1791,
      "step": 287
    },
    {
      "epoch": 2.6979074565281462,
      "grad_norm": 0.23730476200580597,
      "learning_rate": 1.7881619937694706e-05,
      "loss": 1.1958,
      "step": 288
    },
    {
      "epoch": 2.707338638373121,
      "grad_norm": 0.24487213790416718,
      "learning_rate": 1.794392523364486e-05,
      "loss": 1.178,
      "step": 289
    },
    {
      "epoch": 2.7167698202180963,
      "grad_norm": 0.2546743154525757,
      "learning_rate": 1.8006230529595017e-05,
      "loss": 1.1817,
      "step": 290
    },
    {
      "epoch": 2.726201002063071,
      "grad_norm": 0.2513800263404846,
      "learning_rate": 1.8068535825545172e-05,
      "loss": 1.1646,
      "step": 291
    },
    {
      "epoch": 2.735632183908046,
      "grad_norm": 0.23103146255016327,
      "learning_rate": 1.8130841121495328e-05,
      "loss": 1.1623,
      "step": 292
    },
    {
      "epoch": 2.745063365753021,
      "grad_norm": 0.2539042532444,
      "learning_rate": 1.8193146417445483e-05,
      "loss": 1.1357,
      "step": 293
    },
    {
      "epoch": 2.7544945475979956,
      "grad_norm": 0.24006985127925873,
      "learning_rate": 1.825545171339564e-05,
      "loss": 1.1922,
      "step": 294
    },
    {
      "epoch": 2.763925729442971,
      "grad_norm": 0.23085786402225494,
      "learning_rate": 1.8317757009345797e-05,
      "loss": 1.1727,
      "step": 295
    },
    {
      "epoch": 2.7733569112879457,
      "grad_norm": 0.25417327880859375,
      "learning_rate": 1.8380062305295953e-05,
      "loss": 1.1471,
      "step": 296
    },
    {
      "epoch": 2.782788093132921,
      "grad_norm": 0.243643656373024,
      "learning_rate": 1.8442367601246108e-05,
      "loss": 1.1738,
      "step": 297
    },
    {
      "epoch": 2.792219274977896,
      "grad_norm": 0.23967838287353516,
      "learning_rate": 1.8504672897196264e-05,
      "loss": 1.132,
      "step": 298
    },
    {
      "epoch": 2.8016504568228706,
      "grad_norm": 0.24009720981121063,
      "learning_rate": 1.856697819314642e-05,
      "loss": 1.1631,
      "step": 299
    },
    {
      "epoch": 2.8110816386678454,
      "grad_norm": 0.23822054266929626,
      "learning_rate": 1.8629283489096575e-05,
      "loss": 1.1641,
      "step": 300
    },
    {
      "epoch": 2.8205128205128203,
      "grad_norm": 0.24036946892738342,
      "learning_rate": 1.869158878504673e-05,
      "loss": 1.1881,
      "step": 301
    },
    {
      "epoch": 2.8299440023577955,
      "grad_norm": 0.25368309020996094,
      "learning_rate": 1.8753894080996886e-05,
      "loss": 1.1772,
      "step": 302
    },
    {
      "epoch": 2.8393751842027704,
      "grad_norm": 0.26098155975341797,
      "learning_rate": 1.881619937694704e-05,
      "loss": 1.1732,
      "step": 303
    },
    {
      "epoch": 2.8488063660477456,
      "grad_norm": 0.23578251898288727,
      "learning_rate": 1.8878504672897197e-05,
      "loss": 1.1966,
      "step": 304
    },
    {
      "epoch": 2.8582375478927204,
      "grad_norm": 0.23718848824501038,
      "learning_rate": 1.8940809968847352e-05,
      "loss": 1.1614,
      "step": 305
    },
    {
      "epoch": 2.8676687297376953,
      "grad_norm": 0.24909135699272156,
      "learning_rate": 1.900311526479751e-05,
      "loss": 1.1601,
      "step": 306
    },
    {
      "epoch": 2.87709991158267,
      "grad_norm": 0.24738016724586487,
      "learning_rate": 1.9065420560747666e-05,
      "loss": 1.1751,
      "step": 307
    },
    {
      "epoch": 2.886531093427645,
      "grad_norm": 0.261202335357666,
      "learning_rate": 1.9127725856697822e-05,
      "loss": 1.2128,
      "step": 308
    },
    {
      "epoch": 2.89596227527262,
      "grad_norm": 0.26559725403785706,
      "learning_rate": 1.9190031152647977e-05,
      "loss": 1.1829,
      "step": 309
    },
    {
      "epoch": 2.905393457117595,
      "grad_norm": 0.24812161922454834,
      "learning_rate": 1.9252336448598133e-05,
      "loss": 1.1585,
      "step": 310
    },
    {
      "epoch": 2.9148246389625703,
      "grad_norm": 0.23286084830760956,
      "learning_rate": 1.9314641744548288e-05,
      "loss": 1.1925,
      "step": 311
    },
    {
      "epoch": 2.924255820807545,
      "grad_norm": 0.2559203505516052,
      "learning_rate": 1.9376947040498444e-05,
      "loss": 1.1631,
      "step": 312
    },
    {
      "epoch": 2.93368700265252,
      "grad_norm": 0.24360142648220062,
      "learning_rate": 1.94392523364486e-05,
      "loss": 1.1822,
      "step": 313
    },
    {
      "epoch": 2.9431181844974947,
      "grad_norm": 0.24883300065994263,
      "learning_rate": 1.9501557632398755e-05,
      "loss": 1.2003,
      "step": 314
    },
    {
      "epoch": 2.9525493663424696,
      "grad_norm": 0.2535325884819031,
      "learning_rate": 1.956386292834891e-05,
      "loss": 1.1636,
      "step": 315
    },
    {
      "epoch": 2.961980548187445,
      "grad_norm": 0.24936076998710632,
      "learning_rate": 1.9626168224299065e-05,
      "loss": 1.1467,
      "step": 316
    },
    {
      "epoch": 2.9714117300324197,
      "grad_norm": 0.24353483319282532,
      "learning_rate": 1.9688473520249224e-05,
      "loss": 1.1686,
      "step": 317
    },
    {
      "epoch": 2.9808429118773945,
      "grad_norm": 0.25718924403190613,
      "learning_rate": 1.975077881619938e-05,
      "loss": 1.1529,
      "step": 318
    },
    {
      "epoch": 2.9902740937223697,
      "grad_norm": 0.23586733639240265,
      "learning_rate": 1.9813084112149535e-05,
      "loss": 1.1551,
      "step": 319
    },
    {
      "epoch": 2.9997052755673446,
      "grad_norm": 0.2552696466445923,
      "learning_rate": 1.987538940809969e-05,
      "loss": 1.1158,
      "step": 320
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.455202341079712,
      "learning_rate": 1.9937694704049846e-05,
      "loss": 0.7939,
      "step": 321
    },
    {
      "epoch": 3.009431181844975,
      "grad_norm": 0.2339474856853485,
      "learning_rate": 2e-05,
      "loss": 1.1481,
      "step": 322
    },
    {
      "epoch": 3.01886236368995,
      "grad_norm": 0.24249282479286194,
      "learning_rate": 1.9999999541901656e-05,
      "loss": 1.2054,
      "step": 323
    },
    {
      "epoch": 3.028293545534925,
      "grad_norm": 0.24714455008506775,
      "learning_rate": 1.9999998167606664e-05,
      "loss": 1.195,
      "step": 324
    },
    {
      "epoch": 3.0377247273798997,
      "grad_norm": 0.23195713758468628,
      "learning_rate": 1.999999587711515e-05,
      "loss": 1.1602,
      "step": 325
    },
    {
      "epoch": 3.0471559092248746,
      "grad_norm": 0.2546997666358948,
      "learning_rate": 1.9999992670427326e-05,
      "loss": 1.1266,
      "step": 326
    },
    {
      "epoch": 3.05658709106985,
      "grad_norm": 0.24203699827194214,
      "learning_rate": 1.9999988547543482e-05,
      "loss": 1.1666,
      "step": 327
    },
    {
      "epoch": 3.0660182729148246,
      "grad_norm": 0.28365036845207214,
      "learning_rate": 1.9999983508463997e-05,
      "loss": 1.2035,
      "step": 328
    },
    {
      "epoch": 3.0754494547597995,
      "grad_norm": 0.25600045919418335,
      "learning_rate": 1.999997755318934e-05,
      "loss": 1.1676,
      "step": 329
    },
    {
      "epoch": 3.0848806366047747,
      "grad_norm": 0.27817919850349426,
      "learning_rate": 1.9999970681720044e-05,
      "loss": 1.1569,
      "step": 330
    },
    {
      "epoch": 3.0943118184497496,
      "grad_norm": 0.24141646921634674,
      "learning_rate": 1.9999962894056745e-05,
      "loss": 1.1803,
      "step": 331
    },
    {
      "epoch": 3.1037430002947244,
      "grad_norm": 0.24592971801757812,
      "learning_rate": 1.9999954190200156e-05,
      "loss": 1.1786,
      "step": 332
    },
    {
      "epoch": 3.113174182139699,
      "grad_norm": 0.2592411935329437,
      "learning_rate": 1.999994457015108e-05,
      "loss": 1.1779,
      "step": 333
    },
    {
      "epoch": 3.1226053639846745,
      "grad_norm": 0.26128309965133667,
      "learning_rate": 1.9999934033910386e-05,
      "loss": 1.1456,
      "step": 334
    },
    {
      "epoch": 3.1320365458296493,
      "grad_norm": 0.2692318260669708,
      "learning_rate": 1.9999922581479046e-05,
      "loss": 1.1581,
      "step": 335
    },
    {
      "epoch": 3.141467727674624,
      "grad_norm": 0.22544214129447937,
      "learning_rate": 1.9999910212858115e-05,
      "loss": 1.1628,
      "step": 336
    },
    {
      "epoch": 3.150898909519599,
      "grad_norm": 0.2657294273376465,
      "learning_rate": 1.999989692804872e-05,
      "loss": 1.1595,
      "step": 337
    },
    {
      "epoch": 3.160330091364574,
      "grad_norm": 0.243167445063591,
      "learning_rate": 1.9999882727052074e-05,
      "loss": 1.1485,
      "step": 338
    },
    {
      "epoch": 3.169761273209549,
      "grad_norm": 0.2478320300579071,
      "learning_rate": 1.999986760986949e-05,
      "loss": 1.1335,
      "step": 339
    },
    {
      "epoch": 3.179192455054524,
      "grad_norm": 0.24596451222896576,
      "learning_rate": 1.999985157650234e-05,
      "loss": 1.1499,
      "step": 340
    },
    {
      "epoch": 3.188623636899499,
      "grad_norm": 0.24663867056369781,
      "learning_rate": 1.9999834626952104e-05,
      "loss": 1.1843,
      "step": 341
    },
    {
      "epoch": 3.198054818744474,
      "grad_norm": 0.2588484585285187,
      "learning_rate": 1.999981676122033e-05,
      "loss": 1.1541,
      "step": 342
    },
    {
      "epoch": 3.2074860005894488,
      "grad_norm": 0.2407224327325821,
      "learning_rate": 1.9999797979308648e-05,
      "loss": 1.1309,
      "step": 343
    },
    {
      "epoch": 3.2169171824344236,
      "grad_norm": 0.25702977180480957,
      "learning_rate": 1.9999778281218792e-05,
      "loss": 1.1851,
      "step": 344
    },
    {
      "epoch": 3.226348364279399,
      "grad_norm": 0.22993312776088715,
      "learning_rate": 1.999975766695256e-05,
      "loss": 1.16,
      "step": 345
    },
    {
      "epoch": 3.2357795461243737,
      "grad_norm": 0.2726621925830841,
      "learning_rate": 1.999973613651184e-05,
      "loss": 1.1721,
      "step": 346
    },
    {
      "epoch": 3.2452107279693485,
      "grad_norm": 0.2383069396018982,
      "learning_rate": 1.9999713689898605e-05,
      "loss": 1.1811,
      "step": 347
    },
    {
      "epoch": 3.2546419098143238,
      "grad_norm": 0.23498135805130005,
      "learning_rate": 1.9999690327114912e-05,
      "loss": 1.1493,
      "step": 348
    },
    {
      "epoch": 3.2640730916592986,
      "grad_norm": 0.22462166845798492,
      "learning_rate": 1.99996660481629e-05,
      "loss": 1.1648,
      "step": 349
    },
    {
      "epoch": 3.2735042735042734,
      "grad_norm": 0.2470613420009613,
      "learning_rate": 1.99996408530448e-05,
      "loss": 1.1519,
      "step": 350
    },
    {
      "epoch": 3.2829354553492482,
      "grad_norm": 0.29675301909446716,
      "learning_rate": 1.9999614741762912e-05,
      "loss": 1.188,
      "step": 351
    },
    {
      "epoch": 3.2923666371942235,
      "grad_norm": 0.2555227279663086,
      "learning_rate": 1.9999587714319634e-05,
      "loss": 1.1993,
      "step": 352
    },
    {
      "epoch": 3.3017978190391983,
      "grad_norm": 0.26602867245674133,
      "learning_rate": 1.999955977071744e-05,
      "loss": 1.1106,
      "step": 353
    },
    {
      "epoch": 3.311229000884173,
      "grad_norm": 0.234245166182518,
      "learning_rate": 1.999953091095889e-05,
      "loss": 1.1521,
      "step": 354
    },
    {
      "epoch": 3.3206601827291484,
      "grad_norm": 0.24864137172698975,
      "learning_rate": 1.9999501135046623e-05,
      "loss": 1.1759,
      "step": 355
    },
    {
      "epoch": 3.3300913645741232,
      "grad_norm": 0.2633604407310486,
      "learning_rate": 1.9999470442983376e-05,
      "loss": 1.1263,
      "step": 356
    },
    {
      "epoch": 3.339522546419098,
      "grad_norm": 0.23668625950813293,
      "learning_rate": 1.999943883477196e-05,
      "loss": 1.1151,
      "step": 357
    },
    {
      "epoch": 3.348953728264073,
      "grad_norm": 0.2630104720592499,
      "learning_rate": 1.9999406310415268e-05,
      "loss": 1.1314,
      "step": 358
    },
    {
      "epoch": 3.358384910109048,
      "grad_norm": 0.2664058804512024,
      "learning_rate": 1.9999372869916278e-05,
      "loss": 1.1753,
      "step": 359
    },
    {
      "epoch": 3.367816091954023,
      "grad_norm": 0.23079103231430054,
      "learning_rate": 1.9999338513278057e-05,
      "loss": 1.1561,
      "step": 360
    },
    {
      "epoch": 3.377247273798998,
      "grad_norm": 0.25338056683540344,
      "learning_rate": 1.999930324050375e-05,
      "loss": 1.1583,
      "step": 361
    },
    {
      "epoch": 3.386678455643973,
      "grad_norm": 0.24501170217990875,
      "learning_rate": 1.9999267051596595e-05,
      "loss": 1.1427,
      "step": 362
    },
    {
      "epoch": 3.396109637488948,
      "grad_norm": 0.2476009726524353,
      "learning_rate": 1.99992299465599e-05,
      "loss": 1.1801,
      "step": 363
    },
    {
      "epoch": 3.4055408193339227,
      "grad_norm": 0.24752593040466309,
      "learning_rate": 1.9999191925397066e-05,
      "loss": 1.2133,
      "step": 364
    },
    {
      "epoch": 3.4149720011788975,
      "grad_norm": 0.23028194904327393,
      "learning_rate": 1.9999152988111582e-05,
      "loss": 1.1556,
      "step": 365
    },
    {
      "epoch": 3.424403183023873,
      "grad_norm": 0.26808905601501465,
      "learning_rate": 1.9999113134707006e-05,
      "loss": 1.1631,
      "step": 366
    },
    {
      "epoch": 3.4338343648688476,
      "grad_norm": 0.2424982190132141,
      "learning_rate": 1.9999072365187e-05,
      "loss": 1.1981,
      "step": 367
    },
    {
      "epoch": 3.4432655467138225,
      "grad_norm": 0.2506370544433594,
      "learning_rate": 1.999903067955529e-05,
      "loss": 1.0859,
      "step": 368
    },
    {
      "epoch": 3.4526967285587977,
      "grad_norm": 0.2347106635570526,
      "learning_rate": 1.9998988077815704e-05,
      "loss": 1.1809,
      "step": 369
    },
    {
      "epoch": 3.4621279104037725,
      "grad_norm": 0.25090840458869934,
      "learning_rate": 1.999894455997214e-05,
      "loss": 1.1875,
      "step": 370
    },
    {
      "epoch": 3.4715590922487474,
      "grad_norm": 0.2665555477142334,
      "learning_rate": 1.999890012602858e-05,
      "loss": 1.1698,
      "step": 371
    },
    {
      "epoch": 3.480990274093722,
      "grad_norm": 0.2474517822265625,
      "learning_rate": 1.9998854775989106e-05,
      "loss": 1.1288,
      "step": 372
    },
    {
      "epoch": 3.4904214559386975,
      "grad_norm": 0.2658616304397583,
      "learning_rate": 1.9998808509857867e-05,
      "loss": 1.1496,
      "step": 373
    },
    {
      "epoch": 3.4998526377836723,
      "grad_norm": 0.2708302438259125,
      "learning_rate": 1.9998761327639102e-05,
      "loss": 1.1632,
      "step": 374
    },
    {
      "epoch": 3.509283819628647,
      "grad_norm": 0.2555970549583435,
      "learning_rate": 1.9998713229337133e-05,
      "loss": 1.1529,
      "step": 375
    },
    {
      "epoch": 3.5187150014736224,
      "grad_norm": 0.26345497369766235,
      "learning_rate": 1.9998664214956373e-05,
      "loss": 1.1236,
      "step": 376
    },
    {
      "epoch": 3.528146183318597,
      "grad_norm": 0.23429472744464874,
      "learning_rate": 1.99986142845013e-05,
      "loss": 1.1885,
      "step": 377
    },
    {
      "epoch": 3.537577365163572,
      "grad_norm": 0.24943828582763672,
      "learning_rate": 1.9998563437976502e-05,
      "loss": 1.1966,
      "step": 378
    },
    {
      "epoch": 3.547008547008547,
      "grad_norm": 0.2380925863981247,
      "learning_rate": 1.999851167538663e-05,
      "loss": 1.1745,
      "step": 379
    },
    {
      "epoch": 3.556439728853522,
      "grad_norm": 0.2584437429904938,
      "learning_rate": 1.999845899673643e-05,
      "loss": 1.1566,
      "step": 380
    },
    {
      "epoch": 3.565870910698497,
      "grad_norm": 0.22694659233093262,
      "learning_rate": 1.9998405402030726e-05,
      "loss": 1.1903,
      "step": 381
    },
    {
      "epoch": 3.5753020925434718,
      "grad_norm": 0.24482078850269318,
      "learning_rate": 1.9998350891274425e-05,
      "loss": 1.1493,
      "step": 382
    },
    {
      "epoch": 3.584733274388447,
      "grad_norm": 0.24775061011314392,
      "learning_rate": 1.999829546447253e-05,
      "loss": 1.1792,
      "step": 383
    },
    {
      "epoch": 3.594164456233422,
      "grad_norm": 0.27050018310546875,
      "learning_rate": 1.999823912163011e-05,
      "loss": 1.1792,
      "step": 384
    },
    {
      "epoch": 3.6035956380783967,
      "grad_norm": 0.2697140872478485,
      "learning_rate": 1.999818186275234e-05,
      "loss": 1.1943,
      "step": 385
    },
    {
      "epoch": 3.6130268199233715,
      "grad_norm": 0.23728853464126587,
      "learning_rate": 1.9998123687844452e-05,
      "loss": 1.1707,
      "step": 386
    },
    {
      "epoch": 3.6224580017683468,
      "grad_norm": 0.2726808786392212,
      "learning_rate": 1.999806459691178e-05,
      "loss": 1.1543,
      "step": 387
    },
    {
      "epoch": 3.6318891836133216,
      "grad_norm": 0.24973535537719727,
      "learning_rate": 1.9998004589959744e-05,
      "loss": 1.1664,
      "step": 388
    },
    {
      "epoch": 3.6413203654582964,
      "grad_norm": 0.23706494271755219,
      "learning_rate": 1.9997943666993836e-05,
      "loss": 1.1717,
      "step": 389
    },
    {
      "epoch": 3.6507515473032717,
      "grad_norm": 0.26369670033454895,
      "learning_rate": 1.9997881828019638e-05,
      "loss": 1.1567,
      "step": 390
    },
    {
      "epoch": 3.6601827291482465,
      "grad_norm": 0.23059867322444916,
      "learning_rate": 1.9997819073042818e-05,
      "loss": 1.1474,
      "step": 391
    },
    {
      "epoch": 3.6696139109932213,
      "grad_norm": 0.2537585496902466,
      "learning_rate": 1.9997755402069122e-05,
      "loss": 1.167,
      "step": 392
    },
    {
      "epoch": 3.679045092838196,
      "grad_norm": 0.25734877586364746,
      "learning_rate": 1.999769081510439e-05,
      "loss": 1.1579,
      "step": 393
    },
    {
      "epoch": 3.688476274683171,
      "grad_norm": 0.254589706659317,
      "learning_rate": 1.9997625312154533e-05,
      "loss": 1.1948,
      "step": 394
    },
    {
      "epoch": 3.6979074565281462,
      "grad_norm": 0.2623542845249176,
      "learning_rate": 1.9997558893225552e-05,
      "loss": 1.1718,
      "step": 395
    },
    {
      "epoch": 3.707338638373121,
      "grad_norm": 0.2644002437591553,
      "learning_rate": 1.9997491558323537e-05,
      "loss": 1.0872,
      "step": 396
    },
    {
      "epoch": 3.7167698202180963,
      "grad_norm": 0.2724987268447876,
      "learning_rate": 1.9997423307454656e-05,
      "loss": 1.1536,
      "step": 397
    },
    {
      "epoch": 3.726201002063071,
      "grad_norm": 0.24298442900180817,
      "learning_rate": 1.9997354140625162e-05,
      "loss": 1.1749,
      "step": 398
    },
    {
      "epoch": 3.735632183908046,
      "grad_norm": 0.28384917974472046,
      "learning_rate": 1.9997284057841392e-05,
      "loss": 1.1932,
      "step": 399
    },
    {
      "epoch": 3.745063365753021,
      "grad_norm": 0.24091613292694092,
      "learning_rate": 1.9997213059109765e-05,
      "loss": 1.1526,
      "step": 400
    },
    {
      "epoch": 3.7544945475979956,
      "grad_norm": 0.24851132929325104,
      "learning_rate": 1.9997141144436784e-05,
      "loss": 1.1254,
      "step": 401
    },
    {
      "epoch": 3.763925729442971,
      "grad_norm": 0.2271181046962738,
      "learning_rate": 1.9997068313829042e-05,
      "loss": 1.1646,
      "step": 402
    },
    {
      "epoch": 3.7733569112879457,
      "grad_norm": 0.23915345966815948,
      "learning_rate": 1.9996994567293214e-05,
      "loss": 1.1856,
      "step": 403
    },
    {
      "epoch": 3.782788093132921,
      "grad_norm": 0.2556590437889099,
      "learning_rate": 1.999691990483605e-05,
      "loss": 1.146,
      "step": 404
    },
    {
      "epoch": 3.792219274977896,
      "grad_norm": 0.24960549175739288,
      "learning_rate": 1.9996844326464396e-05,
      "loss": 1.1796,
      "step": 405
    },
    {
      "epoch": 3.8016504568228706,
      "grad_norm": 0.24331258237361908,
      "learning_rate": 1.9996767832185176e-05,
      "loss": 1.1352,
      "step": 406
    },
    {
      "epoch": 3.8110816386678454,
      "grad_norm": 0.26910874247550964,
      "learning_rate": 1.9996690422005394e-05,
      "loss": 1.1689,
      "step": 407
    },
    {
      "epoch": 3.8205128205128203,
      "grad_norm": 0.24662186205387115,
      "learning_rate": 1.9996612095932147e-05,
      "loss": 1.162,
      "step": 408
    },
    {
      "epoch": 3.8299440023577955,
      "grad_norm": 0.2644490897655487,
      "learning_rate": 1.9996532853972605e-05,
      "loss": 1.1669,
      "step": 409
    },
    {
      "epoch": 3.8393751842027704,
      "grad_norm": 0.2516016364097595,
      "learning_rate": 1.9996452696134037e-05,
      "loss": 1.1551,
      "step": 410
    },
    {
      "epoch": 3.8488063660477456,
      "grad_norm": 0.2650182843208313,
      "learning_rate": 1.9996371622423782e-05,
      "loss": 1.1431,
      "step": 411
    },
    {
      "epoch": 3.8582375478927204,
      "grad_norm": 0.2691153883934021,
      "learning_rate": 1.9996289632849266e-05,
      "loss": 1.1323,
      "step": 412
    },
    {
      "epoch": 3.8676687297376953,
      "grad_norm": 0.2666041851043701,
      "learning_rate": 1.9996206727418004e-05,
      "loss": 1.1213,
      "step": 413
    },
    {
      "epoch": 3.87709991158267,
      "grad_norm": 0.24536776542663574,
      "learning_rate": 1.999612290613759e-05,
      "loss": 1.18,
      "step": 414
    },
    {
      "epoch": 3.886531093427645,
      "grad_norm": 0.24114175140857697,
      "learning_rate": 1.9996038169015704e-05,
      "loss": 1.1819,
      "step": 415
    },
    {
      "epoch": 3.89596227527262,
      "grad_norm": 0.26230236887931824,
      "learning_rate": 1.9995952516060114e-05,
      "loss": 1.1357,
      "step": 416
    },
    {
      "epoch": 3.905393457117595,
      "grad_norm": 0.25293219089508057,
      "learning_rate": 1.999586594727866e-05,
      "loss": 1.1332,
      "step": 417
    },
    {
      "epoch": 3.9148246389625703,
      "grad_norm": 0.2339089959859848,
      "learning_rate": 1.999577846267928e-05,
      "loss": 1.1718,
      "step": 418
    },
    {
      "epoch": 3.924255820807545,
      "grad_norm": 0.2559797167778015,
      "learning_rate": 1.9995690062269985e-05,
      "loss": 1.1291,
      "step": 419
    },
    {
      "epoch": 3.93368700265252,
      "grad_norm": 0.2726241648197174,
      "learning_rate": 1.9995600746058877e-05,
      "loss": 1.1413,
      "step": 420
    },
    {
      "epoch": 3.9431181844974947,
      "grad_norm": 0.2521967887878418,
      "learning_rate": 1.9995510514054137e-05,
      "loss": 1.1575,
      "step": 421
    },
    {
      "epoch": 3.9525493663424696,
      "grad_norm": 0.2704184055328369,
      "learning_rate": 1.999541936626403e-05,
      "loss": 1.1768,
      "step": 422
    },
    {
      "epoch": 3.961980548187445,
      "grad_norm": 0.25551438331604004,
      "learning_rate": 1.9995327302696916e-05,
      "loss": 1.1321,
      "step": 423
    },
    {
      "epoch": 3.9714117300324197,
      "grad_norm": 0.2741276025772095,
      "learning_rate": 1.9995234323361215e-05,
      "loss": 1.1701,
      "step": 424
    },
    {
      "epoch": 3.9808429118773945,
      "grad_norm": 0.2594124674797058,
      "learning_rate": 1.999514042826546e-05,
      "loss": 1.1514,
      "step": 425
    },
    {
      "epoch": 3.9902740937223697,
      "grad_norm": 0.24058319628238678,
      "learning_rate": 1.9995045617418247e-05,
      "loss": 1.1754,
      "step": 426
    },
    {
      "epoch": 3.9997052755673446,
      "grad_norm": 0.24412544071674347,
      "learning_rate": 1.9994949890828262e-05,
      "loss": 1.1606,
      "step": 427
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.195028305053711,
      "learning_rate": 1.999485324850428e-05,
      "loss": 0.6,
      "step": 428
    },
    {
      "epoch": 4.009431181844975,
      "grad_norm": 0.2472548633813858,
      "learning_rate": 1.9994755690455154e-05,
      "loss": 1.1351,
      "step": 429
    },
    {
      "epoch": 4.01886236368995,
      "grad_norm": 0.2561401426792145,
      "learning_rate": 1.9994657216689814e-05,
      "loss": 1.1575,
      "step": 430
    },
    {
      "epoch": 4.0282935455349245,
      "grad_norm": 0.25465086102485657,
      "learning_rate": 1.999455782721729e-05,
      "loss": 1.1406,
      "step": 431
    },
    {
      "epoch": 4.0377247273799,
      "grad_norm": 0.25173383951187134,
      "learning_rate": 1.9994457522046692e-05,
      "loss": 1.1221,
      "step": 432
    },
    {
      "epoch": 4.047155909224875,
      "grad_norm": 0.24817247688770294,
      "learning_rate": 1.9994356301187195e-05,
      "loss": 1.1307,
      "step": 433
    },
    {
      "epoch": 4.05658709106985,
      "grad_norm": 0.25433459877967834,
      "learning_rate": 1.999425416464809e-05,
      "loss": 1.1247,
      "step": 434
    },
    {
      "epoch": 4.066018272914825,
      "grad_norm": 0.28191715478897095,
      "learning_rate": 1.9994151112438726e-05,
      "loss": 1.1774,
      "step": 435
    },
    {
      "epoch": 4.0754494547597995,
      "grad_norm": 0.27621543407440186,
      "learning_rate": 1.9994047144568545e-05,
      "loss": 1.1181,
      "step": 436
    },
    {
      "epoch": 4.084880636604774,
      "grad_norm": 0.25847506523132324,
      "learning_rate": 1.9993942261047074e-05,
      "loss": 1.1622,
      "step": 437
    },
    {
      "epoch": 4.094311818449749,
      "grad_norm": 0.25455424189567566,
      "learning_rate": 1.999383646188392e-05,
      "loss": 1.1432,
      "step": 438
    },
    {
      "epoch": 4.103743000294725,
      "grad_norm": 0.25335466861724854,
      "learning_rate": 1.999372974708878e-05,
      "loss": 1.2031,
      "step": 439
    },
    {
      "epoch": 4.1131741821397,
      "grad_norm": 0.25624558329582214,
      "learning_rate": 1.9993622116671427e-05,
      "loss": 1.1401,
      "step": 440
    },
    {
      "epoch": 4.1226053639846745,
      "grad_norm": 0.2575918734073639,
      "learning_rate": 1.9993513570641726e-05,
      "loss": 1.1489,
      "step": 441
    },
    {
      "epoch": 4.132036545829649,
      "grad_norm": 0.2630651593208313,
      "learning_rate": 1.999340410900962e-05,
      "loss": 1.1814,
      "step": 442
    },
    {
      "epoch": 4.141467727674624,
      "grad_norm": 0.2638557255268097,
      "learning_rate": 1.9993293731785134e-05,
      "loss": 1.1328,
      "step": 443
    },
    {
      "epoch": 4.150898909519599,
      "grad_norm": 0.24800214171409607,
      "learning_rate": 1.9993182438978387e-05,
      "loss": 1.1647,
      "step": 444
    },
    {
      "epoch": 4.160330091364574,
      "grad_norm": 0.2533898949623108,
      "learning_rate": 1.9993070230599575e-05,
      "loss": 1.131,
      "step": 445
    },
    {
      "epoch": 4.1697612732095495,
      "grad_norm": 0.26801028847694397,
      "learning_rate": 1.9992957106658974e-05,
      "loss": 1.113,
      "step": 446
    },
    {
      "epoch": 4.179192455054524,
      "grad_norm": 0.2616179287433624,
      "learning_rate": 1.9992843067166955e-05,
      "loss": 1.1436,
      "step": 447
    },
    {
      "epoch": 4.188623636899499,
      "grad_norm": 0.26080557703971863,
      "learning_rate": 1.9992728112133958e-05,
      "loss": 1.1689,
      "step": 448
    },
    {
      "epoch": 4.198054818744474,
      "grad_norm": 0.24846932291984558,
      "learning_rate": 1.9992612241570522e-05,
      "loss": 1.1333,
      "step": 449
    },
    {
      "epoch": 4.207486000589449,
      "grad_norm": 0.291733056306839,
      "learning_rate": 1.9992495455487263e-05,
      "loss": 1.1402,
      "step": 450
    },
    {
      "epoch": 4.216917182434424,
      "grad_norm": 0.2785075902938843,
      "learning_rate": 1.9992377753894874e-05,
      "loss": 1.1425,
      "step": 451
    },
    {
      "epoch": 4.226348364279398,
      "grad_norm": 0.2668045163154602,
      "learning_rate": 1.9992259136804145e-05,
      "loss": 1.1084,
      "step": 452
    },
    {
      "epoch": 4.235779546124374,
      "grad_norm": 0.2606316804885864,
      "learning_rate": 1.9992139604225943e-05,
      "loss": 1.1477,
      "step": 453
    },
    {
      "epoch": 4.245210727969349,
      "grad_norm": 0.2691757082939148,
      "learning_rate": 1.9992019156171215e-05,
      "loss": 1.1723,
      "step": 454
    },
    {
      "epoch": 4.254641909814324,
      "grad_norm": 0.2710667848587036,
      "learning_rate": 1.9991897792651002e-05,
      "loss": 1.1689,
      "step": 455
    },
    {
      "epoch": 4.264073091659299,
      "grad_norm": 0.24052558839321136,
      "learning_rate": 1.9991775513676418e-05,
      "loss": 1.1489,
      "step": 456
    },
    {
      "epoch": 4.273504273504273,
      "grad_norm": 0.24777287244796753,
      "learning_rate": 1.9991652319258677e-05,
      "loss": 1.1723,
      "step": 457
    },
    {
      "epoch": 4.282935455349248,
      "grad_norm": 0.29242244362831116,
      "learning_rate": 1.9991528209409052e-05,
      "loss": 1.1683,
      "step": 458
    },
    {
      "epoch": 4.292366637194223,
      "grad_norm": 0.26288294792175293,
      "learning_rate": 1.9991403184138923e-05,
      "loss": 1.1181,
      "step": 459
    },
    {
      "epoch": 4.301797819039198,
      "grad_norm": 0.29905369877815247,
      "learning_rate": 1.999127724345974e-05,
      "loss": 1.1486,
      "step": 460
    },
    {
      "epoch": 4.311229000884174,
      "grad_norm": 0.26807495951652527,
      "learning_rate": 1.9991150387383043e-05,
      "loss": 1.1458,
      "step": 461
    },
    {
      "epoch": 4.320660182729148,
      "grad_norm": 0.26257237792015076,
      "learning_rate": 1.9991022615920457e-05,
      "loss": 1.1621,
      "step": 462
    },
    {
      "epoch": 4.330091364574123,
      "grad_norm": 0.2671409845352173,
      "learning_rate": 1.9990893929083687e-05,
      "loss": 1.1065,
      "step": 463
    },
    {
      "epoch": 4.339522546419098,
      "grad_norm": 0.2971418797969818,
      "learning_rate": 1.9990764326884522e-05,
      "loss": 1.1323,
      "step": 464
    },
    {
      "epoch": 4.348953728264073,
      "grad_norm": 0.27652162313461304,
      "learning_rate": 1.9990633809334836e-05,
      "loss": 1.1397,
      "step": 465
    },
    {
      "epoch": 4.358384910109048,
      "grad_norm": 0.25710198283195496,
      "learning_rate": 1.999050237644659e-05,
      "loss": 1.1589,
      "step": 466
    },
    {
      "epoch": 4.3678160919540225,
      "grad_norm": 0.2507089078426361,
      "learning_rate": 1.9990370028231824e-05,
      "loss": 1.1501,
      "step": 467
    },
    {
      "epoch": 4.377247273798998,
      "grad_norm": 0.2636183202266693,
      "learning_rate": 1.9990236764702658e-05,
      "loss": 1.1279,
      "step": 468
    },
    {
      "epoch": 4.386678455643973,
      "grad_norm": 0.2886471152305603,
      "learning_rate": 1.9990102585871313e-05,
      "loss": 1.1872,
      "step": 469
    },
    {
      "epoch": 4.396109637488948,
      "grad_norm": 0.2702710032463074,
      "learning_rate": 1.9989967491750076e-05,
      "loss": 1.1484,
      "step": 470
    },
    {
      "epoch": 4.405540819333923,
      "grad_norm": 0.27002573013305664,
      "learning_rate": 1.9989831482351322e-05,
      "loss": 1.1293,
      "step": 471
    },
    {
      "epoch": 4.4149720011788975,
      "grad_norm": 0.2893202006816864,
      "learning_rate": 1.9989694557687515e-05,
      "loss": 1.1154,
      "step": 472
    },
    {
      "epoch": 4.424403183023872,
      "grad_norm": 0.24547575414180756,
      "learning_rate": 1.99895567177712e-05,
      "loss": 1.1546,
      "step": 473
    },
    {
      "epoch": 4.433834364868847,
      "grad_norm": 0.2654038369655609,
      "learning_rate": 1.9989417962615006e-05,
      "loss": 1.1663,
      "step": 474
    },
    {
      "epoch": 4.443265546713823,
      "grad_norm": 0.24988961219787598,
      "learning_rate": 1.9989278292231647e-05,
      "loss": 1.1346,
      "step": 475
    },
    {
      "epoch": 4.452696728558798,
      "grad_norm": 0.2462431788444519,
      "learning_rate": 1.9989137706633915e-05,
      "loss": 1.1621,
      "step": 476
    },
    {
      "epoch": 4.4621279104037725,
      "grad_norm": 0.261726438999176,
      "learning_rate": 1.9988996205834695e-05,
      "loss": 1.1601,
      "step": 477
    },
    {
      "epoch": 4.471559092248747,
      "grad_norm": 0.27879616618156433,
      "learning_rate": 1.998885378984695e-05,
      "loss": 1.1566,
      "step": 478
    },
    {
      "epoch": 4.480990274093722,
      "grad_norm": 0.259721964597702,
      "learning_rate": 1.9988710458683728e-05,
      "loss": 1.1545,
      "step": 479
    },
    {
      "epoch": 4.490421455938697,
      "grad_norm": 0.2752620279788971,
      "learning_rate": 1.9988566212358157e-05,
      "loss": 1.1598,
      "step": 480
    },
    {
      "epoch": 4.499852637783672,
      "grad_norm": 0.26882103085517883,
      "learning_rate": 1.998842105088346e-05,
      "loss": 1.1004,
      "step": 481
    },
    {
      "epoch": 4.5092838196286475,
      "grad_norm": 0.28713667392730713,
      "learning_rate": 1.998827497427293e-05,
      "loss": 1.1513,
      "step": 482
    },
    {
      "epoch": 4.518715001473622,
      "grad_norm": 0.26432108879089355,
      "learning_rate": 1.9988127982539957e-05,
      "loss": 1.1483,
      "step": 483
    },
    {
      "epoch": 4.528146183318597,
      "grad_norm": 0.2586536705493927,
      "learning_rate": 1.9987980075698e-05,
      "loss": 1.1554,
      "step": 484
    },
    {
      "epoch": 4.537577365163572,
      "grad_norm": 0.2673826515674591,
      "learning_rate": 1.9987831253760622e-05,
      "loss": 1.1298,
      "step": 485
    },
    {
      "epoch": 4.547008547008547,
      "grad_norm": 0.27045226097106934,
      "learning_rate": 1.9987681516741445e-05,
      "loss": 1.127,
      "step": 486
    },
    {
      "epoch": 4.556439728853522,
      "grad_norm": 0.28373467922210693,
      "learning_rate": 1.9987530864654197e-05,
      "loss": 1.1637,
      "step": 487
    },
    {
      "epoch": 4.5658709106984965,
      "grad_norm": 0.25306832790374756,
      "learning_rate": 1.9987379297512676e-05,
      "loss": 1.1371,
      "step": 488
    },
    {
      "epoch": 4.575302092543472,
      "grad_norm": 0.25129541754722595,
      "learning_rate": 1.9987226815330773e-05,
      "loss": 1.142,
      "step": 489
    },
    {
      "epoch": 4.584733274388447,
      "grad_norm": 0.24957270920276642,
      "learning_rate": 1.9987073418122453e-05,
      "loss": 1.1899,
      "step": 490
    },
    {
      "epoch": 4.594164456233422,
      "grad_norm": 0.26634982228279114,
      "learning_rate": 1.9986919105901773e-05,
      "loss": 1.1248,
      "step": 491
    },
    {
      "epoch": 4.603595638078397,
      "grad_norm": 0.28366583585739136,
      "learning_rate": 1.998676387868287e-05,
      "loss": 1.1653,
      "step": 492
    },
    {
      "epoch": 4.6130268199233715,
      "grad_norm": 0.27519237995147705,
      "learning_rate": 1.998660773647997e-05,
      "loss": 1.1673,
      "step": 493
    },
    {
      "epoch": 4.622458001768346,
      "grad_norm": 0.2738685607910156,
      "learning_rate": 1.998645067930737e-05,
      "loss": 1.1499,
      "step": 494
    },
    {
      "epoch": 4.631889183613321,
      "grad_norm": 0.28892311453819275,
      "learning_rate": 1.998629270717947e-05,
      "loss": 1.1492,
      "step": 495
    },
    {
      "epoch": 4.641320365458297,
      "grad_norm": 0.2639668881893158,
      "learning_rate": 1.9986133820110735e-05,
      "loss": 1.1373,
      "step": 496
    },
    {
      "epoch": 4.650751547303272,
      "grad_norm": 0.28067559003829956,
      "learning_rate": 1.9985974018115728e-05,
      "loss": 1.1389,
      "step": 497
    },
    {
      "epoch": 4.6601827291482465,
      "grad_norm": 0.29011958837509155,
      "learning_rate": 1.9985813301209083e-05,
      "loss": 1.1662,
      "step": 498
    },
    {
      "epoch": 4.669613910993221,
      "grad_norm": 0.26316726207733154,
      "learning_rate": 1.9985651669405534e-05,
      "loss": 1.1923,
      "step": 499
    },
    {
      "epoch": 4.679045092838196,
      "grad_norm": 0.2665063440799713,
      "learning_rate": 1.9985489122719882e-05,
      "loss": 1.1412,
      "step": 500
    },
    {
      "epoch": 4.688476274683171,
      "grad_norm": 0.24381481111049652,
      "learning_rate": 1.9985325661167025e-05,
      "loss": 1.1602,
      "step": 501
    },
    {
      "epoch": 4.697907456528146,
      "grad_norm": 0.2565566599369049,
      "learning_rate": 1.9985161284761933e-05,
      "loss": 1.1165,
      "step": 502
    },
    {
      "epoch": 4.7073386383731215,
      "grad_norm": 0.2725907266139984,
      "learning_rate": 1.9984995993519674e-05,
      "loss": 1.1486,
      "step": 503
    },
    {
      "epoch": 4.716769820218096,
      "grad_norm": 0.2674521207809448,
      "learning_rate": 1.9984829787455385e-05,
      "loss": 1.1412,
      "step": 504
    },
    {
      "epoch": 4.726201002063071,
      "grad_norm": 0.2626359760761261,
      "learning_rate": 1.99846626665843e-05,
      "loss": 1.1628,
      "step": 505
    },
    {
      "epoch": 4.735632183908046,
      "grad_norm": 0.2567939758300781,
      "learning_rate": 1.9984494630921722e-05,
      "loss": 1.134,
      "step": 506
    },
    {
      "epoch": 4.745063365753021,
      "grad_norm": 0.25106364488601685,
      "learning_rate": 1.9984325680483054e-05,
      "loss": 1.1721,
      "step": 507
    },
    {
      "epoch": 4.754494547597996,
      "grad_norm": 0.3094298839569092,
      "learning_rate": 1.9984155815283768e-05,
      "loss": 1.1781,
      "step": 508
    },
    {
      "epoch": 4.76392572944297,
      "grad_norm": 0.2584042251110077,
      "learning_rate": 1.9983985035339434e-05,
      "loss": 1.1456,
      "step": 509
    },
    {
      "epoch": 4.773356911287946,
      "grad_norm": 0.25467678904533386,
      "learning_rate": 1.99838133406657e-05,
      "loss": 1.1608,
      "step": 510
    },
    {
      "epoch": 4.782788093132921,
      "grad_norm": 0.2562759518623352,
      "learning_rate": 1.998364073127829e-05,
      "loss": 1.1626,
      "step": 511
    },
    {
      "epoch": 4.792219274977896,
      "grad_norm": 0.2771031856536865,
      "learning_rate": 1.9983467207193023e-05,
      "loss": 1.1741,
      "step": 512
    },
    {
      "epoch": 4.801650456822871,
      "grad_norm": 0.2562180757522583,
      "learning_rate": 1.9983292768425793e-05,
      "loss": 1.1187,
      "step": 513
    },
    {
      "epoch": 4.811081638667845,
      "grad_norm": 0.2839386463165283,
      "learning_rate": 1.9983117414992586e-05,
      "loss": 1.1409,
      "step": 514
    },
    {
      "epoch": 4.82051282051282,
      "grad_norm": 0.2644241750240326,
      "learning_rate": 1.9982941146909466e-05,
      "loss": 1.1987,
      "step": 515
    },
    {
      "epoch": 4.829944002357795,
      "grad_norm": 0.26137757301330566,
      "learning_rate": 1.9982763964192586e-05,
      "loss": 1.1139,
      "step": 516
    },
    {
      "epoch": 4.839375184202771,
      "grad_norm": 0.2870071530342102,
      "learning_rate": 1.9982585866858173e-05,
      "loss": 1.1627,
      "step": 517
    },
    {
      "epoch": 4.848806366047746,
      "grad_norm": 0.24657125771045685,
      "learning_rate": 1.9982406854922548e-05,
      "loss": 1.1843,
      "step": 518
    },
    {
      "epoch": 4.85823754789272,
      "grad_norm": 0.24908483028411865,
      "learning_rate": 1.9982226928402115e-05,
      "loss": 1.1581,
      "step": 519
    },
    {
      "epoch": 4.867668729737695,
      "grad_norm": 0.287517249584198,
      "learning_rate": 1.998204608731335e-05,
      "loss": 1.1823,
      "step": 520
    },
    {
      "epoch": 4.87709991158267,
      "grad_norm": 0.2577473223209381,
      "learning_rate": 1.998186433167283e-05,
      "loss": 1.1606,
      "step": 521
    },
    {
      "epoch": 4.886531093427645,
      "grad_norm": 0.27965641021728516,
      "learning_rate": 1.9981681661497204e-05,
      "loss": 1.1721,
      "step": 522
    },
    {
      "epoch": 4.89596227527262,
      "grad_norm": 0.2739396393299103,
      "learning_rate": 1.998149807680321e-05,
      "loss": 1.1842,
      "step": 523
    },
    {
      "epoch": 4.9053934571175954,
      "grad_norm": 0.28305160999298096,
      "learning_rate": 1.9981313577607667e-05,
      "loss": 1.2096,
      "step": 524
    },
    {
      "epoch": 4.91482463896257,
      "grad_norm": 0.28943151235580444,
      "learning_rate": 1.9981128163927476e-05,
      "loss": 1.162,
      "step": 525
    },
    {
      "epoch": 4.924255820807545,
      "grad_norm": 0.25589314103126526,
      "learning_rate": 1.998094183577963e-05,
      "loss": 1.1621,
      "step": 526
    },
    {
      "epoch": 4.93368700265252,
      "grad_norm": 0.27715468406677246,
      "learning_rate": 1.9980754593181192e-05,
      "loss": 1.1527,
      "step": 527
    },
    {
      "epoch": 4.943118184497495,
      "grad_norm": 0.2777034640312195,
      "learning_rate": 1.9980566436149326e-05,
      "loss": 1.1454,
      "step": 528
    },
    {
      "epoch": 4.95254936634247,
      "grad_norm": 0.26989421248435974,
      "learning_rate": 1.998037736470127e-05,
      "loss": 1.169,
      "step": 529
    },
    {
      "epoch": 4.961980548187444,
      "grad_norm": 0.27719858288764954,
      "learning_rate": 1.9980187378854337e-05,
      "loss": 1.1108,
      "step": 530
    },
    {
      "epoch": 4.97141173003242,
      "grad_norm": 0.2775389552116394,
      "learning_rate": 1.9979996478625946e-05,
      "loss": 1.1342,
      "step": 531
    },
    {
      "epoch": 4.980842911877395,
      "grad_norm": 0.27332448959350586,
      "learning_rate": 1.997980466403358e-05,
      "loss": 1.1521,
      "step": 532
    },
    {
      "epoch": 4.99027409372237,
      "grad_norm": 0.3204605281352997,
      "learning_rate": 1.9979611935094817e-05,
      "loss": 1.1757,
      "step": 533
    },
    {
      "epoch": 4.999705275567345,
      "grad_norm": 0.2857342064380646,
      "learning_rate": 1.997941829182731e-05,
      "loss": 1.112,
      "step": 534
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.9063878059387207,
      "learning_rate": 1.99792237342488e-05,
      "loss": 0.5254,
      "step": 535
    },
    {
      "epoch": 5.009431181844975,
      "grad_norm": 0.23136506974697113,
      "learning_rate": 1.997902826237712e-05,
      "loss": 1.1503,
      "step": 536
    },
    {
      "epoch": 5.01886236368995,
      "grad_norm": 0.25733277201652527,
      "learning_rate": 1.997883187623017e-05,
      "loss": 1.1583,
      "step": 537
    },
    {
      "epoch": 5.0282935455349245,
      "grad_norm": 0.26119422912597656,
      "learning_rate": 1.997863457582595e-05,
      "loss": 1.0899,
      "step": 538
    },
    {
      "epoch": 5.0377247273799,
      "grad_norm": 0.2896842062473297,
      "learning_rate": 1.9978436361182533e-05,
      "loss": 1.1062,
      "step": 539
    },
    {
      "epoch": 5.047155909224875,
      "grad_norm": 0.25849997997283936,
      "learning_rate": 1.9978237232318077e-05,
      "loss": 1.1551,
      "step": 540
    },
    {
      "epoch": 5.05658709106985,
      "grad_norm": 0.29144784808158875,
      "learning_rate": 1.9978037189250835e-05,
      "loss": 1.1318,
      "step": 541
    },
    {
      "epoch": 5.066018272914825,
      "grad_norm": 0.25536543130874634,
      "learning_rate": 1.9977836231999125e-05,
      "loss": 1.2109,
      "step": 542
    },
    {
      "epoch": 5.0754494547597995,
      "grad_norm": 0.2717997431755066,
      "learning_rate": 1.9977634360581363e-05,
      "loss": 1.1418,
      "step": 543
    },
    {
      "epoch": 5.084880636604774,
      "grad_norm": 0.25424525141716003,
      "learning_rate": 1.9977431575016046e-05,
      "loss": 1.1546,
      "step": 544
    },
    {
      "epoch": 5.094311818449749,
      "grad_norm": 0.25665757060050964,
      "learning_rate": 1.9977227875321747e-05,
      "loss": 1.1585,
      "step": 545
    },
    {
      "epoch": 5.103743000294725,
      "grad_norm": 0.2825110852718353,
      "learning_rate": 1.9977023261517138e-05,
      "loss": 1.1409,
      "step": 546
    },
    {
      "epoch": 5.1131741821397,
      "grad_norm": 0.2810177505016327,
      "learning_rate": 1.997681773362096e-05,
      "loss": 1.1631,
      "step": 547
    },
    {
      "epoch": 5.1226053639846745,
      "grad_norm": 0.26353251934051514,
      "learning_rate": 1.9976611291652042e-05,
      "loss": 1.1242,
      "step": 548
    },
    {
      "epoch": 5.132036545829649,
      "grad_norm": 0.28000104427337646,
      "learning_rate": 1.99764039356293e-05,
      "loss": 1.1409,
      "step": 549
    },
    {
      "epoch": 5.141467727674624,
      "grad_norm": 0.29019448161125183,
      "learning_rate": 1.9976195665571734e-05,
      "loss": 1.2111,
      "step": 550
    },
    {
      "epoch": 5.150898909519599,
      "grad_norm": 0.27413952350616455,
      "learning_rate": 1.9975986481498425e-05,
      "loss": 1.1565,
      "step": 551
    },
    {
      "epoch": 5.160330091364574,
      "grad_norm": 0.2659062147140503,
      "learning_rate": 1.9975776383428532e-05,
      "loss": 1.1573,
      "step": 552
    },
    {
      "epoch": 5.1697612732095495,
      "grad_norm": 0.2959041893482208,
      "learning_rate": 1.9975565371381314e-05,
      "loss": 1.1403,
      "step": 553
    },
    {
      "epoch": 5.179192455054524,
      "grad_norm": 0.26517096161842346,
      "learning_rate": 1.9975353445376097e-05,
      "loss": 1.157,
      "step": 554
    },
    {
      "epoch": 5.188623636899499,
      "grad_norm": 0.28139975666999817,
      "learning_rate": 1.99751406054323e-05,
      "loss": 1.1596,
      "step": 555
    },
    {
      "epoch": 5.198054818744474,
      "grad_norm": 0.28777027130126953,
      "learning_rate": 1.9974926851569424e-05,
      "loss": 1.1468,
      "step": 556
    },
    {
      "epoch": 5.207486000589449,
      "grad_norm": 0.27441516518592834,
      "learning_rate": 1.9974712183807054e-05,
      "loss": 1.1525,
      "step": 557
    },
    {
      "epoch": 5.216917182434424,
      "grad_norm": 0.2545448839664459,
      "learning_rate": 1.9974496602164852e-05,
      "loss": 1.185,
      "step": 558
    },
    {
      "epoch": 5.226348364279398,
      "grad_norm": 0.2770099937915802,
      "learning_rate": 1.9974280106662575e-05,
      "loss": 1.1489,
      "step": 559
    },
    {
      "epoch": 5.235779546124374,
      "grad_norm": 0.2802804708480835,
      "learning_rate": 1.9974062697320055e-05,
      "loss": 1.1325,
      "step": 560
    },
    {
      "epoch": 5.245210727969349,
      "grad_norm": 0.2883077561855316,
      "learning_rate": 1.9973844374157216e-05,
      "loss": 1.1077,
      "step": 561
    },
    {
      "epoch": 5.254641909814324,
      "grad_norm": 0.2876817286014557,
      "learning_rate": 1.997362513719406e-05,
      "loss": 1.0969,
      "step": 562
    },
    {
      "epoch": 5.264073091659299,
      "grad_norm": 0.27500519156455994,
      "learning_rate": 1.9973404986450662e-05,
      "loss": 1.1251,
      "step": 563
    },
    {
      "epoch": 5.273504273504273,
      "grad_norm": 0.2637026607990265,
      "learning_rate": 1.9973183921947205e-05,
      "loss": 1.1573,
      "step": 564
    },
    {
      "epoch": 5.282935455349248,
      "grad_norm": 0.2797510623931885,
      "learning_rate": 1.997296194370394e-05,
      "loss": 1.1723,
      "step": 565
    },
    {
      "epoch": 5.292366637194223,
      "grad_norm": 0.26661109924316406,
      "learning_rate": 1.9972739051741203e-05,
      "loss": 1.1823,
      "step": 566
    },
    {
      "epoch": 5.301797819039198,
      "grad_norm": 0.2862928509712219,
      "learning_rate": 1.9972515246079416e-05,
      "loss": 1.1409,
      "step": 567
    },
    {
      "epoch": 5.311229000884174,
      "grad_norm": 0.26056963205337524,
      "learning_rate": 1.997229052673908e-05,
      "loss": 1.1666,
      "step": 568
    },
    {
      "epoch": 5.320660182729148,
      "grad_norm": 0.2757222056388855,
      "learning_rate": 1.9972064893740793e-05,
      "loss": 1.1327,
      "step": 569
    },
    {
      "epoch": 5.330091364574123,
      "grad_norm": 0.2607435882091522,
      "learning_rate": 1.997183834710522e-05,
      "loss": 1.1451,
      "step": 570
    },
    {
      "epoch": 5.339522546419098,
      "grad_norm": 0.3036711812019348,
      "learning_rate": 1.9971610886853116e-05,
      "loss": 1.154,
      "step": 571
    },
    {
      "epoch": 5.348953728264073,
      "grad_norm": 0.2728152275085449,
      "learning_rate": 1.9971382513005326e-05,
      "loss": 1.1101,
      "step": 572
    },
    {
      "epoch": 5.358384910109048,
      "grad_norm": 0.28689780831336975,
      "learning_rate": 1.997115322558277e-05,
      "loss": 1.1663,
      "step": 573
    },
    {
      "epoch": 5.3678160919540225,
      "grad_norm": 0.27059420943260193,
      "learning_rate": 1.9970923024606458e-05,
      "loss": 1.1095,
      "step": 574
    },
    {
      "epoch": 5.377247273798998,
      "grad_norm": 0.2779182493686676,
      "learning_rate": 1.9970691910097478e-05,
      "loss": 1.1783,
      "step": 575
    },
    {
      "epoch": 5.386678455643973,
      "grad_norm": 0.26629623770713806,
      "learning_rate": 1.9970459882077007e-05,
      "loss": 1.137,
      "step": 576
    },
    {
      "epoch": 5.396109637488948,
      "grad_norm": 0.2550724744796753,
      "learning_rate": 1.99702269405663e-05,
      "loss": 1.1208,
      "step": 577
    },
    {
      "epoch": 5.405540819333923,
      "grad_norm": 0.28392139077186584,
      "learning_rate": 1.9969993085586704e-05,
      "loss": 1.153,
      "step": 578
    },
    {
      "epoch": 5.4149720011788975,
      "grad_norm": 0.3039725720882416,
      "learning_rate": 1.996975831715964e-05,
      "loss": 1.1426,
      "step": 579
    },
    {
      "epoch": 5.424403183023872,
      "grad_norm": 0.28319597244262695,
      "learning_rate": 1.9969522635306623e-05,
      "loss": 1.136,
      "step": 580
    },
    {
      "epoch": 5.433834364868847,
      "grad_norm": 0.26333901286125183,
      "learning_rate": 1.9969286040049243e-05,
      "loss": 1.1433,
      "step": 581
    },
    {
      "epoch": 5.443265546713823,
      "grad_norm": 0.29213181138038635,
      "learning_rate": 1.9969048531409175e-05,
      "loss": 1.1114,
      "step": 582
    },
    {
      "epoch": 5.452696728558798,
      "grad_norm": 0.27767080068588257,
      "learning_rate": 1.9968810109408178e-05,
      "loss": 1.1515,
      "step": 583
    },
    {
      "epoch": 5.4621279104037725,
      "grad_norm": 0.26403871178627014,
      "learning_rate": 1.9968570774068105e-05,
      "loss": 1.1562,
      "step": 584
    },
    {
      "epoch": 5.471559092248747,
      "grad_norm": 0.305964857339859,
      "learning_rate": 1.996833052541087e-05,
      "loss": 1.1464,
      "step": 585
    },
    {
      "epoch": 5.480990274093722,
      "grad_norm": 0.27876919507980347,
      "learning_rate": 1.9968089363458496e-05,
      "loss": 1.1543,
      "step": 586
    },
    {
      "epoch": 5.490421455938697,
      "grad_norm": 0.29019442200660706,
      "learning_rate": 1.9967847288233075e-05,
      "loss": 1.1567,
      "step": 587
    },
    {
      "epoch": 5.499852637783672,
      "grad_norm": 0.29654040932655334,
      "learning_rate": 1.9967604299756787e-05,
      "loss": 1.1001,
      "step": 588
    },
    {
      "epoch": 5.5092838196286475,
      "grad_norm": 0.2778743505477905,
      "learning_rate": 1.996736039805189e-05,
      "loss": 1.1572,
      "step": 589
    },
    {
      "epoch": 5.518715001473622,
      "grad_norm": 0.28873610496520996,
      "learning_rate": 1.9967115583140735e-05,
      "loss": 1.1201,
      "step": 590
    },
    {
      "epoch": 5.528146183318597,
      "grad_norm": 0.2789043188095093,
      "learning_rate": 1.996686985504575e-05,
      "loss": 1.1498,
      "step": 591
    },
    {
      "epoch": 5.537577365163572,
      "grad_norm": 0.27758732438087463,
      "learning_rate": 1.9966623213789448e-05,
      "loss": 1.1187,
      "step": 592
    },
    {
      "epoch": 5.547008547008547,
      "grad_norm": 0.2900662124156952,
      "learning_rate": 1.9966375659394425e-05,
      "loss": 1.1136,
      "step": 593
    },
    {
      "epoch": 5.556439728853522,
      "grad_norm": 0.28729233145713806,
      "learning_rate": 1.9966127191883363e-05,
      "loss": 1.1018,
      "step": 594
    },
    {
      "epoch": 5.5658709106984965,
      "grad_norm": 0.3175833523273468,
      "learning_rate": 1.9965877811279034e-05,
      "loss": 1.1657,
      "step": 595
    },
    {
      "epoch": 5.575302092543472,
      "grad_norm": 0.2700801491737366,
      "learning_rate": 1.996562751760427e-05,
      "loss": 1.1496,
      "step": 596
    },
    {
      "epoch": 5.584733274388447,
      "grad_norm": 0.27089163661003113,
      "learning_rate": 1.9965376310882018e-05,
      "loss": 1.1624,
      "step": 597
    },
    {
      "epoch": 5.594164456233422,
      "grad_norm": 0.27540141344070435,
      "learning_rate": 1.9965124191135287e-05,
      "loss": 1.1232,
      "step": 598
    },
    {
      "epoch": 5.603595638078397,
      "grad_norm": 0.2859240770339966,
      "learning_rate": 1.9964871158387175e-05,
      "loss": 1.1413,
      "step": 599
    },
    {
      "epoch": 5.6130268199233715,
      "grad_norm": 0.2777286767959595,
      "learning_rate": 1.9964617212660867e-05,
      "loss": 1.1557,
      "step": 600
    },
    {
      "epoch": 5.622458001768346,
      "grad_norm": 0.3148115277290344,
      "learning_rate": 1.9964362353979627e-05,
      "loss": 1.1364,
      "step": 601
    },
    {
      "epoch": 5.631889183613321,
      "grad_norm": 0.2701999545097351,
      "learning_rate": 1.996410658236681e-05,
      "loss": 1.133,
      "step": 602
    },
    {
      "epoch": 5.641320365458297,
      "grad_norm": 0.27401822805404663,
      "learning_rate": 1.9963849897845845e-05,
      "loss": 1.1345,
      "step": 603
    },
    {
      "epoch": 5.650751547303272,
      "grad_norm": 0.27954885363578796,
      "learning_rate": 1.996359230044025e-05,
      "loss": 1.1224,
      "step": 604
    },
    {
      "epoch": 5.6601827291482465,
      "grad_norm": 0.2544698715209961,
      "learning_rate": 1.996333379017363e-05,
      "loss": 1.1349,
      "step": 605
    },
    {
      "epoch": 5.669613910993221,
      "grad_norm": 0.2784958779811859,
      "learning_rate": 1.996307436706966e-05,
      "loss": 1.1329,
      "step": 606
    },
    {
      "epoch": 5.679045092838196,
      "grad_norm": 0.2728007435798645,
      "learning_rate": 1.9962814031152123e-05,
      "loss": 1.1367,
      "step": 607
    },
    {
      "epoch": 5.688476274683171,
      "grad_norm": 0.29075682163238525,
      "learning_rate": 1.996255278244486e-05,
      "loss": 1.1237,
      "step": 608
    },
    {
      "epoch": 5.697907456528146,
      "grad_norm": 0.2744593918323517,
      "learning_rate": 1.9962290620971807e-05,
      "loss": 1.1406,
      "step": 609
    },
    {
      "epoch": 5.7073386383731215,
      "grad_norm": 0.29340553283691406,
      "learning_rate": 1.996202754675699e-05,
      "loss": 1.198,
      "step": 610
    },
    {
      "epoch": 5.716769820218096,
      "grad_norm": 0.31183695793151855,
      "learning_rate": 1.99617635598245e-05,
      "loss": 1.1546,
      "step": 611
    },
    {
      "epoch": 5.726201002063071,
      "grad_norm": 0.2750961482524872,
      "learning_rate": 1.9961498660198536e-05,
      "loss": 1.1205,
      "step": 612
    },
    {
      "epoch": 5.735632183908046,
      "grad_norm": 0.28891587257385254,
      "learning_rate": 1.996123284790336e-05,
      "loss": 1.1656,
      "step": 613
    },
    {
      "epoch": 5.745063365753021,
      "grad_norm": 0.27019864320755005,
      "learning_rate": 1.996096612296333e-05,
      "loss": 1.1338,
      "step": 614
    },
    {
      "epoch": 5.754494547597996,
      "grad_norm": 0.26674434542655945,
      "learning_rate": 1.996069848540288e-05,
      "loss": 1.1466,
      "step": 615
    },
    {
      "epoch": 5.76392572944297,
      "grad_norm": 0.30362674593925476,
      "learning_rate": 1.9960429935246535e-05,
      "loss": 1.1382,
      "step": 616
    },
    {
      "epoch": 5.773356911287946,
      "grad_norm": 0.2443380504846573,
      "learning_rate": 1.996016047251889e-05,
      "loss": 1.1579,
      "step": 617
    },
    {
      "epoch": 5.782788093132921,
      "grad_norm": 0.2809804379940033,
      "learning_rate": 1.995989009724465e-05,
      "loss": 1.1098,
      "step": 618
    },
    {
      "epoch": 5.792219274977896,
      "grad_norm": 0.2650749683380127,
      "learning_rate": 1.995961880944857e-05,
      "loss": 1.1255,
      "step": 619
    },
    {
      "epoch": 5.801650456822871,
      "grad_norm": 0.2905789613723755,
      "learning_rate": 1.9959346609155514e-05,
      "loss": 1.1282,
      "step": 620
    },
    {
      "epoch": 5.811081638667845,
      "grad_norm": 0.2917894423007965,
      "learning_rate": 1.9959073496390417e-05,
      "loss": 1.111,
      "step": 621
    },
    {
      "epoch": 5.82051282051282,
      "grad_norm": 0.28200146555900574,
      "learning_rate": 1.9958799471178306e-05,
      "loss": 1.155,
      "step": 622
    },
    {
      "epoch": 5.829944002357795,
      "grad_norm": 0.27315112948417664,
      "learning_rate": 1.9958524533544283e-05,
      "loss": 1.1355,
      "step": 623
    },
    {
      "epoch": 5.839375184202771,
      "grad_norm": 0.27947288751602173,
      "learning_rate": 1.9958248683513538e-05,
      "loss": 1.1342,
      "step": 624
    },
    {
      "epoch": 5.848806366047746,
      "grad_norm": 0.29104888439178467,
      "learning_rate": 1.9957971921111347e-05,
      "loss": 1.1337,
      "step": 625
    },
    {
      "epoch": 5.85823754789272,
      "grad_norm": 0.2955031394958496,
      "learning_rate": 1.9957694246363064e-05,
      "loss": 1.1333,
      "step": 626
    },
    {
      "epoch": 5.867668729737695,
      "grad_norm": 0.30177441239356995,
      "learning_rate": 1.995741565929413e-05,
      "loss": 1.1043,
      "step": 627
    },
    {
      "epoch": 5.87709991158267,
      "grad_norm": 0.3179454207420349,
      "learning_rate": 1.995713615993007e-05,
      "loss": 1.1428,
      "step": 628
    },
    {
      "epoch": 5.886531093427645,
      "grad_norm": 0.3313003182411194,
      "learning_rate": 1.9956855748296494e-05,
      "loss": 1.1221,
      "step": 629
    },
    {
      "epoch": 5.89596227527262,
      "grad_norm": 0.31058600544929504,
      "learning_rate": 1.995657442441909e-05,
      "loss": 1.1568,
      "step": 630
    },
    {
      "epoch": 5.9053934571175954,
      "grad_norm": 0.2711484730243683,
      "learning_rate": 1.9956292188323633e-05,
      "loss": 1.2016,
      "step": 631
    },
    {
      "epoch": 5.91482463896257,
      "grad_norm": 0.286067396402359,
      "learning_rate": 1.995600904003598e-05,
      "loss": 1.1751,
      "step": 632
    },
    {
      "epoch": 5.924255820807545,
      "grad_norm": 0.3116125166416168,
      "learning_rate": 1.9955724979582075e-05,
      "loss": 1.1695,
      "step": 633
    },
    {
      "epoch": 5.93368700265252,
      "grad_norm": 0.27363455295562744,
      "learning_rate": 1.9955440006987942e-05,
      "loss": 1.1547,
      "step": 634
    },
    {
      "epoch": 5.943118184497495,
      "grad_norm": 0.2926158607006073,
      "learning_rate": 1.9955154122279695e-05,
      "loss": 1.1378,
      "step": 635
    },
    {
      "epoch": 5.95254936634247,
      "grad_norm": 0.304308146238327,
      "learning_rate": 1.9954867325483524e-05,
      "loss": 1.1175,
      "step": 636
    },
    {
      "epoch": 5.961980548187444,
      "grad_norm": 0.29885435104370117,
      "learning_rate": 1.99545796166257e-05,
      "loss": 1.1635,
      "step": 637
    },
    {
      "epoch": 5.97141173003242,
      "grad_norm": 0.2895413637161255,
      "learning_rate": 1.995429099573259e-05,
      "loss": 1.1245,
      "step": 638
    },
    {
      "epoch": 5.980842911877395,
      "grad_norm": 0.3032973110675812,
      "learning_rate": 1.9954001462830632e-05,
      "loss": 1.0897,
      "step": 639
    },
    {
      "epoch": 5.99027409372237,
      "grad_norm": 0.2853977680206299,
      "learning_rate": 1.9953711017946357e-05,
      "loss": 1.1228,
      "step": 640
    },
    {
      "epoch": 5.999705275567345,
      "grad_norm": 0.26987114548683167,
      "learning_rate": 1.9953419661106372e-05,
      "loss": 1.1775,
      "step": 641
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.4648364782333374,
      "learning_rate": 1.9953127392337378e-05,
      "loss": 0.9518,
      "step": 642
    },
    {
      "epoch": 6.009431181844975,
      "grad_norm": 0.2976588010787964,
      "learning_rate": 1.995283421166614e-05,
      "loss": 1.159,
      "step": 643
    },
    {
      "epoch": 6.01886236368995,
      "grad_norm": 0.2812289893627167,
      "learning_rate": 1.995254011911953e-05,
      "loss": 1.1294,
      "step": 644
    },
    {
      "epoch": 6.0282935455349245,
      "grad_norm": 0.29609906673431396,
      "learning_rate": 1.9952245114724492e-05,
      "loss": 1.1731,
      "step": 645
    },
    {
      "epoch": 6.0377247273799,
      "grad_norm": 0.2774598300457001,
      "learning_rate": 1.9951949198508048e-05,
      "loss": 1.1318,
      "step": 646
    },
    {
      "epoch": 6.047155909224875,
      "grad_norm": 0.2795136570930481,
      "learning_rate": 1.9951652370497313e-05,
      "loss": 1.1539,
      "step": 647
    },
    {
      "epoch": 6.05658709106985,
      "grad_norm": 0.29758915305137634,
      "learning_rate": 1.9951354630719483e-05,
      "loss": 1.1289,
      "step": 648
    },
    {
      "epoch": 6.066018272914825,
      "grad_norm": 0.26898515224456787,
      "learning_rate": 1.9951055979201837e-05,
      "loss": 1.1576,
      "step": 649
    },
    {
      "epoch": 6.0754494547597995,
      "grad_norm": 0.2876826822757721,
      "learning_rate": 1.9950756415971736e-05,
      "loss": 1.1161,
      "step": 650
    },
    {
      "epoch": 6.084880636604774,
      "grad_norm": 0.29664337635040283,
      "learning_rate": 1.9950455941056625e-05,
      "loss": 1.1331,
      "step": 651
    },
    {
      "epoch": 6.094311818449749,
      "grad_norm": 0.2798660397529602,
      "learning_rate": 1.9950154554484035e-05,
      "loss": 1.15,
      "step": 652
    },
    {
      "epoch": 6.103743000294725,
      "grad_norm": 0.3104180693626404,
      "learning_rate": 1.9949852256281584e-05,
      "loss": 1.1109,
      "step": 653
    },
    {
      "epoch": 6.1131741821397,
      "grad_norm": 0.2517605125904083,
      "learning_rate": 1.9949549046476955e-05,
      "loss": 1.1522,
      "step": 654
    },
    {
      "epoch": 6.1226053639846745,
      "grad_norm": 0.3191726505756378,
      "learning_rate": 1.9949244925097942e-05,
      "loss": 1.1264,
      "step": 655
    },
    {
      "epoch": 6.132036545829649,
      "grad_norm": 0.2934046983718872,
      "learning_rate": 1.99489398921724e-05,
      "loss": 1.1427,
      "step": 656
    },
    {
      "epoch": 6.141467727674624,
      "grad_norm": 0.28122222423553467,
      "learning_rate": 1.9948633947728283e-05,
      "loss": 1.1402,
      "step": 657
    },
    {
      "epoch": 6.150898909519599,
      "grad_norm": 0.28847670555114746,
      "learning_rate": 1.9948327091793615e-05,
      "loss": 1.1262,
      "step": 658
    },
    {
      "epoch": 6.160330091364574,
      "grad_norm": 0.2767079174518585,
      "learning_rate": 1.9948019324396507e-05,
      "loss": 1.1283,
      "step": 659
    },
    {
      "epoch": 6.1697612732095495,
      "grad_norm": 0.28160572052001953,
      "learning_rate": 1.9947710645565167e-05,
      "loss": 1.1222,
      "step": 660
    },
    {
      "epoch": 6.179192455054524,
      "grad_norm": 0.2764461934566498,
      "learning_rate": 1.9947401055327875e-05,
      "loss": 1.1526,
      "step": 661
    },
    {
      "epoch": 6.188623636899499,
      "grad_norm": 0.29330548644065857,
      "learning_rate": 1.9947090553712983e-05,
      "loss": 1.125,
      "step": 662
    },
    {
      "epoch": 6.198054818744474,
      "grad_norm": 0.2660379707813263,
      "learning_rate": 1.9946779140748954e-05,
      "loss": 1.1956,
      "step": 663
    },
    {
      "epoch": 6.207486000589449,
      "grad_norm": 0.3015292286872864,
      "learning_rate": 1.994646681646431e-05,
      "loss": 1.1162,
      "step": 664
    },
    {
      "epoch": 6.216917182434424,
      "grad_norm": 0.2775987684726715,
      "learning_rate": 1.9946153580887667e-05,
      "loss": 1.1291,
      "step": 665
    },
    {
      "epoch": 6.226348364279398,
      "grad_norm": 0.2899509370326996,
      "learning_rate": 1.994583943404773e-05,
      "loss": 1.1341,
      "step": 666
    },
    {
      "epoch": 6.235779546124374,
      "grad_norm": 0.2871028780937195,
      "learning_rate": 1.9945524375973276e-05,
      "loss": 1.1427,
      "step": 667
    },
    {
      "epoch": 6.245210727969349,
      "grad_norm": 0.29947683215141296,
      "learning_rate": 1.9945208406693168e-05,
      "loss": 1.1397,
      "step": 668
    },
    {
      "epoch": 6.254641909814324,
      "grad_norm": 0.2911531925201416,
      "learning_rate": 1.994489152623636e-05,
      "loss": 1.1519,
      "step": 669
    },
    {
      "epoch": 6.264073091659299,
      "grad_norm": 0.30958592891693115,
      "learning_rate": 1.9944573734631883e-05,
      "loss": 1.1296,
      "step": 670
    },
    {
      "epoch": 6.273504273504273,
      "grad_norm": 0.3004133999347687,
      "learning_rate": 1.994425503190885e-05,
      "loss": 1.153,
      "step": 671
    },
    {
      "epoch": 6.282935455349248,
      "grad_norm": 0.29449015855789185,
      "learning_rate": 1.994393541809647e-05,
      "loss": 1.1583,
      "step": 672
    },
    {
      "epoch": 6.292366637194223,
      "grad_norm": 0.2978440821170807,
      "learning_rate": 1.9943614893224014e-05,
      "loss": 1.1283,
      "step": 673
    },
    {
      "epoch": 6.301797819039198,
      "grad_norm": 0.28348615765571594,
      "learning_rate": 1.9943293457320854e-05,
      "loss": 1.192,
      "step": 674
    },
    {
      "epoch": 6.311229000884174,
      "grad_norm": 0.28280675411224365,
      "learning_rate": 1.994297111041644e-05,
      "loss": 1.1125,
      "step": 675
    },
    {
      "epoch": 6.320660182729148,
      "grad_norm": 0.28954342007637024,
      "learning_rate": 1.99426478525403e-05,
      "loss": 1.128,
      "step": 676
    },
    {
      "epoch": 6.330091364574123,
      "grad_norm": 0.3068341612815857,
      "learning_rate": 1.994232368372206e-05,
      "loss": 1.1285,
      "step": 677
    },
    {
      "epoch": 6.339522546419098,
      "grad_norm": 0.2877725660800934,
      "learning_rate": 1.9941998603991416e-05,
      "loss": 1.1492,
      "step": 678
    },
    {
      "epoch": 6.348953728264073,
      "grad_norm": 0.3063173294067383,
      "learning_rate": 1.9941672613378152e-05,
      "loss": 1.1371,
      "step": 679
    },
    {
      "epoch": 6.358384910109048,
      "grad_norm": 0.2550581991672516,
      "learning_rate": 1.994134571191213e-05,
      "loss": 1.1347,
      "step": 680
    },
    {
      "epoch": 6.3678160919540225,
      "grad_norm": 0.31204888224601746,
      "learning_rate": 1.994101789962331e-05,
      "loss": 1.1057,
      "step": 681
    },
    {
      "epoch": 6.377247273798998,
      "grad_norm": 0.28355252742767334,
      "learning_rate": 1.9940689176541717e-05,
      "loss": 1.1074,
      "step": 682
    },
    {
      "epoch": 6.386678455643973,
      "grad_norm": 0.2846810221672058,
      "learning_rate": 1.9940359542697476e-05,
      "loss": 1.1254,
      "step": 683
    },
    {
      "epoch": 6.396109637488948,
      "grad_norm": 0.288790762424469,
      "learning_rate": 1.994002899812078e-05,
      "loss": 1.1198,
      "step": 684
    },
    {
      "epoch": 6.405540819333923,
      "grad_norm": 0.2860358655452728,
      "learning_rate": 1.993969754284192e-05,
      "loss": 1.1258,
      "step": 685
    },
    {
      "epoch": 6.4149720011788975,
      "grad_norm": 0.2984943687915802,
      "learning_rate": 1.9939365176891266e-05,
      "loss": 1.1247,
      "step": 686
    },
    {
      "epoch": 6.424403183023872,
      "grad_norm": 0.2928933799266815,
      "learning_rate": 1.993903190029926e-05,
      "loss": 1.146,
      "step": 687
    },
    {
      "epoch": 6.433834364868847,
      "grad_norm": 0.3034203052520752,
      "learning_rate": 1.9938697713096443e-05,
      "loss": 1.1216,
      "step": 688
    },
    {
      "epoch": 6.443265546713823,
      "grad_norm": 0.3096190094947815,
      "learning_rate": 1.9938362615313432e-05,
      "loss": 1.1391,
      "step": 689
    },
    {
      "epoch": 6.452696728558798,
      "grad_norm": 0.30415159463882446,
      "learning_rate": 1.993802660698093e-05,
      "loss": 1.1591,
      "step": 690
    },
    {
      "epoch": 6.4621279104037725,
      "grad_norm": 0.27310097217559814,
      "learning_rate": 1.9937689688129716e-05,
      "loss": 1.1281,
      "step": 691
    },
    {
      "epoch": 6.471559092248747,
      "grad_norm": 0.2851615846157074,
      "learning_rate": 1.9937351858790665e-05,
      "loss": 1.1349,
      "step": 692
    },
    {
      "epoch": 6.480990274093722,
      "grad_norm": 0.299983948469162,
      "learning_rate": 1.9937013118994727e-05,
      "loss": 1.0892,
      "step": 693
    },
    {
      "epoch": 6.490421455938697,
      "grad_norm": 0.2792853116989136,
      "learning_rate": 1.9936673468772937e-05,
      "loss": 1.1784,
      "step": 694
    },
    {
      "epoch": 6.499852637783672,
      "grad_norm": 0.28103357553482056,
      "learning_rate": 1.9936332908156412e-05,
      "loss": 1.1331,
      "step": 695
    },
    {
      "epoch": 6.5092838196286475,
      "grad_norm": 0.28038260340690613,
      "learning_rate": 1.993599143717636e-05,
      "loss": 1.1439,
      "step": 696
    },
    {
      "epoch": 6.518715001473622,
      "grad_norm": 0.2844087779521942,
      "learning_rate": 1.9935649055864056e-05,
      "loss": 1.1024,
      "step": 697
    },
    {
      "epoch": 6.528146183318597,
      "grad_norm": 0.27724727988243103,
      "learning_rate": 1.9935305764250875e-05,
      "loss": 1.1757,
      "step": 698
    },
    {
      "epoch": 6.537577365163572,
      "grad_norm": 0.2874237596988678,
      "learning_rate": 1.9934961562368272e-05,
      "loss": 1.1353,
      "step": 699
    },
    {
      "epoch": 6.547008547008547,
      "grad_norm": 0.30274319648742676,
      "learning_rate": 1.993461645024778e-05,
      "loss": 1.1282,
      "step": 700
    },
    {
      "epoch": 6.556439728853522,
      "grad_norm": 0.3078702390193939,
      "learning_rate": 1.9934270427921016e-05,
      "loss": 1.195,
      "step": 701
    },
    {
      "epoch": 6.5658709106984965,
      "grad_norm": 0.2992122769355774,
      "learning_rate": 1.9933923495419686e-05,
      "loss": 1.145,
      "step": 702
    },
    {
      "epoch": 6.575302092543472,
      "grad_norm": 0.28610992431640625,
      "learning_rate": 1.993357565277557e-05,
      "loss": 1.125,
      "step": 703
    },
    {
      "epoch": 6.584733274388447,
      "grad_norm": 0.28095531463623047,
      "learning_rate": 1.9933226900020545e-05,
      "loss": 1.154,
      "step": 704
    },
    {
      "epoch": 6.594164456233422,
      "grad_norm": 0.30057358741760254,
      "learning_rate": 1.993287723718656e-05,
      "loss": 1.1529,
      "step": 705
    },
    {
      "epoch": 6.603595638078397,
      "grad_norm": 0.30095916986465454,
      "learning_rate": 1.993252666430565e-05,
      "loss": 1.1253,
      "step": 706
    },
    {
      "epoch": 6.6130268199233715,
      "grad_norm": 0.3126285672187805,
      "learning_rate": 1.9932175181409934e-05,
      "loss": 1.1153,
      "step": 707
    },
    {
      "epoch": 6.622458001768346,
      "grad_norm": 0.27720731496810913,
      "learning_rate": 1.9931822788531615e-05,
      "loss": 1.144,
      "step": 708
    },
    {
      "epoch": 6.631889183613321,
      "grad_norm": 0.2976920008659363,
      "learning_rate": 1.9931469485702982e-05,
      "loss": 1.1059,
      "step": 709
    },
    {
      "epoch": 6.641320365458297,
      "grad_norm": 0.2982141077518463,
      "learning_rate": 1.9931115272956405e-05,
      "loss": 1.1567,
      "step": 710
    },
    {
      "epoch": 6.650751547303272,
      "grad_norm": 0.28802770376205444,
      "learning_rate": 1.993076015032433e-05,
      "loss": 1.1404,
      "step": 711
    },
    {
      "epoch": 6.6601827291482465,
      "grad_norm": 0.3090329170227051,
      "learning_rate": 1.9930404117839304e-05,
      "loss": 1.1108,
      "step": 712
    },
    {
      "epoch": 6.669613910993221,
      "grad_norm": 0.30991438031196594,
      "learning_rate": 1.9930047175533933e-05,
      "loss": 1.1292,
      "step": 713
    },
    {
      "epoch": 6.679045092838196,
      "grad_norm": 0.28077200055122375,
      "learning_rate": 1.992968932344093e-05,
      "loss": 1.1758,
      "step": 714
    },
    {
      "epoch": 6.688476274683171,
      "grad_norm": 0.29487887024879456,
      "learning_rate": 1.992933056159308e-05,
      "loss": 1.1597,
      "step": 715
    },
    {
      "epoch": 6.697907456528146,
      "grad_norm": 0.2797034978866577,
      "learning_rate": 1.992897089002325e-05,
      "loss": 1.0995,
      "step": 716
    },
    {
      "epoch": 6.7073386383731215,
      "grad_norm": 0.2567651867866516,
      "learning_rate": 1.9928610308764392e-05,
      "loss": 1.1366,
      "step": 717
    },
    {
      "epoch": 6.716769820218096,
      "grad_norm": 0.2890252470970154,
      "learning_rate": 1.992824881784955e-05,
      "loss": 1.1256,
      "step": 718
    },
    {
      "epoch": 6.726201002063071,
      "grad_norm": 0.3427819609642029,
      "learning_rate": 1.9927886417311833e-05,
      "loss": 1.1556,
      "step": 719
    },
    {
      "epoch": 6.735632183908046,
      "grad_norm": 0.3076249957084656,
      "learning_rate": 1.9927523107184452e-05,
      "loss": 1.1619,
      "step": 720
    },
    {
      "epoch": 6.745063365753021,
      "grad_norm": 0.2989400029182434,
      "learning_rate": 1.9927158887500688e-05,
      "loss": 1.1282,
      "step": 721
    },
    {
      "epoch": 6.754494547597996,
      "grad_norm": 0.3329310715198517,
      "learning_rate": 1.9926793758293918e-05,
      "loss": 1.1248,
      "step": 722
    },
    {
      "epoch": 6.76392572944297,
      "grad_norm": 0.2901914417743683,
      "learning_rate": 1.9926427719597586e-05,
      "loss": 1.1501,
      "step": 723
    },
    {
      "epoch": 6.773356911287946,
      "grad_norm": 0.29209184646606445,
      "learning_rate": 1.9926060771445234e-05,
      "loss": 1.0872,
      "step": 724
    },
    {
      "epoch": 6.782788093132921,
      "grad_norm": 0.28621336817741394,
      "learning_rate": 1.9925692913870477e-05,
      "loss": 1.1747,
      "step": 725
    },
    {
      "epoch": 6.792219274977896,
      "grad_norm": 0.29696208238601685,
      "learning_rate": 1.9925324146907022e-05,
      "loss": 1.1483,
      "step": 726
    },
    {
      "epoch": 6.801650456822871,
      "grad_norm": 0.28290995955467224,
      "learning_rate": 1.9924954470588657e-05,
      "loss": 1.1361,
      "step": 727
    },
    {
      "epoch": 6.811081638667845,
      "grad_norm": 0.29629427194595337,
      "learning_rate": 1.9924583884949248e-05,
      "loss": 1.1503,
      "step": 728
    },
    {
      "epoch": 6.82051282051282,
      "grad_norm": 0.2813538908958435,
      "learning_rate": 1.992421239002275e-05,
      "loss": 1.1475,
      "step": 729
    },
    {
      "epoch": 6.829944002357795,
      "grad_norm": 0.29059967398643494,
      "learning_rate": 1.99238399858432e-05,
      "loss": 1.1631,
      "step": 730
    },
    {
      "epoch": 6.839375184202771,
      "grad_norm": 0.2738023102283478,
      "learning_rate": 1.9923466672444712e-05,
      "loss": 1.0927,
      "step": 731
    },
    {
      "epoch": 6.848806366047746,
      "grad_norm": 0.30425408482551575,
      "learning_rate": 1.9923092449861493e-05,
      "loss": 1.177,
      "step": 732
    },
    {
      "epoch": 6.85823754789272,
      "grad_norm": 0.31151673197746277,
      "learning_rate": 1.9922717318127833e-05,
      "loss": 1.0973,
      "step": 733
    },
    {
      "epoch": 6.867668729737695,
      "grad_norm": 0.29532456398010254,
      "learning_rate": 1.992234127727809e-05,
      "loss": 1.0944,
      "step": 734
    },
    {
      "epoch": 6.87709991158267,
      "grad_norm": 0.29471760988235474,
      "learning_rate": 1.992196432734673e-05,
      "loss": 1.132,
      "step": 735
    },
    {
      "epoch": 6.886531093427645,
      "grad_norm": 0.2858657240867615,
      "learning_rate": 1.9921586468368282e-05,
      "loss": 1.1296,
      "step": 736
    },
    {
      "epoch": 6.89596227527262,
      "grad_norm": 0.30053871870040894,
      "learning_rate": 1.992120770037737e-05,
      "loss": 1.1007,
      "step": 737
    },
    {
      "epoch": 6.9053934571175954,
      "grad_norm": 0.2973736822605133,
      "learning_rate": 1.9920828023408685e-05,
      "loss": 1.1284,
      "step": 738
    },
    {
      "epoch": 6.91482463896257,
      "grad_norm": 0.2950553596019745,
      "learning_rate": 1.9920447437497022e-05,
      "loss": 1.1739,
      "step": 739
    },
    {
      "epoch": 6.924255820807545,
      "grad_norm": 0.3023276627063751,
      "learning_rate": 1.9920065942677255e-05,
      "loss": 1.1146,
      "step": 740
    },
    {
      "epoch": 6.93368700265252,
      "grad_norm": 0.3153594732284546,
      "learning_rate": 1.9919683538984325e-05,
      "loss": 1.0851,
      "step": 741
    },
    {
      "epoch": 6.943118184497495,
      "grad_norm": 0.29804447293281555,
      "learning_rate": 1.9919300226453277e-05,
      "loss": 1.1352,
      "step": 742
    },
    {
      "epoch": 6.95254936634247,
      "grad_norm": 0.30292707681655884,
      "learning_rate": 1.9918916005119224e-05,
      "loss": 1.1502,
      "step": 743
    },
    {
      "epoch": 6.961980548187444,
      "grad_norm": 0.2925572395324707,
      "learning_rate": 1.991853087501737e-05,
      "loss": 1.1703,
      "step": 744
    },
    {
      "epoch": 6.97141173003242,
      "grad_norm": 0.2915116548538208,
      "learning_rate": 1.9918144836183003e-05,
      "loss": 1.1301,
      "step": 745
    },
    {
      "epoch": 6.980842911877395,
      "grad_norm": 0.3155469596385956,
      "learning_rate": 1.9917757888651488e-05,
      "loss": 1.1237,
      "step": 746
    },
    {
      "epoch": 6.99027409372237,
      "grad_norm": 0.3001948893070221,
      "learning_rate": 1.991737003245828e-05,
      "loss": 1.1446,
      "step": 747
    },
    {
      "epoch": 6.999705275567345,
      "grad_norm": 0.31477728486061096,
      "learning_rate": 1.9916981267638912e-05,
      "loss": 1.1605,
      "step": 748
    },
    {
      "epoch": 7.0,
      "grad_norm": 1.691961407661438,
      "learning_rate": 1.9916591594229002e-05,
      "loss": 1.0164,
      "step": 749
    },
    {
      "epoch": 7.009431181844975,
      "grad_norm": 0.298705518245697,
      "learning_rate": 1.9916201012264255e-05,
      "loss": 1.1274,
      "step": 750
    },
    {
      "epoch": 7.01886236368995,
      "grad_norm": 0.2926231920719147,
      "learning_rate": 1.9915809521780452e-05,
      "loss": 1.1349,
      "step": 751
    },
    {
      "epoch": 7.0282935455349245,
      "grad_norm": 0.30090242624282837,
      "learning_rate": 1.9915417122813464e-05,
      "loss": 1.128,
      "step": 752
    },
    {
      "epoch": 7.0377247273799,
      "grad_norm": 0.2813696563243866,
      "learning_rate": 1.9915023815399243e-05,
      "loss": 1.1696,
      "step": 753
    },
    {
      "epoch": 7.047155909224875,
      "grad_norm": 0.3074609339237213,
      "learning_rate": 1.991462959957382e-05,
      "loss": 1.1279,
      "step": 754
    },
    {
      "epoch": 7.05658709106985,
      "grad_norm": 0.31694865226745605,
      "learning_rate": 1.9914234475373318e-05,
      "loss": 1.1236,
      "step": 755
    },
    {
      "epoch": 7.066018272914825,
      "grad_norm": 0.3016173243522644,
      "learning_rate": 1.9913838442833934e-05,
      "loss": 1.172,
      "step": 756
    },
    {
      "epoch": 7.0754494547597995,
      "grad_norm": 0.27980944514274597,
      "learning_rate": 1.9913441501991956e-05,
      "loss": 1.1472,
      "step": 757
    },
    {
      "epoch": 7.084880636604774,
      "grad_norm": 0.3050203323364258,
      "learning_rate": 1.9913043652883748e-05,
      "loss": 1.1289,
      "step": 758
    },
    {
      "epoch": 7.094311818449749,
      "grad_norm": 0.2868805229663849,
      "learning_rate": 1.991264489554576e-05,
      "loss": 1.1421,
      "step": 759
    },
    {
      "epoch": 7.103743000294725,
      "grad_norm": 0.3081061840057373,
      "learning_rate": 1.9912245230014533e-05,
      "loss": 1.1686,
      "step": 760
    },
    {
      "epoch": 7.1131741821397,
      "grad_norm": 0.2825002670288086,
      "learning_rate": 1.9911844656326676e-05,
      "loss": 1.1299,
      "step": 761
    },
    {
      "epoch": 7.1226053639846745,
      "grad_norm": 0.32215824723243713,
      "learning_rate": 1.9911443174518896e-05,
      "loss": 1.0827,
      "step": 762
    },
    {
      "epoch": 7.132036545829649,
      "grad_norm": 0.2720104157924652,
      "learning_rate": 1.991104078462797e-05,
      "loss": 1.1279,
      "step": 763
    },
    {
      "epoch": 7.141467727674624,
      "grad_norm": 0.3187060058116913,
      "learning_rate": 1.9910637486690773e-05,
      "loss": 1.1344,
      "step": 764
    },
    {
      "epoch": 7.150898909519599,
      "grad_norm": 0.3060894012451172,
      "learning_rate": 1.9910233280744244e-05,
      "loss": 1.1268,
      "step": 765
    },
    {
      "epoch": 7.160330091364574,
      "grad_norm": 0.29930925369262695,
      "learning_rate": 1.9909828166825426e-05,
      "loss": 1.1534,
      "step": 766
    },
    {
      "epoch": 7.1697612732095495,
      "grad_norm": 0.3437637984752655,
      "learning_rate": 1.9909422144971433e-05,
      "loss": 1.1212,
      "step": 767
    },
    {
      "epoch": 7.179192455054524,
      "grad_norm": 0.31885167956352234,
      "learning_rate": 1.9909015215219464e-05,
      "loss": 1.1391,
      "step": 768
    },
    {
      "epoch": 7.188623636899499,
      "grad_norm": 0.28641650080680847,
      "learning_rate": 1.99086073776068e-05,
      "loss": 1.1807,
      "step": 769
    },
    {
      "epoch": 7.198054818744474,
      "grad_norm": 0.3094464838504791,
      "learning_rate": 1.9908198632170807e-05,
      "loss": 1.1693,
      "step": 770
    },
    {
      "epoch": 7.207486000589449,
      "grad_norm": 0.2834804356098175,
      "learning_rate": 1.990778897894894e-05,
      "loss": 1.1162,
      "step": 771
    },
    {
      "epoch": 7.216917182434424,
      "grad_norm": 0.28369027376174927,
      "learning_rate": 1.9907378417978723e-05,
      "loss": 1.1052,
      "step": 772
    },
    {
      "epoch": 7.226348364279398,
      "grad_norm": 0.29375767707824707,
      "learning_rate": 1.9906966949297777e-05,
      "loss": 1.1687,
      "step": 773
    },
    {
      "epoch": 7.235779546124374,
      "grad_norm": 0.34326478838920593,
      "learning_rate": 1.9906554572943796e-05,
      "loss": 1.1143,
      "step": 774
    },
    {
      "epoch": 7.245210727969349,
      "grad_norm": 0.3039381802082062,
      "learning_rate": 1.990614128895457e-05,
      "loss": 1.1469,
      "step": 775
    },
    {
      "epoch": 7.254641909814324,
      "grad_norm": 0.3372262716293335,
      "learning_rate": 1.9905727097367954e-05,
      "loss": 1.1103,
      "step": 776
    },
    {
      "epoch": 7.264073091659299,
      "grad_norm": 0.3213746249675751,
      "learning_rate": 1.9905311998221902e-05,
      "loss": 1.1284,
      "step": 777
    },
    {
      "epoch": 7.273504273504273,
      "grad_norm": 0.29482758045196533,
      "learning_rate": 1.9904895991554447e-05,
      "loss": 1.1279,
      "step": 778
    },
    {
      "epoch": 7.282935455349248,
      "grad_norm": 0.2901090383529663,
      "learning_rate": 1.9904479077403695e-05,
      "loss": 1.1354,
      "step": 779
    },
    {
      "epoch": 7.292366637194223,
      "grad_norm": 0.2885685861110687,
      "learning_rate": 1.990406125580785e-05,
      "loss": 1.1465,
      "step": 780
    },
    {
      "epoch": 7.301797819039198,
      "grad_norm": 0.30120721459388733,
      "learning_rate": 1.9903642526805194e-05,
      "loss": 1.1353,
      "step": 781
    },
    {
      "epoch": 7.311229000884174,
      "grad_norm": 0.3437204658985138,
      "learning_rate": 1.990322289043409e-05,
      "loss": 1.1421,
      "step": 782
    },
    {
      "epoch": 7.320660182729148,
      "grad_norm": 0.2968297004699707,
      "learning_rate": 1.9902802346732982e-05,
      "loss": 1.1273,
      "step": 783
    },
    {
      "epoch": 7.330091364574123,
      "grad_norm": 0.31573331356048584,
      "learning_rate": 1.9902380895740397e-05,
      "loss": 1.1488,
      "step": 784
    },
    {
      "epoch": 7.339522546419098,
      "grad_norm": 0.3044077754020691,
      "learning_rate": 1.9901958537494957e-05,
      "loss": 1.1704,
      "step": 785
    },
    {
      "epoch": 7.348953728264073,
      "grad_norm": 0.3121173083782196,
      "learning_rate": 1.990153527203535e-05,
      "loss": 1.1336,
      "step": 786
    },
    {
      "epoch": 7.358384910109048,
      "grad_norm": 0.3370584547519684,
      "learning_rate": 1.9901111099400365e-05,
      "loss": 1.0984,
      "step": 787
    },
    {
      "epoch": 7.3678160919540225,
      "grad_norm": 0.31617480516433716,
      "learning_rate": 1.9900686019628853e-05,
      "loss": 1.1391,
      "step": 788
    },
    {
      "epoch": 7.377247273798998,
      "grad_norm": 0.3083356022834778,
      "learning_rate": 1.9900260032759772e-05,
      "loss": 1.0878,
      "step": 789
    },
    {
      "epoch": 7.386678455643973,
      "grad_norm": 0.3417675197124481,
      "learning_rate": 1.9899833138832138e-05,
      "loss": 1.1197,
      "step": 790
    },
    {
      "epoch": 7.396109637488948,
      "grad_norm": 0.3129981756210327,
      "learning_rate": 1.9899405337885073e-05,
      "loss": 1.1443,
      "step": 791
    },
    {
      "epoch": 7.405540819333923,
      "grad_norm": 0.3011592924594879,
      "learning_rate": 1.9898976629957767e-05,
      "loss": 1.0983,
      "step": 792
    },
    {
      "epoch": 7.4149720011788975,
      "grad_norm": 0.33389249444007874,
      "learning_rate": 1.98985470150895e-05,
      "loss": 1.1176,
      "step": 793
    },
    {
      "epoch": 7.424403183023872,
      "grad_norm": 0.3061431348323822,
      "learning_rate": 1.989811649331963e-05,
      "loss": 1.0609,
      "step": 794
    },
    {
      "epoch": 7.433834364868847,
      "grad_norm": 0.3106611371040344,
      "learning_rate": 1.9897685064687608e-05,
      "loss": 1.115,
      "step": 795
    },
    {
      "epoch": 7.443265546713823,
      "grad_norm": 0.3124815821647644,
      "learning_rate": 1.9897252729232954e-05,
      "loss": 1.1215,
      "step": 796
    },
    {
      "epoch": 7.452696728558798,
      "grad_norm": 0.31633245944976807,
      "learning_rate": 1.9896819486995282e-05,
      "loss": 1.1813,
      "step": 797
    },
    {
      "epoch": 7.4621279104037725,
      "grad_norm": 0.3172498941421509,
      "learning_rate": 1.9896385338014285e-05,
      "loss": 1.1321,
      "step": 798
    },
    {
      "epoch": 7.471559092248747,
      "grad_norm": 0.29450657963752747,
      "learning_rate": 1.989595028232974e-05,
      "loss": 1.0899,
      "step": 799
    },
    {
      "epoch": 7.480990274093722,
      "grad_norm": 0.33151915669441223,
      "learning_rate": 1.9895514319981507e-05,
      "loss": 1.1126,
      "step": 800
    },
    {
      "epoch": 7.490421455938697,
      "grad_norm": 0.31156280636787415,
      "learning_rate": 1.989507745100953e-05,
      "loss": 1.1512,
      "step": 801
    },
    {
      "epoch": 7.499852637783672,
      "grad_norm": 0.30025148391723633,
      "learning_rate": 1.9894639675453828e-05,
      "loss": 1.1076,
      "step": 802
    },
    {
      "epoch": 7.5092838196286475,
      "grad_norm": 0.2869744896888733,
      "learning_rate": 1.9894200993354514e-05,
      "loss": 1.0946,
      "step": 803
    },
    {
      "epoch": 7.518715001473622,
      "grad_norm": 0.31786268949508667,
      "learning_rate": 1.9893761404751783e-05,
      "loss": 1.1582,
      "step": 804
    },
    {
      "epoch": 7.528146183318597,
      "grad_norm": 0.3058665096759796,
      "learning_rate": 1.9893320909685907e-05,
      "loss": 1.1277,
      "step": 805
    },
    {
      "epoch": 7.537577365163572,
      "grad_norm": 0.29756730794906616,
      "learning_rate": 1.9892879508197247e-05,
      "loss": 1.1236,
      "step": 806
    },
    {
      "epoch": 7.547008547008547,
      "grad_norm": 0.30074283480644226,
      "learning_rate": 1.989243720032624e-05,
      "loss": 1.1142,
      "step": 807
    },
    {
      "epoch": 7.556439728853522,
      "grad_norm": 0.32405680418014526,
      "learning_rate": 1.9891993986113412e-05,
      "loss": 1.1361,
      "step": 808
    },
    {
      "epoch": 7.5658709106984965,
      "grad_norm": 0.2920079231262207,
      "learning_rate": 1.9891549865599366e-05,
      "loss": 1.1449,
      "step": 809
    },
    {
      "epoch": 7.575302092543472,
      "grad_norm": 0.33918827772140503,
      "learning_rate": 1.9891104838824803e-05,
      "loss": 1.1353,
      "step": 810
    },
    {
      "epoch": 7.584733274388447,
      "grad_norm": 0.2959598898887634,
      "learning_rate": 1.9890658905830486e-05,
      "loss": 1.1913,
      "step": 811
    },
    {
      "epoch": 7.594164456233422,
      "grad_norm": 0.28424370288848877,
      "learning_rate": 1.9890212066657272e-05,
      "loss": 1.1478,
      "step": 812
    },
    {
      "epoch": 7.603595638078397,
      "grad_norm": 0.3293933570384979,
      "learning_rate": 1.9889764321346105e-05,
      "loss": 1.1093,
      "step": 813
    },
    {
      "epoch": 7.6130268199233715,
      "grad_norm": 0.30919957160949707,
      "learning_rate": 1.9889315669938003e-05,
      "loss": 1.1344,
      "step": 814
    },
    {
      "epoch": 7.622458001768346,
      "grad_norm": 0.2916177809238434,
      "learning_rate": 1.9888866112474075e-05,
      "loss": 1.1576,
      "step": 815
    },
    {
      "epoch": 7.631889183613321,
      "grad_norm": 0.29905998706817627,
      "learning_rate": 1.9888415648995508e-05,
      "loss": 1.1321,
      "step": 816
    },
    {
      "epoch": 7.641320365458297,
      "grad_norm": 0.3274576663970947,
      "learning_rate": 1.988796427954357e-05,
      "loss": 1.1519,
      "step": 817
    },
    {
      "epoch": 7.650751547303272,
      "grad_norm": 0.31401771306991577,
      "learning_rate": 1.9887512004159618e-05,
      "loss": 1.1147,
      "step": 818
    },
    {
      "epoch": 7.6601827291482465,
      "grad_norm": 0.3007761836051941,
      "learning_rate": 1.9887058822885095e-05,
      "loss": 1.2147,
      "step": 819
    },
    {
      "epoch": 7.669613910993221,
      "grad_norm": 0.2921323776245117,
      "learning_rate": 1.9886604735761513e-05,
      "loss": 1.1398,
      "step": 820
    },
    {
      "epoch": 7.679045092838196,
      "grad_norm": 0.2909519672393799,
      "learning_rate": 1.9886149742830476e-05,
      "loss": 1.1227,
      "step": 821
    },
    {
      "epoch": 7.688476274683171,
      "grad_norm": 0.29912832379341125,
      "learning_rate": 1.9885693844133675e-05,
      "loss": 1.0992,
      "step": 822
    },
    {
      "epoch": 7.697907456528146,
      "grad_norm": 0.32213249802589417,
      "learning_rate": 1.9885237039712874e-05,
      "loss": 1.1287,
      "step": 823
    },
    {
      "epoch": 7.7073386383731215,
      "grad_norm": 0.2944284975528717,
      "learning_rate": 1.988477932960993e-05,
      "loss": 1.087,
      "step": 824
    },
    {
      "epoch": 7.716769820218096,
      "grad_norm": 0.3133915960788727,
      "learning_rate": 1.9884320713866774e-05,
      "loss": 1.095,
      "step": 825
    },
    {
      "epoch": 7.726201002063071,
      "grad_norm": 0.3160547614097595,
      "learning_rate": 1.9883861192525433e-05,
      "loss": 1.1314,
      "step": 826
    },
    {
      "epoch": 7.735632183908046,
      "grad_norm": 0.29428908228874207,
      "learning_rate": 1.9883400765627996e-05,
      "loss": 1.1306,
      "step": 827
    },
    {
      "epoch": 7.745063365753021,
      "grad_norm": 0.3112960755825043,
      "learning_rate": 1.9882939433216653e-05,
      "loss": 1.1218,
      "step": 828
    },
    {
      "epoch": 7.754494547597996,
      "grad_norm": 0.30988243222236633,
      "learning_rate": 1.988247719533367e-05,
      "loss": 1.1044,
      "step": 829
    },
    {
      "epoch": 7.76392572944297,
      "grad_norm": 0.33407390117645264,
      "learning_rate": 1.9882014052021405e-05,
      "loss": 1.0945,
      "step": 830
    },
    {
      "epoch": 7.773356911287946,
      "grad_norm": 0.3426600992679596,
      "learning_rate": 1.9881550003322276e-05,
      "loss": 1.1054,
      "step": 831
    },
    {
      "epoch": 7.782788093132921,
      "grad_norm": 0.29079872369766235,
      "learning_rate": 1.988108504927881e-05,
      "loss": 1.1667,
      "step": 832
    },
    {
      "epoch": 7.792219274977896,
      "grad_norm": 0.3079555630683899,
      "learning_rate": 1.9880619189933602e-05,
      "loss": 1.1323,
      "step": 833
    },
    {
      "epoch": 7.801650456822871,
      "grad_norm": 0.32263270020484924,
      "learning_rate": 1.9880152425329336e-05,
      "loss": 1.1574,
      "step": 834
    },
    {
      "epoch": 7.811081638667845,
      "grad_norm": 0.3372173607349396,
      "learning_rate": 1.987968475550878e-05,
      "loss": 1.1495,
      "step": 835
    },
    {
      "epoch": 7.82051282051282,
      "grad_norm": 0.3138803541660309,
      "learning_rate": 1.987921618051477e-05,
      "loss": 1.1636,
      "step": 836
    },
    {
      "epoch": 7.829944002357795,
      "grad_norm": 0.34180426597595215,
      "learning_rate": 1.987874670039025e-05,
      "loss": 1.1062,
      "step": 837
    },
    {
      "epoch": 7.839375184202771,
      "grad_norm": 0.2851787209510803,
      "learning_rate": 1.9878276315178226e-05,
      "loss": 1.1601,
      "step": 838
    },
    {
      "epoch": 7.848806366047746,
      "grad_norm": 0.3064446449279785,
      "learning_rate": 1.987780502492179e-05,
      "loss": 1.1338,
      "step": 839
    },
    {
      "epoch": 7.85823754789272,
      "grad_norm": 0.2875286042690277,
      "learning_rate": 1.9877332829664135e-05,
      "loss": 1.1207,
      "step": 840
    },
    {
      "epoch": 7.867668729737695,
      "grad_norm": 0.3380201756954193,
      "learning_rate": 1.9876859729448515e-05,
      "loss": 1.1113,
      "step": 841
    },
    {
      "epoch": 7.87709991158267,
      "grad_norm": 0.31326764822006226,
      "learning_rate": 1.9876385724318274e-05,
      "loss": 1.186,
      "step": 842
    },
    {
      "epoch": 7.886531093427645,
      "grad_norm": 0.3070063889026642,
      "learning_rate": 1.9875910814316846e-05,
      "loss": 1.1428,
      "step": 843
    },
    {
      "epoch": 7.89596227527262,
      "grad_norm": 0.29900458455085754,
      "learning_rate": 1.9875434999487737e-05,
      "loss": 1.1154,
      "step": 844
    },
    {
      "epoch": 7.9053934571175954,
      "grad_norm": 0.2876986861228943,
      "learning_rate": 1.9874958279874543e-05,
      "loss": 1.1358,
      "step": 845
    },
    {
      "epoch": 7.91482463896257,
      "grad_norm": 0.30100923776626587,
      "learning_rate": 1.987448065552094e-05,
      "loss": 1.1084,
      "step": 846
    },
    {
      "epoch": 7.924255820807545,
      "grad_norm": 0.281020849943161,
      "learning_rate": 1.9874002126470688e-05,
      "loss": 1.1035,
      "step": 847
    },
    {
      "epoch": 7.93368700265252,
      "grad_norm": 0.32907453179359436,
      "learning_rate": 1.987352269276763e-05,
      "loss": 1.0949,
      "step": 848
    },
    {
      "epoch": 7.943118184497495,
      "grad_norm": 0.29373398423194885,
      "learning_rate": 1.987304235445569e-05,
      "loss": 1.1416,
      "step": 849
    },
    {
      "epoch": 7.95254936634247,
      "grad_norm": 0.31025463342666626,
      "learning_rate": 1.9872561111578885e-05,
      "loss": 1.1556,
      "step": 850
    },
    {
      "epoch": 7.961980548187444,
      "grad_norm": 0.31871578097343445,
      "learning_rate": 1.9872078964181292e-05,
      "loss": 1.1482,
      "step": 851
    },
    {
      "epoch": 7.97141173003242,
      "grad_norm": 0.28061309456825256,
      "learning_rate": 1.9871595912307096e-05,
      "loss": 1.1573,
      "step": 852
    },
    {
      "epoch": 7.980842911877395,
      "grad_norm": 0.276934951543808,
      "learning_rate": 1.9871111956000545e-05,
      "loss": 1.1539,
      "step": 853
    },
    {
      "epoch": 7.99027409372237,
      "grad_norm": 0.3141954839229584,
      "learning_rate": 1.9870627095305992e-05,
      "loss": 1.0883,
      "step": 854
    },
    {
      "epoch": 7.999705275567345,
      "grad_norm": 0.31258994340896606,
      "learning_rate": 1.987014133026785e-05,
      "loss": 1.1006,
      "step": 855
    },
    {
      "epoch": 8.0,
      "grad_norm": 1.3840771913528442,
      "learning_rate": 1.9869654660930625e-05,
      "loss": 0.8346,
      "step": 856
    },
    {
      "epoch": 8.009431181844976,
      "grad_norm": 0.3582943379878998,
      "learning_rate": 1.9869167087338908e-05,
      "loss": 1.1197,
      "step": 857
    },
    {
      "epoch": 8.01886236368995,
      "grad_norm": 0.30885636806488037,
      "learning_rate": 1.9868678609537372e-05,
      "loss": 1.104,
      "step": 858
    },
    {
      "epoch": 8.028293545534925,
      "grad_norm": 0.2909170985221863,
      "learning_rate": 1.9868189227570767e-05,
      "loss": 1.162,
      "step": 859
    },
    {
      "epoch": 8.0377247273799,
      "grad_norm": 0.30390292406082153,
      "learning_rate": 1.9867698941483932e-05,
      "loss": 1.1244,
      "step": 860
    },
    {
      "epoch": 8.047155909224875,
      "grad_norm": 0.27787649631500244,
      "learning_rate": 1.9867207751321786e-05,
      "loss": 1.1291,
      "step": 861
    },
    {
      "epoch": 8.056587091069849,
      "grad_norm": 0.3079860210418701,
      "learning_rate": 1.9866715657129335e-05,
      "loss": 1.1211,
      "step": 862
    },
    {
      "epoch": 8.066018272914825,
      "grad_norm": 0.30087438225746155,
      "learning_rate": 1.9866222658951656e-05,
      "loss": 1.0975,
      "step": 863
    },
    {
      "epoch": 8.0754494547598,
      "grad_norm": 0.3013684153556824,
      "learning_rate": 1.986572875683393e-05,
      "loss": 1.1365,
      "step": 864
    },
    {
      "epoch": 8.084880636604774,
      "grad_norm": 0.3340030610561371,
      "learning_rate": 1.98652339508214e-05,
      "loss": 1.1438,
      "step": 865
    },
    {
      "epoch": 8.09431181844975,
      "grad_norm": 0.2895887792110443,
      "learning_rate": 1.9864738240959396e-05,
      "loss": 1.123,
      "step": 866
    },
    {
      "epoch": 8.103743000294724,
      "grad_norm": 0.33631661534309387,
      "learning_rate": 1.9864241627293347e-05,
      "loss": 1.1504,
      "step": 867
    },
    {
      "epoch": 8.1131741821397,
      "grad_norm": 0.3055470287799835,
      "learning_rate": 1.9863744109868743e-05,
      "loss": 1.1633,
      "step": 868
    },
    {
      "epoch": 8.122605363984674,
      "grad_norm": 0.352252721786499,
      "learning_rate": 1.986324568873117e-05,
      "loss": 1.1117,
      "step": 869
    },
    {
      "epoch": 8.13203654582965,
      "grad_norm": 0.2935343086719513,
      "learning_rate": 1.986274636392629e-05,
      "loss": 1.1191,
      "step": 870
    },
    {
      "epoch": 8.141467727674625,
      "grad_norm": 0.32181090116500854,
      "learning_rate": 1.9862246135499857e-05,
      "loss": 1.1523,
      "step": 871
    },
    {
      "epoch": 8.150898909519599,
      "grad_norm": 0.30801522731781006,
      "learning_rate": 1.9861745003497693e-05,
      "loss": 1.1636,
      "step": 872
    },
    {
      "epoch": 8.160330091364575,
      "grad_norm": 0.3301963210105896,
      "learning_rate": 1.9861242967965722e-05,
      "loss": 1.101,
      "step": 873
    },
    {
      "epoch": 8.169761273209549,
      "grad_norm": 0.30058571696281433,
      "learning_rate": 1.9860740028949933e-05,
      "loss": 1.1281,
      "step": 874
    },
    {
      "epoch": 8.179192455054524,
      "grad_norm": 0.29529333114624023,
      "learning_rate": 1.9860236186496406e-05,
      "loss": 1.1136,
      "step": 875
    },
    {
      "epoch": 8.188623636899498,
      "grad_norm": 0.30874213576316833,
      "learning_rate": 1.9859731440651303e-05,
      "loss": 1.1431,
      "step": 876
    },
    {
      "epoch": 8.198054818744474,
      "grad_norm": 0.3081565797328949,
      "learning_rate": 1.9859225791460874e-05,
      "loss": 1.1461,
      "step": 877
    },
    {
      "epoch": 8.20748600058945,
      "grad_norm": 0.3118036091327667,
      "learning_rate": 1.9858719238971436e-05,
      "loss": 1.1038,
      "step": 878
    },
    {
      "epoch": 8.216917182434424,
      "grad_norm": 0.3372817635536194,
      "learning_rate": 1.9858211783229407e-05,
      "loss": 1.1579,
      "step": 879
    },
    {
      "epoch": 8.2263483642794,
      "grad_norm": 0.291151762008667,
      "learning_rate": 1.985770342428128e-05,
      "loss": 1.0935,
      "step": 880
    },
    {
      "epoch": 8.235779546124373,
      "grad_norm": 0.30195292830467224,
      "learning_rate": 1.9857194162173627e-05,
      "loss": 1.1195,
      "step": 881
    },
    {
      "epoch": 8.245210727969349,
      "grad_norm": 0.33139723539352417,
      "learning_rate": 1.985668399695311e-05,
      "loss": 1.1123,
      "step": 882
    },
    {
      "epoch": 8.254641909814323,
      "grad_norm": 0.32879218459129333,
      "learning_rate": 1.9856172928666467e-05,
      "loss": 1.0968,
      "step": 883
    },
    {
      "epoch": 8.264073091659299,
      "grad_norm": 0.3262989819049835,
      "learning_rate": 1.9855660957360524e-05,
      "loss": 1.1212,
      "step": 884
    },
    {
      "epoch": 8.273504273504274,
      "grad_norm": 0.29864901304244995,
      "learning_rate": 1.9855148083082186e-05,
      "loss": 1.1471,
      "step": 885
    },
    {
      "epoch": 8.282935455349248,
      "grad_norm": 0.28844597935676575,
      "learning_rate": 1.9854634305878445e-05,
      "loss": 1.1367,
      "step": 886
    },
    {
      "epoch": 8.292366637194224,
      "grad_norm": 0.3240841031074524,
      "learning_rate": 1.9854119625796367e-05,
      "loss": 1.1391,
      "step": 887
    },
    {
      "epoch": 8.301797819039198,
      "grad_norm": 0.3447263538837433,
      "learning_rate": 1.9853604042883113e-05,
      "loss": 1.1423,
      "step": 888
    },
    {
      "epoch": 8.311229000884174,
      "grad_norm": 0.30541878938674927,
      "learning_rate": 1.9853087557185922e-05,
      "loss": 1.0906,
      "step": 889
    },
    {
      "epoch": 8.320660182729148,
      "grad_norm": 0.33410006761550903,
      "learning_rate": 1.985257016875211e-05,
      "loss": 1.1159,
      "step": 890
    },
    {
      "epoch": 8.330091364574123,
      "grad_norm": 0.352668821811676,
      "learning_rate": 1.9852051877629077e-05,
      "loss": 1.0971,
      "step": 891
    },
    {
      "epoch": 8.339522546419099,
      "grad_norm": 0.33648231625556946,
      "learning_rate": 1.9851532683864314e-05,
      "loss": 1.1354,
      "step": 892
    },
    {
      "epoch": 8.348953728264073,
      "grad_norm": 0.2774215340614319,
      "learning_rate": 1.9851012587505393e-05,
      "loss": 1.1055,
      "step": 893
    },
    {
      "epoch": 8.358384910109049,
      "grad_norm": 0.3235042691230774,
      "learning_rate": 1.9850491588599954e-05,
      "loss": 1.1208,
      "step": 894
    },
    {
      "epoch": 8.367816091954023,
      "grad_norm": 0.2978575527667999,
      "learning_rate": 1.984996968719574e-05,
      "loss": 1.141,
      "step": 895
    },
    {
      "epoch": 8.377247273798998,
      "grad_norm": 0.3200622498989105,
      "learning_rate": 1.9849446883340563e-05,
      "loss": 1.1131,
      "step": 896
    },
    {
      "epoch": 8.386678455643972,
      "grad_norm": 0.3313077688217163,
      "learning_rate": 1.9848923177082323e-05,
      "loss": 1.085,
      "step": 897
    },
    {
      "epoch": 8.396109637488948,
      "grad_norm": 0.3079144060611725,
      "learning_rate": 1.9848398568469e-05,
      "loss": 1.1505,
      "step": 898
    },
    {
      "epoch": 8.405540819333924,
      "grad_norm": 0.30789315700531006,
      "learning_rate": 1.9847873057548665e-05,
      "loss": 1.1628,
      "step": 899
    },
    {
      "epoch": 8.414972001178898,
      "grad_norm": 0.32638052105903625,
      "learning_rate": 1.9847346644369457e-05,
      "loss": 1.0911,
      "step": 900
    },
    {
      "epoch": 8.424403183023873,
      "grad_norm": 0.337313711643219,
      "learning_rate": 1.984681932897961e-05,
      "loss": 1.1409,
      "step": 901
    },
    {
      "epoch": 8.433834364868847,
      "grad_norm": 0.30280250310897827,
      "learning_rate": 1.9846291111427437e-05,
      "loss": 1.1327,
      "step": 902
    },
    {
      "epoch": 8.443265546713823,
      "grad_norm": 0.3031136393547058,
      "learning_rate": 1.9845761991761332e-05,
      "loss": 1.0816,
      "step": 903
    },
    {
      "epoch": 8.452696728558797,
      "grad_norm": 0.32366272807121277,
      "learning_rate": 1.9845231970029774e-05,
      "loss": 1.1422,
      "step": 904
    },
    {
      "epoch": 8.462127910403773,
      "grad_norm": 0.30693158507347107,
      "learning_rate": 1.984470104628132e-05,
      "loss": 1.1445,
      "step": 905
    },
    {
      "epoch": 8.471559092248748,
      "grad_norm": 0.2920929193496704,
      "learning_rate": 1.9844169220564615e-05,
      "loss": 1.1136,
      "step": 906
    },
    {
      "epoch": 8.480990274093722,
      "grad_norm": 0.2927638292312622,
      "learning_rate": 1.984363649292839e-05,
      "loss": 1.1367,
      "step": 907
    },
    {
      "epoch": 8.490421455938698,
      "grad_norm": 0.30173829197883606,
      "learning_rate": 1.984310286342144e-05,
      "loss": 1.1265,
      "step": 908
    },
    {
      "epoch": 8.499852637783672,
      "grad_norm": 0.3451744616031647,
      "learning_rate": 1.984256833209267e-05,
      "loss": 1.1505,
      "step": 909
    },
    {
      "epoch": 8.509283819628648,
      "grad_norm": 0.328769326210022,
      "learning_rate": 1.9842032898991048e-05,
      "loss": 1.1116,
      "step": 910
    },
    {
      "epoch": 8.518715001473621,
      "grad_norm": 0.3437176048755646,
      "learning_rate": 1.984149656416563e-05,
      "loss": 1.1206,
      "step": 911
    },
    {
      "epoch": 8.528146183318597,
      "grad_norm": 0.32404428720474243,
      "learning_rate": 1.984095932766555e-05,
      "loss": 1.1607,
      "step": 912
    },
    {
      "epoch": 8.537577365163571,
      "grad_norm": 0.3057379424571991,
      "learning_rate": 1.9840421189540042e-05,
      "loss": 1.1205,
      "step": 913
    },
    {
      "epoch": 8.547008547008547,
      "grad_norm": 0.30314305424690247,
      "learning_rate": 1.9839882149838397e-05,
      "loss": 1.1581,
      "step": 914
    },
    {
      "epoch": 8.556439728853523,
      "grad_norm": 0.3177058696746826,
      "learning_rate": 1.9839342208610008e-05,
      "loss": 1.1339,
      "step": 915
    },
    {
      "epoch": 8.565870910698496,
      "grad_norm": 0.32213515043258667,
      "learning_rate": 1.9838801365904342e-05,
      "loss": 1.1253,
      "step": 916
    },
    {
      "epoch": 8.575302092543472,
      "grad_norm": 0.32328057289123535,
      "learning_rate": 1.9838259621770956e-05,
      "loss": 1.1242,
      "step": 917
    },
    {
      "epoch": 8.584733274388446,
      "grad_norm": 0.3405260443687439,
      "learning_rate": 1.9837716976259477e-05,
      "loss": 1.1884,
      "step": 918
    },
    {
      "epoch": 8.594164456233422,
      "grad_norm": 0.30424028635025024,
      "learning_rate": 1.983717342941963e-05,
      "loss": 1.09,
      "step": 919
    },
    {
      "epoch": 8.603595638078396,
      "grad_norm": 0.3215782940387726,
      "learning_rate": 1.9836628981301204e-05,
      "loss": 1.1384,
      "step": 920
    },
    {
      "epoch": 8.613026819923371,
      "grad_norm": 0.3256554901599884,
      "learning_rate": 1.9836083631954092e-05,
      "loss": 1.1053,
      "step": 921
    },
    {
      "epoch": 8.622458001768347,
      "grad_norm": 0.3186770975589752,
      "learning_rate": 1.9835537381428248e-05,
      "loss": 1.1051,
      "step": 922
    },
    {
      "epoch": 8.631889183613321,
      "grad_norm": 0.30663394927978516,
      "learning_rate": 1.983499022977373e-05,
      "loss": 1.1336,
      "step": 923
    },
    {
      "epoch": 8.641320365458297,
      "grad_norm": 0.3267298638820648,
      "learning_rate": 1.9834442177040666e-05,
      "loss": 1.163,
      "step": 924
    },
    {
      "epoch": 8.65075154730327,
      "grad_norm": 0.29237011075019836,
      "learning_rate": 1.983389322327926e-05,
      "loss": 1.1217,
      "step": 925
    },
    {
      "epoch": 8.660182729148246,
      "grad_norm": 0.31643229722976685,
      "learning_rate": 1.9833343368539813e-05,
      "loss": 1.0874,
      "step": 926
    },
    {
      "epoch": 8.66961391099322,
      "grad_norm": 0.3271484673023224,
      "learning_rate": 1.9832792612872704e-05,
      "loss": 1.1166,
      "step": 927
    },
    {
      "epoch": 8.679045092838196,
      "grad_norm": 0.3450450599193573,
      "learning_rate": 1.983224095632839e-05,
      "loss": 1.1327,
      "step": 928
    },
    {
      "epoch": 8.688476274683172,
      "grad_norm": 0.34141966700553894,
      "learning_rate": 1.9831688398957416e-05,
      "loss": 1.124,
      "step": 929
    },
    {
      "epoch": 8.697907456528146,
      "grad_norm": 0.3355785012245178,
      "learning_rate": 1.9831134940810405e-05,
      "loss": 1.1203,
      "step": 930
    },
    {
      "epoch": 8.707338638373121,
      "grad_norm": 0.32210665941238403,
      "learning_rate": 1.9830580581938064e-05,
      "loss": 1.1435,
      "step": 931
    },
    {
      "epoch": 8.716769820218095,
      "grad_norm": 0.3013294041156769,
      "learning_rate": 1.9830025322391185e-05,
      "loss": 1.1304,
      "step": 932
    },
    {
      "epoch": 8.726201002063071,
      "grad_norm": 0.3177154064178467,
      "learning_rate": 1.9829469162220642e-05,
      "loss": 1.1406,
      "step": 933
    },
    {
      "epoch": 8.735632183908045,
      "grad_norm": 0.3534151017665863,
      "learning_rate": 1.9828912101477385e-05,
      "loss": 1.1753,
      "step": 934
    },
    {
      "epoch": 8.74506336575302,
      "grad_norm": 0.3145313858985901,
      "learning_rate": 1.982835414021246e-05,
      "loss": 1.1314,
      "step": 935
    },
    {
      "epoch": 8.754494547597997,
      "grad_norm": 0.30095723271369934,
      "learning_rate": 1.982779527847698e-05,
      "loss": 1.11,
      "step": 936
    },
    {
      "epoch": 8.76392572944297,
      "grad_norm": 0.36475008726119995,
      "learning_rate": 1.9827235516322148e-05,
      "loss": 1.1295,
      "step": 937
    },
    {
      "epoch": 8.773356911287946,
      "grad_norm": 0.32005575299263,
      "learning_rate": 1.9826674853799256e-05,
      "loss": 1.1234,
      "step": 938
    },
    {
      "epoch": 8.78278809313292,
      "grad_norm": 0.3251902461051941,
      "learning_rate": 1.982611329095966e-05,
      "loss": 1.1376,
      "step": 939
    },
    {
      "epoch": 8.792219274977896,
      "grad_norm": 0.32449856400489807,
      "learning_rate": 1.9825550827854826e-05,
      "loss": 1.1089,
      "step": 940
    },
    {
      "epoch": 8.80165045682287,
      "grad_norm": 0.30900728702545166,
      "learning_rate": 1.9824987464536272e-05,
      "loss": 1.1264,
      "step": 941
    },
    {
      "epoch": 8.811081638667845,
      "grad_norm": 0.34929314255714417,
      "learning_rate": 1.982442320105562e-05,
      "loss": 1.1741,
      "step": 942
    },
    {
      "epoch": 8.820512820512821,
      "grad_norm": 0.3470143675804138,
      "learning_rate": 1.982385803746457e-05,
      "loss": 1.1466,
      "step": 943
    },
    {
      "epoch": 8.829944002357795,
      "grad_norm": 0.33177703619003296,
      "learning_rate": 1.9823291973814897e-05,
      "loss": 1.0922,
      "step": 944
    },
    {
      "epoch": 8.83937518420277,
      "grad_norm": 0.3137301206588745,
      "learning_rate": 1.9822725010158464e-05,
      "loss": 1.143,
      "step": 945
    },
    {
      "epoch": 8.848806366047745,
      "grad_norm": 0.29711881279945374,
      "learning_rate": 1.9822157146547217e-05,
      "loss": 1.112,
      "step": 946
    },
    {
      "epoch": 8.85823754789272,
      "grad_norm": 0.3209730386734009,
      "learning_rate": 1.9821588383033185e-05,
      "loss": 1.1376,
      "step": 947
    },
    {
      "epoch": 8.867668729737694,
      "grad_norm": 0.3335842192173004,
      "learning_rate": 1.982101871966848e-05,
      "loss": 1.1494,
      "step": 948
    },
    {
      "epoch": 8.87709991158267,
      "grad_norm": 0.2997051775455475,
      "learning_rate": 1.9820448156505287e-05,
      "loss": 1.1426,
      "step": 949
    },
    {
      "epoch": 8.886531093427646,
      "grad_norm": 0.3393019735813141,
      "learning_rate": 1.9819876693595884e-05,
      "loss": 1.1145,
      "step": 950
    },
    {
      "epoch": 8.89596227527262,
      "grad_norm": 0.3156786859035492,
      "learning_rate": 1.9819304330992635e-05,
      "loss": 1.1136,
      "step": 951
    },
    {
      "epoch": 8.905393457117595,
      "grad_norm": 0.3412819802761078,
      "learning_rate": 1.981873106874797e-05,
      "loss": 1.1333,
      "step": 952
    },
    {
      "epoch": 8.91482463896257,
      "grad_norm": 0.3190101683139801,
      "learning_rate": 1.9818156906914415e-05,
      "loss": 1.132,
      "step": 953
    },
    {
      "epoch": 8.924255820807545,
      "grad_norm": 0.3377256691455841,
      "learning_rate": 1.9817581845544574e-05,
      "loss": 1.093,
      "step": 954
    },
    {
      "epoch": 8.933687002652519,
      "grad_norm": 0.32697659730911255,
      "learning_rate": 1.9817005884691135e-05,
      "loss": 1.1693,
      "step": 955
    },
    {
      "epoch": 8.943118184497495,
      "grad_norm": 0.3018835484981537,
      "learning_rate": 1.981642902440687e-05,
      "loss": 1.1376,
      "step": 956
    },
    {
      "epoch": 8.95254936634247,
      "grad_norm": 0.3153565227985382,
      "learning_rate": 1.9815851264744624e-05,
      "loss": 1.1193,
      "step": 957
    },
    {
      "epoch": 8.961980548187444,
      "grad_norm": 0.3602859377861023,
      "learning_rate": 1.9815272605757337e-05,
      "loss": 1.1183,
      "step": 958
    },
    {
      "epoch": 8.97141173003242,
      "grad_norm": 0.32320189476013184,
      "learning_rate": 1.9814693047498022e-05,
      "loss": 1.1433,
      "step": 959
    },
    {
      "epoch": 8.980842911877394,
      "grad_norm": 0.3273150324821472,
      "learning_rate": 1.981411259001978e-05,
      "loss": 1.1059,
      "step": 960
    },
    {
      "epoch": 8.99027409372237,
      "grad_norm": 0.2994784712791443,
      "learning_rate": 1.981353123337579e-05,
      "loss": 1.1538,
      "step": 961
    },
    {
      "epoch": 8.999705275567344,
      "grad_norm": 0.3237287104129791,
      "learning_rate": 1.9812948977619318e-05,
      "loss": 1.1192,
      "step": 962
    },
    {
      "epoch": 9.0,
      "grad_norm": 1.4942028522491455,
      "learning_rate": 1.9812365822803707e-05,
      "loss": 0.9416,
      "step": 963
    },
    {
      "epoch": 9.009431181844976,
      "grad_norm": 0.29814067482948303,
      "learning_rate": 1.9811781768982392e-05,
      "loss": 1.1309,
      "step": 964
    },
    {
      "epoch": 9.01886236368995,
      "grad_norm": 0.3161367177963257,
      "learning_rate": 1.9811196816208876e-05,
      "loss": 1.1556,
      "step": 965
    },
    {
      "epoch": 9.028293545534925,
      "grad_norm": 0.33322376012802124,
      "learning_rate": 1.981061096453676e-05,
      "loss": 1.0979,
      "step": 966
    },
    {
      "epoch": 9.0377247273799,
      "grad_norm": 0.32565703988075256,
      "learning_rate": 1.9810024214019714e-05,
      "loss": 1.0941,
      "step": 967
    },
    {
      "epoch": 9.047155909224875,
      "grad_norm": 0.3158816397190094,
      "learning_rate": 1.9809436564711495e-05,
      "loss": 1.0955,
      "step": 968
    },
    {
      "epoch": 9.056587091069849,
      "grad_norm": 0.32280150055885315,
      "learning_rate": 1.9808848016665946e-05,
      "loss": 1.1698,
      "step": 969
    },
    {
      "epoch": 9.066018272914825,
      "grad_norm": 0.32347047328948975,
      "learning_rate": 1.9808258569936988e-05,
      "loss": 1.0975,
      "step": 970
    },
    {
      "epoch": 9.0754494547598,
      "grad_norm": 0.3057040870189667,
      "learning_rate": 1.980766822457863e-05,
      "loss": 1.1291,
      "step": 971
    },
    {
      "epoch": 9.084880636604774,
      "grad_norm": 0.3289203941822052,
      "learning_rate": 1.9807076980644954e-05,
      "loss": 1.1272,
      "step": 972
    },
    {
      "epoch": 9.09431181844975,
      "grad_norm": 0.34666383266448975,
      "learning_rate": 1.9806484838190134e-05,
      "loss": 1.1069,
      "step": 973
    },
    {
      "epoch": 9.103743000294724,
      "grad_norm": 0.326106995344162,
      "learning_rate": 1.9805891797268418e-05,
      "loss": 1.1307,
      "step": 974
    },
    {
      "epoch": 9.1131741821397,
      "grad_norm": 0.3441641628742218,
      "learning_rate": 1.980529785793414e-05,
      "loss": 1.1473,
      "step": 975
    },
    {
      "epoch": 9.122605363984674,
      "grad_norm": 0.3393020033836365,
      "learning_rate": 1.980470302024172e-05,
      "loss": 1.1379,
      "step": 976
    },
    {
      "epoch": 9.13203654582965,
      "grad_norm": 0.3226853013038635,
      "learning_rate": 1.9804107284245658e-05,
      "loss": 1.1084,
      "step": 977
    },
    {
      "epoch": 9.141467727674625,
      "grad_norm": 0.3308658003807068,
      "learning_rate": 1.980351065000053e-05,
      "loss": 1.1232,
      "step": 978
    },
    {
      "epoch": 9.150898909519599,
      "grad_norm": 0.3319736123085022,
      "learning_rate": 1.9802913117561e-05,
      "loss": 1.1616,
      "step": 979
    },
    {
      "epoch": 9.160330091364575,
      "grad_norm": 0.31637251377105713,
      "learning_rate": 1.9802314686981816e-05,
      "loss": 1.1382,
      "step": 980
    },
    {
      "epoch": 9.169761273209549,
      "grad_norm": 0.31496942043304443,
      "learning_rate": 1.9801715358317808e-05,
      "loss": 1.1202,
      "step": 981
    },
    {
      "epoch": 9.179192455054524,
      "grad_norm": 0.3173936605453491,
      "learning_rate": 1.9801115131623882e-05,
      "loss": 1.1392,
      "step": 982
    },
    {
      "epoch": 9.188623636899498,
      "grad_norm": 0.32043910026550293,
      "learning_rate": 1.980051400695503e-05,
      "loss": 1.1026,
      "step": 983
    },
    {
      "epoch": 9.198054818744474,
      "grad_norm": 0.29564058780670166,
      "learning_rate": 1.9799911984366333e-05,
      "loss": 1.1343,
      "step": 984
    },
    {
      "epoch": 9.20748600058945,
      "grad_norm": 0.30059120059013367,
      "learning_rate": 1.9799309063912942e-05,
      "loss": 1.1284,
      "step": 985
    },
    {
      "epoch": 9.216917182434424,
      "grad_norm": 0.32138922810554504,
      "learning_rate": 1.9798705245650103e-05,
      "loss": 1.1284,
      "step": 986
    },
    {
      "epoch": 9.2263483642794,
      "grad_norm": 0.3336201310157776,
      "learning_rate": 1.979810052963313e-05,
      "loss": 1.1403,
      "step": 987
    },
    {
      "epoch": 9.235779546124373,
      "grad_norm": 0.32085028290748596,
      "learning_rate": 1.979749491591743e-05,
      "loss": 1.149,
      "step": 988
    },
    {
      "epoch": 9.245210727969349,
      "grad_norm": 0.32607004046440125,
      "learning_rate": 1.979688840455849e-05,
      "loss": 1.1722,
      "step": 989
    },
    {
      "epoch": 9.254641909814323,
      "grad_norm": 0.32348841428756714,
      "learning_rate": 1.9796280995611875e-05,
      "loss": 1.1395,
      "step": 990
    },
    {
      "epoch": 9.264073091659299,
      "grad_norm": 0.3777458667755127,
      "learning_rate": 1.979567268913324e-05,
      "loss": 1.1219,
      "step": 991
    },
    {
      "epoch": 9.273504273504274,
      "grad_norm": 0.35210666060447693,
      "learning_rate": 1.979506348517832e-05,
      "loss": 1.143,
      "step": 992
    },
    {
      "epoch": 9.282935455349248,
      "grad_norm": 0.3490491807460785,
      "learning_rate": 1.979445338380292e-05,
      "loss": 1.0957,
      "step": 993
    },
    {
      "epoch": 9.292366637194224,
      "grad_norm": 0.294016033411026,
      "learning_rate": 1.9793842385062948e-05,
      "loss": 1.1155,
      "step": 994
    },
    {
      "epoch": 9.301797819039198,
      "grad_norm": 0.3395389914512634,
      "learning_rate": 1.9793230489014378e-05,
      "loss": 1.1075,
      "step": 995
    },
    {
      "epoch": 9.311229000884174,
      "grad_norm": 0.33736076951026917,
      "learning_rate": 1.9792617695713273e-05,
      "loss": 1.1615,
      "step": 996
    },
    {
      "epoch": 9.320660182729148,
      "grad_norm": 0.31844058632850647,
      "learning_rate": 1.9792004005215775e-05,
      "loss": 1.1044,
      "step": 997
    },
    {
      "epoch": 9.330091364574123,
      "grad_norm": 0.33042362332344055,
      "learning_rate": 1.9791389417578114e-05,
      "loss": 1.1386,
      "step": 998
    },
    {
      "epoch": 9.339522546419099,
      "grad_norm": 0.298462450504303,
      "learning_rate": 1.9790773932856595e-05,
      "loss": 1.1292,
      "step": 999
    },
    {
      "epoch": 9.348953728264073,
      "grad_norm": 0.336055725812912,
      "learning_rate": 1.9790157551107606e-05,
      "loss": 1.1131,
      "step": 1000
    },
    {
      "epoch": 9.358384910109049,
      "grad_norm": 0.3152976930141449,
      "learning_rate": 1.978954027238763e-05,
      "loss": 1.0969,
      "step": 1001
    },
    {
      "epoch": 9.367816091954023,
      "grad_norm": 0.3209478259086609,
      "learning_rate": 1.9788922096753212e-05,
      "loss": 1.1085,
      "step": 1002
    },
    {
      "epoch": 9.377247273798998,
      "grad_norm": 0.3049681782722473,
      "learning_rate": 1.9788303024260992e-05,
      "loss": 1.1299,
      "step": 1003
    },
    {
      "epoch": 9.386678455643972,
      "grad_norm": 0.3237234950065613,
      "learning_rate": 1.978768305496769e-05,
      "loss": 1.1004,
      "step": 1004
    },
    {
      "epoch": 9.396109637488948,
      "grad_norm": 0.3378226161003113,
      "learning_rate": 1.9787062188930104e-05,
      "loss": 1.0643,
      "step": 1005
    },
    {
      "epoch": 9.405540819333924,
      "grad_norm": 0.3088867664337158,
      "learning_rate": 1.9786440426205122e-05,
      "loss": 1.0879,
      "step": 1006
    },
    {
      "epoch": 9.414972001178898,
      "grad_norm": 0.3393961489200592,
      "learning_rate": 1.9785817766849708e-05,
      "loss": 1.0879,
      "step": 1007
    },
    {
      "epoch": 9.424403183023873,
      "grad_norm": 0.3305712640285492,
      "learning_rate": 1.978519421092091e-05,
      "loss": 1.1275,
      "step": 1008
    },
    {
      "epoch": 9.433834364868847,
      "grad_norm": 0.34086254239082336,
      "learning_rate": 1.978456975847586e-05,
      "loss": 1.1096,
      "step": 1009
    },
    {
      "epoch": 9.443265546713823,
      "grad_norm": 0.31619158387184143,
      "learning_rate": 1.9783944409571766e-05,
      "loss": 1.1625,
      "step": 1010
    },
    {
      "epoch": 9.452696728558797,
      "grad_norm": 0.32607409358024597,
      "learning_rate": 1.9783318164265926e-05,
      "loss": 1.1474,
      "step": 1011
    },
    {
      "epoch": 9.462127910403773,
      "grad_norm": 0.3415587842464447,
      "learning_rate": 1.9782691022615714e-05,
      "loss": 1.121,
      "step": 1012
    },
    {
      "epoch": 9.471559092248748,
      "grad_norm": 0.34320521354675293,
      "learning_rate": 1.9782062984678587e-05,
      "loss": 1.0944,
      "step": 1013
    },
    {
      "epoch": 9.480990274093722,
      "grad_norm": 0.3159911036491394,
      "learning_rate": 1.9781434050512088e-05,
      "loss": 1.0819,
      "step": 1014
    },
    {
      "epoch": 9.490421455938698,
      "grad_norm": 0.31452399492263794,
      "learning_rate": 1.978080422017384e-05,
      "loss": 1.0942,
      "step": 1015
    },
    {
      "epoch": 9.499852637783672,
      "grad_norm": 0.34394705295562744,
      "learning_rate": 1.978017349372155e-05,
      "loss": 1.1259,
      "step": 1016
    },
    {
      "epoch": 9.509283819628648,
      "grad_norm": 0.3110506236553192,
      "learning_rate": 1.9779541871213e-05,
      "loss": 1.1173,
      "step": 1017
    },
    {
      "epoch": 9.518715001473621,
      "grad_norm": 0.322788804769516,
      "learning_rate": 1.977890935270606e-05,
      "loss": 1.1508,
      "step": 1018
    },
    {
      "epoch": 9.528146183318597,
      "grad_norm": 0.3259046971797943,
      "learning_rate": 1.9778275938258685e-05,
      "loss": 1.1519,
      "step": 1019
    },
    {
      "epoch": 9.537577365163571,
      "grad_norm": 0.3625785708427429,
      "learning_rate": 1.9777641627928906e-05,
      "loss": 1.0886,
      "step": 1020
    },
    {
      "epoch": 9.547008547008547,
      "grad_norm": 0.38208118081092834,
      "learning_rate": 1.9777006421774835e-05,
      "loss": 1.1192,
      "step": 1021
    },
    {
      "epoch": 9.556439728853523,
      "grad_norm": 0.3308458924293518,
      "learning_rate": 1.9776370319854675e-05,
      "loss": 1.1252,
      "step": 1022
    },
    {
      "epoch": 9.565870910698496,
      "grad_norm": 0.31205910444259644,
      "learning_rate": 1.97757333222267e-05,
      "loss": 1.1135,
      "step": 1023
    },
    {
      "epoch": 9.575302092543472,
      "grad_norm": 0.297140508890152,
      "learning_rate": 1.9775095428949278e-05,
      "loss": 1.0969,
      "step": 1024
    },
    {
      "epoch": 9.584733274388446,
      "grad_norm": 0.2954461872577667,
      "learning_rate": 1.9774456640080845e-05,
      "loss": 1.1643,
      "step": 1025
    },
    {
      "epoch": 9.594164456233422,
      "grad_norm": 0.3319762945175171,
      "learning_rate": 1.9773816955679933e-05,
      "loss": 1.1123,
      "step": 1026
    },
    {
      "epoch": 9.603595638078396,
      "grad_norm": 0.3239365518093109,
      "learning_rate": 1.9773176375805147e-05,
      "loss": 1.1327,
      "step": 1027
    },
    {
      "epoch": 9.613026819923371,
      "grad_norm": 0.323219358921051,
      "learning_rate": 1.977253490051518e-05,
      "loss": 1.1377,
      "step": 1028
    },
    {
      "epoch": 9.622458001768347,
      "grad_norm": 0.3392476439476013,
      "learning_rate": 1.9771892529868793e-05,
      "loss": 1.0767,
      "step": 1029
    },
    {
      "epoch": 9.631889183613321,
      "grad_norm": 0.30634698271751404,
      "learning_rate": 1.9771249263924853e-05,
      "loss": 1.1185,
      "step": 1030
    },
    {
      "epoch": 9.641320365458297,
      "grad_norm": 0.34170156717300415,
      "learning_rate": 1.977060510274229e-05,
      "loss": 1.1405,
      "step": 1031
    },
    {
      "epoch": 9.65075154730327,
      "grad_norm": 0.29931777715682983,
      "learning_rate": 1.9769960046380122e-05,
      "loss": 1.1407,
      "step": 1032
    },
    {
      "epoch": 9.660182729148246,
      "grad_norm": 0.3369739055633545,
      "learning_rate": 1.976931409489745e-05,
      "loss": 1.1522,
      "step": 1033
    },
    {
      "epoch": 9.66961391099322,
      "grad_norm": 0.337089478969574,
      "learning_rate": 1.976866724835345e-05,
      "loss": 1.1418,
      "step": 1034
    },
    {
      "epoch": 9.679045092838196,
      "grad_norm": 0.3494974970817566,
      "learning_rate": 1.9768019506807395e-05,
      "loss": 1.0934,
      "step": 1035
    },
    {
      "epoch": 9.688476274683172,
      "grad_norm": 0.3430628776550293,
      "learning_rate": 1.9767370870318626e-05,
      "loss": 1.1253,
      "step": 1036
    },
    {
      "epoch": 9.697907456528146,
      "grad_norm": 0.2990545928478241,
      "learning_rate": 1.9766721338946572e-05,
      "loss": 1.1201,
      "step": 1037
    },
    {
      "epoch": 9.707338638373121,
      "grad_norm": 0.3381965458393097,
      "learning_rate": 1.976607091275074e-05,
      "loss": 1.1145,
      "step": 1038
    },
    {
      "epoch": 9.716769820218095,
      "grad_norm": 0.3019287884235382,
      "learning_rate": 1.9765419591790723e-05,
      "loss": 1.1489,
      "step": 1039
    },
    {
      "epoch": 9.726201002063071,
      "grad_norm": 0.30612096190452576,
      "learning_rate": 1.97647673761262e-05,
      "loss": 1.1476,
      "step": 1040
    },
    {
      "epoch": 9.735632183908045,
      "grad_norm": 0.32876160740852356,
      "learning_rate": 1.976411426581692e-05,
      "loss": 1.0914,
      "step": 1041
    },
    {
      "epoch": 9.74506336575302,
      "grad_norm": 0.3135070204734802,
      "learning_rate": 1.9763460260922725e-05,
      "loss": 1.1063,
      "step": 1042
    },
    {
      "epoch": 9.754494547597997,
      "grad_norm": 0.3163871169090271,
      "learning_rate": 1.976280536150353e-05,
      "loss": 1.1285,
      "step": 1043
    },
    {
      "epoch": 9.76392572944297,
      "grad_norm": 0.321896493434906,
      "learning_rate": 1.9762149567619345e-05,
      "loss": 1.1209,
      "step": 1044
    },
    {
      "epoch": 9.773356911287946,
      "grad_norm": 0.3656555712223053,
      "learning_rate": 1.9761492879330248e-05,
      "loss": 1.175,
      "step": 1045
    },
    {
      "epoch": 9.78278809313292,
      "grad_norm": 0.3257378339767456,
      "learning_rate": 1.9760835296696398e-05,
      "loss": 1.1105,
      "step": 1046
    },
    {
      "epoch": 9.792219274977896,
      "grad_norm": 0.3423345685005188,
      "learning_rate": 1.9760176819778055e-05,
      "loss": 1.0839,
      "step": 1047
    },
    {
      "epoch": 9.80165045682287,
      "grad_norm": 0.3589416742324829,
      "learning_rate": 1.9759517448635543e-05,
      "loss": 1.1164,
      "step": 1048
    },
    {
      "epoch": 9.811081638667845,
      "grad_norm": 0.3540821075439453,
      "learning_rate": 1.9758857183329274e-05,
      "loss": 1.1222,
      "step": 1049
    },
    {
      "epoch": 9.820512820512821,
      "grad_norm": 0.31931307911872864,
      "learning_rate": 1.975819602391974e-05,
      "loss": 1.1257,
      "step": 1050
    },
    {
      "epoch": 9.829944002357795,
      "grad_norm": 0.34903889894485474,
      "learning_rate": 1.9757533970467513e-05,
      "loss": 1.1784,
      "step": 1051
    },
    {
      "epoch": 9.83937518420277,
      "grad_norm": 0.3331470787525177,
      "learning_rate": 1.9756871023033256e-05,
      "loss": 1.1233,
      "step": 1052
    },
    {
      "epoch": 9.848806366047745,
      "grad_norm": 0.3602074086666107,
      "learning_rate": 1.975620718167771e-05,
      "loss": 1.145,
      "step": 1053
    },
    {
      "epoch": 9.85823754789272,
      "grad_norm": 0.3193846046924591,
      "learning_rate": 1.9755542446461687e-05,
      "loss": 1.106,
      "step": 1054
    },
    {
      "epoch": 9.867668729737694,
      "grad_norm": 0.3402208089828491,
      "learning_rate": 1.9754876817446097e-05,
      "loss": 1.1144,
      "step": 1055
    },
    {
      "epoch": 9.87709991158267,
      "grad_norm": 0.3478154242038727,
      "learning_rate": 1.975421029469192e-05,
      "loss": 1.1267,
      "step": 1056
    },
    {
      "epoch": 9.886531093427646,
      "grad_norm": 0.3254580497741699,
      "learning_rate": 1.975354287826023e-05,
      "loss": 1.1324,
      "step": 1057
    },
    {
      "epoch": 9.89596227527262,
      "grad_norm": 0.3202641010284424,
      "learning_rate": 1.9752874568212165e-05,
      "loss": 1.1278,
      "step": 1058
    },
    {
      "epoch": 9.905393457117595,
      "grad_norm": 0.32604584097862244,
      "learning_rate": 1.9752205364608965e-05,
      "loss": 1.1364,
      "step": 1059
    },
    {
      "epoch": 9.91482463896257,
      "grad_norm": 0.31243661046028137,
      "learning_rate": 1.9751535267511936e-05,
      "loss": 1.1332,
      "step": 1060
    },
    {
      "epoch": 9.924255820807545,
      "grad_norm": 0.3489837050437927,
      "learning_rate": 1.9750864276982473e-05,
      "loss": 1.1222,
      "step": 1061
    },
    {
      "epoch": 9.933687002652519,
      "grad_norm": 0.3276897966861725,
      "learning_rate": 1.9750192393082056e-05,
      "loss": 1.1217,
      "step": 1062
    },
    {
      "epoch": 9.943118184497495,
      "grad_norm": 0.3590974807739258,
      "learning_rate": 1.9749519615872238e-05,
      "loss": 1.1412,
      "step": 1063
    },
    {
      "epoch": 9.95254936634247,
      "grad_norm": 0.3440941870212555,
      "learning_rate": 1.9748845945414658e-05,
      "loss": 1.1244,
      "step": 1064
    },
    {
      "epoch": 9.961980548187444,
      "grad_norm": 0.34693431854248047,
      "learning_rate": 1.9748171381771043e-05,
      "loss": 1.1489,
      "step": 1065
    },
    {
      "epoch": 9.97141173003242,
      "grad_norm": 0.3440100848674774,
      "learning_rate": 1.9747495925003194e-05,
      "loss": 1.1754,
      "step": 1066
    },
    {
      "epoch": 9.980842911877394,
      "grad_norm": 0.3333016633987427,
      "learning_rate": 1.9746819575172994e-05,
      "loss": 1.1461,
      "step": 1067
    },
    {
      "epoch": 9.99027409372237,
      "grad_norm": 0.3210360109806061,
      "learning_rate": 1.974614233234241e-05,
      "loss": 1.1228,
      "step": 1068
    },
    {
      "epoch": 9.999705275567344,
      "grad_norm": 0.32681193947792053,
      "learning_rate": 1.9745464196573495e-05,
      "loss": 1.123,
      "step": 1069
    },
    {
      "epoch": 10.0,
      "grad_norm": 2.1303908824920654,
      "learning_rate": 1.9744785167928375e-05,
      "loss": 0.7223,
      "step": 1070
    },
    {
      "epoch": 10.009431181844976,
      "grad_norm": 0.3287103772163391,
      "learning_rate": 1.9744105246469264e-05,
      "loss": 1.1456,
      "step": 1071
    },
    {
      "epoch": 10.01886236368995,
      "grad_norm": 0.33117175102233887,
      "learning_rate": 1.9743424432258457e-05,
      "loss": 1.1602,
      "step": 1072
    },
    {
      "epoch": 10.028293545534925,
      "grad_norm": 0.33360859751701355,
      "learning_rate": 1.9742742725358326e-05,
      "loss": 1.1404,
      "step": 1073
    },
    {
      "epoch": 10.0377247273799,
      "grad_norm": 0.32992252707481384,
      "learning_rate": 1.9742060125831334e-05,
      "loss": 1.0719,
      "step": 1074
    },
    {
      "epoch": 10.047155909224875,
      "grad_norm": 0.334869921207428,
      "learning_rate": 1.974137663374002e-05,
      "loss": 1.1023,
      "step": 1075
    },
    {
      "epoch": 10.056587091069849,
      "grad_norm": 0.31745269894599915,
      "learning_rate": 1.9740692249147e-05,
      "loss": 1.085,
      "step": 1076
    },
    {
      "epoch": 10.066018272914825,
      "grad_norm": 0.32197239995002747,
      "learning_rate": 1.974000697211498e-05,
      "loss": 1.1291,
      "step": 1077
    },
    {
      "epoch": 10.0754494547598,
      "grad_norm": 0.32814353704452515,
      "learning_rate": 1.973932080270675e-05,
      "loss": 1.1353,
      "step": 1078
    },
    {
      "epoch": 10.084880636604774,
      "grad_norm": 0.3304601013660431,
      "learning_rate": 1.9738633740985172e-05,
      "loss": 1.1237,
      "step": 1079
    },
    {
      "epoch": 10.09431181844975,
      "grad_norm": 0.33138349652290344,
      "learning_rate": 1.9737945787013195e-05,
      "loss": 1.1229,
      "step": 1080
    },
    {
      "epoch": 10.103743000294724,
      "grad_norm": 0.3092903792858124,
      "learning_rate": 1.9737256940853848e-05,
      "loss": 1.1098,
      "step": 1081
    },
    {
      "epoch": 10.1131741821397,
      "grad_norm": 0.33177605271339417,
      "learning_rate": 1.9736567202570243e-05,
      "loss": 1.1429,
      "step": 1082
    },
    {
      "epoch": 10.122605363984674,
      "grad_norm": 0.34932491183280945,
      "learning_rate": 1.9735876572225575e-05,
      "loss": 1.1426,
      "step": 1083
    },
    {
      "epoch": 10.13203654582965,
      "grad_norm": 0.3337744176387787,
      "learning_rate": 1.9735185049883118e-05,
      "loss": 1.0957,
      "step": 1084
    },
    {
      "epoch": 10.141467727674625,
      "grad_norm": 0.33670514822006226,
      "learning_rate": 1.973449263560623e-05,
      "loss": 1.1124,
      "step": 1085
    },
    {
      "epoch": 10.150898909519599,
      "grad_norm": 0.3315304219722748,
      "learning_rate": 1.973379932945835e-05,
      "loss": 1.148,
      "step": 1086
    },
    {
      "epoch": 10.160330091364575,
      "grad_norm": 0.33011165261268616,
      "learning_rate": 1.9733105131502996e-05,
      "loss": 1.1423,
      "step": 1087
    },
    {
      "epoch": 10.169761273209549,
      "grad_norm": 0.3263069689273834,
      "learning_rate": 1.9732410041803775e-05,
      "loss": 1.0923,
      "step": 1088
    },
    {
      "epoch": 10.179192455054524,
      "grad_norm": 0.3167310655117035,
      "learning_rate": 1.9731714060424364e-05,
      "loss": 1.0925,
      "step": 1089
    },
    {
      "epoch": 10.188623636899498,
      "grad_norm": 0.3105847239494324,
      "learning_rate": 1.9731017187428535e-05,
      "loss": 1.1049,
      "step": 1090
    },
    {
      "epoch": 10.198054818744474,
      "grad_norm": 0.32775548100471497,
      "learning_rate": 1.9730319422880133e-05,
      "loss": 1.1438,
      "step": 1091
    },
    {
      "epoch": 10.20748600058945,
      "grad_norm": 0.3413657248020172,
      "learning_rate": 1.9729620766843088e-05,
      "loss": 1.1165,
      "step": 1092
    },
    {
      "epoch": 10.216917182434424,
      "grad_norm": 0.3214491009712219,
      "learning_rate": 1.9728921219381405e-05,
      "loss": 1.2011,
      "step": 1093
    },
    {
      "epoch": 10.2263483642794,
      "grad_norm": 0.3110083341598511,
      "learning_rate": 1.9728220780559187e-05,
      "loss": 1.1583,
      "step": 1094
    },
    {
      "epoch": 10.235779546124373,
      "grad_norm": 0.3542344272136688,
      "learning_rate": 1.9727519450440597e-05,
      "loss": 1.1555,
      "step": 1095
    },
    {
      "epoch": 10.245210727969349,
      "grad_norm": 0.3408867418766022,
      "learning_rate": 1.9726817229089895e-05,
      "loss": 1.1472,
      "step": 1096
    },
    {
      "epoch": 10.254641909814323,
      "grad_norm": 0.32141172885894775,
      "learning_rate": 1.972611411657142e-05,
      "loss": 1.1158,
      "step": 1097
    },
    {
      "epoch": 10.264073091659299,
      "grad_norm": 0.3276224434375763,
      "learning_rate": 1.972541011294959e-05,
      "loss": 1.1187,
      "step": 1098
    },
    {
      "epoch": 10.273504273504274,
      "grad_norm": 0.32611164450645447,
      "learning_rate": 1.9724705218288903e-05,
      "loss": 1.065,
      "step": 1099
    },
    {
      "epoch": 10.282935455349248,
      "grad_norm": 0.304199755191803,
      "learning_rate": 1.9723999432653947e-05,
      "loss": 1.1437,
      "step": 1100
    },
    {
      "epoch": 10.292366637194224,
      "grad_norm": 0.3580639064311981,
      "learning_rate": 1.972329275610938e-05,
      "loss": 1.1255,
      "step": 1101
    },
    {
      "epoch": 10.301797819039198,
      "grad_norm": 0.3399190604686737,
      "learning_rate": 1.972258518871995e-05,
      "loss": 1.1203,
      "step": 1102
    },
    {
      "epoch": 10.311229000884174,
      "grad_norm": 0.3315814435482025,
      "learning_rate": 1.9721876730550483e-05,
      "loss": 1.1554,
      "step": 1103
    },
    {
      "epoch": 10.320660182729148,
      "grad_norm": 0.3371059000492096,
      "learning_rate": 1.972116738166589e-05,
      "loss": 1.1296,
      "step": 1104
    },
    {
      "epoch": 10.330091364574123,
      "grad_norm": 0.35531091690063477,
      "learning_rate": 1.9720457142131157e-05,
      "loss": 1.1243,
      "step": 1105
    },
    {
      "epoch": 10.339522546419099,
      "grad_norm": 0.33311110734939575,
      "learning_rate": 1.971974601201136e-05,
      "loss": 1.1053,
      "step": 1106
    },
    {
      "epoch": 10.348953728264073,
      "grad_norm": 0.3483726382255554,
      "learning_rate": 1.9719033991371653e-05,
      "loss": 1.1436,
      "step": 1107
    },
    {
      "epoch": 10.358384910109049,
      "grad_norm": 0.3431892395019531,
      "learning_rate": 1.971832108027727e-05,
      "loss": 1.1264,
      "step": 1108
    },
    {
      "epoch": 10.367816091954023,
      "grad_norm": 0.3364507853984833,
      "learning_rate": 1.9717607278793524e-05,
      "loss": 1.0801,
      "step": 1109
    },
    {
      "epoch": 10.377247273798998,
      "grad_norm": 0.32525816559791565,
      "learning_rate": 1.9716892586985815e-05,
      "loss": 1.1001,
      "step": 1110
    },
    {
      "epoch": 10.386678455643972,
      "grad_norm": 0.32847967743873596,
      "learning_rate": 1.9716177004919625e-05,
      "loss": 1.085,
      "step": 1111
    },
    {
      "epoch": 10.396109637488948,
      "grad_norm": 0.3196488916873932,
      "learning_rate": 1.9715460532660516e-05,
      "loss": 1.1047,
      "step": 1112
    },
    {
      "epoch": 10.405540819333924,
      "grad_norm": 0.3444192707538605,
      "learning_rate": 1.971474317027413e-05,
      "loss": 1.1177,
      "step": 1113
    },
    {
      "epoch": 10.414972001178898,
      "grad_norm": 0.32241132855415344,
      "learning_rate": 1.9714024917826193e-05,
      "loss": 1.1132,
      "step": 1114
    },
    {
      "epoch": 10.424403183023873,
      "grad_norm": 0.322923868894577,
      "learning_rate": 1.97133057753825e-05,
      "loss": 1.0938,
      "step": 1115
    },
    {
      "epoch": 10.433834364868847,
      "grad_norm": 0.3165452778339386,
      "learning_rate": 1.9712585743008957e-05,
      "loss": 1.1569,
      "step": 1116
    },
    {
      "epoch": 10.443265546713823,
      "grad_norm": 0.35334402322769165,
      "learning_rate": 1.971186482077152e-05,
      "loss": 1.1151,
      "step": 1117
    },
    {
      "epoch": 10.452696728558797,
      "grad_norm": 0.3155128061771393,
      "learning_rate": 1.971114300873624e-05,
      "loss": 1.1444,
      "step": 1118
    },
    {
      "epoch": 10.462127910403773,
      "grad_norm": 0.34099647402763367,
      "learning_rate": 1.9710420306969258e-05,
      "loss": 1.1177,
      "step": 1119
    },
    {
      "epoch": 10.471559092248748,
      "grad_norm": 0.3335476517677307,
      "learning_rate": 1.970969671553678e-05,
      "loss": 1.133,
      "step": 1120
    },
    {
      "epoch": 10.480990274093722,
      "grad_norm": 0.341154009103775,
      "learning_rate": 1.97089722345051e-05,
      "loss": 1.1119,
      "step": 1121
    },
    {
      "epoch": 10.490421455938698,
      "grad_norm": 0.31618428230285645,
      "learning_rate": 1.9708246863940604e-05,
      "loss": 1.0872,
      "step": 1122
    },
    {
      "epoch": 10.499852637783672,
      "grad_norm": 0.32555925846099854,
      "learning_rate": 1.9707520603909743e-05,
      "loss": 1.1083,
      "step": 1123
    },
    {
      "epoch": 10.509283819628648,
      "grad_norm": 0.32909464836120605,
      "learning_rate": 1.9706793454479055e-05,
      "loss": 1.1299,
      "step": 1124
    },
    {
      "epoch": 10.518715001473621,
      "grad_norm": 0.3366670310497284,
      "learning_rate": 1.9706065415715167e-05,
      "loss": 1.1255,
      "step": 1125
    },
    {
      "epoch": 10.528146183318597,
      "grad_norm": 0.34271475672721863,
      "learning_rate": 1.9705336487684777e-05,
      "loss": 1.1171,
      "step": 1126
    },
    {
      "epoch": 10.537577365163571,
      "grad_norm": 0.32770034670829773,
      "learning_rate": 1.9704606670454673e-05,
      "loss": 1.1432,
      "step": 1127
    },
    {
      "epoch": 10.547008547008547,
      "grad_norm": 0.3646000921726227,
      "learning_rate": 1.9703875964091716e-05,
      "loss": 1.1029,
      "step": 1128
    },
    {
      "epoch": 10.556439728853523,
      "grad_norm": 0.35055986046791077,
      "learning_rate": 1.9703144368662855e-05,
      "loss": 1.1092,
      "step": 1129
    },
    {
      "epoch": 10.565870910698496,
      "grad_norm": 0.34334224462509155,
      "learning_rate": 1.9702411884235122e-05,
      "loss": 1.1064,
      "step": 1130
    },
    {
      "epoch": 10.575302092543472,
      "grad_norm": 0.3248698115348816,
      "learning_rate": 1.9701678510875624e-05,
      "loss": 1.1086,
      "step": 1131
    },
    {
      "epoch": 10.584733274388446,
      "grad_norm": 0.32977238297462463,
      "learning_rate": 1.9700944248651553e-05,
      "loss": 1.1397,
      "step": 1132
    },
    {
      "epoch": 10.594164456233422,
      "grad_norm": 0.3353089690208435,
      "learning_rate": 1.970020909763018e-05,
      "loss": 1.1019,
      "step": 1133
    },
    {
      "epoch": 10.603595638078396,
      "grad_norm": 0.2983211874961853,
      "learning_rate": 1.9699473057878862e-05,
      "loss": 1.1075,
      "step": 1134
    },
    {
      "epoch": 10.613026819923371,
      "grad_norm": 0.3500257730484009,
      "learning_rate": 1.9698736129465032e-05,
      "loss": 1.0711,
      "step": 1135
    },
    {
      "epoch": 10.622458001768347,
      "grad_norm": 0.3799750506877899,
      "learning_rate": 1.9697998312456212e-05,
      "loss": 1.1075,
      "step": 1136
    },
    {
      "epoch": 10.631889183613321,
      "grad_norm": 0.34590956568717957,
      "learning_rate": 1.969725960691999e-05,
      "loss": 1.114,
      "step": 1137
    },
    {
      "epoch": 10.641320365458297,
      "grad_norm": 0.336953729391098,
      "learning_rate": 1.9696520012924057e-05,
      "loss": 1.146,
      "step": 1138
    },
    {
      "epoch": 10.65075154730327,
      "grad_norm": 0.34128937125205994,
      "learning_rate": 1.9695779530536174e-05,
      "loss": 1.1109,
      "step": 1139
    },
    {
      "epoch": 10.660182729148246,
      "grad_norm": 0.33180683851242065,
      "learning_rate": 1.9695038159824178e-05,
      "loss": 1.1164,
      "step": 1140
    },
    {
      "epoch": 10.66961391099322,
      "grad_norm": 0.35120007395744324,
      "learning_rate": 1.9694295900855994e-05,
      "loss": 1.123,
      "step": 1141
    },
    {
      "epoch": 10.679045092838196,
      "grad_norm": 0.3432934582233429,
      "learning_rate": 1.9693552753699633e-05,
      "loss": 1.0743,
      "step": 1142
    },
    {
      "epoch": 10.688476274683172,
      "grad_norm": 0.3784763813018799,
      "learning_rate": 1.9692808718423172e-05,
      "loss": 1.1227,
      "step": 1143
    },
    {
      "epoch": 10.697907456528146,
      "grad_norm": 0.3699739873409271,
      "learning_rate": 1.969206379509479e-05,
      "loss": 1.1451,
      "step": 1144
    },
    {
      "epoch": 10.707338638373121,
      "grad_norm": 0.35090172290802,
      "learning_rate": 1.969131798378273e-05,
      "loss": 1.0707,
      "step": 1145
    },
    {
      "epoch": 10.716769820218095,
      "grad_norm": 0.3303532004356384,
      "learning_rate": 1.9690571284555323e-05,
      "loss": 1.1467,
      "step": 1146
    },
    {
      "epoch": 10.726201002063071,
      "grad_norm": 0.33435899019241333,
      "learning_rate": 1.9689823697480985e-05,
      "loss": 1.1102,
      "step": 1147
    },
    {
      "epoch": 10.735632183908045,
      "grad_norm": 0.3470252454280853,
      "learning_rate": 1.968907522262821e-05,
      "loss": 1.1064,
      "step": 1148
    },
    {
      "epoch": 10.74506336575302,
      "grad_norm": 0.31846365332603455,
      "learning_rate": 1.9688325860065567e-05,
      "loss": 1.1249,
      "step": 1149
    },
    {
      "epoch": 10.754494547597997,
      "grad_norm": 0.338206946849823,
      "learning_rate": 1.9687575609861716e-05,
      "loss": 1.1564,
      "step": 1150
    },
    {
      "epoch": 10.76392572944297,
      "grad_norm": 0.36096081137657166,
      "learning_rate": 1.9686824472085395e-05,
      "loss": 1.0837,
      "step": 1151
    },
    {
      "epoch": 10.773356911287946,
      "grad_norm": 0.3373167812824249,
      "learning_rate": 1.9686072446805425e-05,
      "loss": 1.1555,
      "step": 1152
    },
    {
      "epoch": 10.78278809313292,
      "grad_norm": 0.3354816138744354,
      "learning_rate": 1.96853195340907e-05,
      "loss": 1.1613,
      "step": 1153
    },
    {
      "epoch": 10.792219274977896,
      "grad_norm": 0.3517622947692871,
      "learning_rate": 1.968456573401021e-05,
      "loss": 1.1377,
      "step": 1154
    },
    {
      "epoch": 10.80165045682287,
      "grad_norm": 0.35303163528442383,
      "learning_rate": 1.9683811046633012e-05,
      "loss": 1.1159,
      "step": 1155
    },
    {
      "epoch": 10.811081638667845,
      "grad_norm": 0.3458373248577118,
      "learning_rate": 1.968305547202825e-05,
      "loss": 1.0905,
      "step": 1156
    },
    {
      "epoch": 10.820512820512821,
      "grad_norm": 0.3195459544658661,
      "learning_rate": 1.9682299010265152e-05,
      "loss": 1.1488,
      "step": 1157
    },
    {
      "epoch": 10.829944002357795,
      "grad_norm": 0.318263977766037,
      "learning_rate": 1.968154166141303e-05,
      "loss": 1.1363,
      "step": 1158
    },
    {
      "epoch": 10.83937518420277,
      "grad_norm": 0.36983856558799744,
      "learning_rate": 1.9680783425541258e-05,
      "loss": 1.0911,
      "step": 1159
    },
    {
      "epoch": 10.848806366047745,
      "grad_norm": 0.3604796528816223,
      "learning_rate": 1.968002430271932e-05,
      "loss": 1.1214,
      "step": 1160
    },
    {
      "epoch": 10.85823754789272,
      "grad_norm": 0.35543522238731384,
      "learning_rate": 1.9679264293016755e-05,
      "loss": 1.1553,
      "step": 1161
    },
    {
      "epoch": 10.867668729737694,
      "grad_norm": 0.33969902992248535,
      "learning_rate": 1.96785033965032e-05,
      "loss": 1.1239,
      "step": 1162
    },
    {
      "epoch": 10.87709991158267,
      "grad_norm": 0.3577300012111664,
      "learning_rate": 1.9677741613248373e-05,
      "loss": 1.0958,
      "step": 1163
    },
    {
      "epoch": 10.886531093427646,
      "grad_norm": 0.37591397762298584,
      "learning_rate": 1.9676978943322055e-05,
      "loss": 1.1244,
      "step": 1164
    },
    {
      "epoch": 10.89596227527262,
      "grad_norm": 0.3413938581943512,
      "learning_rate": 1.9676215386794137e-05,
      "loss": 1.1357,
      "step": 1165
    },
    {
      "epoch": 10.905393457117595,
      "grad_norm": 0.35482072830200195,
      "learning_rate": 1.9675450943734567e-05,
      "loss": 1.1205,
      "step": 1166
    },
    {
      "epoch": 10.91482463896257,
      "grad_norm": 0.32422298192977905,
      "learning_rate": 1.9674685614213384e-05,
      "loss": 1.1542,
      "step": 1167
    },
    {
      "epoch": 10.924255820807545,
      "grad_norm": 0.33138424158096313,
      "learning_rate": 1.9673919398300706e-05,
      "loss": 1.1305,
      "step": 1168
    },
    {
      "epoch": 10.933687002652519,
      "grad_norm": 0.3523331880569458,
      "learning_rate": 1.9673152296066738e-05,
      "loss": 1.1131,
      "step": 1169
    },
    {
      "epoch": 10.943118184497495,
      "grad_norm": 0.3162321150302887,
      "learning_rate": 1.9672384307581757e-05,
      "loss": 1.13,
      "step": 1170
    },
    {
      "epoch": 10.95254936634247,
      "grad_norm": 0.31900742650032043,
      "learning_rate": 1.9671615432916128e-05,
      "loss": 1.17,
      "step": 1171
    },
    {
      "epoch": 10.961980548187444,
      "grad_norm": 0.3432539403438568,
      "learning_rate": 1.9670845672140296e-05,
      "loss": 1.1239,
      "step": 1172
    },
    {
      "epoch": 10.97141173003242,
      "grad_norm": 0.3387048542499542,
      "learning_rate": 1.9670075025324786e-05,
      "loss": 1.0764,
      "step": 1173
    },
    {
      "epoch": 10.980842911877394,
      "grad_norm": 0.3263111412525177,
      "learning_rate": 1.9669303492540202e-05,
      "loss": 1.131,
      "step": 1174
    },
    {
      "epoch": 10.99027409372237,
      "grad_norm": 0.3256021738052368,
      "learning_rate": 1.9668531073857236e-05,
      "loss": 1.1366,
      "step": 1175
    },
    {
      "epoch": 10.999705275567344,
      "grad_norm": 0.35615262389183044,
      "learning_rate": 1.966775776934665e-05,
      "loss": 1.1388,
      "step": 1176
    },
    {
      "epoch": 11.0,
      "grad_norm": 1.6935187578201294,
      "learning_rate": 1.96669835790793e-05,
      "loss": 0.6931,
      "step": 1177
    },
    {
      "epoch": 11.009431181844976,
      "grad_norm": 0.3545399308204651,
      "learning_rate": 1.9666208503126115e-05,
      "loss": 1.1543,
      "step": 1178
    },
    {
      "epoch": 11.01886236368995,
      "grad_norm": 0.33502769470214844,
      "learning_rate": 1.9665432541558105e-05,
      "loss": 1.1302,
      "step": 1179
    },
    {
      "epoch": 11.028293545534925,
      "grad_norm": 0.34005847573280334,
      "learning_rate": 1.9664655694446367e-05,
      "loss": 1.1324,
      "step": 1180
    },
    {
      "epoch": 11.0377247273799,
      "grad_norm": 0.3357713222503662,
      "learning_rate": 1.9663877961862074e-05,
      "loss": 1.0933,
      "step": 1181
    },
    {
      "epoch": 11.047155909224875,
      "grad_norm": 0.34975507855415344,
      "learning_rate": 1.966309934387648e-05,
      "loss": 1.1035,
      "step": 1182
    },
    {
      "epoch": 11.056587091069849,
      "grad_norm": 0.3259058892726898,
      "learning_rate": 1.9662319840560923e-05,
      "loss": 1.1468,
      "step": 1183
    },
    {
      "epoch": 11.066018272914825,
      "grad_norm": 0.36397841572761536,
      "learning_rate": 1.966153945198682e-05,
      "loss": 1.1234,
      "step": 1184
    },
    {
      "epoch": 11.0754494547598,
      "grad_norm": 0.34289494156837463,
      "learning_rate": 1.9660758178225675e-05,
      "loss": 1.1308,
      "step": 1185
    },
    {
      "epoch": 11.084880636604774,
      "grad_norm": 0.3257163465023041,
      "learning_rate": 1.9659976019349064e-05,
      "loss": 1.1019,
      "step": 1186
    },
    {
      "epoch": 11.09431181844975,
      "grad_norm": 0.3627397418022156,
      "learning_rate": 1.9659192975428642e-05,
      "loss": 1.1385,
      "step": 1187
    },
    {
      "epoch": 11.103743000294724,
      "grad_norm": 0.33348962664604187,
      "learning_rate": 1.9658409046536162e-05,
      "loss": 1.1457,
      "step": 1188
    },
    {
      "epoch": 11.1131741821397,
      "grad_norm": 0.30707818269729614,
      "learning_rate": 1.9657624232743443e-05,
      "loss": 1.1314,
      "step": 1189
    },
    {
      "epoch": 11.122605363984674,
      "grad_norm": 0.3443623483181,
      "learning_rate": 1.9656838534122387e-05,
      "loss": 1.1175,
      "step": 1190
    },
    {
      "epoch": 11.13203654582965,
      "grad_norm": 0.3655952215194702,
      "learning_rate": 1.9656051950744982e-05,
      "loss": 1.1311,
      "step": 1191
    },
    {
      "epoch": 11.141467727674625,
      "grad_norm": 0.3404823839664459,
      "learning_rate": 1.9655264482683293e-05,
      "loss": 1.0964,
      "step": 1192
    },
    {
      "epoch": 11.150898909519599,
      "grad_norm": 0.3233632445335388,
      "learning_rate": 1.965447613000947e-05,
      "loss": 1.1048,
      "step": 1193
    },
    {
      "epoch": 11.160330091364575,
      "grad_norm": 0.33819976449012756,
      "learning_rate": 1.965368689279574e-05,
      "loss": 1.1221,
      "step": 1194
    },
    {
      "epoch": 11.169761273209549,
      "grad_norm": 0.32687047123908997,
      "learning_rate": 1.9652896771114416e-05,
      "loss": 1.1132,
      "step": 1195
    },
    {
      "epoch": 11.179192455054524,
      "grad_norm": 0.3674697279930115,
      "learning_rate": 1.9652105765037883e-05,
      "loss": 1.1023,
      "step": 1196
    },
    {
      "epoch": 11.188623636899498,
      "grad_norm": 0.34745335578918457,
      "learning_rate": 1.965131387463861e-05,
      "loss": 1.1367,
      "step": 1197
    },
    {
      "epoch": 11.198054818744474,
      "grad_norm": 0.34042438864707947,
      "learning_rate": 1.965052109998916e-05,
      "loss": 1.1523,
      "step": 1198
    },
    {
      "epoch": 11.20748600058945,
      "grad_norm": 0.3212912082672119,
      "learning_rate": 1.9649727441162162e-05,
      "loss": 1.1189,
      "step": 1199
    },
    {
      "epoch": 11.216917182434424,
      "grad_norm": 0.3339918553829193,
      "learning_rate": 1.964893289823033e-05,
      "loss": 1.0809,
      "step": 1200
    },
    {
      "epoch": 11.2263483642794,
      "grad_norm": 0.33783629536628723,
      "learning_rate": 1.964813747126646e-05,
      "loss": 1.1144,
      "step": 1201
    },
    {
      "epoch": 11.235779546124373,
      "grad_norm": 0.39996570348739624,
      "learning_rate": 1.9647341160343428e-05,
      "loss": 1.132,
      "step": 1202
    },
    {
      "epoch": 11.245210727969349,
      "grad_norm": 0.3449597954750061,
      "learning_rate": 1.9646543965534195e-05,
      "loss": 1.0866,
      "step": 1203
    },
    {
      "epoch": 11.254641909814323,
      "grad_norm": 0.3902365565299988,
      "learning_rate": 1.9645745886911794e-05,
      "loss": 1.1076,
      "step": 1204
    },
    {
      "epoch": 11.264073091659299,
      "grad_norm": 0.34230291843414307,
      "learning_rate": 1.9644946924549352e-05,
      "loss": 1.0875,
      "step": 1205
    },
    {
      "epoch": 11.273504273504274,
      "grad_norm": 0.3403967320919037,
      "learning_rate": 1.9644147078520065e-05,
      "loss": 1.1179,
      "step": 1206
    },
    {
      "epoch": 11.282935455349248,
      "grad_norm": 0.36026090383529663,
      "learning_rate": 1.9643346348897216e-05,
      "loss": 1.1142,
      "step": 1207
    },
    {
      "epoch": 11.292366637194224,
      "grad_norm": 0.3251414895057678,
      "learning_rate": 1.9642544735754164e-05,
      "loss": 1.154,
      "step": 1208
    },
    {
      "epoch": 11.301797819039198,
      "grad_norm": 0.32509076595306396,
      "learning_rate": 1.9641742239164357e-05,
      "loss": 1.1029,
      "step": 1209
    },
    {
      "epoch": 11.311229000884174,
      "grad_norm": 0.35267403721809387,
      "learning_rate": 1.964093885920132e-05,
      "loss": 1.1651,
      "step": 1210
    },
    {
      "epoch": 11.320660182729148,
      "grad_norm": 0.3343515992164612,
      "learning_rate": 1.9640134595938648e-05,
      "loss": 1.1571,
      "step": 1211
    },
    {
      "epoch": 11.330091364574123,
      "grad_norm": 0.31700897216796875,
      "learning_rate": 1.9639329449450044e-05,
      "loss": 1.1141,
      "step": 1212
    },
    {
      "epoch": 11.339522546419099,
      "grad_norm": 0.3293388783931732,
      "learning_rate": 1.9638523419809266e-05,
      "loss": 1.135,
      "step": 1213
    },
    {
      "epoch": 11.348953728264073,
      "grad_norm": 0.3454344868659973,
      "learning_rate": 1.9637716507090162e-05,
      "loss": 1.1286,
      "step": 1214
    },
    {
      "epoch": 11.358384910109049,
      "grad_norm": 0.36902227997779846,
      "learning_rate": 1.9636908711366662e-05,
      "loss": 1.088,
      "step": 1215
    },
    {
      "epoch": 11.367816091954023,
      "grad_norm": 0.4071858823299408,
      "learning_rate": 1.9636100032712778e-05,
      "loss": 1.077,
      "step": 1216
    },
    {
      "epoch": 11.377247273798998,
      "grad_norm": 0.3332138657569885,
      "learning_rate": 1.9635290471202596e-05,
      "loss": 1.0725,
      "step": 1217
    },
    {
      "epoch": 11.386678455643972,
      "grad_norm": 0.3362845778465271,
      "learning_rate": 1.9634480026910293e-05,
      "loss": 1.1233,
      "step": 1218
    },
    {
      "epoch": 11.396109637488948,
      "grad_norm": 0.3299788236618042,
      "learning_rate": 1.963366869991012e-05,
      "loss": 1.1404,
      "step": 1219
    },
    {
      "epoch": 11.405540819333924,
      "grad_norm": 0.3383111357688904,
      "learning_rate": 1.9632856490276408e-05,
      "loss": 1.0994,
      "step": 1220
    },
    {
      "epoch": 11.414972001178898,
      "grad_norm": 0.37016940116882324,
      "learning_rate": 1.9632043398083573e-05,
      "loss": 1.1298,
      "step": 1221
    },
    {
      "epoch": 11.424403183023873,
      "grad_norm": 0.37602126598358154,
      "learning_rate": 1.9631229423406114e-05,
      "loss": 1.1551,
      "step": 1222
    },
    {
      "epoch": 11.433834364868847,
      "grad_norm": 0.3435502052307129,
      "learning_rate": 1.96304145663186e-05,
      "loss": 1.1153,
      "step": 1223
    },
    {
      "epoch": 11.443265546713823,
      "grad_norm": 0.33022287487983704,
      "learning_rate": 1.9629598826895698e-05,
      "loss": 1.1411,
      "step": 1224
    },
    {
      "epoch": 11.452696728558797,
      "grad_norm": 0.3283100426197052,
      "learning_rate": 1.9628782205212134e-05,
      "loss": 1.0827,
      "step": 1225
    },
    {
      "epoch": 11.462127910403773,
      "grad_norm": 0.37720999121665955,
      "learning_rate": 1.9627964701342732e-05,
      "loss": 1.1135,
      "step": 1226
    },
    {
      "epoch": 11.471559092248748,
      "grad_norm": 0.35840556025505066,
      "learning_rate": 1.962714631536239e-05,
      "loss": 1.1087,
      "step": 1227
    },
    {
      "epoch": 11.480990274093722,
      "grad_norm": 0.32472479343414307,
      "learning_rate": 1.9626327047346095e-05,
      "loss": 1.0676,
      "step": 1228
    },
    {
      "epoch": 11.490421455938698,
      "grad_norm": 0.41812029480934143,
      "learning_rate": 1.9625506897368897e-05,
      "loss": 1.103,
      "step": 1229
    },
    {
      "epoch": 11.499852637783672,
      "grad_norm": 0.3422810435295105,
      "learning_rate": 1.9624685865505947e-05,
      "loss": 1.1563,
      "step": 1230
    },
    {
      "epoch": 11.509283819628648,
      "grad_norm": 0.3437187373638153,
      "learning_rate": 1.9623863951832467e-05,
      "loss": 1.1259,
      "step": 1231
    },
    {
      "epoch": 11.518715001473621,
      "grad_norm": 0.3377770483493805,
      "learning_rate": 1.9623041156423753e-05,
      "loss": 1.1581,
      "step": 1232
    },
    {
      "epoch": 11.528146183318597,
      "grad_norm": 0.33552831411361694,
      "learning_rate": 1.9622217479355196e-05,
      "loss": 1.1133,
      "step": 1233
    },
    {
      "epoch": 11.537577365163571,
      "grad_norm": 0.3203872740268707,
      "learning_rate": 1.962139292070226e-05,
      "loss": 1.1002,
      "step": 1234
    },
    {
      "epoch": 11.547008547008547,
      "grad_norm": 0.3372959494590759,
      "learning_rate": 1.9620567480540486e-05,
      "loss": 1.1156,
      "step": 1235
    },
    {
      "epoch": 11.556439728853523,
      "grad_norm": 0.33533310890197754,
      "learning_rate": 1.961974115894551e-05,
      "loss": 1.1593,
      "step": 1236
    },
    {
      "epoch": 11.565870910698496,
      "grad_norm": 0.3301073908805847,
      "learning_rate": 1.9618913955993028e-05,
      "loss": 1.1073,
      "step": 1237
    },
    {
      "epoch": 11.575302092543472,
      "grad_norm": 0.35930004715919495,
      "learning_rate": 1.961808587175884e-05,
      "loss": 1.1642,
      "step": 1238
    },
    {
      "epoch": 11.584733274388446,
      "grad_norm": 0.39501523971557617,
      "learning_rate": 1.96172569063188e-05,
      "loss": 1.1171,
      "step": 1239
    },
    {
      "epoch": 11.594164456233422,
      "grad_norm": 0.3323787450790405,
      "learning_rate": 1.9616427059748875e-05,
      "loss": 1.1154,
      "step": 1240
    },
    {
      "epoch": 11.603595638078396,
      "grad_norm": 0.35207295417785645,
      "learning_rate": 1.9615596332125083e-05,
      "loss": 1.1205,
      "step": 1241
    },
    {
      "epoch": 11.613026819923371,
      "grad_norm": 0.33099156618118286,
      "learning_rate": 1.9614764723523535e-05,
      "loss": 1.137,
      "step": 1242
    },
    {
      "epoch": 11.622458001768347,
      "grad_norm": 0.33013221621513367,
      "learning_rate": 1.961393223402043e-05,
      "loss": 1.168,
      "step": 1243
    },
    {
      "epoch": 11.631889183613321,
      "grad_norm": 0.3490018844604492,
      "learning_rate": 1.9613098863692037e-05,
      "loss": 1.1317,
      "step": 1244
    },
    {
      "epoch": 11.641320365458297,
      "grad_norm": 0.3404841721057892,
      "learning_rate": 1.9612264612614706e-05,
      "loss": 1.1438,
      "step": 1245
    },
    {
      "epoch": 11.65075154730327,
      "grad_norm": 0.32349589467048645,
      "learning_rate": 1.9611429480864876e-05,
      "loss": 1.123,
      "step": 1246
    },
    {
      "epoch": 11.660182729148246,
      "grad_norm": 0.3672853708267212,
      "learning_rate": 1.9610593468519055e-05,
      "loss": 1.0663,
      "step": 1247
    },
    {
      "epoch": 11.66961391099322,
      "grad_norm": 0.41167572140693665,
      "learning_rate": 1.9609756575653846e-05,
      "loss": 1.1626,
      "step": 1248
    },
    {
      "epoch": 11.679045092838196,
      "grad_norm": 0.34717637300491333,
      "learning_rate": 1.960891880234592e-05,
      "loss": 1.0761,
      "step": 1249
    },
    {
      "epoch": 11.688476274683172,
      "grad_norm": 0.3531663119792938,
      "learning_rate": 1.9608080148672034e-05,
      "loss": 1.0984,
      "step": 1250
    },
    {
      "epoch": 11.697907456528146,
      "grad_norm": 0.3780469596385956,
      "learning_rate": 1.9607240614709025e-05,
      "loss": 1.1214,
      "step": 1251
    },
    {
      "epoch": 11.707338638373121,
      "grad_norm": 0.33700913190841675,
      "learning_rate": 1.960640020053381e-05,
      "loss": 1.135,
      "step": 1252
    },
    {
      "epoch": 11.716769820218095,
      "grad_norm": 0.36966049671173096,
      "learning_rate": 1.9605558906223392e-05,
      "loss": 1.1103,
      "step": 1253
    },
    {
      "epoch": 11.726201002063071,
      "grad_norm": 0.35400381684303284,
      "learning_rate": 1.9604716731854846e-05,
      "loss": 1.0918,
      "step": 1254
    },
    {
      "epoch": 11.735632183908045,
      "grad_norm": 0.3589797019958496,
      "learning_rate": 1.960387367750533e-05,
      "loss": 1.1109,
      "step": 1255
    },
    {
      "epoch": 11.74506336575302,
      "grad_norm": 0.36683496832847595,
      "learning_rate": 1.960302974325209e-05,
      "loss": 1.0943,
      "step": 1256
    },
    {
      "epoch": 11.754494547597997,
      "grad_norm": 0.3487492501735687,
      "learning_rate": 1.9602184929172446e-05,
      "loss": 1.071,
      "step": 1257
    },
    {
      "epoch": 11.76392572944297,
      "grad_norm": 0.3704026937484741,
      "learning_rate": 1.9601339235343796e-05,
      "loss": 1.1313,
      "step": 1258
    },
    {
      "epoch": 11.773356911287946,
      "grad_norm": 0.3529439866542816,
      "learning_rate": 1.960049266184362e-05,
      "loss": 1.098,
      "step": 1259
    },
    {
      "epoch": 11.78278809313292,
      "grad_norm": 0.3483370244503021,
      "learning_rate": 1.959964520874949e-05,
      "loss": 1.1226,
      "step": 1260
    },
    {
      "epoch": 11.792219274977896,
      "grad_norm": 0.3522011935710907,
      "learning_rate": 1.959879687613904e-05,
      "loss": 1.1381,
      "step": 1261
    },
    {
      "epoch": 11.80165045682287,
      "grad_norm": 0.3864879608154297,
      "learning_rate": 1.959794766409e-05,
      "loss": 1.1332,
      "step": 1262
    },
    {
      "epoch": 11.811081638667845,
      "grad_norm": 0.39322373270988464,
      "learning_rate": 1.9597097572680176e-05,
      "loss": 1.1536,
      "step": 1263
    },
    {
      "epoch": 11.820512820512821,
      "grad_norm": 0.32169109582901,
      "learning_rate": 1.9596246601987444e-05,
      "loss": 1.1123,
      "step": 1264
    },
    {
      "epoch": 11.829944002357795,
      "grad_norm": 0.3514584004878998,
      "learning_rate": 1.9595394752089775e-05,
      "loss": 1.1336,
      "step": 1265
    },
    {
      "epoch": 11.83937518420277,
      "grad_norm": 0.34154975414276123,
      "learning_rate": 1.959454202306522e-05,
      "loss": 1.1355,
      "step": 1266
    },
    {
      "epoch": 11.848806366047745,
      "grad_norm": 0.3451805114746094,
      "learning_rate": 1.9593688414991896e-05,
      "loss": 1.1235,
      "step": 1267
    },
    {
      "epoch": 11.85823754789272,
      "grad_norm": 0.40009355545043945,
      "learning_rate": 1.959283392794802e-05,
      "loss": 1.136,
      "step": 1268
    },
    {
      "epoch": 11.867668729737694,
      "grad_norm": 0.3743019700050354,
      "learning_rate": 1.9591978562011874e-05,
      "loss": 1.105,
      "step": 1269
    },
    {
      "epoch": 11.87709991158267,
      "grad_norm": 0.34589144587516785,
      "learning_rate": 1.9591122317261825e-05,
      "loss": 1.1164,
      "step": 1270
    },
    {
      "epoch": 11.886531093427646,
      "grad_norm": 0.36051905155181885,
      "learning_rate": 1.959026519377633e-05,
      "loss": 1.1077,
      "step": 1271
    },
    {
      "epoch": 11.89596227527262,
      "grad_norm": 0.38235563039779663,
      "learning_rate": 1.9589407191633908e-05,
      "loss": 1.1351,
      "step": 1272
    },
    {
      "epoch": 11.905393457117595,
      "grad_norm": 0.35961517691612244,
      "learning_rate": 1.9588548310913173e-05,
      "loss": 1.0904,
      "step": 1273
    },
    {
      "epoch": 11.91482463896257,
      "grad_norm": 0.4152693450450897,
      "learning_rate": 1.958768855169282e-05,
      "loss": 1.0841,
      "step": 1274
    },
    {
      "epoch": 11.924255820807545,
      "grad_norm": 0.38303229212760925,
      "learning_rate": 1.9586827914051614e-05,
      "loss": 1.093,
      "step": 1275
    },
    {
      "epoch": 11.933687002652519,
      "grad_norm": 0.39092230796813965,
      "learning_rate": 1.958596639806841e-05,
      "loss": 1.1147,
      "step": 1276
    },
    {
      "epoch": 11.943118184497495,
      "grad_norm": 0.34519192576408386,
      "learning_rate": 1.9585104003822135e-05,
      "loss": 1.1593,
      "step": 1277
    },
    {
      "epoch": 11.95254936634247,
      "grad_norm": 0.3244064748287201,
      "learning_rate": 1.9584240731391805e-05,
      "loss": 1.1241,
      "step": 1278
    },
    {
      "epoch": 11.961980548187444,
      "grad_norm": 0.3836977481842041,
      "learning_rate": 1.9583376580856514e-05,
      "loss": 1.162,
      "step": 1279
    },
    {
      "epoch": 11.97141173003242,
      "grad_norm": 0.35891082882881165,
      "learning_rate": 1.9582511552295434e-05,
      "loss": 1.1084,
      "step": 1280
    },
    {
      "epoch": 11.980842911877394,
      "grad_norm": 0.3906620740890503,
      "learning_rate": 1.9581645645787818e-05,
      "loss": 1.1149,
      "step": 1281
    },
    {
      "epoch": 11.99027409372237,
      "grad_norm": 0.3618229031562805,
      "learning_rate": 1.9580778861412996e-05,
      "loss": 1.0997,
      "step": 1282
    },
    {
      "epoch": 11.999705275567344,
      "grad_norm": 0.3449534475803375,
      "learning_rate": 1.9579911199250388e-05,
      "loss": 1.1056,
      "step": 1283
    },
    {
      "epoch": 12.0,
      "grad_norm": 1.6370986700057983,
      "learning_rate": 1.9579042659379488e-05,
      "loss": 0.811,
      "step": 1284
    },
    {
      "epoch": 12.009431181844976,
      "grad_norm": 0.35149991512298584,
      "learning_rate": 1.957817324187987e-05,
      "loss": 1.147,
      "step": 1285
    },
    {
      "epoch": 12.01886236368995,
      "grad_norm": 0.3333313465118408,
      "learning_rate": 1.9577302946831194e-05,
      "loss": 1.1206,
      "step": 1286
    },
    {
      "epoch": 12.028293545534925,
      "grad_norm": 0.3411003649234772,
      "learning_rate": 1.9576431774313193e-05,
      "loss": 1.1216,
      "step": 1287
    },
    {
      "epoch": 12.0377247273799,
      "grad_norm": 0.3381842076778412,
      "learning_rate": 1.957555972440568e-05,
      "loss": 1.0911,
      "step": 1288
    },
    {
      "epoch": 12.047155909224875,
      "grad_norm": 0.3509841561317444,
      "learning_rate": 1.9574686797188554e-05,
      "loss": 1.1536,
      "step": 1289
    },
    {
      "epoch": 12.056587091069849,
      "grad_norm": 0.3645651042461395,
      "learning_rate": 1.9573812992741796e-05,
      "loss": 1.1189,
      "step": 1290
    },
    {
      "epoch": 12.066018272914825,
      "grad_norm": 0.3840332329273224,
      "learning_rate": 1.957293831114546e-05,
      "loss": 1.1042,
      "step": 1291
    },
    {
      "epoch": 12.0754494547598,
      "grad_norm": 0.33427342772483826,
      "learning_rate": 1.9572062752479684e-05,
      "loss": 1.1357,
      "step": 1292
    },
    {
      "epoch": 12.084880636604774,
      "grad_norm": 0.3378201723098755,
      "learning_rate": 1.957118631682469e-05,
      "loss": 1.1421,
      "step": 1293
    },
    {
      "epoch": 12.09431181844975,
      "grad_norm": 0.37995365262031555,
      "learning_rate": 1.957030900426077e-05,
      "loss": 1.1417,
      "step": 1294
    },
    {
      "epoch": 12.103743000294724,
      "grad_norm": 0.3383643627166748,
      "learning_rate": 1.9569430814868312e-05,
      "loss": 1.1096,
      "step": 1295
    },
    {
      "epoch": 12.1131741821397,
      "grad_norm": 0.34418758749961853,
      "learning_rate": 1.9568551748727767e-05,
      "loss": 1.1155,
      "step": 1296
    },
    {
      "epoch": 12.122605363984674,
      "grad_norm": 0.3567563593387604,
      "learning_rate": 1.9567671805919682e-05,
      "loss": 1.166,
      "step": 1297
    },
    {
      "epoch": 12.13203654582965,
      "grad_norm": 0.38670629262924194,
      "learning_rate": 1.9566790986524674e-05,
      "loss": 1.1045,
      "step": 1298
    },
    {
      "epoch": 12.141467727674625,
      "grad_norm": 0.3499082922935486,
      "learning_rate": 1.9565909290623438e-05,
      "loss": 1.0862,
      "step": 1299
    },
    {
      "epoch": 12.150898909519599,
      "grad_norm": 0.3603803515434265,
      "learning_rate": 1.9565026718296764e-05,
      "loss": 1.088,
      "step": 1300
    },
    {
      "epoch": 12.160330091364575,
      "grad_norm": 0.34434404969215393,
      "learning_rate": 1.9564143269625507e-05,
      "loss": 1.0891,
      "step": 1301
    },
    {
      "epoch": 12.169761273209549,
      "grad_norm": 0.3502150774002075,
      "learning_rate": 1.9563258944690613e-05,
      "loss": 1.12,
      "step": 1302
    },
    {
      "epoch": 12.179192455054524,
      "grad_norm": 0.3257521986961365,
      "learning_rate": 1.9562373743573094e-05,
      "loss": 1.1145,
      "step": 1303
    },
    {
      "epoch": 12.188623636899498,
      "grad_norm": 0.33531272411346436,
      "learning_rate": 1.9561487666354067e-05,
      "loss": 1.1309,
      "step": 1304
    },
    {
      "epoch": 12.198054818744474,
      "grad_norm": 0.33458149433135986,
      "learning_rate": 1.9560600713114697e-05,
      "loss": 1.0868,
      "step": 1305
    },
    {
      "epoch": 12.20748600058945,
      "grad_norm": 0.39071640372276306,
      "learning_rate": 1.955971288393626e-05,
      "loss": 1.1195,
      "step": 1306
    },
    {
      "epoch": 12.216917182434424,
      "grad_norm": 0.36544501781463623,
      "learning_rate": 1.955882417890009e-05,
      "loss": 1.0706,
      "step": 1307
    },
    {
      "epoch": 12.2263483642794,
      "grad_norm": 0.35698992013931274,
      "learning_rate": 1.9557934598087617e-05,
      "loss": 1.1363,
      "step": 1308
    },
    {
      "epoch": 12.235779546124373,
      "grad_norm": 0.3465011417865753,
      "learning_rate": 1.9557044141580337e-05,
      "loss": 1.0924,
      "step": 1309
    },
    {
      "epoch": 12.245210727969349,
      "grad_norm": 0.36331984400749207,
      "learning_rate": 1.955615280945984e-05,
      "loss": 1.1342,
      "step": 1310
    },
    {
      "epoch": 12.254641909814323,
      "grad_norm": 0.363540917634964,
      "learning_rate": 1.955526060180778e-05,
      "loss": 1.101,
      "step": 1311
    },
    {
      "epoch": 12.264073091659299,
      "grad_norm": 0.37589606642723083,
      "learning_rate": 1.9554367518705914e-05,
      "loss": 1.1007,
      "step": 1312
    },
    {
      "epoch": 12.273504273504274,
      "grad_norm": 0.35350802540779114,
      "learning_rate": 1.9553473560236056e-05,
      "loss": 1.1018,
      "step": 1313
    },
    {
      "epoch": 12.282935455349248,
      "grad_norm": 0.36852747201919556,
      "learning_rate": 1.9552578726480114e-05,
      "loss": 1.1529,
      "step": 1314
    },
    {
      "epoch": 12.292366637194224,
      "grad_norm": 0.3348575234413147,
      "learning_rate": 1.955168301752007e-05,
      "loss": 1.1334,
      "step": 1315
    },
    {
      "epoch": 12.301797819039198,
      "grad_norm": 0.34904277324676514,
      "learning_rate": 1.9550786433437992e-05,
      "loss": 1.1059,
      "step": 1316
    },
    {
      "epoch": 12.311229000884174,
      "grad_norm": 0.40083369612693787,
      "learning_rate": 1.9549888974316024e-05,
      "loss": 1.1077,
      "step": 1317
    },
    {
      "epoch": 12.320660182729148,
      "grad_norm": 0.35534706711769104,
      "learning_rate": 1.9548990640236383e-05,
      "loss": 1.1509,
      "step": 1318
    },
    {
      "epoch": 12.330091364574123,
      "grad_norm": 0.34918212890625,
      "learning_rate": 1.954809143128139e-05,
      "loss": 1.112,
      "step": 1319
    },
    {
      "epoch": 12.339522546419099,
      "grad_norm": 0.3373749852180481,
      "learning_rate": 1.9547191347533414e-05,
      "loss": 1.1247,
      "step": 1320
    },
    {
      "epoch": 12.348953728264073,
      "grad_norm": 0.35321134328842163,
      "learning_rate": 1.954629038907493e-05,
      "loss": 1.1059,
      "step": 1321
    },
    {
      "epoch": 12.358384910109049,
      "grad_norm": 0.33455830812454224,
      "learning_rate": 1.954538855598848e-05,
      "loss": 1.1625,
      "step": 1322
    },
    {
      "epoch": 12.367816091954023,
      "grad_norm": 0.37916669249534607,
      "learning_rate": 1.954448584835669e-05,
      "loss": 1.0715,
      "step": 1323
    },
    {
      "epoch": 12.377247273798998,
      "grad_norm": 0.3409891128540039,
      "learning_rate": 1.954358226626227e-05,
      "loss": 1.1169,
      "step": 1324
    },
    {
      "epoch": 12.386678455643972,
      "grad_norm": 0.36701154708862305,
      "learning_rate": 1.9542677809787996e-05,
      "loss": 1.1381,
      "step": 1325
    },
    {
      "epoch": 12.396109637488948,
      "grad_norm": 0.3677269518375397,
      "learning_rate": 1.954177247901675e-05,
      "loss": 1.1221,
      "step": 1326
    },
    {
      "epoch": 12.405540819333924,
      "grad_norm": 0.3520985543727875,
      "learning_rate": 1.9540866274031457e-05,
      "loss": 1.1088,
      "step": 1327
    },
    {
      "epoch": 12.414972001178898,
      "grad_norm": 0.33595195412635803,
      "learning_rate": 1.9539959194915163e-05,
      "loss": 1.1086,
      "step": 1328
    },
    {
      "epoch": 12.424403183023873,
      "grad_norm": 0.33077412843704224,
      "learning_rate": 1.9539051241750965e-05,
      "loss": 1.0959,
      "step": 1329
    },
    {
      "epoch": 12.433834364868847,
      "grad_norm": 0.36492714285850525,
      "learning_rate": 1.9538142414622045e-05,
      "loss": 1.145,
      "step": 1330
    },
    {
      "epoch": 12.443265546713823,
      "grad_norm": 0.320036381483078,
      "learning_rate": 1.953723271361168e-05,
      "loss": 1.1171,
      "step": 1331
    },
    {
      "epoch": 12.452696728558797,
      "grad_norm": 0.4091443717479706,
      "learning_rate": 1.9536322138803206e-05,
      "loss": 1.111,
      "step": 1332
    },
    {
      "epoch": 12.462127910403773,
      "grad_norm": 0.3567791283130646,
      "learning_rate": 1.9535410690280058e-05,
      "loss": 1.083,
      "step": 1333
    },
    {
      "epoch": 12.471559092248748,
      "grad_norm": 0.35502198338508606,
      "learning_rate": 1.9534498368125742e-05,
      "loss": 1.0473,
      "step": 1334
    },
    {
      "epoch": 12.480990274093722,
      "grad_norm": 0.3416888415813446,
      "learning_rate": 1.9533585172423834e-05,
      "loss": 1.0975,
      "step": 1335
    },
    {
      "epoch": 12.490421455938698,
      "grad_norm": 0.34922271966934204,
      "learning_rate": 1.9532671103258013e-05,
      "loss": 1.1426,
      "step": 1336
    },
    {
      "epoch": 12.499852637783672,
      "grad_norm": 0.33529335260391235,
      "learning_rate": 1.9531756160712024e-05,
      "loss": 1.1035,
      "step": 1337
    },
    {
      "epoch": 12.509283819628648,
      "grad_norm": 0.392479807138443,
      "learning_rate": 1.953084034486969e-05,
      "loss": 1.1138,
      "step": 1338
    },
    {
      "epoch": 12.518715001473621,
      "grad_norm": 0.37244582176208496,
      "learning_rate": 1.9529923655814918e-05,
      "loss": 1.1215,
      "step": 1339
    },
    {
      "epoch": 12.528146183318597,
      "grad_norm": 0.33935993909835815,
      "learning_rate": 1.9529006093631695e-05,
      "loss": 1.1655,
      "step": 1340
    },
    {
      "epoch": 12.537577365163571,
      "grad_norm": 0.33208397030830383,
      "learning_rate": 1.9528087658404088e-05,
      "loss": 1.0971,
      "step": 1341
    },
    {
      "epoch": 12.547008547008547,
      "grad_norm": 0.37345367670059204,
      "learning_rate": 1.9527168350216246e-05,
      "loss": 1.1625,
      "step": 1342
    },
    {
      "epoch": 12.556439728853523,
      "grad_norm": 0.3442283570766449,
      "learning_rate": 1.9526248169152392e-05,
      "loss": 1.1378,
      "step": 1343
    },
    {
      "epoch": 12.565870910698496,
      "grad_norm": 0.3839608132839203,
      "learning_rate": 1.9525327115296835e-05,
      "loss": 1.1193,
      "step": 1344
    },
    {
      "epoch": 12.575302092543472,
      "grad_norm": 0.3773917257785797,
      "learning_rate": 1.9524405188733963e-05,
      "loss": 1.159,
      "step": 1345
    },
    {
      "epoch": 12.584733274388446,
      "grad_norm": 0.3750702738761902,
      "learning_rate": 1.9523482389548238e-05,
      "loss": 1.1074,
      "step": 1346
    },
    {
      "epoch": 12.594164456233422,
      "grad_norm": 0.31979233026504517,
      "learning_rate": 1.952255871782421e-05,
      "loss": 1.1323,
      "step": 1347
    },
    {
      "epoch": 12.603595638078396,
      "grad_norm": 0.3329947590827942,
      "learning_rate": 1.9521634173646504e-05,
      "loss": 1.1559,
      "step": 1348
    },
    {
      "epoch": 12.613026819923371,
      "grad_norm": 0.35906949639320374,
      "learning_rate": 1.952070875709983e-05,
      "loss": 1.1108,
      "step": 1349
    },
    {
      "epoch": 12.622458001768347,
      "grad_norm": 0.4065130650997162,
      "learning_rate": 1.9519782468268968e-05,
      "loss": 1.0979,
      "step": 1350
    },
    {
      "epoch": 12.631889183613321,
      "grad_norm": 0.35444048047065735,
      "learning_rate": 1.951885530723879e-05,
      "loss": 1.1538,
      "step": 1351
    },
    {
      "epoch": 12.641320365458297,
      "grad_norm": 0.35060685873031616,
      "learning_rate": 1.951792727409424e-05,
      "loss": 1.1148,
      "step": 1352
    },
    {
      "epoch": 12.65075154730327,
      "grad_norm": 0.33534374833106995,
      "learning_rate": 1.9516998368920342e-05,
      "loss": 1.1316,
      "step": 1353
    },
    {
      "epoch": 12.660182729148246,
      "grad_norm": 0.3423806428909302,
      "learning_rate": 1.9516068591802208e-05,
      "loss": 1.094,
      "step": 1354
    },
    {
      "epoch": 12.66961391099322,
      "grad_norm": 0.37049487233161926,
      "learning_rate": 1.9515137942825018e-05,
      "loss": 1.1275,
      "step": 1355
    },
    {
      "epoch": 12.679045092838196,
      "grad_norm": 0.3666110336780548,
      "learning_rate": 1.951420642207404e-05,
      "loss": 1.1142,
      "step": 1356
    },
    {
      "epoch": 12.688476274683172,
      "grad_norm": 0.340651273727417,
      "learning_rate": 1.951327402963462e-05,
      "loss": 1.1465,
      "step": 1357
    },
    {
      "epoch": 12.697907456528146,
      "grad_norm": 0.3659946322441101,
      "learning_rate": 1.9512340765592182e-05,
      "loss": 1.1718,
      "step": 1358
    },
    {
      "epoch": 12.707338638373121,
      "grad_norm": 0.3449133336544037,
      "learning_rate": 1.951140663003223e-05,
      "loss": 1.1324,
      "step": 1359
    },
    {
      "epoch": 12.716769820218095,
      "grad_norm": 0.37318500876426697,
      "learning_rate": 1.9510471623040353e-05,
      "loss": 1.1231,
      "step": 1360
    },
    {
      "epoch": 12.726201002063071,
      "grad_norm": 0.3655708432197571,
      "learning_rate": 1.9509535744702215e-05,
      "loss": 1.0859,
      "step": 1361
    },
    {
      "epoch": 12.735632183908045,
      "grad_norm": 0.3458182215690613,
      "learning_rate": 1.950859899510356e-05,
      "loss": 1.1036,
      "step": 1362
    },
    {
      "epoch": 12.74506336575302,
      "grad_norm": 0.3664512634277344,
      "learning_rate": 1.9507661374330212e-05,
      "loss": 1.0988,
      "step": 1363
    },
    {
      "epoch": 12.754494547597997,
      "grad_norm": 0.3451266586780548,
      "learning_rate": 1.950672288246808e-05,
      "loss": 1.0952,
      "step": 1364
    },
    {
      "epoch": 12.76392572944297,
      "grad_norm": 0.32681500911712646,
      "learning_rate": 1.950578351960314e-05,
      "loss": 1.1044,
      "step": 1365
    },
    {
      "epoch": 12.773356911287946,
      "grad_norm": 0.38990193605422974,
      "learning_rate": 1.9504843285821464e-05,
      "loss": 1.1148,
      "step": 1366
    },
    {
      "epoch": 12.78278809313292,
      "grad_norm": 0.3776232600212097,
      "learning_rate": 1.9503902181209194e-05,
      "loss": 1.1234,
      "step": 1367
    },
    {
      "epoch": 12.792219274977896,
      "grad_norm": 0.3580773174762726,
      "learning_rate": 1.9502960205852547e-05,
      "loss": 1.0899,
      "step": 1368
    },
    {
      "epoch": 12.80165045682287,
      "grad_norm": 0.3590937554836273,
      "learning_rate": 1.9502017359837838e-05,
      "loss": 1.1542,
      "step": 1369
    },
    {
      "epoch": 12.811081638667845,
      "grad_norm": 0.3659215271472931,
      "learning_rate": 1.950107364325144e-05,
      "loss": 1.126,
      "step": 1370
    },
    {
      "epoch": 12.820512820512821,
      "grad_norm": 0.37782666087150574,
      "learning_rate": 1.9500129056179825e-05,
      "loss": 1.0719,
      "step": 1371
    },
    {
      "epoch": 12.829944002357795,
      "grad_norm": 0.40494614839553833,
      "learning_rate": 1.9499183598709526e-05,
      "loss": 1.0963,
      "step": 1372
    },
    {
      "epoch": 12.83937518420277,
      "grad_norm": 0.32791465520858765,
      "learning_rate": 1.949823727092717e-05,
      "loss": 1.1564,
      "step": 1373
    },
    {
      "epoch": 12.848806366047745,
      "grad_norm": 0.3710528612136841,
      "learning_rate": 1.9497290072919464e-05,
      "loss": 1.0827,
      "step": 1374
    },
    {
      "epoch": 12.85823754789272,
      "grad_norm": 0.34777578711509705,
      "learning_rate": 1.9496342004773186e-05,
      "loss": 1.1143,
      "step": 1375
    },
    {
      "epoch": 12.867668729737694,
      "grad_norm": 0.35241633653640747,
      "learning_rate": 1.949539306657519e-05,
      "loss": 1.0892,
      "step": 1376
    },
    {
      "epoch": 12.87709991158267,
      "grad_norm": 0.38031288981437683,
      "learning_rate": 1.949444325841243e-05,
      "loss": 1.1091,
      "step": 1377
    },
    {
      "epoch": 12.886531093427646,
      "grad_norm": 0.3323776423931122,
      "learning_rate": 1.9493492580371926e-05,
      "loss": 1.1544,
      "step": 1378
    },
    {
      "epoch": 12.89596227527262,
      "grad_norm": 0.36274269223213196,
      "learning_rate": 1.949254103254077e-05,
      "loss": 1.1166,
      "step": 1379
    },
    {
      "epoch": 12.905393457117595,
      "grad_norm": 0.3615570664405823,
      "learning_rate": 1.9491588615006146e-05,
      "loss": 1.1105,
      "step": 1380
    },
    {
      "epoch": 12.91482463896257,
      "grad_norm": 0.3646501302719116,
      "learning_rate": 1.9490635327855315e-05,
      "loss": 1.1167,
      "step": 1381
    },
    {
      "epoch": 12.924255820807545,
      "grad_norm": 0.3607137203216553,
      "learning_rate": 1.948968117117562e-05,
      "loss": 1.0818,
      "step": 1382
    },
    {
      "epoch": 12.933687002652519,
      "grad_norm": 0.37604475021362305,
      "learning_rate": 1.9488726145054477e-05,
      "loss": 1.1263,
      "step": 1383
    },
    {
      "epoch": 12.943118184497495,
      "grad_norm": 0.3585360646247864,
      "learning_rate": 1.9487770249579387e-05,
      "loss": 1.1157,
      "step": 1384
    },
    {
      "epoch": 12.95254936634247,
      "grad_norm": 0.32835373282432556,
      "learning_rate": 1.948681348483793e-05,
      "loss": 1.0845,
      "step": 1385
    },
    {
      "epoch": 12.961980548187444,
      "grad_norm": 0.3938957750797272,
      "learning_rate": 1.9485855850917755e-05,
      "loss": 1.1567,
      "step": 1386
    },
    {
      "epoch": 12.97141173003242,
      "grad_norm": 0.3630099296569824,
      "learning_rate": 1.9484897347906612e-05,
      "loss": 1.057,
      "step": 1387
    },
    {
      "epoch": 12.980842911877394,
      "grad_norm": 0.3667352497577667,
      "learning_rate": 1.948393797589231e-05,
      "loss": 1.1136,
      "step": 1388
    },
    {
      "epoch": 12.99027409372237,
      "grad_norm": 0.3691384494304657,
      "learning_rate": 1.9482977734962753e-05,
      "loss": 1.139,
      "step": 1389
    },
    {
      "epoch": 12.999705275567344,
      "grad_norm": 0.3409498333930969,
      "learning_rate": 1.9482016625205916e-05,
      "loss": 1.0806,
      "step": 1390
    },
    {
      "epoch": 13.0,
      "grad_norm": 2.376415491104126,
      "learning_rate": 1.9481054646709855e-05,
      "loss": 0.5294,
      "step": 1391
    },
    {
      "epoch": 13.009431181844976,
      "grad_norm": 0.38661280274391174,
      "learning_rate": 1.9480091799562706e-05,
      "loss": 1.0987,
      "step": 1392
    },
    {
      "epoch": 13.01886236368995,
      "grad_norm": 0.3505675196647644,
      "learning_rate": 1.9479128083852682e-05,
      "loss": 1.1049,
      "step": 1393
    },
    {
      "epoch": 13.028293545534925,
      "grad_norm": 0.3357957899570465,
      "learning_rate": 1.9478163499668084e-05,
      "loss": 1.1294,
      "step": 1394
    },
    {
      "epoch": 13.0377247273799,
      "grad_norm": 0.36973389983177185,
      "learning_rate": 1.9477198047097287e-05,
      "loss": 1.1106,
      "step": 1395
    },
    {
      "epoch": 13.047155909224875,
      "grad_norm": 0.32697492837905884,
      "learning_rate": 1.9476231726228735e-05,
      "loss": 1.1351,
      "step": 1396
    },
    {
      "epoch": 13.056587091069849,
      "grad_norm": 0.3292542099952698,
      "learning_rate": 1.9475264537150976e-05,
      "loss": 1.0727,
      "step": 1397
    },
    {
      "epoch": 13.066018272914825,
      "grad_norm": 0.35328155755996704,
      "learning_rate": 1.9474296479952612e-05,
      "loss": 1.1182,
      "step": 1398
    },
    {
      "epoch": 13.0754494547598,
      "grad_norm": 0.3727162182331085,
      "learning_rate": 1.9473327554722344e-05,
      "loss": 1.1219,
      "step": 1399
    },
    {
      "epoch": 13.084880636604774,
      "grad_norm": 0.3503969609737396,
      "learning_rate": 1.947235776154894e-05,
      "loss": 1.137,
      "step": 1400
    },
    {
      "epoch": 13.09431181844975,
      "grad_norm": 0.3609474301338196,
      "learning_rate": 1.9471387100521255e-05,
      "loss": 1.1733,
      "step": 1401
    },
    {
      "epoch": 13.103743000294724,
      "grad_norm": 0.3726101517677307,
      "learning_rate": 1.9470415571728222e-05,
      "loss": 1.1279,
      "step": 1402
    },
    {
      "epoch": 13.1131741821397,
      "grad_norm": 0.3548479974269867,
      "learning_rate": 1.9469443175258846e-05,
      "loss": 1.1461,
      "step": 1403
    },
    {
      "epoch": 13.122605363984674,
      "grad_norm": 0.3648587465286255,
      "learning_rate": 1.9468469911202222e-05,
      "loss": 1.1126,
      "step": 1404
    },
    {
      "epoch": 13.13203654582965,
      "grad_norm": 0.3984470069408417,
      "learning_rate": 1.9467495779647518e-05,
      "loss": 1.1648,
      "step": 1405
    },
    {
      "epoch": 13.141467727674625,
      "grad_norm": 0.37446433305740356,
      "learning_rate": 1.9466520780683988e-05,
      "loss": 1.1069,
      "step": 1406
    },
    {
      "epoch": 13.150898909519599,
      "grad_norm": 0.35477539896965027,
      "learning_rate": 1.9465544914400953e-05,
      "loss": 1.1322,
      "step": 1407
    },
    {
      "epoch": 13.160330091364575,
      "grad_norm": 0.36796483397483826,
      "learning_rate": 1.9464568180887832e-05,
      "loss": 1.1311,
      "step": 1408
    },
    {
      "epoch": 13.169761273209549,
      "grad_norm": 0.3581152558326721,
      "learning_rate": 1.9463590580234103e-05,
      "loss": 1.081,
      "step": 1409
    },
    {
      "epoch": 13.179192455054524,
      "grad_norm": 0.35327115654945374,
      "learning_rate": 1.9462612112529342e-05,
      "loss": 1.1141,
      "step": 1410
    },
    {
      "epoch": 13.188623636899498,
      "grad_norm": 0.36066243052482605,
      "learning_rate": 1.9461632777863187e-05,
      "loss": 1.13,
      "step": 1411
    },
    {
      "epoch": 13.198054818744474,
      "grad_norm": 0.38627979159355164,
      "learning_rate": 1.9460652576325375e-05,
      "loss": 1.1257,
      "step": 1412
    },
    {
      "epoch": 13.20748600058945,
      "grad_norm": 0.369072288274765,
      "learning_rate": 1.9459671508005703e-05,
      "loss": 1.1176,
      "step": 1413
    },
    {
      "epoch": 13.216917182434424,
      "grad_norm": 0.3740125298500061,
      "learning_rate": 1.9458689572994062e-05,
      "loss": 1.1063,
      "step": 1414
    },
    {
      "epoch": 13.2263483642794,
      "grad_norm": 0.34421664476394653,
      "learning_rate": 1.9457706771380408e-05,
      "loss": 1.1734,
      "step": 1415
    },
    {
      "epoch": 13.235779546124373,
      "grad_norm": 0.346334844827652,
      "learning_rate": 1.9456723103254796e-05,
      "loss": 1.1241,
      "step": 1416
    },
    {
      "epoch": 13.245210727969349,
      "grad_norm": 0.3613956570625305,
      "learning_rate": 1.945573856870734e-05,
      "loss": 1.1103,
      "step": 1417
    },
    {
      "epoch": 13.254641909814323,
      "grad_norm": 0.3488287627696991,
      "learning_rate": 1.9454753167828252e-05,
      "loss": 1.126,
      "step": 1418
    },
    {
      "epoch": 13.264073091659299,
      "grad_norm": 0.37981849908828735,
      "learning_rate": 1.9453766900707806e-05,
      "loss": 1.1405,
      "step": 1419
    },
    {
      "epoch": 13.273504273504274,
      "grad_norm": 0.3913114368915558,
      "learning_rate": 1.9452779767436364e-05,
      "loss": 1.1429,
      "step": 1420
    },
    {
      "epoch": 13.282935455349248,
      "grad_norm": 0.35687994956970215,
      "learning_rate": 1.9451791768104372e-05,
      "loss": 1.1497,
      "step": 1421
    },
    {
      "epoch": 13.292366637194224,
      "grad_norm": 0.3391557037830353,
      "learning_rate": 1.9450802902802348e-05,
      "loss": 1.1259,
      "step": 1422
    },
    {
      "epoch": 13.301797819039198,
      "grad_norm": 0.37908798456192017,
      "learning_rate": 1.9449813171620886e-05,
      "loss": 1.1226,
      "step": 1423
    },
    {
      "epoch": 13.311229000884174,
      "grad_norm": 0.3801570236682892,
      "learning_rate": 1.9448822574650675e-05,
      "loss": 1.0838,
      "step": 1424
    },
    {
      "epoch": 13.320660182729148,
      "grad_norm": 0.35979682207107544,
      "learning_rate": 1.9447831111982464e-05,
      "loss": 1.1323,
      "step": 1425
    },
    {
      "epoch": 13.330091364574123,
      "grad_norm": 0.3531459867954254,
      "learning_rate": 1.9446838783707096e-05,
      "loss": 1.1128,
      "step": 1426
    },
    {
      "epoch": 13.339522546419099,
      "grad_norm": 0.3728228509426117,
      "learning_rate": 1.9445845589915485e-05,
      "loss": 1.0897,
      "step": 1427
    },
    {
      "epoch": 13.348953728264073,
      "grad_norm": 0.35282042622566223,
      "learning_rate": 1.944485153069863e-05,
      "loss": 1.1356,
      "step": 1428
    },
    {
      "epoch": 13.358384910109049,
      "grad_norm": 0.382694274187088,
      "learning_rate": 1.94438566061476e-05,
      "loss": 1.1337,
      "step": 1429
    },
    {
      "epoch": 13.367816091954023,
      "grad_norm": 0.3601239025592804,
      "learning_rate": 1.944286081635356e-05,
      "loss": 1.099,
      "step": 1430
    },
    {
      "epoch": 13.377247273798998,
      "grad_norm": 0.3541124761104584,
      "learning_rate": 1.9441864161407738e-05,
      "loss": 1.0822,
      "step": 1431
    },
    {
      "epoch": 13.386678455643972,
      "grad_norm": 0.35351940989494324,
      "learning_rate": 1.9440866641401445e-05,
      "loss": 1.0892,
      "step": 1432
    },
    {
      "epoch": 13.396109637488948,
      "grad_norm": 0.3530522584915161,
      "learning_rate": 1.9439868256426075e-05,
      "loss": 1.1119,
      "step": 1433
    },
    {
      "epoch": 13.405540819333924,
      "grad_norm": 0.3970246911048889,
      "learning_rate": 1.94388690065731e-05,
      "loss": 1.1263,
      "step": 1434
    },
    {
      "epoch": 13.414972001178898,
      "grad_norm": 0.37219882011413574,
      "learning_rate": 1.9437868891934074e-05,
      "loss": 1.0864,
      "step": 1435
    },
    {
      "epoch": 13.424403183023873,
      "grad_norm": 0.3821824789047241,
      "learning_rate": 1.9436867912600622e-05,
      "loss": 1.1047,
      "step": 1436
    },
    {
      "epoch": 13.433834364868847,
      "grad_norm": 0.37335291504859924,
      "learning_rate": 1.943586606866446e-05,
      "loss": 1.0976,
      "step": 1437
    },
    {
      "epoch": 13.443265546713823,
      "grad_norm": 0.36660274863243103,
      "learning_rate": 1.9434863360217367e-05,
      "loss": 1.1337,
      "step": 1438
    },
    {
      "epoch": 13.452696728558797,
      "grad_norm": 0.35275140404701233,
      "learning_rate": 1.943385978735122e-05,
      "loss": 1.129,
      "step": 1439
    },
    {
      "epoch": 13.462127910403773,
      "grad_norm": 0.37188196182250977,
      "learning_rate": 1.943285535015796e-05,
      "loss": 1.1296,
      "step": 1440
    },
    {
      "epoch": 13.471559092248748,
      "grad_norm": 0.3548787832260132,
      "learning_rate": 1.943185004872962e-05,
      "loss": 1.1549,
      "step": 1441
    },
    {
      "epoch": 13.480990274093722,
      "grad_norm": 0.38789981603622437,
      "learning_rate": 1.94308438831583e-05,
      "loss": 1.0968,
      "step": 1442
    },
    {
      "epoch": 13.490421455938698,
      "grad_norm": 0.36786985397338867,
      "learning_rate": 1.9429836853536185e-05,
      "loss": 1.0937,
      "step": 1443
    },
    {
      "epoch": 13.499852637783672,
      "grad_norm": 0.38389474153518677,
      "learning_rate": 1.9428828959955537e-05,
      "loss": 1.1146,
      "step": 1444
    },
    {
      "epoch": 13.509283819628648,
      "grad_norm": 0.325699120759964,
      "learning_rate": 1.9427820202508703e-05,
      "loss": 1.124,
      "step": 1445
    },
    {
      "epoch": 13.518715001473621,
      "grad_norm": 0.3628537654876709,
      "learning_rate": 1.9426810581288102e-05,
      "loss": 1.1031,
      "step": 1446
    },
    {
      "epoch": 13.528146183318597,
      "grad_norm": 0.3845818042755127,
      "learning_rate": 1.9425800096386237e-05,
      "loss": 1.125,
      "step": 1447
    },
    {
      "epoch": 13.537577365163571,
      "grad_norm": 0.3670876920223236,
      "learning_rate": 1.9424788747895687e-05,
      "loss": 1.0972,
      "step": 1448
    },
    {
      "epoch": 13.547008547008547,
      "grad_norm": 0.37615081667900085,
      "learning_rate": 1.9423776535909112e-05,
      "loss": 1.1245,
      "step": 1449
    },
    {
      "epoch": 13.556439728853523,
      "grad_norm": 0.41003596782684326,
      "learning_rate": 1.9422763460519256e-05,
      "loss": 1.1383,
      "step": 1450
    },
    {
      "epoch": 13.565870910698496,
      "grad_norm": 0.36933621764183044,
      "learning_rate": 1.9421749521818924e-05,
      "loss": 1.0694,
      "step": 1451
    },
    {
      "epoch": 13.575302092543472,
      "grad_norm": 0.37573137879371643,
      "learning_rate": 1.9420734719901025e-05,
      "loss": 1.1022,
      "step": 1452
    },
    {
      "epoch": 13.584733274388446,
      "grad_norm": 0.3670099675655365,
      "learning_rate": 1.9419719054858524e-05,
      "loss": 1.1257,
      "step": 1453
    },
    {
      "epoch": 13.594164456233422,
      "grad_norm": 0.37779438495635986,
      "learning_rate": 1.9418702526784487e-05,
      "loss": 1.0836,
      "step": 1454
    },
    {
      "epoch": 13.603595638078396,
      "grad_norm": 0.3671523630619049,
      "learning_rate": 1.941768513577204e-05,
      "loss": 1.1037,
      "step": 1455
    },
    {
      "epoch": 13.613026819923371,
      "grad_norm": 0.3588135838508606,
      "learning_rate": 1.9416666881914398e-05,
      "loss": 1.1003,
      "step": 1456
    },
    {
      "epoch": 13.622458001768347,
      "grad_norm": 0.3818010985851288,
      "learning_rate": 1.9415647765304855e-05,
      "loss": 1.1275,
      "step": 1457
    },
    {
      "epoch": 13.631889183613321,
      "grad_norm": 0.3837841749191284,
      "learning_rate": 1.9414627786036783e-05,
      "loss": 1.116,
      "step": 1458
    },
    {
      "epoch": 13.641320365458297,
      "grad_norm": 0.4073801338672638,
      "learning_rate": 1.9413606944203628e-05,
      "loss": 1.1017,
      "step": 1459
    },
    {
      "epoch": 13.65075154730327,
      "grad_norm": 0.35240569710731506,
      "learning_rate": 1.941258523989892e-05,
      "loss": 1.1069,
      "step": 1460
    },
    {
      "epoch": 13.660182729148246,
      "grad_norm": 0.3557018041610718,
      "learning_rate": 1.9411562673216272e-05,
      "loss": 1.1425,
      "step": 1461
    },
    {
      "epoch": 13.66961391099322,
      "grad_norm": 0.35238122940063477,
      "learning_rate": 1.9410539244249364e-05,
      "loss": 1.126,
      "step": 1462
    },
    {
      "epoch": 13.679045092838196,
      "grad_norm": 0.3449111878871918,
      "learning_rate": 1.9409514953091965e-05,
      "loss": 1.1216,
      "step": 1463
    },
    {
      "epoch": 13.688476274683172,
      "grad_norm": 0.3739182949066162,
      "learning_rate": 1.9408489799837924e-05,
      "loss": 1.1041,
      "step": 1464
    },
    {
      "epoch": 13.697907456528146,
      "grad_norm": 0.3635525107383728,
      "learning_rate": 1.9407463784581163e-05,
      "loss": 1.1101,
      "step": 1465
    },
    {
      "epoch": 13.707338638373121,
      "grad_norm": 0.3712218403816223,
      "learning_rate": 1.9406436907415683e-05,
      "loss": 1.0809,
      "step": 1466
    },
    {
      "epoch": 13.716769820218095,
      "grad_norm": 0.3622626066207886,
      "learning_rate": 1.9405409168435568e-05,
      "loss": 1.1013,
      "step": 1467
    },
    {
      "epoch": 13.726201002063071,
      "grad_norm": 0.3443532884120941,
      "learning_rate": 1.940438056773498e-05,
      "loss": 1.0789,
      "step": 1468
    },
    {
      "epoch": 13.735632183908045,
      "grad_norm": 0.3752964437007904,
      "learning_rate": 1.9403351105408152e-05,
      "loss": 1.095,
      "step": 1469
    },
    {
      "epoch": 13.74506336575302,
      "grad_norm": 0.3928656280040741,
      "learning_rate": 1.9402320781549413e-05,
      "loss": 1.1177,
      "step": 1470
    },
    {
      "epoch": 13.754494547597997,
      "grad_norm": 0.4112391173839569,
      "learning_rate": 1.9401289596253156e-05,
      "loss": 1.0743,
      "step": 1471
    },
    {
      "epoch": 13.76392572944297,
      "grad_norm": 0.3885609805583954,
      "learning_rate": 1.940025754961386e-05,
      "loss": 1.0795,
      "step": 1472
    },
    {
      "epoch": 13.773356911287946,
      "grad_norm": 0.3529418110847473,
      "learning_rate": 1.9399224641726077e-05,
      "loss": 1.0887,
      "step": 1473
    },
    {
      "epoch": 13.78278809313292,
      "grad_norm": 0.38333505392074585,
      "learning_rate": 1.9398190872684445e-05,
      "loss": 1.1511,
      "step": 1474
    },
    {
      "epoch": 13.792219274977896,
      "grad_norm": 0.35104337334632874,
      "learning_rate": 1.9397156242583678e-05,
      "loss": 1.1036,
      "step": 1475
    },
    {
      "epoch": 13.80165045682287,
      "grad_norm": 0.3407571017742157,
      "learning_rate": 1.9396120751518568e-05,
      "loss": 1.0784,
      "step": 1476
    },
    {
      "epoch": 13.811081638667845,
      "grad_norm": 0.36426520347595215,
      "learning_rate": 1.9395084399583984e-05,
      "loss": 1.1083,
      "step": 1477
    },
    {
      "epoch": 13.820512820512821,
      "grad_norm": 0.3834168016910553,
      "learning_rate": 1.9394047186874875e-05,
      "loss": 1.1218,
      "step": 1478
    },
    {
      "epoch": 13.829944002357795,
      "grad_norm": 0.4304836094379425,
      "learning_rate": 1.9393009113486277e-05,
      "loss": 1.1183,
      "step": 1479
    },
    {
      "epoch": 13.83937518420277,
      "grad_norm": 0.3736785650253296,
      "learning_rate": 1.939197017951329e-05,
      "loss": 1.1346,
      "step": 1480
    },
    {
      "epoch": 13.848806366047745,
      "grad_norm": 0.3564755916595459,
      "learning_rate": 1.9390930385051105e-05,
      "loss": 1.1165,
      "step": 1481
    },
    {
      "epoch": 13.85823754789272,
      "grad_norm": 0.37082841992378235,
      "learning_rate": 1.938988973019499e-05,
      "loss": 1.145,
      "step": 1482
    },
    {
      "epoch": 13.867668729737694,
      "grad_norm": 0.3701366186141968,
      "learning_rate": 1.9388848215040284e-05,
      "loss": 1.1244,
      "step": 1483
    },
    {
      "epoch": 13.87709991158267,
      "grad_norm": 0.37199848890304565,
      "learning_rate": 1.9387805839682414e-05,
      "loss": 1.0841,
      "step": 1484
    },
    {
      "epoch": 13.886531093427646,
      "grad_norm": 0.35956916213035583,
      "learning_rate": 1.9386762604216877e-05,
      "loss": 1.1255,
      "step": 1485
    },
    {
      "epoch": 13.89596227527262,
      "grad_norm": 0.3970869779586792,
      "learning_rate": 1.9385718508739263e-05,
      "loss": 1.1474,
      "step": 1486
    },
    {
      "epoch": 13.905393457117595,
      "grad_norm": 0.38674047589302063,
      "learning_rate": 1.9384673553345222e-05,
      "loss": 1.0978,
      "step": 1487
    },
    {
      "epoch": 13.91482463896257,
      "grad_norm": 0.35968711972236633,
      "learning_rate": 1.93836277381305e-05,
      "loss": 1.106,
      "step": 1488
    },
    {
      "epoch": 13.924255820807545,
      "grad_norm": 0.3723371922969818,
      "learning_rate": 1.9382581063190912e-05,
      "loss": 1.0904,
      "step": 1489
    },
    {
      "epoch": 13.933687002652519,
      "grad_norm": 0.3778955340385437,
      "learning_rate": 1.938153352862235e-05,
      "loss": 1.0903,
      "step": 1490
    },
    {
      "epoch": 13.943118184497495,
      "grad_norm": 0.41141390800476074,
      "learning_rate": 1.9380485134520793e-05,
      "loss": 1.0946,
      "step": 1491
    },
    {
      "epoch": 13.95254936634247,
      "grad_norm": 0.34914642572402954,
      "learning_rate": 1.9379435880982297e-05,
      "loss": 1.0846,
      "step": 1492
    },
    {
      "epoch": 13.961980548187444,
      "grad_norm": 0.34230348467826843,
      "learning_rate": 1.9378385768102985e-05,
      "loss": 1.0766,
      "step": 1493
    },
    {
      "epoch": 13.97141173003242,
      "grad_norm": 0.4085959792137146,
      "learning_rate": 1.9377334795979074e-05,
      "loss": 1.0874,
      "step": 1494
    },
    {
      "epoch": 13.980842911877394,
      "grad_norm": 0.375917911529541,
      "learning_rate": 1.9376282964706857e-05,
      "loss": 1.0949,
      "step": 1495
    },
    {
      "epoch": 13.99027409372237,
      "grad_norm": 0.33106720447540283,
      "learning_rate": 1.9375230274382695e-05,
      "loss": 1.0927,
      "step": 1496
    },
    {
      "epoch": 13.999705275567344,
      "grad_norm": 0.35379138588905334,
      "learning_rate": 1.9374176725103045e-05,
      "loss": 1.1113,
      "step": 1497
    },
    {
      "epoch": 14.0,
      "grad_norm": 1.855554223060608,
      "learning_rate": 1.9373122316964424e-05,
      "loss": 1.0581,
      "step": 1498
    },
    {
      "epoch": 14.009431181844976,
      "grad_norm": 0.3909223675727844,
      "learning_rate": 1.937206705006344e-05,
      "loss": 1.116,
      "step": 1499
    },
    {
      "epoch": 14.01886236368995,
      "grad_norm": 0.41198280453681946,
      "learning_rate": 1.9371010924496772e-05,
      "loss": 1.129,
      "step": 1500
    },
    {
      "epoch": 14.028293545534925,
      "grad_norm": 0.35767942667007446,
      "learning_rate": 1.936995394036119e-05,
      "loss": 1.132,
      "step": 1501
    },
    {
      "epoch": 14.0377247273799,
      "grad_norm": 0.3763447403907776,
      "learning_rate": 1.9368896097753524e-05,
      "loss": 1.1755,
      "step": 1502
    },
    {
      "epoch": 14.047155909224875,
      "grad_norm": 0.36367329955101013,
      "learning_rate": 1.9367837396770706e-05,
      "loss": 1.06,
      "step": 1503
    },
    {
      "epoch": 14.056587091069849,
      "grad_norm": 0.33702337741851807,
      "learning_rate": 1.936677783750972e-05,
      "loss": 1.0838,
      "step": 1504
    },
    {
      "epoch": 14.066018272914825,
      "grad_norm": 0.35923999547958374,
      "learning_rate": 1.9365717420067657e-05,
      "loss": 1.1074,
      "step": 1505
    },
    {
      "epoch": 14.0754494547598,
      "grad_norm": 0.37421539425849915,
      "learning_rate": 1.936465614454166e-05,
      "loss": 1.0929,
      "step": 1506
    },
    {
      "epoch": 14.084880636604774,
      "grad_norm": 0.37915098667144775,
      "learning_rate": 1.9363594011028968e-05,
      "loss": 1.1074,
      "step": 1507
    },
    {
      "epoch": 14.09431181844975,
      "grad_norm": 0.3930465877056122,
      "learning_rate": 1.9362531019626893e-05,
      "loss": 1.1179,
      "step": 1508
    },
    {
      "epoch": 14.103743000294724,
      "grad_norm": 0.3356602191925049,
      "learning_rate": 1.9361467170432825e-05,
      "loss": 1.0834,
      "step": 1509
    },
    {
      "epoch": 14.1131741821397,
      "grad_norm": 0.45276832580566406,
      "learning_rate": 1.9360402463544236e-05,
      "loss": 1.0899,
      "step": 1510
    },
    {
      "epoch": 14.122605363984674,
      "grad_norm": 0.38381001353263855,
      "learning_rate": 1.9359336899058668e-05,
      "loss": 1.0996,
      "step": 1511
    },
    {
      "epoch": 14.13203654582965,
      "grad_norm": 0.3467874825000763,
      "learning_rate": 1.9358270477073756e-05,
      "loss": 1.1339,
      "step": 1512
    },
    {
      "epoch": 14.141467727674625,
      "grad_norm": 0.38555052876472473,
      "learning_rate": 1.93572031976872e-05,
      "loss": 1.1142,
      "step": 1513
    },
    {
      "epoch": 14.150898909519599,
      "grad_norm": 0.3509141802787781,
      "learning_rate": 1.9356135060996782e-05,
      "loss": 1.1623,
      "step": 1514
    },
    {
      "epoch": 14.160330091364575,
      "grad_norm": 0.3810410499572754,
      "learning_rate": 1.935506606710037e-05,
      "loss": 1.0869,
      "step": 1515
    },
    {
      "epoch": 14.169761273209549,
      "grad_norm": 0.4186238944530487,
      "learning_rate": 1.93539962160959e-05,
      "loss": 1.0934,
      "step": 1516
    },
    {
      "epoch": 14.179192455054524,
      "grad_norm": 0.3539169430732727,
      "learning_rate": 1.9352925508081394e-05,
      "loss": 1.1081,
      "step": 1517
    },
    {
      "epoch": 14.188623636899498,
      "grad_norm": 0.35329365730285645,
      "learning_rate": 1.9351853943154952e-05,
      "loss": 1.1434,
      "step": 1518
    },
    {
      "epoch": 14.198054818744474,
      "grad_norm": 0.37139010429382324,
      "learning_rate": 1.9350781521414745e-05,
      "loss": 1.1053,
      "step": 1519
    },
    {
      "epoch": 14.20748600058945,
      "grad_norm": 0.39608442783355713,
      "learning_rate": 1.934970824295903e-05,
      "loss": 1.1059,
      "step": 1520
    },
    {
      "epoch": 14.216917182434424,
      "grad_norm": 0.361419141292572,
      "learning_rate": 1.9348634107886145e-05,
      "loss": 1.0933,
      "step": 1521
    },
    {
      "epoch": 14.2263483642794,
      "grad_norm": 0.3688527047634125,
      "learning_rate": 1.9347559116294495e-05,
      "loss": 1.137,
      "step": 1522
    },
    {
      "epoch": 14.235779546124373,
      "grad_norm": 0.37657564878463745,
      "learning_rate": 1.9346483268282574e-05,
      "loss": 1.1414,
      "step": 1523
    },
    {
      "epoch": 14.245210727969349,
      "grad_norm": 0.3531215786933899,
      "learning_rate": 1.934540656394895e-05,
      "loss": 1.1344,
      "step": 1524
    },
    {
      "epoch": 14.254641909814323,
      "grad_norm": 0.3673568367958069,
      "learning_rate": 1.934432900339227e-05,
      "loss": 1.1304,
      "step": 1525
    },
    {
      "epoch": 14.264073091659299,
      "grad_norm": 0.3563488721847534,
      "learning_rate": 1.9343250586711257e-05,
      "loss": 1.1022,
      "step": 1526
    },
    {
      "epoch": 14.273504273504274,
      "grad_norm": 0.37991392612457275,
      "learning_rate": 1.9342171314004722e-05,
      "loss": 1.0891,
      "step": 1527
    },
    {
      "epoch": 14.282935455349248,
      "grad_norm": 0.38266944885253906,
      "learning_rate": 1.9341091185371547e-05,
      "loss": 1.0844,
      "step": 1528
    },
    {
      "epoch": 14.292366637194224,
      "grad_norm": 0.37334969639778137,
      "learning_rate": 1.9340010200910684e-05,
      "loss": 1.1198,
      "step": 1529
    },
    {
      "epoch": 14.301797819039198,
      "grad_norm": 0.3675945997238159,
      "learning_rate": 1.9338928360721183e-05,
      "loss": 1.1073,
      "step": 1530
    },
    {
      "epoch": 14.311229000884174,
      "grad_norm": 0.371572345495224,
      "learning_rate": 1.9337845664902154e-05,
      "loss": 1.0912,
      "step": 1531
    },
    {
      "epoch": 14.320660182729148,
      "grad_norm": 0.37958818674087524,
      "learning_rate": 1.9336762113552797e-05,
      "loss": 1.1252,
      "step": 1532
    },
    {
      "epoch": 14.330091364574123,
      "grad_norm": 0.387008398771286,
      "learning_rate": 1.933567770677239e-05,
      "loss": 1.1023,
      "step": 1533
    },
    {
      "epoch": 14.339522546419099,
      "grad_norm": 0.35109943151474,
      "learning_rate": 1.933459244466028e-05,
      "loss": 1.0859,
      "step": 1534
    },
    {
      "epoch": 14.348953728264073,
      "grad_norm": 0.368906170129776,
      "learning_rate": 1.93335063273159e-05,
      "loss": 1.0829,
      "step": 1535
    },
    {
      "epoch": 14.358384910109049,
      "grad_norm": 0.3819480836391449,
      "learning_rate": 1.933241935483876e-05,
      "loss": 1.1144,
      "step": 1536
    },
    {
      "epoch": 14.367816091954023,
      "grad_norm": 0.3769046664237976,
      "learning_rate": 1.933133152732845e-05,
      "loss": 1.1419,
      "step": 1537
    },
    {
      "epoch": 14.377247273798998,
      "grad_norm": 0.370897114276886,
      "learning_rate": 1.9330242844884633e-05,
      "loss": 1.1009,
      "step": 1538
    },
    {
      "epoch": 14.386678455643972,
      "grad_norm": 0.34862565994262695,
      "learning_rate": 1.9329153307607055e-05,
      "loss": 1.0952,
      "step": 1539
    },
    {
      "epoch": 14.396109637488948,
      "grad_norm": 0.3612026870250702,
      "learning_rate": 1.932806291559554e-05,
      "loss": 1.1084,
      "step": 1540
    },
    {
      "epoch": 14.405540819333924,
      "grad_norm": 0.3709704279899597,
      "learning_rate": 1.9326971668949986e-05,
      "loss": 1.065,
      "step": 1541
    },
    {
      "epoch": 14.414972001178898,
      "grad_norm": 0.3850926160812378,
      "learning_rate": 1.932587956777038e-05,
      "loss": 1.1279,
      "step": 1542
    },
    {
      "epoch": 14.424403183023873,
      "grad_norm": 0.36045801639556885,
      "learning_rate": 1.9324786612156772e-05,
      "loss": 1.1077,
      "step": 1543
    },
    {
      "epoch": 14.433834364868847,
      "grad_norm": 0.384869784116745,
      "learning_rate": 1.9323692802209306e-05,
      "loss": 1.0963,
      "step": 1544
    },
    {
      "epoch": 14.443265546713823,
      "grad_norm": 0.3382855951786041,
      "learning_rate": 1.9322598138028188e-05,
      "loss": 1.0967,
      "step": 1545
    },
    {
      "epoch": 14.452696728558797,
      "grad_norm": 0.36585676670074463,
      "learning_rate": 1.9321502619713716e-05,
      "loss": 1.1374,
      "step": 1546
    },
    {
      "epoch": 14.462127910403773,
      "grad_norm": 0.36526015400886536,
      "learning_rate": 1.932040624736626e-05,
      "loss": 1.0953,
      "step": 1547
    },
    {
      "epoch": 14.471559092248748,
      "grad_norm": 0.41313520073890686,
      "learning_rate": 1.931930902108627e-05,
      "loss": 1.1436,
      "step": 1548
    },
    {
      "epoch": 14.480990274093722,
      "grad_norm": 0.3928343653678894,
      "learning_rate": 1.9318210940974273e-05,
      "loss": 1.0915,
      "step": 1549
    },
    {
      "epoch": 14.490421455938698,
      "grad_norm": 0.35344696044921875,
      "learning_rate": 1.9317112007130873e-05,
      "loss": 1.1315,
      "step": 1550
    },
    {
      "epoch": 14.499852637783672,
      "grad_norm": 0.40079402923583984,
      "learning_rate": 1.9316012219656757e-05,
      "loss": 1.1003,
      "step": 1551
    },
    {
      "epoch": 14.509283819628648,
      "grad_norm": 0.3422422707080841,
      "learning_rate": 1.9314911578652683e-05,
      "loss": 1.1287,
      "step": 1552
    },
    {
      "epoch": 14.518715001473621,
      "grad_norm": 0.39867904782295227,
      "learning_rate": 1.9313810084219495e-05,
      "loss": 1.135,
      "step": 1553
    },
    {
      "epoch": 14.528146183318597,
      "grad_norm": 0.3838579058647156,
      "learning_rate": 1.931270773645811e-05,
      "loss": 1.1066,
      "step": 1554
    },
    {
      "epoch": 14.537577365163571,
      "grad_norm": 0.3686832785606384,
      "learning_rate": 1.9311604535469527e-05,
      "loss": 1.0722,
      "step": 1555
    },
    {
      "epoch": 14.547008547008547,
      "grad_norm": 0.36536702513694763,
      "learning_rate": 1.9310500481354818e-05,
      "loss": 1.104,
      "step": 1556
    },
    {
      "epoch": 14.556439728853523,
      "grad_norm": 0.346871554851532,
      "learning_rate": 1.930939557421514e-05,
      "loss": 1.0658,
      "step": 1557
    },
    {
      "epoch": 14.565870910698496,
      "grad_norm": 0.4227096140384674,
      "learning_rate": 1.930828981415172e-05,
      "loss": 1.1317,
      "step": 1558
    },
    {
      "epoch": 14.575302092543472,
      "grad_norm": 0.36811184883117676,
      "learning_rate": 1.9307183201265867e-05,
      "loss": 1.0849,
      "step": 1559
    },
    {
      "epoch": 14.584733274388446,
      "grad_norm": 0.3514645993709564,
      "learning_rate": 1.930607573565897e-05,
      "loss": 1.0748,
      "step": 1560
    },
    {
      "epoch": 14.594164456233422,
      "grad_norm": 0.3802592158317566,
      "learning_rate": 1.93049674174325e-05,
      "loss": 1.0992,
      "step": 1561
    },
    {
      "epoch": 14.603595638078396,
      "grad_norm": 0.3620220720767975,
      "learning_rate": 1.930385824668799e-05,
      "loss": 1.0919,
      "step": 1562
    },
    {
      "epoch": 14.613026819923371,
      "grad_norm": 0.38618698716163635,
      "learning_rate": 1.9302748223527067e-05,
      "loss": 1.1223,
      "step": 1563
    },
    {
      "epoch": 14.622458001768347,
      "grad_norm": 0.375652551651001,
      "learning_rate": 1.9301637348051435e-05,
      "loss": 1.1095,
      "step": 1564
    },
    {
      "epoch": 14.631889183613321,
      "grad_norm": 0.35693109035491943,
      "learning_rate": 1.930052562036287e-05,
      "loss": 1.0829,
      "step": 1565
    },
    {
      "epoch": 14.641320365458297,
      "grad_norm": 0.39899709820747375,
      "learning_rate": 1.929941304056322e-05,
      "loss": 1.112,
      "step": 1566
    },
    {
      "epoch": 14.65075154730327,
      "grad_norm": 0.38338735699653625,
      "learning_rate": 1.9298299608754433e-05,
      "loss": 1.1079,
      "step": 1567
    },
    {
      "epoch": 14.660182729148246,
      "grad_norm": 0.4020153284072876,
      "learning_rate": 1.929718532503851e-05,
      "loss": 1.124,
      "step": 1568
    },
    {
      "epoch": 14.66961391099322,
      "grad_norm": 0.3843649923801422,
      "learning_rate": 1.9296070189517545e-05,
      "loss": 1.1302,
      "step": 1569
    },
    {
      "epoch": 14.679045092838196,
      "grad_norm": 0.37574467062950134,
      "learning_rate": 1.929495420229371e-05,
      "loss": 1.1147,
      "step": 1570
    },
    {
      "epoch": 14.688476274683172,
      "grad_norm": 0.3620656728744507,
      "learning_rate": 1.9293837363469247e-05,
      "loss": 1.1211,
      "step": 1571
    },
    {
      "epoch": 14.697907456528146,
      "grad_norm": 0.39702078700065613,
      "learning_rate": 1.929271967314648e-05,
      "loss": 1.1049,
      "step": 1572
    },
    {
      "epoch": 14.707338638373121,
      "grad_norm": 0.38634786009788513,
      "learning_rate": 1.9291601131427814e-05,
      "loss": 1.0863,
      "step": 1573
    },
    {
      "epoch": 14.716769820218095,
      "grad_norm": 0.36181122064590454,
      "learning_rate": 1.9290481738415726e-05,
      "loss": 1.0993,
      "step": 1574
    },
    {
      "epoch": 14.726201002063071,
      "grad_norm": 0.3797849118709564,
      "learning_rate": 1.9289361494212783e-05,
      "loss": 1.0892,
      "step": 1575
    },
    {
      "epoch": 14.735632183908045,
      "grad_norm": 0.3649861216545105,
      "learning_rate": 1.928824039892161e-05,
      "loss": 1.0782,
      "step": 1576
    },
    {
      "epoch": 14.74506336575302,
      "grad_norm": 0.3917198181152344,
      "learning_rate": 1.9287118452644927e-05,
      "loss": 1.0821,
      "step": 1577
    },
    {
      "epoch": 14.754494547597997,
      "grad_norm": 0.37021398544311523,
      "learning_rate": 1.9285995655485527e-05,
      "loss": 1.0991,
      "step": 1578
    },
    {
      "epoch": 14.76392572944297,
      "grad_norm": 0.3511815071105957,
      "learning_rate": 1.9284872007546277e-05,
      "loss": 1.1188,
      "step": 1579
    },
    {
      "epoch": 14.773356911287946,
      "grad_norm": 0.39760980010032654,
      "learning_rate": 1.928374750893013e-05,
      "loss": 1.1064,
      "step": 1580
    },
    {
      "epoch": 14.78278809313292,
      "grad_norm": 0.36537960171699524,
      "learning_rate": 1.928262215974011e-05,
      "loss": 1.1397,
      "step": 1581
    },
    {
      "epoch": 14.792219274977896,
      "grad_norm": 0.41734713315963745,
      "learning_rate": 1.928149596007932e-05,
      "loss": 1.1214,
      "step": 1582
    },
    {
      "epoch": 14.80165045682287,
      "grad_norm": 0.3728983700275421,
      "learning_rate": 1.9280368910050943e-05,
      "loss": 1.1189,
      "step": 1583
    },
    {
      "epoch": 14.811081638667845,
      "grad_norm": 0.3569949269294739,
      "learning_rate": 1.927924100975824e-05,
      "loss": 1.1465,
      "step": 1584
    },
    {
      "epoch": 14.820512820512821,
      "grad_norm": 0.3779425323009491,
      "learning_rate": 1.9278112259304548e-05,
      "loss": 1.0979,
      "step": 1585
    },
    {
      "epoch": 14.829944002357795,
      "grad_norm": 0.3587665259838104,
      "learning_rate": 1.927698265879328e-05,
      "loss": 1.143,
      "step": 1586
    },
    {
      "epoch": 14.83937518420277,
      "grad_norm": 0.3731856644153595,
      "learning_rate": 1.927585220832793e-05,
      "loss": 1.1237,
      "step": 1587
    },
    {
      "epoch": 14.848806366047745,
      "grad_norm": 0.39419612288475037,
      "learning_rate": 1.9274720908012074e-05,
      "loss": 1.1161,
      "step": 1588
    },
    {
      "epoch": 14.85823754789272,
      "grad_norm": 0.3590352535247803,
      "learning_rate": 1.9273588757949362e-05,
      "loss": 1.0936,
      "step": 1589
    },
    {
      "epoch": 14.867668729737694,
      "grad_norm": 0.3922674357891083,
      "learning_rate": 1.9272455758243514e-05,
      "loss": 1.1397,
      "step": 1590
    },
    {
      "epoch": 14.87709991158267,
      "grad_norm": 0.34205105900764465,
      "learning_rate": 1.927132190899834e-05,
      "loss": 1.1076,
      "step": 1591
    },
    {
      "epoch": 14.886531093427646,
      "grad_norm": 0.38046571612358093,
      "learning_rate": 1.927018721031772e-05,
      "loss": 1.1659,
      "step": 1592
    },
    {
      "epoch": 14.89596227527262,
      "grad_norm": 0.3736892342567444,
      "learning_rate": 1.926905166230562e-05,
      "loss": 1.1298,
      "step": 1593
    },
    {
      "epoch": 14.905393457117595,
      "grad_norm": 0.37442129850387573,
      "learning_rate": 1.9267915265066076e-05,
      "loss": 1.1503,
      "step": 1594
    },
    {
      "epoch": 14.91482463896257,
      "grad_norm": 0.3666459918022156,
      "learning_rate": 1.9266778018703202e-05,
      "loss": 1.1141,
      "step": 1595
    },
    {
      "epoch": 14.924255820807545,
      "grad_norm": 0.3691643476486206,
      "learning_rate": 1.9265639923321194e-05,
      "loss": 1.1412,
      "step": 1596
    },
    {
      "epoch": 14.933687002652519,
      "grad_norm": 0.40101268887519836,
      "learning_rate": 1.9264500979024324e-05,
      "loss": 1.1481,
      "step": 1597
    },
    {
      "epoch": 14.943118184497495,
      "grad_norm": 0.3671351969242096,
      "learning_rate": 1.926336118591694e-05,
      "loss": 1.078,
      "step": 1598
    },
    {
      "epoch": 14.95254936634247,
      "grad_norm": 0.3644125461578369,
      "learning_rate": 1.926222054410347e-05,
      "loss": 1.0951,
      "step": 1599
    },
    {
      "epoch": 14.961980548187444,
      "grad_norm": 0.3615366220474243,
      "learning_rate": 1.926107905368842e-05,
      "loss": 1.1137,
      "step": 1600
    },
    {
      "epoch": 14.97141173003242,
      "grad_norm": 0.393562376499176,
      "learning_rate": 1.9259936714776376e-05,
      "loss": 1.1142,
      "step": 1601
    },
    {
      "epoch": 14.980842911877394,
      "grad_norm": 0.4017179608345032,
      "learning_rate": 1.9258793527471997e-05,
      "loss": 1.1519,
      "step": 1602
    },
    {
      "epoch": 14.99027409372237,
      "grad_norm": 0.3805179297924042,
      "learning_rate": 1.9257649491880016e-05,
      "loss": 1.1558,
      "step": 1603
    },
    {
      "epoch": 14.999705275567344,
      "grad_norm": 0.3850933909416199,
      "learning_rate": 1.9256504608105257e-05,
      "loss": 1.0992,
      "step": 1604
    },
    {
      "epoch": 15.0,
      "grad_norm": 1.4855059385299683,
      "learning_rate": 1.9255358876252606e-05,
      "loss": 0.7657,
      "step": 1605
    },
    {
      "epoch": 15.009431181844976,
      "grad_norm": 0.3611797094345093,
      "learning_rate": 1.9254212296427043e-05,
      "loss": 1.0784,
      "step": 1606
    },
    {
      "epoch": 15.01886236368995,
      "grad_norm": 0.3962307870388031,
      "learning_rate": 1.925306486873361e-05,
      "loss": 1.1003,
      "step": 1607
    },
    {
      "epoch": 15.028293545534925,
      "grad_norm": 0.37608709931373596,
      "learning_rate": 1.925191659327744e-05,
      "loss": 1.0952,
      "step": 1608
    },
    {
      "epoch": 15.0377247273799,
      "grad_norm": 0.45835819840431213,
      "learning_rate": 1.9250767470163732e-05,
      "loss": 1.1034,
      "step": 1609
    },
    {
      "epoch": 15.047155909224875,
      "grad_norm": 0.3901990056037903,
      "learning_rate": 1.924961749949777e-05,
      "loss": 1.1068,
      "step": 1610
    },
    {
      "epoch": 15.056587091069849,
      "grad_norm": 0.4075550436973572,
      "learning_rate": 1.9248466681384918e-05,
      "loss": 1.1136,
      "step": 1611
    },
    {
      "epoch": 15.066018272914825,
      "grad_norm": 0.3705393075942993,
      "learning_rate": 1.9247315015930608e-05,
      "loss": 1.1255,
      "step": 1612
    },
    {
      "epoch": 15.0754494547598,
      "grad_norm": 0.34093809127807617,
      "learning_rate": 1.924616250324036e-05,
      "loss": 1.0872,
      "step": 1613
    },
    {
      "epoch": 15.084880636604774,
      "grad_norm": 0.36543479561805725,
      "learning_rate": 1.9245009143419765e-05,
      "loss": 1.1409,
      "step": 1614
    },
    {
      "epoch": 15.09431181844975,
      "grad_norm": 0.37895330786705017,
      "learning_rate": 1.9243854936574492e-05,
      "loss": 1.12,
      "step": 1615
    },
    {
      "epoch": 15.103743000294724,
      "grad_norm": 0.431609570980072,
      "learning_rate": 1.9242699882810286e-05,
      "loss": 1.1236,
      "step": 1616
    },
    {
      "epoch": 15.1131741821397,
      "grad_norm": 0.40375062823295593,
      "learning_rate": 1.924154398223298e-05,
      "loss": 1.1291,
      "step": 1617
    },
    {
      "epoch": 15.122605363984674,
      "grad_norm": 0.3693370819091797,
      "learning_rate": 1.9240387234948475e-05,
      "loss": 1.0957,
      "step": 1618
    },
    {
      "epoch": 15.13203654582965,
      "grad_norm": 0.3788278102874756,
      "learning_rate": 1.9239229641062747e-05,
      "loss": 1.0768,
      "step": 1619
    },
    {
      "epoch": 15.141467727674625,
      "grad_norm": 0.3830548822879791,
      "learning_rate": 1.9238071200681863e-05,
      "loss": 1.1353,
      "step": 1620
    },
    {
      "epoch": 15.150898909519599,
      "grad_norm": 0.3977244794368744,
      "learning_rate": 1.923691191391195e-05,
      "loss": 1.1168,
      "step": 1621
    },
    {
      "epoch": 15.160330091364575,
      "grad_norm": 0.37378329038619995,
      "learning_rate": 1.9235751780859226e-05,
      "loss": 1.1441,
      "step": 1622
    },
    {
      "epoch": 15.169761273209549,
      "grad_norm": 0.42115524411201477,
      "learning_rate": 1.9234590801629982e-05,
      "loss": 1.0743,
      "step": 1623
    },
    {
      "epoch": 15.179192455054524,
      "grad_norm": 0.3615235984325409,
      "learning_rate": 1.9233428976330586e-05,
      "loss": 1.117,
      "step": 1624
    },
    {
      "epoch": 15.188623636899498,
      "grad_norm": 0.3700472116470337,
      "learning_rate": 1.9232266305067484e-05,
      "loss": 1.1106,
      "step": 1625
    },
    {
      "epoch": 15.198054818744474,
      "grad_norm": 0.41888728737831116,
      "learning_rate": 1.9231102787947197e-05,
      "loss": 1.0769,
      "step": 1626
    },
    {
      "epoch": 15.20748600058945,
      "grad_norm": 0.38166606426239014,
      "learning_rate": 1.9229938425076333e-05,
      "loss": 1.0917,
      "step": 1627
    },
    {
      "epoch": 15.216917182434424,
      "grad_norm": 0.39197248220443726,
      "learning_rate": 1.922877321656156e-05,
      "loss": 1.0808,
      "step": 1628
    },
    {
      "epoch": 15.2263483642794,
      "grad_norm": 0.369179904460907,
      "learning_rate": 1.922760716250965e-05,
      "loss": 1.1173,
      "step": 1629
    },
    {
      "epoch": 15.235779546124373,
      "grad_norm": 0.3711191415786743,
      "learning_rate": 1.922644026302742e-05,
      "loss": 1.089,
      "step": 1630
    },
    {
      "epoch": 15.245210727969349,
      "grad_norm": 0.39768290519714355,
      "learning_rate": 1.9225272518221788e-05,
      "loss": 1.1068,
      "step": 1631
    },
    {
      "epoch": 15.254641909814323,
      "grad_norm": 0.37686777114868164,
      "learning_rate": 1.9224103928199743e-05,
      "loss": 1.1139,
      "step": 1632
    },
    {
      "epoch": 15.264073091659299,
      "grad_norm": 0.38927948474884033,
      "learning_rate": 1.922293449306835e-05,
      "loss": 1.1398,
      "step": 1633
    },
    {
      "epoch": 15.273504273504274,
      "grad_norm": 0.38372600078582764,
      "learning_rate": 1.922176421293475e-05,
      "loss": 1.1016,
      "step": 1634
    },
    {
      "epoch": 15.282935455349248,
      "grad_norm": 0.35042667388916016,
      "learning_rate": 1.9220593087906168e-05,
      "loss": 1.1368,
      "step": 1635
    },
    {
      "epoch": 15.292366637194224,
      "grad_norm": 0.35673850774765015,
      "learning_rate": 1.92194211180899e-05,
      "loss": 1.104,
      "step": 1636
    },
    {
      "epoch": 15.301797819039198,
      "grad_norm": 0.36617398262023926,
      "learning_rate": 1.9218248303593315e-05,
      "loss": 1.0529,
      "step": 1637
    },
    {
      "epoch": 15.311229000884174,
      "grad_norm": 0.40522658824920654,
      "learning_rate": 1.9217074644523878e-05,
      "loss": 1.1218,
      "step": 1638
    },
    {
      "epoch": 15.320660182729148,
      "grad_norm": 0.3762759268283844,
      "learning_rate": 1.9215900140989115e-05,
      "loss": 1.1009,
      "step": 1639
    },
    {
      "epoch": 15.330091364574123,
      "grad_norm": 0.35953986644744873,
      "learning_rate": 1.921472479309663e-05,
      "loss": 1.0759,
      "step": 1640
    },
    {
      "epoch": 15.339522546419099,
      "grad_norm": 0.3763536512851715,
      "learning_rate": 1.921354860095411e-05,
      "loss": 1.1186,
      "step": 1641
    },
    {
      "epoch": 15.348953728264073,
      "grad_norm": 0.37316006422042847,
      "learning_rate": 1.9212371564669317e-05,
      "loss": 1.0867,
      "step": 1642
    },
    {
      "epoch": 15.358384910109049,
      "grad_norm": 0.40438124537467957,
      "learning_rate": 1.921119368435009e-05,
      "loss": 1.0775,
      "step": 1643
    },
    {
      "epoch": 15.367816091954023,
      "grad_norm": 0.3704136610031128,
      "learning_rate": 1.921001496010435e-05,
      "loss": 1.1053,
      "step": 1644
    },
    {
      "epoch": 15.377247273798998,
      "grad_norm": 0.375020831823349,
      "learning_rate": 1.9208835392040086e-05,
      "loss": 1.1182,
      "step": 1645
    },
    {
      "epoch": 15.386678455643972,
      "grad_norm": 0.37181970477104187,
      "learning_rate": 1.9207654980265372e-05,
      "loss": 1.1524,
      "step": 1646
    },
    {
      "epoch": 15.396109637488948,
      "grad_norm": 0.39058974385261536,
      "learning_rate": 1.9206473724888356e-05,
      "loss": 1.1016,
      "step": 1647
    },
    {
      "epoch": 15.405540819333924,
      "grad_norm": 0.35133805871009827,
      "learning_rate": 1.920529162601727e-05,
      "loss": 1.0987,
      "step": 1648
    },
    {
      "epoch": 15.414972001178898,
      "grad_norm": 0.3888801634311676,
      "learning_rate": 1.920410868376041e-05,
      "loss": 1.1271,
      "step": 1649
    },
    {
      "epoch": 15.424403183023873,
      "grad_norm": 0.3805564045906067,
      "learning_rate": 1.920292489822616e-05,
      "loss": 1.145,
      "step": 1650
    },
    {
      "epoch": 15.433834364868847,
      "grad_norm": 0.41368475556373596,
      "learning_rate": 1.9201740269522976e-05,
      "loss": 1.1136,
      "step": 1651
    },
    {
      "epoch": 15.443265546713823,
      "grad_norm": 0.41968289017677307,
      "learning_rate": 1.9200554797759395e-05,
      "loss": 1.1154,
      "step": 1652
    },
    {
      "epoch": 15.452696728558797,
      "grad_norm": 0.42128488421440125,
      "learning_rate": 1.919936848304403e-05,
      "loss": 1.1385,
      "step": 1653
    },
    {
      "epoch": 15.462127910403773,
      "grad_norm": 0.3596537411212921,
      "learning_rate": 1.919818132548557e-05,
      "loss": 1.1666,
      "step": 1654
    },
    {
      "epoch": 15.471559092248748,
      "grad_norm": 0.36195576190948486,
      "learning_rate": 1.919699332519278e-05,
      "loss": 1.1106,
      "step": 1655
    },
    {
      "epoch": 15.480990274093722,
      "grad_norm": 0.4034750759601593,
      "learning_rate": 1.919580448227451e-05,
      "loss": 1.1198,
      "step": 1656
    },
    {
      "epoch": 15.490421455938698,
      "grad_norm": 0.4114519953727722,
      "learning_rate": 1.9194614796839675e-05,
      "loss": 1.0926,
      "step": 1657
    },
    {
      "epoch": 15.499852637783672,
      "grad_norm": 0.41219255328178406,
      "learning_rate": 1.9193424268997278e-05,
      "loss": 1.1188,
      "step": 1658
    },
    {
      "epoch": 15.509283819628648,
      "grad_norm": 0.3719993829727173,
      "learning_rate": 1.9192232898856395e-05,
      "loss": 1.1048,
      "step": 1659
    },
    {
      "epoch": 15.518715001473621,
      "grad_norm": 0.36451593041419983,
      "learning_rate": 1.919104068652617e-05,
      "loss": 1.135,
      "step": 1660
    },
    {
      "epoch": 15.528146183318597,
      "grad_norm": 0.4076788127422333,
      "learning_rate": 1.9189847632115846e-05,
      "loss": 1.0955,
      "step": 1661
    },
    {
      "epoch": 15.537577365163571,
      "grad_norm": 0.38475942611694336,
      "learning_rate": 1.918865373573472e-05,
      "loss": 1.1119,
      "step": 1662
    },
    {
      "epoch": 15.547008547008547,
      "grad_norm": 0.34955403208732605,
      "learning_rate": 1.9187458997492186e-05,
      "loss": 1.1052,
      "step": 1663
    },
    {
      "epoch": 15.556439728853523,
      "grad_norm": 0.3706059455871582,
      "learning_rate": 1.9186263417497696e-05,
      "loss": 1.103,
      "step": 1664
    },
    {
      "epoch": 15.565870910698496,
      "grad_norm": 0.3784816563129425,
      "learning_rate": 1.9185066995860797e-05,
      "loss": 1.1145,
      "step": 1665
    },
    {
      "epoch": 15.575302092543472,
      "grad_norm": 0.38665416836738586,
      "learning_rate": 1.9183869732691095e-05,
      "loss": 1.1053,
      "step": 1666
    },
    {
      "epoch": 15.584733274388446,
      "grad_norm": 0.3881756067276001,
      "learning_rate": 1.9182671628098294e-05,
      "loss": 1.0939,
      "step": 1667
    },
    {
      "epoch": 15.594164456233422,
      "grad_norm": 0.35117238759994507,
      "learning_rate": 1.9181472682192156e-05,
      "loss": 1.1338,
      "step": 1668
    },
    {
      "epoch": 15.603595638078396,
      "grad_norm": 0.40026986598968506,
      "learning_rate": 1.918027289508253e-05,
      "loss": 1.0653,
      "step": 1669
    },
    {
      "epoch": 15.613026819923371,
      "grad_norm": 0.36633071303367615,
      "learning_rate": 1.917907226687934e-05,
      "loss": 1.0855,
      "step": 1670
    },
    {
      "epoch": 15.622458001768347,
      "grad_norm": 0.35741835832595825,
      "learning_rate": 1.917787079769259e-05,
      "loss": 1.1135,
      "step": 1671
    },
    {
      "epoch": 15.631889183613321,
      "grad_norm": 0.38603079319000244,
      "learning_rate": 1.9176668487632356e-05,
      "loss": 1.1148,
      "step": 1672
    },
    {
      "epoch": 15.641320365458297,
      "grad_norm": 0.40599268674850464,
      "learning_rate": 1.917546533680879e-05,
      "loss": 1.0745,
      "step": 1673
    },
    {
      "epoch": 15.65075154730327,
      "grad_norm": 0.38610202074050903,
      "learning_rate": 1.9174261345332128e-05,
      "loss": 1.0866,
      "step": 1674
    },
    {
      "epoch": 15.660182729148246,
      "grad_norm": 0.3992762863636017,
      "learning_rate": 1.917305651331268e-05,
      "loss": 1.1337,
      "step": 1675
    },
    {
      "epoch": 15.66961391099322,
      "grad_norm": 0.3561934232711792,
      "learning_rate": 1.917185084086083e-05,
      "loss": 1.103,
      "step": 1676
    },
    {
      "epoch": 15.679045092838196,
      "grad_norm": 0.438741534948349,
      "learning_rate": 1.9170644328087044e-05,
      "loss": 1.1292,
      "step": 1677
    },
    {
      "epoch": 15.688476274683172,
      "grad_norm": 0.35902222990989685,
      "learning_rate": 1.916943697510186e-05,
      "loss": 1.1112,
      "step": 1678
    },
    {
      "epoch": 15.697907456528146,
      "grad_norm": 0.4235311448574066,
      "learning_rate": 1.9168228782015895e-05,
      "loss": 1.1277,
      "step": 1679
    },
    {
      "epoch": 15.707338638373121,
      "grad_norm": 0.4235726296901703,
      "learning_rate": 1.9167019748939847e-05,
      "loss": 1.1381,
      "step": 1680
    },
    {
      "epoch": 15.716769820218095,
      "grad_norm": 0.3932773172855377,
      "learning_rate": 1.9165809875984483e-05,
      "loss": 1.1088,
      "step": 1681
    },
    {
      "epoch": 15.726201002063071,
      "grad_norm": 0.38724732398986816,
      "learning_rate": 1.916459916326065e-05,
      "loss": 1.1304,
      "step": 1682
    },
    {
      "epoch": 15.735632183908045,
      "grad_norm": 0.4205908179283142,
      "learning_rate": 1.9163387610879282e-05,
      "loss": 1.1028,
      "step": 1683
    },
    {
      "epoch": 15.74506336575302,
      "grad_norm": 0.40330570936203003,
      "learning_rate": 1.9162175218951365e-05,
      "loss": 1.1027,
      "step": 1684
    },
    {
      "epoch": 15.754494547597997,
      "grad_norm": 0.41861212253570557,
      "learning_rate": 1.9160961987587997e-05,
      "loss": 1.1173,
      "step": 1685
    },
    {
      "epoch": 15.76392572944297,
      "grad_norm": 0.3729829490184784,
      "learning_rate": 1.915974791690032e-05,
      "loss": 1.1027,
      "step": 1686
    },
    {
      "epoch": 15.773356911287946,
      "grad_norm": 0.36739957332611084,
      "learning_rate": 1.9158533006999572e-05,
      "loss": 1.117,
      "step": 1687
    },
    {
      "epoch": 15.78278809313292,
      "grad_norm": 0.4162713885307312,
      "learning_rate": 1.915731725799706e-05,
      "loss": 1.112,
      "step": 1688
    },
    {
      "epoch": 15.792219274977896,
      "grad_norm": 0.3839196562767029,
      "learning_rate": 1.9156100670004174e-05,
      "loss": 1.1578,
      "step": 1689
    },
    {
      "epoch": 15.80165045682287,
      "grad_norm": 0.39778971672058105,
      "learning_rate": 1.9154883243132376e-05,
      "loss": 1.1244,
      "step": 1690
    },
    {
      "epoch": 15.811081638667845,
      "grad_norm": 0.37273818254470825,
      "learning_rate": 1.9153664977493207e-05,
      "loss": 1.1099,
      "step": 1691
    },
    {
      "epoch": 15.820512820512821,
      "grad_norm": 0.38322213292121887,
      "learning_rate": 1.9152445873198282e-05,
      "loss": 1.1069,
      "step": 1692
    },
    {
      "epoch": 15.829944002357795,
      "grad_norm": 0.38808345794677734,
      "learning_rate": 1.9151225930359297e-05,
      "loss": 1.1003,
      "step": 1693
    },
    {
      "epoch": 15.83937518420277,
      "grad_norm": 0.41693973541259766,
      "learning_rate": 1.915000514908802e-05,
      "loss": 1.0752,
      "step": 1694
    },
    {
      "epoch": 15.848806366047745,
      "grad_norm": 0.3714955151081085,
      "learning_rate": 1.9148783529496303e-05,
      "loss": 1.1256,
      "step": 1695
    },
    {
      "epoch": 15.85823754789272,
      "grad_norm": 0.3576268255710602,
      "learning_rate": 1.9147561071696066e-05,
      "loss": 1.0936,
      "step": 1696
    },
    {
      "epoch": 15.867668729737694,
      "grad_norm": 0.43345004320144653,
      "learning_rate": 1.914633777579931e-05,
      "loss": 1.1013,
      "step": 1697
    },
    {
      "epoch": 15.87709991158267,
      "grad_norm": 0.3859037756919861,
      "learning_rate": 1.914511364191812e-05,
      "loss": 1.1027,
      "step": 1698
    },
    {
      "epoch": 15.886531093427646,
      "grad_norm": 0.36545246839523315,
      "learning_rate": 1.9143888670164643e-05,
      "loss": 1.1487,
      "step": 1699
    },
    {
      "epoch": 15.89596227527262,
      "grad_norm": 0.3962048888206482,
      "learning_rate": 1.9142662860651114e-05,
      "loss": 1.1071,
      "step": 1700
    },
    {
      "epoch": 15.905393457117595,
      "grad_norm": 0.37667182087898254,
      "learning_rate": 1.9141436213489838e-05,
      "loss": 1.1264,
      "step": 1701
    },
    {
      "epoch": 15.91482463896257,
      "grad_norm": 0.3981523811817169,
      "learning_rate": 1.9140208728793204e-05,
      "loss": 1.12,
      "step": 1702
    },
    {
      "epoch": 15.924255820807545,
      "grad_norm": 0.38155242800712585,
      "learning_rate": 1.913898040667367e-05,
      "loss": 1.0885,
      "step": 1703
    },
    {
      "epoch": 15.933687002652519,
      "grad_norm": 0.3750587999820709,
      "learning_rate": 1.9137751247243782e-05,
      "loss": 1.0983,
      "step": 1704
    },
    {
      "epoch": 15.943118184497495,
      "grad_norm": 0.36654695868492126,
      "learning_rate": 1.9136521250616144e-05,
      "loss": 1.1008,
      "step": 1705
    },
    {
      "epoch": 15.95254936634247,
      "grad_norm": 0.3626682162284851,
      "learning_rate": 1.913529041690346e-05,
      "loss": 1.1124,
      "step": 1706
    },
    {
      "epoch": 15.961980548187444,
      "grad_norm": 0.3951921761035919,
      "learning_rate": 1.9134058746218488e-05,
      "loss": 1.0963,
      "step": 1707
    },
    {
      "epoch": 15.97141173003242,
      "grad_norm": 0.3886764347553253,
      "learning_rate": 1.913282623867408e-05,
      "loss": 1.1424,
      "step": 1708
    },
    {
      "epoch": 15.980842911877394,
      "grad_norm": 0.3638828694820404,
      "learning_rate": 1.913159289438315e-05,
      "loss": 1.0588,
      "step": 1709
    },
    {
      "epoch": 15.99027409372237,
      "grad_norm": 0.36597877740859985,
      "learning_rate": 1.9130358713458705e-05,
      "loss": 1.1201,
      "step": 1710
    },
    {
      "epoch": 15.999705275567344,
      "grad_norm": 0.3858240246772766,
      "learning_rate": 1.912912369601382e-05,
      "loss": 1.1026,
      "step": 1711
    },
    {
      "epoch": 16.0,
      "grad_norm": 1.8527456521987915,
      "learning_rate": 1.9127887842161642e-05,
      "loss": 0.8551,
      "step": 1712
    },
    {
      "epoch": 16.009431181844974,
      "grad_norm": 0.37873929738998413,
      "learning_rate": 1.9126651152015404e-05,
      "loss": 1.1003,
      "step": 1713
    },
    {
      "epoch": 16.01886236368995,
      "grad_norm": 0.34816601872444153,
      "learning_rate": 1.9125413625688405e-05,
      "loss": 1.1112,
      "step": 1714
    },
    {
      "epoch": 16.028293545534925,
      "grad_norm": 0.3780088424682617,
      "learning_rate": 1.9124175263294033e-05,
      "loss": 1.0796,
      "step": 1715
    },
    {
      "epoch": 16.0377247273799,
      "grad_norm": 0.382697194814682,
      "learning_rate": 1.9122936064945744e-05,
      "loss": 1.0882,
      "step": 1716
    },
    {
      "epoch": 16.047155909224873,
      "grad_norm": 0.36984848976135254,
      "learning_rate": 1.9121696030757073e-05,
      "loss": 1.0968,
      "step": 1717
    },
    {
      "epoch": 16.05658709106985,
      "grad_norm": 0.37728744745254517,
      "learning_rate": 1.912045516084163e-05,
      "loss": 1.0833,
      "step": 1718
    },
    {
      "epoch": 16.066018272914825,
      "grad_norm": 0.39289960265159607,
      "learning_rate": 1.911921345531311e-05,
      "loss": 1.1334,
      "step": 1719
    },
    {
      "epoch": 16.0754494547598,
      "grad_norm": 0.36075359582901,
      "learning_rate": 1.9117970914285266e-05,
      "loss": 1.1223,
      "step": 1720
    },
    {
      "epoch": 16.084880636604776,
      "grad_norm": 0.3706780672073364,
      "learning_rate": 1.911672753787195e-05,
      "loss": 1.0869,
      "step": 1721
    },
    {
      "epoch": 16.09431181844975,
      "grad_norm": 0.4048968255519867,
      "learning_rate": 1.911548332618707e-05,
      "loss": 1.0788,
      "step": 1722
    },
    {
      "epoch": 16.103743000294724,
      "grad_norm": 0.3742941617965698,
      "learning_rate": 1.9114238279344627e-05,
      "loss": 1.1583,
      "step": 1723
    },
    {
      "epoch": 16.113174182139698,
      "grad_norm": 0.431035578250885,
      "learning_rate": 1.911299239745869e-05,
      "loss": 1.0224,
      "step": 1724
    },
    {
      "epoch": 16.122605363984675,
      "grad_norm": 0.35016635060310364,
      "learning_rate": 1.911174568064341e-05,
      "loss": 1.1168,
      "step": 1725
    },
    {
      "epoch": 16.13203654582965,
      "grad_norm": 0.38559430837631226,
      "learning_rate": 1.9110498129013005e-05,
      "loss": 1.0903,
      "step": 1726
    },
    {
      "epoch": 16.141467727674623,
      "grad_norm": 0.40773993730545044,
      "learning_rate": 1.9109249742681775e-05,
      "loss": 1.0907,
      "step": 1727
    },
    {
      "epoch": 16.1508989095196,
      "grad_norm": 0.35280492901802063,
      "learning_rate": 1.9108000521764104e-05,
      "loss": 1.1379,
      "step": 1728
    },
    {
      "epoch": 16.160330091364575,
      "grad_norm": 0.3534437417984009,
      "learning_rate": 1.9106750466374438e-05,
      "loss": 1.0957,
      "step": 1729
    },
    {
      "epoch": 16.16976127320955,
      "grad_norm": 0.3698105216026306,
      "learning_rate": 1.9105499576627307e-05,
      "loss": 1.1023,
      "step": 1730
    },
    {
      "epoch": 16.179192455054523,
      "grad_norm": 0.4195781946182251,
      "learning_rate": 1.910424785263732e-05,
      "loss": 1.129,
      "step": 1731
    },
    {
      "epoch": 16.1886236368995,
      "grad_norm": 0.38079994916915894,
      "learning_rate": 1.9102995294519166e-05,
      "loss": 1.097,
      "step": 1732
    },
    {
      "epoch": 16.198054818744474,
      "grad_norm": 0.36091360449790955,
      "learning_rate": 1.9101741902387586e-05,
      "loss": 1.1022,
      "step": 1733
    },
    {
      "epoch": 16.207486000589448,
      "grad_norm": 0.395447701215744,
      "learning_rate": 1.9100487676357434e-05,
      "loss": 1.1257,
      "step": 1734
    },
    {
      "epoch": 16.216917182434425,
      "grad_norm": 0.40266144275665283,
      "learning_rate": 1.909923261654361e-05,
      "loss": 1.1097,
      "step": 1735
    },
    {
      "epoch": 16.2263483642794,
      "grad_norm": 0.3885805904865265,
      "learning_rate": 1.9097976723061107e-05,
      "loss": 1.1005,
      "step": 1736
    },
    {
      "epoch": 16.235779546124373,
      "grad_norm": 0.37655559182167053,
      "learning_rate": 1.9096719996024987e-05,
      "loss": 1.1101,
      "step": 1737
    },
    {
      "epoch": 16.245210727969347,
      "grad_norm": 0.4004645049571991,
      "learning_rate": 1.9095462435550396e-05,
      "loss": 1.142,
      "step": 1738
    },
    {
      "epoch": 16.254641909814325,
      "grad_norm": 0.4094249904155731,
      "learning_rate": 1.9094204041752546e-05,
      "loss": 1.1091,
      "step": 1739
    },
    {
      "epoch": 16.2640730916593,
      "grad_norm": 0.3689297139644623,
      "learning_rate": 1.9092944814746733e-05,
      "loss": 1.1343,
      "step": 1740
    },
    {
      "epoch": 16.273504273504273,
      "grad_norm": 0.4222695827484131,
      "learning_rate": 1.9091684754648322e-05,
      "loss": 1.118,
      "step": 1741
    },
    {
      "epoch": 16.28293545534925,
      "grad_norm": 0.37435683608055115,
      "learning_rate": 1.9090423861572768e-05,
      "loss": 1.0737,
      "step": 1742
    },
    {
      "epoch": 16.292366637194224,
      "grad_norm": 0.38921067118644714,
      "learning_rate": 1.9089162135635592e-05,
      "loss": 1.1091,
      "step": 1743
    },
    {
      "epoch": 16.301797819039198,
      "grad_norm": 0.37960168719291687,
      "learning_rate": 1.9087899576952383e-05,
      "loss": 1.0925,
      "step": 1744
    },
    {
      "epoch": 16.311229000884172,
      "grad_norm": 0.411627858877182,
      "learning_rate": 1.9086636185638827e-05,
      "loss": 1.1395,
      "step": 1745
    },
    {
      "epoch": 16.32066018272915,
      "grad_norm": 0.3988538980484009,
      "learning_rate": 1.908537196181067e-05,
      "loss": 1.0885,
      "step": 1746
    },
    {
      "epoch": 16.330091364574123,
      "grad_norm": 0.36266160011291504,
      "learning_rate": 1.9084106905583746e-05,
      "loss": 1.126,
      "step": 1747
    },
    {
      "epoch": 16.339522546419097,
      "grad_norm": 0.4018574059009552,
      "learning_rate": 1.9082841017073953e-05,
      "loss": 1.0683,
      "step": 1748
    },
    {
      "epoch": 16.348953728264075,
      "grad_norm": 0.395160973072052,
      "learning_rate": 1.908157429639727e-05,
      "loss": 1.1408,
      "step": 1749
    },
    {
      "epoch": 16.35838491010905,
      "grad_norm": 0.39534270763397217,
      "learning_rate": 1.9080306743669758e-05,
      "loss": 1.1745,
      "step": 1750
    },
    {
      "epoch": 16.367816091954023,
      "grad_norm": 0.39719387888908386,
      "learning_rate": 1.907903835900755e-05,
      "loss": 1.1131,
      "step": 1751
    },
    {
      "epoch": 16.377247273798996,
      "grad_norm": 0.4271213412284851,
      "learning_rate": 1.9077769142526853e-05,
      "loss": 1.1217,
      "step": 1752
    },
    {
      "epoch": 16.386678455643974,
      "grad_norm": 0.39333540201187134,
      "learning_rate": 1.9076499094343953e-05,
      "loss": 1.0912,
      "step": 1753
    },
    {
      "epoch": 16.396109637488948,
      "grad_norm": 0.37078389525413513,
      "learning_rate": 1.9075228214575207e-05,
      "loss": 1.0876,
      "step": 1754
    },
    {
      "epoch": 16.405540819333922,
      "grad_norm": 0.3604731857776642,
      "learning_rate": 1.9073956503337057e-05,
      "loss": 1.0896,
      "step": 1755
    },
    {
      "epoch": 16.4149720011789,
      "grad_norm": 0.39769038558006287,
      "learning_rate": 1.907268396074602e-05,
      "loss": 1.1118,
      "step": 1756
    },
    {
      "epoch": 16.424403183023873,
      "grad_norm": 0.41000431776046753,
      "learning_rate": 1.9071410586918683e-05,
      "loss": 1.1074,
      "step": 1757
    },
    {
      "epoch": 16.433834364868847,
      "grad_norm": 0.3701789081096649,
      "learning_rate": 1.9070136381971704e-05,
      "loss": 1.0922,
      "step": 1758
    },
    {
      "epoch": 16.44326554671382,
      "grad_norm": 0.4092119634151459,
      "learning_rate": 1.906886134602184e-05,
      "loss": 1.1104,
      "step": 1759
    },
    {
      "epoch": 16.4526967285588,
      "grad_norm": 0.3869176208972931,
      "learning_rate": 1.9067585479185897e-05,
      "loss": 1.1188,
      "step": 1760
    },
    {
      "epoch": 16.462127910403773,
      "grad_norm": 0.40560054779052734,
      "learning_rate": 1.9066308781580773e-05,
      "loss": 1.0844,
      "step": 1761
    },
    {
      "epoch": 16.471559092248746,
      "grad_norm": 0.36967843770980835,
      "learning_rate": 1.9065031253323444e-05,
      "loss": 1.0949,
      "step": 1762
    },
    {
      "epoch": 16.480990274093724,
      "grad_norm": 0.3946782052516937,
      "learning_rate": 1.906375289453095e-05,
      "loss": 1.1073,
      "step": 1763
    },
    {
      "epoch": 16.490421455938698,
      "grad_norm": 0.3838956356048584,
      "learning_rate": 1.906247370532042e-05,
      "loss": 1.073,
      "step": 1764
    },
    {
      "epoch": 16.499852637783672,
      "grad_norm": 0.375808447599411,
      "learning_rate": 1.9061193685809046e-05,
      "loss": 1.1176,
      "step": 1765
    },
    {
      "epoch": 16.509283819628646,
      "grad_norm": 0.3722324073314667,
      "learning_rate": 1.9059912836114106e-05,
      "loss": 1.1114,
      "step": 1766
    },
    {
      "epoch": 16.518715001473623,
      "grad_norm": 0.385024756193161,
      "learning_rate": 1.9058631156352953e-05,
      "loss": 1.1049,
      "step": 1767
    },
    {
      "epoch": 16.528146183318597,
      "grad_norm": 0.3804810047149658,
      "learning_rate": 1.9057348646643013e-05,
      "loss": 1.1025,
      "step": 1768
    },
    {
      "epoch": 16.53757736516357,
      "grad_norm": 0.4144582450389862,
      "learning_rate": 1.9056065307101786e-05,
      "loss": 1.1291,
      "step": 1769
    },
    {
      "epoch": 16.54700854700855,
      "grad_norm": 0.41321665048599243,
      "learning_rate": 1.9054781137846857e-05,
      "loss": 1.1174,
      "step": 1770
    },
    {
      "epoch": 16.556439728853523,
      "grad_norm": 0.3655484616756439,
      "learning_rate": 1.9053496138995876e-05,
      "loss": 1.1309,
      "step": 1771
    },
    {
      "epoch": 16.565870910698496,
      "grad_norm": 0.39649951457977295,
      "learning_rate": 1.9052210310666573e-05,
      "loss": 1.0848,
      "step": 1772
    },
    {
      "epoch": 16.57530209254347,
      "grad_norm": 0.3740030527114868,
      "learning_rate": 1.9050923652976764e-05,
      "loss": 1.1543,
      "step": 1773
    },
    {
      "epoch": 16.584733274388448,
      "grad_norm": 0.3883468508720398,
      "learning_rate": 1.904963616604432e-05,
      "loss": 1.1392,
      "step": 1774
    },
    {
      "epoch": 16.594164456233422,
      "grad_norm": 0.4275365173816681,
      "learning_rate": 1.904834784998721e-05,
      "loss": 1.1027,
      "step": 1775
    },
    {
      "epoch": 16.603595638078396,
      "grad_norm": 0.36983001232147217,
      "learning_rate": 1.9047058704923466e-05,
      "loss": 1.0817,
      "step": 1776
    },
    {
      "epoch": 16.613026819923373,
      "grad_norm": 0.3724665641784668,
      "learning_rate": 1.9045768730971198e-05,
      "loss": 1.1109,
      "step": 1777
    },
    {
      "epoch": 16.622458001768347,
      "grad_norm": 0.38258498907089233,
      "learning_rate": 1.904447792824859e-05,
      "loss": 1.0936,
      "step": 1778
    },
    {
      "epoch": 16.63188918361332,
      "grad_norm": 0.3771880269050598,
      "learning_rate": 1.9043186296873914e-05,
      "loss": 1.0601,
      "step": 1779
    },
    {
      "epoch": 16.641320365458295,
      "grad_norm": 0.42243704199790955,
      "learning_rate": 1.9041893836965502e-05,
      "loss": 1.0926,
      "step": 1780
    },
    {
      "epoch": 16.650751547303273,
      "grad_norm": 0.3854147493839264,
      "learning_rate": 1.9040600548641768e-05,
      "loss": 1.1213,
      "step": 1781
    },
    {
      "epoch": 16.660182729148246,
      "grad_norm": 0.4062083959579468,
      "learning_rate": 1.9039306432021205e-05,
      "loss": 1.1272,
      "step": 1782
    },
    {
      "epoch": 16.66961391099322,
      "grad_norm": 0.38418442010879517,
      "learning_rate": 1.903801148722238e-05,
      "loss": 1.1204,
      "step": 1783
    },
    {
      "epoch": 16.679045092838198,
      "grad_norm": 0.36241844296455383,
      "learning_rate": 1.9036715714363936e-05,
      "loss": 1.1307,
      "step": 1784
    },
    {
      "epoch": 16.688476274683172,
      "grad_norm": 0.3830927312374115,
      "learning_rate": 1.903541911356459e-05,
      "loss": 1.0973,
      "step": 1785
    },
    {
      "epoch": 16.697907456528146,
      "grad_norm": 0.3786696791648865,
      "learning_rate": 1.9034121684943132e-05,
      "loss": 1.0991,
      "step": 1786
    },
    {
      "epoch": 16.70733863837312,
      "grad_norm": 0.375102698802948,
      "learning_rate": 1.903282342861844e-05,
      "loss": 1.1306,
      "step": 1787
    },
    {
      "epoch": 16.716769820218097,
      "grad_norm": 0.3963320255279541,
      "learning_rate": 1.9031524344709454e-05,
      "loss": 1.073,
      "step": 1788
    },
    {
      "epoch": 16.72620100206307,
      "grad_norm": 0.429811030626297,
      "learning_rate": 1.9030224433335197e-05,
      "loss": 1.1414,
      "step": 1789
    },
    {
      "epoch": 16.735632183908045,
      "grad_norm": 0.4060599207878113,
      "learning_rate": 1.902892369461477e-05,
      "loss": 1.1195,
      "step": 1790
    },
    {
      "epoch": 16.745063365753023,
      "grad_norm": 0.409574955701828,
      "learning_rate": 1.9027622128667338e-05,
      "loss": 1.1163,
      "step": 1791
    },
    {
      "epoch": 16.754494547597997,
      "grad_norm": 0.4069734811782837,
      "learning_rate": 1.902631973561216e-05,
      "loss": 1.1153,
      "step": 1792
    },
    {
      "epoch": 16.76392572944297,
      "grad_norm": 0.3930249810218811,
      "learning_rate": 1.9025016515568554e-05,
      "loss": 1.0758,
      "step": 1793
    },
    {
      "epoch": 16.773356911287944,
      "grad_norm": 0.4035309851169586,
      "learning_rate": 1.902371246865592e-05,
      "loss": 1.0906,
      "step": 1794
    },
    {
      "epoch": 16.782788093132922,
      "grad_norm": 0.3699020445346832,
      "learning_rate": 1.9022407594993742e-05,
      "loss": 1.1266,
      "step": 1795
    },
    {
      "epoch": 16.792219274977896,
      "grad_norm": 0.37370559573173523,
      "learning_rate": 1.9021101894701565e-05,
      "loss": 1.0906,
      "step": 1796
    },
    {
      "epoch": 16.80165045682287,
      "grad_norm": 0.3604868948459625,
      "learning_rate": 1.901979536789902e-05,
      "loss": 1.0594,
      "step": 1797
    },
    {
      "epoch": 16.811081638667847,
      "grad_norm": 0.416215181350708,
      "learning_rate": 1.9018488014705808e-05,
      "loss": 1.0694,
      "step": 1798
    },
    {
      "epoch": 16.82051282051282,
      "grad_norm": 0.3920131027698517,
      "learning_rate": 1.901717983524171e-05,
      "loss": 1.0957,
      "step": 1799
    },
    {
      "epoch": 16.829944002357795,
      "grad_norm": 0.4251244366168976,
      "learning_rate": 1.901587082962658e-05,
      "loss": 1.114,
      "step": 1800
    },
    {
      "epoch": 16.83937518420277,
      "grad_norm": 0.3818094730377197,
      "learning_rate": 1.901456099798035e-05,
      "loss": 1.1051,
      "step": 1801
    },
    {
      "epoch": 16.848806366047747,
      "grad_norm": 0.3902384638786316,
      "learning_rate": 1.9013250340423026e-05,
      "loss": 1.1241,
      "step": 1802
    },
    {
      "epoch": 16.85823754789272,
      "grad_norm": 0.3845222592353821,
      "learning_rate": 1.901193885707469e-05,
      "loss": 1.1187,
      "step": 1803
    },
    {
      "epoch": 16.867668729737694,
      "grad_norm": 0.377600759267807,
      "learning_rate": 1.90106265480555e-05,
      "loss": 1.0849,
      "step": 1804
    },
    {
      "epoch": 16.877099911582672,
      "grad_norm": 0.3625732362270355,
      "learning_rate": 1.9009313413485686e-05,
      "loss": 1.0848,
      "step": 1805
    },
    {
      "epoch": 16.886531093427646,
      "grad_norm": 0.3918282687664032,
      "learning_rate": 1.9007999453485562e-05,
      "loss": 1.1189,
      "step": 1806
    },
    {
      "epoch": 16.89596227527262,
      "grad_norm": 0.3796353340148926,
      "learning_rate": 1.900668466817551e-05,
      "loss": 1.0885,
      "step": 1807
    },
    {
      "epoch": 16.905393457117594,
      "grad_norm": 0.38562464714050293,
      "learning_rate": 1.9005369057675992e-05,
      "loss": 1.1034,
      "step": 1808
    },
    {
      "epoch": 16.91482463896257,
      "grad_norm": 0.3523655831813812,
      "learning_rate": 1.9004052622107538e-05,
      "loss": 1.1283,
      "step": 1809
    },
    {
      "epoch": 16.924255820807545,
      "grad_norm": 0.39222055673599243,
      "learning_rate": 1.9002735361590765e-05,
      "loss": 1.1194,
      "step": 1810
    },
    {
      "epoch": 16.93368700265252,
      "grad_norm": 0.4216935634613037,
      "learning_rate": 1.900141727624636e-05,
      "loss": 1.1071,
      "step": 1811
    },
    {
      "epoch": 16.943118184497497,
      "grad_norm": 0.3762516677379608,
      "learning_rate": 1.9000098366195083e-05,
      "loss": 1.0919,
      "step": 1812
    },
    {
      "epoch": 16.95254936634247,
      "grad_norm": 0.4257918894290924,
      "learning_rate": 1.8998778631557772e-05,
      "loss": 1.0819,
      "step": 1813
    },
    {
      "epoch": 16.961980548187444,
      "grad_norm": 0.38666394352912903,
      "learning_rate": 1.8997458072455345e-05,
      "loss": 1.0989,
      "step": 1814
    },
    {
      "epoch": 16.97141173003242,
      "grad_norm": 0.39274856448173523,
      "learning_rate": 1.8996136689008788e-05,
      "loss": 1.1174,
      "step": 1815
    },
    {
      "epoch": 16.980842911877396,
      "grad_norm": 0.3940792381763458,
      "learning_rate": 1.8994814481339166e-05,
      "loss": 1.1198,
      "step": 1816
    },
    {
      "epoch": 16.99027409372237,
      "grad_norm": 0.4001094102859497,
      "learning_rate": 1.8993491449567615e-05,
      "loss": 1.0992,
      "step": 1817
    },
    {
      "epoch": 16.999705275567344,
      "grad_norm": 0.39141038060188293,
      "learning_rate": 1.899216759381536e-05,
      "loss": 1.1638,
      "step": 1818
    },
    {
      "epoch": 17.0,
      "grad_norm": 2.5193207263946533,
      "learning_rate": 1.8990842914203683e-05,
      "loss": 0.3707,
      "step": 1819
    },
    {
      "epoch": 17.009431181844974,
      "grad_norm": 0.3967444896697998,
      "learning_rate": 1.8989517410853956e-05,
      "loss": 1.1051,
      "step": 1820
    },
    {
      "epoch": 17.01886236368995,
      "grad_norm": 0.3734991252422333,
      "learning_rate": 1.8988191083887617e-05,
      "loss": 1.129,
      "step": 1821
    },
    {
      "epoch": 17.028293545534925,
      "grad_norm": 0.3689441978931427,
      "learning_rate": 1.8986863933426193e-05,
      "loss": 1.1284,
      "step": 1822
    },
    {
      "epoch": 17.0377247273799,
      "grad_norm": 0.37835198640823364,
      "learning_rate": 1.8985535959591265e-05,
      "loss": 1.1338,
      "step": 1823
    },
    {
      "epoch": 17.047155909224873,
      "grad_norm": 0.37626099586486816,
      "learning_rate": 1.898420716250451e-05,
      "loss": 1.1334,
      "step": 1824
    },
    {
      "epoch": 17.05658709106985,
      "grad_norm": 0.3732674717903137,
      "learning_rate": 1.8982877542287665e-05,
      "loss": 1.1067,
      "step": 1825
    },
    {
      "epoch": 17.066018272914825,
      "grad_norm": 0.37567317485809326,
      "learning_rate": 1.898154709906256e-05,
      "loss": 1.0902,
      "step": 1826
    },
    {
      "epoch": 17.0754494547598,
      "grad_norm": 0.3912375867366791,
      "learning_rate": 1.8980215832951078e-05,
      "loss": 1.0851,
      "step": 1827
    },
    {
      "epoch": 17.084880636604776,
      "grad_norm": 0.41180604696273804,
      "learning_rate": 1.8978883744075197e-05,
      "loss": 1.1138,
      "step": 1828
    },
    {
      "epoch": 17.09431181844975,
      "grad_norm": 0.4251348674297333,
      "learning_rate": 1.897755083255696e-05,
      "loss": 1.1318,
      "step": 1829
    },
    {
      "epoch": 17.103743000294724,
      "grad_norm": 0.4134153723716736,
      "learning_rate": 1.8976217098518486e-05,
      "loss": 1.1034,
      "step": 1830
    },
    {
      "epoch": 17.113174182139698,
      "grad_norm": 0.39914804697036743,
      "learning_rate": 1.8974882542081975e-05,
      "loss": 1.1255,
      "step": 1831
    },
    {
      "epoch": 17.122605363984675,
      "grad_norm": 0.4011973440647125,
      "learning_rate": 1.8973547163369693e-05,
      "loss": 1.1749,
      "step": 1832
    },
    {
      "epoch": 17.13203654582965,
      "grad_norm": 0.3927399516105652,
      "learning_rate": 1.8972210962503994e-05,
      "loss": 1.1663,
      "step": 1833
    },
    {
      "epoch": 17.141467727674623,
      "grad_norm": 0.4091062545776367,
      "learning_rate": 1.8970873939607293e-05,
      "loss": 1.1199,
      "step": 1834
    },
    {
      "epoch": 17.1508989095196,
      "grad_norm": 0.37669873237609863,
      "learning_rate": 1.8969536094802096e-05,
      "loss": 1.1299,
      "step": 1835
    },
    {
      "epoch": 17.160330091364575,
      "grad_norm": 0.366487979888916,
      "learning_rate": 1.896819742821097e-05,
      "loss": 1.0919,
      "step": 1836
    },
    {
      "epoch": 17.16976127320955,
      "grad_norm": 0.40362411737442017,
      "learning_rate": 1.8966857939956567e-05,
      "loss": 1.1148,
      "step": 1837
    },
    {
      "epoch": 17.179192455054523,
      "grad_norm": 0.36310330033302307,
      "learning_rate": 1.8965517630161603e-05,
      "loss": 1.1077,
      "step": 1838
    },
    {
      "epoch": 17.1886236368995,
      "grad_norm": 0.4232923984527588,
      "learning_rate": 1.8964176498948886e-05,
      "loss": 1.0857,
      "step": 1839
    },
    {
      "epoch": 17.198054818744474,
      "grad_norm": 0.3626581132411957,
      "learning_rate": 1.8962834546441284e-05,
      "loss": 1.1387,
      "step": 1840
    },
    {
      "epoch": 17.207486000589448,
      "grad_norm": 0.40877366065979004,
      "learning_rate": 1.8961491772761748e-05,
      "loss": 1.1287,
      "step": 1841
    },
    {
      "epoch": 17.216917182434425,
      "grad_norm": 0.37382233142852783,
      "learning_rate": 1.89601481780333e-05,
      "loss": 1.0795,
      "step": 1842
    },
    {
      "epoch": 17.2263483642794,
      "grad_norm": 0.4272134602069855,
      "learning_rate": 1.895880376237905e-05,
      "loss": 1.0324,
      "step": 1843
    },
    {
      "epoch": 17.235779546124373,
      "grad_norm": 0.39328494668006897,
      "learning_rate": 1.895745852592216e-05,
      "loss": 1.1318,
      "step": 1844
    },
    {
      "epoch": 17.245210727969347,
      "grad_norm": 0.38368162512779236,
      "learning_rate": 1.8956112468785884e-05,
      "loss": 1.0963,
      "step": 1845
    },
    {
      "epoch": 17.254641909814325,
      "grad_norm": 0.39169520139694214,
      "learning_rate": 1.895476559109355e-05,
      "loss": 1.0638,
      "step": 1846
    },
    {
      "epoch": 17.2640730916593,
      "grad_norm": 0.4040938913822174,
      "learning_rate": 1.8953417892968557e-05,
      "loss": 1.0837,
      "step": 1847
    },
    {
      "epoch": 17.273504273504273,
      "grad_norm": 0.40922361612319946,
      "learning_rate": 1.895206937453438e-05,
      "loss": 1.0694,
      "step": 1848
    },
    {
      "epoch": 17.28293545534925,
      "grad_norm": 0.3885379433631897,
      "learning_rate": 1.8950720035914572e-05,
      "loss": 1.0907,
      "step": 1849
    },
    {
      "epoch": 17.292366637194224,
      "grad_norm": 0.3780617117881775,
      "learning_rate": 1.894936987723276e-05,
      "loss": 1.0994,
      "step": 1850
    },
    {
      "epoch": 17.301797819039198,
      "grad_norm": 0.37475699186325073,
      "learning_rate": 1.894801889861264e-05,
      "loss": 1.1526,
      "step": 1851
    },
    {
      "epoch": 17.311229000884172,
      "grad_norm": 0.36486202478408813,
      "learning_rate": 1.894666710017799e-05,
      "loss": 1.1395,
      "step": 1852
    },
    {
      "epoch": 17.32066018272915,
      "grad_norm": 0.40777257084846497,
      "learning_rate": 1.8945314482052662e-05,
      "loss": 1.0658,
      "step": 1853
    },
    {
      "epoch": 17.330091364574123,
      "grad_norm": 0.4614691138267517,
      "learning_rate": 1.8943961044360583e-05,
      "loss": 1.118,
      "step": 1854
    },
    {
      "epoch": 17.339522546419097,
      "grad_norm": 0.41212040185928345,
      "learning_rate": 1.8942606787225754e-05,
      "loss": 1.0652,
      "step": 1855
    },
    {
      "epoch": 17.348953728264075,
      "grad_norm": 0.37825626134872437,
      "learning_rate": 1.894125171077225e-05,
      "loss": 1.1332,
      "step": 1856
    },
    {
      "epoch": 17.35838491010905,
      "grad_norm": 0.3684513568878174,
      "learning_rate": 1.893989581512423e-05,
      "loss": 1.1048,
      "step": 1857
    },
    {
      "epoch": 17.367816091954023,
      "grad_norm": 0.39837008714675903,
      "learning_rate": 1.8938539100405912e-05,
      "loss": 1.1411,
      "step": 1858
    },
    {
      "epoch": 17.377247273798996,
      "grad_norm": 0.42889171838760376,
      "learning_rate": 1.8937181566741597e-05,
      "loss": 1.1123,
      "step": 1859
    },
    {
      "epoch": 17.386678455643974,
      "grad_norm": 0.3731585443019867,
      "learning_rate": 1.893582321425567e-05,
      "loss": 1.0933,
      "step": 1860
    },
    {
      "epoch": 17.396109637488948,
      "grad_norm": 0.42729198932647705,
      "learning_rate": 1.893446404307258e-05,
      "loss": 1.0856,
      "step": 1861
    },
    {
      "epoch": 17.405540819333922,
      "grad_norm": 0.36377912759780884,
      "learning_rate": 1.8933104053316846e-05,
      "loss": 1.0948,
      "step": 1862
    },
    {
      "epoch": 17.4149720011789,
      "grad_norm": 0.3903421461582184,
      "learning_rate": 1.8931743245113078e-05,
      "loss": 1.0952,
      "step": 1863
    },
    {
      "epoch": 17.424403183023873,
      "grad_norm": 0.3953242599964142,
      "learning_rate": 1.8930381618585955e-05,
      "loss": 1.1003,
      "step": 1864
    },
    {
      "epoch": 17.433834364868847,
      "grad_norm": 0.3920479118824005,
      "learning_rate": 1.892901917386022e-05,
      "loss": 1.1406,
      "step": 1865
    },
    {
      "epoch": 17.44326554671382,
      "grad_norm": 0.39093855023384094,
      "learning_rate": 1.892765591106071e-05,
      "loss": 1.09,
      "step": 1866
    },
    {
      "epoch": 17.4526967285588,
      "grad_norm": 0.4337351322174072,
      "learning_rate": 1.8926291830312314e-05,
      "loss": 1.0892,
      "step": 1867
    },
    {
      "epoch": 17.462127910403773,
      "grad_norm": 0.40682485699653625,
      "learning_rate": 1.8924926931740017e-05,
      "loss": 1.0851,
      "step": 1868
    },
    {
      "epoch": 17.471559092248746,
      "grad_norm": 0.39602988958358765,
      "learning_rate": 1.8923561215468874e-05,
      "loss": 1.1288,
      "step": 1869
    },
    {
      "epoch": 17.480990274093724,
      "grad_norm": 0.40243467688560486,
      "learning_rate": 1.8922194681624e-05,
      "loss": 1.147,
      "step": 1870
    },
    {
      "epoch": 17.490421455938698,
      "grad_norm": 0.36493271589279175,
      "learning_rate": 1.892082733033061e-05,
      "loss": 1.121,
      "step": 1871
    },
    {
      "epoch": 17.499852637783672,
      "grad_norm": 0.4075433313846588,
      "learning_rate": 1.891945916171397e-05,
      "loss": 1.0672,
      "step": 1872
    },
    {
      "epoch": 17.509283819628646,
      "grad_norm": 0.408877432346344,
      "learning_rate": 1.8918090175899433e-05,
      "loss": 1.0574,
      "step": 1873
    },
    {
      "epoch": 17.518715001473623,
      "grad_norm": 0.39950621128082275,
      "learning_rate": 1.8916720373012425e-05,
      "loss": 1.1032,
      "step": 1874
    },
    {
      "epoch": 17.528146183318597,
      "grad_norm": 0.3763718008995056,
      "learning_rate": 1.8915349753178453e-05,
      "loss": 1.1206,
      "step": 1875
    },
    {
      "epoch": 17.53757736516357,
      "grad_norm": 0.38191652297973633,
      "learning_rate": 1.8913978316523085e-05,
      "loss": 1.0779,
      "step": 1876
    },
    {
      "epoch": 17.54700854700855,
      "grad_norm": 0.41770556569099426,
      "learning_rate": 1.8912606063171977e-05,
      "loss": 1.0723,
      "step": 1877
    },
    {
      "epoch": 17.556439728853523,
      "grad_norm": 0.37678828835487366,
      "learning_rate": 1.891123299325085e-05,
      "loss": 1.138,
      "step": 1878
    },
    {
      "epoch": 17.565870910698496,
      "grad_norm": 0.3815603256225586,
      "learning_rate": 1.8909859106885507e-05,
      "loss": 1.0809,
      "step": 1879
    },
    {
      "epoch": 17.57530209254347,
      "grad_norm": 0.3804151713848114,
      "learning_rate": 1.8908484404201822e-05,
      "loss": 1.1127,
      "step": 1880
    },
    {
      "epoch": 17.584733274388448,
      "grad_norm": 0.3559873402118683,
      "learning_rate": 1.8907108885325747e-05,
      "loss": 1.112,
      "step": 1881
    },
    {
      "epoch": 17.594164456233422,
      "grad_norm": 0.4047723412513733,
      "learning_rate": 1.8905732550383305e-05,
      "loss": 1.0678,
      "step": 1882
    },
    {
      "epoch": 17.603595638078396,
      "grad_norm": 0.3839244246482849,
      "learning_rate": 1.890435539950059e-05,
      "loss": 1.1054,
      "step": 1883
    },
    {
      "epoch": 17.613026819923373,
      "grad_norm": 0.3785078525543213,
      "learning_rate": 1.8902977432803786e-05,
      "loss": 1.1081,
      "step": 1884
    },
    {
      "epoch": 17.622458001768347,
      "grad_norm": 0.41827264428138733,
      "learning_rate": 1.8901598650419134e-05,
      "loss": 1.1004,
      "step": 1885
    },
    {
      "epoch": 17.63188918361332,
      "grad_norm": 0.43419158458709717,
      "learning_rate": 1.8900219052472964e-05,
      "loss": 1.1106,
      "step": 1886
    },
    {
      "epoch": 17.641320365458295,
      "grad_norm": 0.44771909713745117,
      "learning_rate": 1.8898838639091668e-05,
      "loss": 1.1113,
      "step": 1887
    },
    {
      "epoch": 17.650751547303273,
      "grad_norm": 0.4107382893562317,
      "learning_rate": 1.8897457410401723e-05,
      "loss": 1.1523,
      "step": 1888
    },
    {
      "epoch": 17.660182729148246,
      "grad_norm": 0.43263089656829834,
      "learning_rate": 1.8896075366529674e-05,
      "loss": 1.1195,
      "step": 1889
    },
    {
      "epoch": 17.66961391099322,
      "grad_norm": 0.3817659020423889,
      "learning_rate": 1.8894692507602148e-05,
      "loss": 1.111,
      "step": 1890
    },
    {
      "epoch": 17.679045092838198,
      "grad_norm": 0.369667649269104,
      "learning_rate": 1.889330883374584e-05,
      "loss": 1.1087,
      "step": 1891
    },
    {
      "epoch": 17.688476274683172,
      "grad_norm": 0.42971980571746826,
      "learning_rate": 1.8891924345087518e-05,
      "loss": 1.1086,
      "step": 1892
    },
    {
      "epoch": 17.697907456528146,
      "grad_norm": 0.4143795669078827,
      "learning_rate": 1.889053904175403e-05,
      "loss": 1.1169,
      "step": 1893
    },
    {
      "epoch": 17.70733863837312,
      "grad_norm": 0.4340727925300598,
      "learning_rate": 1.88891529238723e-05,
      "loss": 1.1226,
      "step": 1894
    },
    {
      "epoch": 17.716769820218097,
      "grad_norm": 0.4045574367046356,
      "learning_rate": 1.888776599156932e-05,
      "loss": 1.1318,
      "step": 1895
    },
    {
      "epoch": 17.72620100206307,
      "grad_norm": 0.42418622970581055,
      "learning_rate": 1.8886378244972166e-05,
      "loss": 1.1191,
      "step": 1896
    },
    {
      "epoch": 17.735632183908045,
      "grad_norm": 0.4141570031642914,
      "learning_rate": 1.8884989684207976e-05,
      "loss": 1.0976,
      "step": 1897
    },
    {
      "epoch": 17.745063365753023,
      "grad_norm": 0.4098469316959381,
      "learning_rate": 1.8883600309403975e-05,
      "loss": 1.1293,
      "step": 1898
    },
    {
      "epoch": 17.754494547597997,
      "grad_norm": 0.3718825578689575,
      "learning_rate": 1.8882210120687454e-05,
      "loss": 1.0924,
      "step": 1899
    },
    {
      "epoch": 17.76392572944297,
      "grad_norm": 0.3887348175048828,
      "learning_rate": 1.888081911818578e-05,
      "loss": 1.0979,
      "step": 1900
    },
    {
      "epoch": 17.773356911287944,
      "grad_norm": 0.41047224402427673,
      "learning_rate": 1.88794273020264e-05,
      "loss": 1.0556,
      "step": 1901
    },
    {
      "epoch": 17.782788093132922,
      "grad_norm": 0.36983534693717957,
      "learning_rate": 1.887803467233683e-05,
      "loss": 1.1345,
      "step": 1902
    },
    {
      "epoch": 17.792219274977896,
      "grad_norm": 0.38718295097351074,
      "learning_rate": 1.8876641229244663e-05,
      "loss": 1.0573,
      "step": 1903
    },
    {
      "epoch": 17.80165045682287,
      "grad_norm": 0.4065624177455902,
      "learning_rate": 1.8875246972877565e-05,
      "loss": 1.0907,
      "step": 1904
    },
    {
      "epoch": 17.811081638667847,
      "grad_norm": 0.40605252981185913,
      "learning_rate": 1.887385190336328e-05,
      "loss": 1.1317,
      "step": 1905
    },
    {
      "epoch": 17.82051282051282,
      "grad_norm": 0.3820152282714844,
      "learning_rate": 1.887245602082962e-05,
      "loss": 1.0786,
      "step": 1906
    },
    {
      "epoch": 17.829944002357795,
      "grad_norm": 0.44013628363609314,
      "learning_rate": 1.8871059325404472e-05,
      "loss": 1.1188,
      "step": 1907
    },
    {
      "epoch": 17.83937518420277,
      "grad_norm": 0.4202350676059723,
      "learning_rate": 1.8869661817215812e-05,
      "loss": 1.1079,
      "step": 1908
    },
    {
      "epoch": 17.848806366047747,
      "grad_norm": 0.4212225079536438,
      "learning_rate": 1.8868263496391667e-05,
      "loss": 1.1011,
      "step": 1909
    },
    {
      "epoch": 17.85823754789272,
      "grad_norm": 0.39752131700515747,
      "learning_rate": 1.886686436306016e-05,
      "loss": 1.1096,
      "step": 1910
    },
    {
      "epoch": 17.867668729737694,
      "grad_norm": 0.4106908440589905,
      "learning_rate": 1.8865464417349477e-05,
      "loss": 1.067,
      "step": 1911
    },
    {
      "epoch": 17.877099911582672,
      "grad_norm": 0.37951692938804626,
      "learning_rate": 1.8864063659387875e-05,
      "loss": 1.0807,
      "step": 1912
    },
    {
      "epoch": 17.886531093427646,
      "grad_norm": 0.39991676807403564,
      "learning_rate": 1.8862662089303698e-05,
      "loss": 1.0953,
      "step": 1913
    },
    {
      "epoch": 17.89596227527262,
      "grad_norm": 0.40608394145965576,
      "learning_rate": 1.886125970722535e-05,
      "loss": 1.0956,
      "step": 1914
    },
    {
      "epoch": 17.905393457117594,
      "grad_norm": 0.44880953431129456,
      "learning_rate": 1.8859856513281327e-05,
      "loss": 1.0681,
      "step": 1915
    },
    {
      "epoch": 17.91482463896257,
      "grad_norm": 0.3901723027229309,
      "learning_rate": 1.885845250760018e-05,
      "loss": 1.0751,
      "step": 1916
    },
    {
      "epoch": 17.924255820807545,
      "grad_norm": 0.41937196254730225,
      "learning_rate": 1.8857047690310545e-05,
      "loss": 1.1185,
      "step": 1917
    },
    {
      "epoch": 17.93368700265252,
      "grad_norm": 0.41733652353286743,
      "learning_rate": 1.8855642061541136e-05,
      "loss": 1.1396,
      "step": 1918
    },
    {
      "epoch": 17.943118184497497,
      "grad_norm": 0.38152509927749634,
      "learning_rate": 1.8854235621420733e-05,
      "loss": 1.0839,
      "step": 1919
    },
    {
      "epoch": 17.95254936634247,
      "grad_norm": 0.41039496660232544,
      "learning_rate": 1.885282837007819e-05,
      "loss": 1.0575,
      "step": 1920
    },
    {
      "epoch": 17.961980548187444,
      "grad_norm": 0.4155566096305847,
      "learning_rate": 1.885142030764245e-05,
      "loss": 1.0833,
      "step": 1921
    },
    {
      "epoch": 17.97141173003242,
      "grad_norm": 0.38011470437049866,
      "learning_rate": 1.8850011434242507e-05,
      "loss": 1.0783,
      "step": 1922
    },
    {
      "epoch": 17.980842911877396,
      "grad_norm": 0.39162081480026245,
      "learning_rate": 1.8848601750007448e-05,
      "loss": 1.1269,
      "step": 1923
    },
    {
      "epoch": 17.99027409372237,
      "grad_norm": 0.436692476272583,
      "learning_rate": 1.884719125506642e-05,
      "loss": 1.0548,
      "step": 1924
    },
    {
      "epoch": 17.999705275567344,
      "grad_norm": 0.38621804118156433,
      "learning_rate": 1.8845779949548665e-05,
      "loss": 1.0969,
      "step": 1925
    },
    {
      "epoch": 18.0,
      "grad_norm": 4.647227764129639,
      "learning_rate": 1.8844367833583475e-05,
      "loss": 0.2797,
      "step": 1926
    },
    {
      "epoch": 18.009431181844974,
      "grad_norm": 0.3769613802433014,
      "learning_rate": 1.8842954907300236e-05,
      "loss": 1.1101,
      "step": 1927
    },
    {
      "epoch": 18.01886236368995,
      "grad_norm": 0.375202476978302,
      "learning_rate": 1.8841541170828397e-05,
      "loss": 1.1074,
      "step": 1928
    },
    {
      "epoch": 18.028293545534925,
      "grad_norm": 0.41251340508461,
      "learning_rate": 1.884012662429748e-05,
      "loss": 1.1426,
      "step": 1929
    },
    {
      "epoch": 18.0377247273799,
      "grad_norm": 0.4038643538951874,
      "learning_rate": 1.8838711267837086e-05,
      "loss": 1.0912,
      "step": 1930
    },
    {
      "epoch": 18.047155909224873,
      "grad_norm": 0.4197443127632141,
      "learning_rate": 1.8837295101576895e-05,
      "loss": 1.1165,
      "step": 1931
    },
    {
      "epoch": 18.05658709106985,
      "grad_norm": 0.40973007678985596,
      "learning_rate": 1.8835878125646655e-05,
      "loss": 1.1466,
      "step": 1932
    },
    {
      "epoch": 18.066018272914825,
      "grad_norm": 0.43991538882255554,
      "learning_rate": 1.8834460340176178e-05,
      "loss": 1.0794,
      "step": 1933
    },
    {
      "epoch": 18.0754494547598,
      "grad_norm": 0.3827593922615051,
      "learning_rate": 1.8833041745295376e-05,
      "loss": 1.1235,
      "step": 1934
    },
    {
      "epoch": 18.084880636604776,
      "grad_norm": 0.3963157832622528,
      "learning_rate": 1.8831622341134213e-05,
      "loss": 1.0748,
      "step": 1935
    },
    {
      "epoch": 18.09431181844975,
      "grad_norm": 0.3911871612071991,
      "learning_rate": 1.8830202127822734e-05,
      "loss": 1.0922,
      "step": 1936
    },
    {
      "epoch": 18.103743000294724,
      "grad_norm": 0.3769371807575226,
      "learning_rate": 1.882878110549106e-05,
      "loss": 1.0762,
      "step": 1937
    },
    {
      "epoch": 18.113174182139698,
      "grad_norm": 0.4227392375469208,
      "learning_rate": 1.8827359274269383e-05,
      "loss": 1.0891,
      "step": 1938
    },
    {
      "epoch": 18.122605363984675,
      "grad_norm": 0.38572362065315247,
      "learning_rate": 1.882593663428797e-05,
      "loss": 1.1512,
      "step": 1939
    },
    {
      "epoch": 18.13203654582965,
      "grad_norm": 0.4000500738620758,
      "learning_rate": 1.8824513185677165e-05,
      "loss": 1.1207,
      "step": 1940
    },
    {
      "epoch": 18.141467727674623,
      "grad_norm": 0.3833293914794922,
      "learning_rate": 1.8823088928567387e-05,
      "loss": 1.1473,
      "step": 1941
    },
    {
      "epoch": 18.1508989095196,
      "grad_norm": 0.3964836001396179,
      "learning_rate": 1.882166386308912e-05,
      "loss": 1.115,
      "step": 1942
    },
    {
      "epoch": 18.160330091364575,
      "grad_norm": 0.39940041303634644,
      "learning_rate": 1.882023798937293e-05,
      "loss": 1.0788,
      "step": 1943
    },
    {
      "epoch": 18.16976127320955,
      "grad_norm": 0.3954622149467468,
      "learning_rate": 1.8818811307549457e-05,
      "loss": 1.1066,
      "step": 1944
    },
    {
      "epoch": 18.179192455054523,
      "grad_norm": 0.4015710949897766,
      "learning_rate": 1.881738381774941e-05,
      "loss": 1.1328,
      "step": 1945
    },
    {
      "epoch": 18.1886236368995,
      "grad_norm": 0.4063354432582855,
      "learning_rate": 1.8815955520103574e-05,
      "loss": 1.0925,
      "step": 1946
    },
    {
      "epoch": 18.198054818744474,
      "grad_norm": 0.37912020087242126,
      "learning_rate": 1.8814526414742817e-05,
      "loss": 1.0798,
      "step": 1947
    },
    {
      "epoch": 18.207486000589448,
      "grad_norm": 0.37466633319854736,
      "learning_rate": 1.8813096501798066e-05,
      "loss": 1.0811,
      "step": 1948
    },
    {
      "epoch": 18.216917182434425,
      "grad_norm": 0.4162711799144745,
      "learning_rate": 1.8811665781400327e-05,
      "loss": 1.1186,
      "step": 1949
    },
    {
      "epoch": 18.2263483642794,
      "grad_norm": 0.42572611570358276,
      "learning_rate": 1.8810234253680694e-05,
      "loss": 1.0893,
      "step": 1950
    },
    {
      "epoch": 18.235779546124373,
      "grad_norm": 0.3866846561431885,
      "learning_rate": 1.880880191877031e-05,
      "loss": 1.0848,
      "step": 1951
    },
    {
      "epoch": 18.245210727969347,
      "grad_norm": 0.3661072850227356,
      "learning_rate": 1.8807368776800412e-05,
      "loss": 1.1451,
      "step": 1952
    },
    {
      "epoch": 18.254641909814325,
      "grad_norm": 0.3872124254703522,
      "learning_rate": 1.88059348279023e-05,
      "loss": 1.1087,
      "step": 1953
    },
    {
      "epoch": 18.2640730916593,
      "grad_norm": 0.4395657181739807,
      "learning_rate": 1.880450007220736e-05,
      "loss": 1.0745,
      "step": 1954
    },
    {
      "epoch": 18.273504273504273,
      "grad_norm": 0.3996078670024872,
      "learning_rate": 1.880306450984703e-05,
      "loss": 1.1221,
      "step": 1955
    },
    {
      "epoch": 18.28293545534925,
      "grad_norm": 0.3953399956226349,
      "learning_rate": 1.880162814095285e-05,
      "loss": 1.0522,
      "step": 1956
    },
    {
      "epoch": 18.292366637194224,
      "grad_norm": 0.4039020538330078,
      "learning_rate": 1.8800190965656414e-05,
      "loss": 1.0852,
      "step": 1957
    },
    {
      "epoch": 18.301797819039198,
      "grad_norm": 0.3840382397174835,
      "learning_rate": 1.8798752984089393e-05,
      "loss": 1.0768,
      "step": 1958
    },
    {
      "epoch": 18.311229000884172,
      "grad_norm": 0.4011639952659607,
      "learning_rate": 1.8797314196383537e-05,
      "loss": 1.0514,
      "step": 1959
    },
    {
      "epoch": 18.32066018272915,
      "grad_norm": 0.41725286841392517,
      "learning_rate": 1.8795874602670667e-05,
      "loss": 1.1039,
      "step": 1960
    },
    {
      "epoch": 18.330091364574123,
      "grad_norm": 0.41775447130203247,
      "learning_rate": 1.8794434203082675e-05,
      "loss": 1.0876,
      "step": 1961
    },
    {
      "epoch": 18.339522546419097,
      "grad_norm": 0.3888823091983795,
      "learning_rate": 1.879299299775154e-05,
      "loss": 1.0825,
      "step": 1962
    },
    {
      "epoch": 18.348953728264075,
      "grad_norm": 0.38429516553878784,
      "learning_rate": 1.879155098680929e-05,
      "loss": 1.1122,
      "step": 1963
    },
    {
      "epoch": 18.35838491010905,
      "grad_norm": 0.3932879567146301,
      "learning_rate": 1.8790108170388053e-05,
      "loss": 1.094,
      "step": 1964
    },
    {
      "epoch": 18.367816091954023,
      "grad_norm": 0.3659825026988983,
      "learning_rate": 1.8788664548620014e-05,
      "loss": 1.1093,
      "step": 1965
    },
    {
      "epoch": 18.377247273798996,
      "grad_norm": 0.41030624508857727,
      "learning_rate": 1.8787220121637438e-05,
      "loss": 1.0911,
      "step": 1966
    },
    {
      "epoch": 18.386678455643974,
      "grad_norm": 0.42341816425323486,
      "learning_rate": 1.8785774889572664e-05,
      "loss": 1.101,
      "step": 1967
    },
    {
      "epoch": 18.396109637488948,
      "grad_norm": 0.3987632095813751,
      "learning_rate": 1.8784328852558105e-05,
      "loss": 1.1473,
      "step": 1968
    },
    {
      "epoch": 18.405540819333922,
      "grad_norm": 0.41668960452079773,
      "learning_rate": 1.8782882010726246e-05,
      "loss": 1.1126,
      "step": 1969
    },
    {
      "epoch": 18.4149720011789,
      "grad_norm": 0.42664262652397156,
      "learning_rate": 1.8781434364209638e-05,
      "loss": 1.1028,
      "step": 1970
    },
    {
      "epoch": 18.424403183023873,
      "grad_norm": 0.3845374584197998,
      "learning_rate": 1.8779985913140927e-05,
      "loss": 1.1034,
      "step": 1971
    },
    {
      "epoch": 18.433834364868847,
      "grad_norm": 0.4076400101184845,
      "learning_rate": 1.8778536657652808e-05,
      "loss": 1.0773,
      "step": 1972
    },
    {
      "epoch": 18.44326554671382,
      "grad_norm": 0.39931488037109375,
      "learning_rate": 1.8777086597878072e-05,
      "loss": 1.124,
      "step": 1973
    },
    {
      "epoch": 18.4526967285588,
      "grad_norm": 0.41329488158226013,
      "learning_rate": 1.8775635733949566e-05,
      "loss": 1.1072,
      "step": 1974
    },
    {
      "epoch": 18.462127910403773,
      "grad_norm": 0.4499553143978119,
      "learning_rate": 1.8774184066000218e-05,
      "loss": 1.0538,
      "step": 1975
    },
    {
      "epoch": 18.471559092248746,
      "grad_norm": 0.39240801334381104,
      "learning_rate": 1.877273159416303e-05,
      "loss": 1.1073,
      "step": 1976
    },
    {
      "epoch": 18.480990274093724,
      "grad_norm": 0.3948584794998169,
      "learning_rate": 1.877127831857108e-05,
      "loss": 1.0598,
      "step": 1977
    },
    {
      "epoch": 18.490421455938698,
      "grad_norm": 0.4006405770778656,
      "learning_rate": 1.876982423935751e-05,
      "loss": 1.0866,
      "step": 1978
    },
    {
      "epoch": 18.499852637783672,
      "grad_norm": 0.38885340094566345,
      "learning_rate": 1.8768369356655555e-05,
      "loss": 1.0642,
      "step": 1979
    },
    {
      "epoch": 18.509283819628646,
      "grad_norm": 0.42861810326576233,
      "learning_rate": 1.8766913670598495e-05,
      "loss": 1.0722,
      "step": 1980
    },
    {
      "epoch": 18.518715001473623,
      "grad_norm": 0.4400515854358673,
      "learning_rate": 1.876545718131971e-05,
      "loss": 1.1028,
      "step": 1981
    },
    {
      "epoch": 18.528146183318597,
      "grad_norm": 0.3808777928352356,
      "learning_rate": 1.8763999888952637e-05,
      "loss": 1.0938,
      "step": 1982
    },
    {
      "epoch": 18.53757736516357,
      "grad_norm": 0.45860111713409424,
      "learning_rate": 1.8762541793630797e-05,
      "loss": 1.0983,
      "step": 1983
    },
    {
      "epoch": 18.54700854700855,
      "grad_norm": 0.469788134098053,
      "learning_rate": 1.876108289548778e-05,
      "loss": 1.0652,
      "step": 1984
    },
    {
      "epoch": 18.556439728853523,
      "grad_norm": 0.4049084186553955,
      "learning_rate": 1.8759623194657247e-05,
      "loss": 1.0831,
      "step": 1985
    },
    {
      "epoch": 18.565870910698496,
      "grad_norm": 0.3912694454193115,
      "learning_rate": 1.8758162691272938e-05,
      "loss": 1.0911,
      "step": 1986
    },
    {
      "epoch": 18.57530209254347,
      "grad_norm": 0.3761162757873535,
      "learning_rate": 1.875670138546866e-05,
      "loss": 1.1105,
      "step": 1987
    },
    {
      "epoch": 18.584733274388448,
      "grad_norm": 0.377590537071228,
      "learning_rate": 1.8755239277378306e-05,
      "loss": 1.1115,
      "step": 1988
    },
    {
      "epoch": 18.594164456233422,
      "grad_norm": 0.3837859034538269,
      "learning_rate": 1.8753776367135824e-05,
      "loss": 1.1175,
      "step": 1989
    },
    {
      "epoch": 18.603595638078396,
      "grad_norm": 0.3629072904586792,
      "learning_rate": 1.8752312654875245e-05,
      "loss": 1.1234,
      "step": 1990
    },
    {
      "epoch": 18.613026819923373,
      "grad_norm": 0.406203955411911,
      "learning_rate": 1.8750848140730683e-05,
      "loss": 1.1192,
      "step": 1991
    },
    {
      "epoch": 18.622458001768347,
      "grad_norm": 0.3855498433113098,
      "learning_rate": 1.874938282483631e-05,
      "loss": 1.0903,
      "step": 1992
    },
    {
      "epoch": 18.63188918361332,
      "grad_norm": 0.4099408686161041,
      "learning_rate": 1.874791670732638e-05,
      "loss": 1.1196,
      "step": 1993
    },
    {
      "epoch": 18.641320365458295,
      "grad_norm": 0.3933411240577698,
      "learning_rate": 1.874644978833522e-05,
      "loss": 1.0899,
      "step": 1994
    },
    {
      "epoch": 18.650751547303273,
      "grad_norm": 0.41544055938720703,
      "learning_rate": 1.8744982067997225e-05,
      "loss": 1.1114,
      "step": 1995
    },
    {
      "epoch": 18.660182729148246,
      "grad_norm": 0.39959269762039185,
      "learning_rate": 1.8743513546446867e-05,
      "loss": 1.1292,
      "step": 1996
    },
    {
      "epoch": 18.66961391099322,
      "grad_norm": 0.41302189230918884,
      "learning_rate": 1.874204422381869e-05,
      "loss": 1.1295,
      "step": 1997
    },
    {
      "epoch": 18.679045092838198,
      "grad_norm": 0.38041916489601135,
      "learning_rate": 1.874057410024732e-05,
      "loss": 1.085,
      "step": 1998
    },
    {
      "epoch": 18.688476274683172,
      "grad_norm": 0.39195966720581055,
      "learning_rate": 1.8739103175867438e-05,
      "loss": 1.0932,
      "step": 1999
    },
    {
      "epoch": 18.697907456528146,
      "grad_norm": 0.3747360110282898,
      "learning_rate": 1.873763145081382e-05,
      "loss": 1.1116,
      "step": 2000
    },
    {
      "epoch": 18.70733863837312,
      "grad_norm": 0.4162326455116272,
      "learning_rate": 1.8736158925221302e-05,
      "loss": 1.0641,
      "step": 2001
    },
    {
      "epoch": 18.716769820218097,
      "grad_norm": 0.3775438666343689,
      "learning_rate": 1.8734685599224793e-05,
      "loss": 1.0951,
      "step": 2002
    },
    {
      "epoch": 18.72620100206307,
      "grad_norm": 0.380603551864624,
      "learning_rate": 1.8733211472959283e-05,
      "loss": 1.0749,
      "step": 2003
    },
    {
      "epoch": 18.735632183908045,
      "grad_norm": 0.4181482791900635,
      "learning_rate": 1.8731736546559828e-05,
      "loss": 1.0972,
      "step": 2004
    },
    {
      "epoch": 18.745063365753023,
      "grad_norm": 0.3802022635936737,
      "learning_rate": 1.8730260820161562e-05,
      "loss": 1.0934,
      "step": 2005
    },
    {
      "epoch": 18.754494547597997,
      "grad_norm": 0.4536462426185608,
      "learning_rate": 1.872878429389969e-05,
      "loss": 1.1644,
      "step": 2006
    },
    {
      "epoch": 18.76392572944297,
      "grad_norm": 0.39201778173446655,
      "learning_rate": 1.8727306967909488e-05,
      "loss": 1.1002,
      "step": 2007
    },
    {
      "epoch": 18.773356911287944,
      "grad_norm": 0.40757158398628235,
      "learning_rate": 1.8725828842326313e-05,
      "loss": 1.0594,
      "step": 2008
    },
    {
      "epoch": 18.782788093132922,
      "grad_norm": 0.4007664918899536,
      "learning_rate": 1.8724349917285593e-05,
      "loss": 1.089,
      "step": 2009
    },
    {
      "epoch": 18.792219274977896,
      "grad_norm": 0.438967227935791,
      "learning_rate": 1.8722870192922814e-05,
      "loss": 1.078,
      "step": 2010
    },
    {
      "epoch": 18.80165045682287,
      "grad_norm": 0.4015951156616211,
      "learning_rate": 1.872138966937356e-05,
      "loss": 1.1181,
      "step": 2011
    },
    {
      "epoch": 18.811081638667847,
      "grad_norm": 0.380262553691864,
      "learning_rate": 1.871990834677347e-05,
      "loss": 1.1054,
      "step": 2012
    },
    {
      "epoch": 18.82051282051282,
      "grad_norm": 0.3758790194988251,
      "learning_rate": 1.8718426225258266e-05,
      "loss": 1.1266,
      "step": 2013
    },
    {
      "epoch": 18.829944002357795,
      "grad_norm": 0.3736661970615387,
      "learning_rate": 1.8716943304963738e-05,
      "loss": 1.1267,
      "step": 2014
    },
    {
      "epoch": 18.83937518420277,
      "grad_norm": 0.389828085899353,
      "learning_rate": 1.8715459586025747e-05,
      "loss": 1.1042,
      "step": 2015
    },
    {
      "epoch": 18.848806366047747,
      "grad_norm": 0.401605486869812,
      "learning_rate": 1.8713975068580235e-05,
      "loss": 1.1384,
      "step": 2016
    },
    {
      "epoch": 18.85823754789272,
      "grad_norm": 0.41619542241096497,
      "learning_rate": 1.8712489752763212e-05,
      "loss": 1.1304,
      "step": 2017
    },
    {
      "epoch": 18.867668729737694,
      "grad_norm": 0.41874220967292786,
      "learning_rate": 1.8711003638710763e-05,
      "loss": 1.1079,
      "step": 2018
    },
    {
      "epoch": 18.877099911582672,
      "grad_norm": 0.4087580442428589,
      "learning_rate": 1.8709516726559043e-05,
      "loss": 1.0914,
      "step": 2019
    },
    {
      "epoch": 18.886531093427646,
      "grad_norm": 0.4011191725730896,
      "learning_rate": 1.8708029016444285e-05,
      "loss": 1.1264,
      "step": 2020
    },
    {
      "epoch": 18.89596227527262,
      "grad_norm": 0.41208186745643616,
      "learning_rate": 1.8706540508502793e-05,
      "loss": 1.0894,
      "step": 2021
    },
    {
      "epoch": 18.905393457117594,
      "grad_norm": 0.3864780068397522,
      "learning_rate": 1.8705051202870938e-05,
      "loss": 1.0748,
      "step": 2022
    },
    {
      "epoch": 18.91482463896257,
      "grad_norm": 0.41134992241859436,
      "learning_rate": 1.8703561099685178e-05,
      "loss": 1.1529,
      "step": 2023
    },
    {
      "epoch": 18.924255820807545,
      "grad_norm": 0.42699483036994934,
      "learning_rate": 1.870207019908203e-05,
      "loss": 1.107,
      "step": 2024
    },
    {
      "epoch": 18.93368700265252,
      "grad_norm": 0.3987249433994293,
      "learning_rate": 1.870057850119809e-05,
      "loss": 1.1095,
      "step": 2025
    },
    {
      "epoch": 18.943118184497497,
      "grad_norm": 0.4200994074344635,
      "learning_rate": 1.8699086006170032e-05,
      "loss": 1.1123,
      "step": 2026
    },
    {
      "epoch": 18.95254936634247,
      "grad_norm": 0.4051104187965393,
      "learning_rate": 1.869759271413459e-05,
      "loss": 1.1056,
      "step": 2027
    },
    {
      "epoch": 18.961980548187444,
      "grad_norm": 0.3779805600643158,
      "learning_rate": 1.869609862522858e-05,
      "loss": 1.0981,
      "step": 2028
    },
    {
      "epoch": 18.97141173003242,
      "grad_norm": 0.40340206027030945,
      "learning_rate": 1.86946037395889e-05,
      "loss": 1.0883,
      "step": 2029
    },
    {
      "epoch": 18.980842911877396,
      "grad_norm": 0.42218250036239624,
      "learning_rate": 1.8693108057352497e-05,
      "loss": 1.131,
      "step": 2030
    },
    {
      "epoch": 18.99027409372237,
      "grad_norm": 0.4312974214553833,
      "learning_rate": 1.8691611578656415e-05,
      "loss": 1.1,
      "step": 2031
    },
    {
      "epoch": 18.999705275567344,
      "grad_norm": 0.44961556792259216,
      "learning_rate": 1.869011430363776e-05,
      "loss": 1.1157,
      "step": 2032
    },
    {
      "epoch": 19.0,
      "grad_norm": 1.945188283920288,
      "learning_rate": 1.8688616232433708e-05,
      "loss": 0.9002,
      "step": 2033
    },
    {
      "epoch": 19.009431181844974,
      "grad_norm": 0.40878450870513916,
      "learning_rate": 1.8687117365181514e-05,
      "loss": 1.0683,
      "step": 2034
    },
    {
      "epoch": 19.01886236368995,
      "grad_norm": 0.47334954142570496,
      "learning_rate": 1.86856177020185e-05,
      "loss": 1.0633,
      "step": 2035
    },
    {
      "epoch": 19.028293545534925,
      "grad_norm": 0.3971922993659973,
      "learning_rate": 1.868411724308207e-05,
      "loss": 1.1173,
      "step": 2036
    },
    {
      "epoch": 19.0377247273799,
      "grad_norm": 0.41009145975112915,
      "learning_rate": 1.868261598850969e-05,
      "loss": 1.0443,
      "step": 2037
    },
    {
      "epoch": 19.047155909224873,
      "grad_norm": 0.40838906168937683,
      "learning_rate": 1.868111393843891e-05,
      "loss": 1.0823,
      "step": 2038
    },
    {
      "epoch": 19.05658709106985,
      "grad_norm": 0.39222952723503113,
      "learning_rate": 1.8679611093007345e-05,
      "loss": 1.1056,
      "step": 2039
    },
    {
      "epoch": 19.066018272914825,
      "grad_norm": 0.4306339919567108,
      "learning_rate": 1.8678107452352683e-05,
      "loss": 1.1216,
      "step": 2040
    },
    {
      "epoch": 19.0754494547598,
      "grad_norm": 0.4043695628643036,
      "learning_rate": 1.8676603016612693e-05,
      "loss": 1.0785,
      "step": 2041
    },
    {
      "epoch": 19.084880636604776,
      "grad_norm": 0.4495309293270111,
      "learning_rate": 1.8675097785925204e-05,
      "loss": 1.0692,
      "step": 2042
    },
    {
      "epoch": 19.09431181844975,
      "grad_norm": 0.3838580548763275,
      "learning_rate": 1.867359176042813e-05,
      "loss": 1.1288,
      "step": 2043
    },
    {
      "epoch": 19.103743000294724,
      "grad_norm": 0.41152796149253845,
      "learning_rate": 1.867208494025945e-05,
      "loss": 1.1417,
      "step": 2044
    },
    {
      "epoch": 19.113174182139698,
      "grad_norm": 0.3957788348197937,
      "learning_rate": 1.8670577325557216e-05,
      "loss": 1.1379,
      "step": 2045
    },
    {
      "epoch": 19.122605363984675,
      "grad_norm": 0.41325533390045166,
      "learning_rate": 1.8669068916459562e-05,
      "loss": 1.0844,
      "step": 2046
    },
    {
      "epoch": 19.13203654582965,
      "grad_norm": 0.38283485174179077,
      "learning_rate": 1.866755971310468e-05,
      "loss": 1.1423,
      "step": 2047
    },
    {
      "epoch": 19.141467727674623,
      "grad_norm": 0.4231797754764557,
      "learning_rate": 1.8666049715630847e-05,
      "loss": 1.1361,
      "step": 2048
    },
    {
      "epoch": 19.1508989095196,
      "grad_norm": 0.4398069679737091,
      "learning_rate": 1.8664538924176406e-05,
      "loss": 1.1253,
      "step": 2049
    },
    {
      "epoch": 19.160330091364575,
      "grad_norm": 0.41795969009399414,
      "learning_rate": 1.866302733887978e-05,
      "loss": 1.1044,
      "step": 2050
    },
    {
      "epoch": 19.16976127320955,
      "grad_norm": 0.39898380637168884,
      "learning_rate": 1.866151495987946e-05,
      "loss": 1.084,
      "step": 2051
    },
    {
      "epoch": 19.179192455054523,
      "grad_norm": 0.4063754975795746,
      "learning_rate": 1.8660001787314008e-05,
      "loss": 1.1215,
      "step": 2052
    },
    {
      "epoch": 19.1886236368995,
      "grad_norm": 0.4305720329284668,
      "learning_rate": 1.865848782132205e-05,
      "loss": 1.0972,
      "step": 2053
    },
    {
      "epoch": 19.198054818744474,
      "grad_norm": 0.43614545464515686,
      "learning_rate": 1.865697306204231e-05,
      "loss": 1.0602,
      "step": 2054
    },
    {
      "epoch": 19.207486000589448,
      "grad_norm": 0.41990143060684204,
      "learning_rate": 1.8655457509613565e-05,
      "loss": 1.099,
      "step": 2055
    },
    {
      "epoch": 19.216917182434425,
      "grad_norm": 0.3744896352291107,
      "learning_rate": 1.8653941164174667e-05,
      "loss": 1.0976,
      "step": 2056
    },
    {
      "epoch": 19.2263483642794,
      "grad_norm": 0.40421760082244873,
      "learning_rate": 1.8652424025864548e-05,
      "loss": 1.0869,
      "step": 2057
    },
    {
      "epoch": 19.235779546124373,
      "grad_norm": 0.38549551367759705,
      "learning_rate": 1.86509060948222e-05,
      "loss": 1.0957,
      "step": 2058
    },
    {
      "epoch": 19.245210727969347,
      "grad_norm": 0.4131612479686737,
      "learning_rate": 1.86493873711867e-05,
      "loss": 1.1315,
      "step": 2059
    },
    {
      "epoch": 19.254641909814325,
      "grad_norm": 0.39992913603782654,
      "learning_rate": 1.8647867855097193e-05,
      "loss": 1.0903,
      "step": 2060
    },
    {
      "epoch": 19.2640730916593,
      "grad_norm": 0.3970986604690552,
      "learning_rate": 1.8646347546692896e-05,
      "loss": 1.0694,
      "step": 2061
    },
    {
      "epoch": 19.273504273504273,
      "grad_norm": 0.43722105026245117,
      "learning_rate": 1.86448264461131e-05,
      "loss": 1.1307,
      "step": 2062
    },
    {
      "epoch": 19.28293545534925,
      "grad_norm": 0.420945942401886,
      "learning_rate": 1.8643304553497165e-05,
      "loss": 1.1062,
      "step": 2063
    },
    {
      "epoch": 19.292366637194224,
      "grad_norm": 0.39732322096824646,
      "learning_rate": 1.8641781868984527e-05,
      "loss": 1.1434,
      "step": 2064
    },
    {
      "epoch": 19.301797819039198,
      "grad_norm": 0.4226488769054413,
      "learning_rate": 1.86402583927147e-05,
      "loss": 1.0539,
      "step": 2065
    },
    {
      "epoch": 19.311229000884172,
      "grad_norm": 0.42144644260406494,
      "learning_rate": 1.8638734124827257e-05,
      "loss": 1.0785,
      "step": 2066
    },
    {
      "epoch": 19.32066018272915,
      "grad_norm": 0.406650573015213,
      "learning_rate": 1.8637209065461853e-05,
      "loss": 1.1141,
      "step": 2067
    },
    {
      "epoch": 19.330091364574123,
      "grad_norm": 0.39630943536758423,
      "learning_rate": 1.8635683214758213e-05,
      "loss": 1.1176,
      "step": 2068
    },
    {
      "epoch": 19.339522546419097,
      "grad_norm": 0.3917084336280823,
      "learning_rate": 1.8634156572856136e-05,
      "loss": 1.0852,
      "step": 2069
    },
    {
      "epoch": 19.348953728264075,
      "grad_norm": 0.40162232518196106,
      "learning_rate": 1.8632629139895492e-05,
      "loss": 1.1106,
      "step": 2070
    },
    {
      "epoch": 19.35838491010905,
      "grad_norm": 0.42168518900871277,
      "learning_rate": 1.8631100916016227e-05,
      "loss": 1.1217,
      "step": 2071
    },
    {
      "epoch": 19.367816091954023,
      "grad_norm": 0.3914658725261688,
      "learning_rate": 1.8629571901358353e-05,
      "loss": 1.1099,
      "step": 2072
    },
    {
      "epoch": 19.377247273798996,
      "grad_norm": 0.4102499783039093,
      "learning_rate": 1.8628042096061956e-05,
      "loss": 1.1616,
      "step": 2073
    },
    {
      "epoch": 19.386678455643974,
      "grad_norm": 0.38175442814826965,
      "learning_rate": 1.86265115002672e-05,
      "loss": 1.1357,
      "step": 2074
    },
    {
      "epoch": 19.396109637488948,
      "grad_norm": 0.3977317810058594,
      "learning_rate": 1.8624980114114316e-05,
      "loss": 1.0478,
      "step": 2075
    },
    {
      "epoch": 19.405540819333922,
      "grad_norm": 0.3670051097869873,
      "learning_rate": 1.8623447937743607e-05,
      "loss": 1.0743,
      "step": 2076
    },
    {
      "epoch": 19.4149720011789,
      "grad_norm": 0.42949795722961426,
      "learning_rate": 1.8621914971295456e-05,
      "loss": 1.0966,
      "step": 2077
    },
    {
      "epoch": 19.424403183023873,
      "grad_norm": 0.39285561442375183,
      "learning_rate": 1.8620381214910307e-05,
      "loss": 1.0958,
      "step": 2078
    },
    {
      "epoch": 19.433834364868847,
      "grad_norm": 0.4089043438434601,
      "learning_rate": 1.8618846668728688e-05,
      "loss": 1.0907,
      "step": 2079
    },
    {
      "epoch": 19.44326554671382,
      "grad_norm": 0.40042948722839355,
      "learning_rate": 1.8617311332891188e-05,
      "loss": 1.1159,
      "step": 2080
    },
    {
      "epoch": 19.4526967285588,
      "grad_norm": 0.38732773065567017,
      "learning_rate": 1.861577520753848e-05,
      "loss": 1.073,
      "step": 2081
    },
    {
      "epoch": 19.462127910403773,
      "grad_norm": 0.37678104639053345,
      "learning_rate": 1.8614238292811297e-05,
      "loss": 1.1032,
      "step": 2082
    },
    {
      "epoch": 19.471559092248746,
      "grad_norm": 0.38750290870666504,
      "learning_rate": 1.8612700588850453e-05,
      "loss": 1.0725,
      "step": 2083
    },
    {
      "epoch": 19.480990274093724,
      "grad_norm": 0.41355714201927185,
      "learning_rate": 1.861116209579683e-05,
      "loss": 1.1108,
      "step": 2084
    },
    {
      "epoch": 19.490421455938698,
      "grad_norm": 0.431112676858902,
      "learning_rate": 1.860962281379139e-05,
      "loss": 1.1252,
      "step": 2085
    },
    {
      "epoch": 19.499852637783672,
      "grad_norm": 0.42724964022636414,
      "learning_rate": 1.860808274297516e-05,
      "loss": 1.1203,
      "step": 2086
    },
    {
      "epoch": 19.509283819628646,
      "grad_norm": 0.38441428542137146,
      "learning_rate": 1.8606541883489236e-05,
      "loss": 1.1349,
      "step": 2087
    },
    {
      "epoch": 19.518715001473623,
      "grad_norm": 0.400790274143219,
      "learning_rate": 1.8605000235474792e-05,
      "loss": 1.0869,
      "step": 2088
    },
    {
      "epoch": 19.528146183318597,
      "grad_norm": 0.4636096954345703,
      "learning_rate": 1.860345779907308e-05,
      "loss": 1.076,
      "step": 2089
    },
    {
      "epoch": 19.53757736516357,
      "grad_norm": 0.4099372327327728,
      "learning_rate": 1.860191457442541e-05,
      "loss": 1.0995,
      "step": 2090
    },
    {
      "epoch": 19.54700854700855,
      "grad_norm": 0.40646064281463623,
      "learning_rate": 1.8600370561673172e-05,
      "loss": 1.0815,
      "step": 2091
    },
    {
      "epoch": 19.556439728853523,
      "grad_norm": 0.3985152542591095,
      "learning_rate": 1.8598825760957836e-05,
      "loss": 1.0719,
      "step": 2092
    },
    {
      "epoch": 19.565870910698496,
      "grad_norm": 0.45663851499557495,
      "learning_rate": 1.859728017242093e-05,
      "loss": 1.1063,
      "step": 2093
    },
    {
      "epoch": 19.57530209254347,
      "grad_norm": 0.39736711978912354,
      "learning_rate": 1.8595733796204058e-05,
      "loss": 1.0637,
      "step": 2094
    },
    {
      "epoch": 19.584733274388448,
      "grad_norm": 0.40922626852989197,
      "learning_rate": 1.85941866324489e-05,
      "loss": 1.1015,
      "step": 2095
    },
    {
      "epoch": 19.594164456233422,
      "grad_norm": 0.3999945819377899,
      "learning_rate": 1.859263868129721e-05,
      "loss": 1.1068,
      "step": 2096
    },
    {
      "epoch": 19.603595638078396,
      "grad_norm": 0.38856038451194763,
      "learning_rate": 1.8591089942890808e-05,
      "loss": 1.0858,
      "step": 2097
    },
    {
      "epoch": 19.613026819923373,
      "grad_norm": 0.4039693772792816,
      "learning_rate": 1.858954041737159e-05,
      "loss": 1.0871,
      "step": 2098
    },
    {
      "epoch": 19.622458001768347,
      "grad_norm": 0.40067437291145325,
      "learning_rate": 1.8587990104881522e-05,
      "loss": 1.0591,
      "step": 2099
    },
    {
      "epoch": 19.63188918361332,
      "grad_norm": 0.4069940149784088,
      "learning_rate": 1.858643900556264e-05,
      "loss": 1.1554,
      "step": 2100
    },
    {
      "epoch": 19.641320365458295,
      "grad_norm": 0.41903677582740784,
      "learning_rate": 1.8584887119557064e-05,
      "loss": 1.0522,
      "step": 2101
    },
    {
      "epoch": 19.650751547303273,
      "grad_norm": 0.3925575017929077,
      "learning_rate": 1.8583334447006972e-05,
      "loss": 1.1261,
      "step": 2102
    },
    {
      "epoch": 19.660182729148246,
      "grad_norm": 0.43862172961235046,
      "learning_rate": 1.858178098805462e-05,
      "loss": 1.1134,
      "step": 2103
    },
    {
      "epoch": 19.66961391099322,
      "grad_norm": 0.37439751625061035,
      "learning_rate": 1.8580226742842335e-05,
      "loss": 1.1159,
      "step": 2104
    },
    {
      "epoch": 19.679045092838198,
      "grad_norm": 0.41044002771377563,
      "learning_rate": 1.8578671711512513e-05,
      "loss": 1.1215,
      "step": 2105
    },
    {
      "epoch": 19.688476274683172,
      "grad_norm": 0.382313996553421,
      "learning_rate": 1.857711589420763e-05,
      "loss": 1.1567,
      "step": 2106
    },
    {
      "epoch": 19.697907456528146,
      "grad_norm": 0.42440736293792725,
      "learning_rate": 1.857555929107023e-05,
      "loss": 1.0862,
      "step": 2107
    },
    {
      "epoch": 19.70733863837312,
      "grad_norm": 0.420729398727417,
      "learning_rate": 1.8574001902242926e-05,
      "loss": 1.1317,
      "step": 2108
    },
    {
      "epoch": 19.716769820218097,
      "grad_norm": 0.3996119201183319,
      "learning_rate": 1.8572443727868408e-05,
      "loss": 1.0966,
      "step": 2109
    },
    {
      "epoch": 19.72620100206307,
      "grad_norm": 0.4272107779979706,
      "learning_rate": 1.857088476808943e-05,
      "loss": 1.0927,
      "step": 2110
    },
    {
      "epoch": 19.735632183908045,
      "grad_norm": 0.4087047576904297,
      "learning_rate": 1.8569325023048827e-05,
      "loss": 1.1005,
      "step": 2111
    },
    {
      "epoch": 19.745063365753023,
      "grad_norm": 0.3999117910861969,
      "learning_rate": 1.8567764492889506e-05,
      "loss": 1.0957,
      "step": 2112
    },
    {
      "epoch": 19.754494547597997,
      "grad_norm": 0.4100914001464844,
      "learning_rate": 1.8566203177754435e-05,
      "loss": 1.0785,
      "step": 2113
    },
    {
      "epoch": 19.76392572944297,
      "grad_norm": 0.4005824625492096,
      "learning_rate": 1.8564641077786668e-05,
      "loss": 1.1388,
      "step": 2114
    },
    {
      "epoch": 19.773356911287944,
      "grad_norm": 0.40672701597213745,
      "learning_rate": 1.856307819312932e-05,
      "loss": 1.089,
      "step": 2115
    },
    {
      "epoch": 19.782788093132922,
      "grad_norm": 0.3895840644836426,
      "learning_rate": 1.856151452392558e-05,
      "loss": 1.0833,
      "step": 2116
    },
    {
      "epoch": 19.792219274977896,
      "grad_norm": 0.39586925506591797,
      "learning_rate": 1.855995007031872e-05,
      "loss": 1.0963,
      "step": 2117
    },
    {
      "epoch": 19.80165045682287,
      "grad_norm": 0.39364784955978394,
      "learning_rate": 1.8558384832452062e-05,
      "loss": 1.0722,
      "step": 2118
    },
    {
      "epoch": 19.811081638667847,
      "grad_norm": 0.4168493151664734,
      "learning_rate": 1.855681881046902e-05,
      "loss": 1.1155,
      "step": 2119
    },
    {
      "epoch": 19.82051282051282,
      "grad_norm": 0.4328179955482483,
      "learning_rate": 1.855525200451307e-05,
      "loss": 1.1246,
      "step": 2120
    },
    {
      "epoch": 19.829944002357795,
      "grad_norm": 0.3817809224128723,
      "learning_rate": 1.8553684414727766e-05,
      "loss": 1.1419,
      "step": 2121
    },
    {
      "epoch": 19.83937518420277,
      "grad_norm": 0.3846319913864136,
      "learning_rate": 1.855211604125673e-05,
      "loss": 1.0893,
      "step": 2122
    },
    {
      "epoch": 19.848806366047747,
      "grad_norm": 0.40752485394477844,
      "learning_rate": 1.855054688424365e-05,
      "loss": 1.0835,
      "step": 2123
    },
    {
      "epoch": 19.85823754789272,
      "grad_norm": 0.440380722284317,
      "learning_rate": 1.85489769438323e-05,
      "loss": 1.1316,
      "step": 2124
    },
    {
      "epoch": 19.867668729737694,
      "grad_norm": 0.39681389927864075,
      "learning_rate": 1.8547406220166507e-05,
      "loss": 1.0956,
      "step": 2125
    },
    {
      "epoch": 19.877099911582672,
      "grad_norm": 0.4206267297267914,
      "learning_rate": 1.8545834713390187e-05,
      "loss": 1.0665,
      "step": 2126
    },
    {
      "epoch": 19.886531093427646,
      "grad_norm": 0.40979939699172974,
      "learning_rate": 1.854426242364732e-05,
      "loss": 1.1005,
      "step": 2127
    },
    {
      "epoch": 19.89596227527262,
      "grad_norm": 0.4175611138343811,
      "learning_rate": 1.854268935108196e-05,
      "loss": 1.0897,
      "step": 2128
    },
    {
      "epoch": 19.905393457117594,
      "grad_norm": 0.4304584562778473,
      "learning_rate": 1.8541115495838226e-05,
      "loss": 1.0676,
      "step": 2129
    },
    {
      "epoch": 19.91482463896257,
      "grad_norm": 0.4055556356906891,
      "learning_rate": 1.853954085806032e-05,
      "loss": 1.0728,
      "step": 2130
    },
    {
      "epoch": 19.924255820807545,
      "grad_norm": 0.4223739504814148,
      "learning_rate": 1.8537965437892512e-05,
      "loss": 1.1322,
      "step": 2131
    },
    {
      "epoch": 19.93368700265252,
      "grad_norm": 0.42380157113075256,
      "learning_rate": 1.853638923547913e-05,
      "loss": 1.0591,
      "step": 2132
    },
    {
      "epoch": 19.943118184497497,
      "grad_norm": 0.41222095489501953,
      "learning_rate": 1.8534812250964597e-05,
      "loss": 1.1061,
      "step": 2133
    },
    {
      "epoch": 19.95254936634247,
      "grad_norm": 0.4198763072490692,
      "learning_rate": 1.853323448449339e-05,
      "loss": 1.0875,
      "step": 2134
    },
    {
      "epoch": 19.961980548187444,
      "grad_norm": 0.4113416373729706,
      "learning_rate": 1.8531655936210064e-05,
      "loss": 1.0808,
      "step": 2135
    },
    {
      "epoch": 19.97141173003242,
      "grad_norm": 0.3946627080440521,
      "learning_rate": 1.8530076606259246e-05,
      "loss": 1.1139,
      "step": 2136
    },
    {
      "epoch": 19.980842911877396,
      "grad_norm": 0.40358901023864746,
      "learning_rate": 1.8528496494785636e-05,
      "loss": 1.1229,
      "step": 2137
    },
    {
      "epoch": 19.99027409372237,
      "grad_norm": 0.38049760460853577,
      "learning_rate": 1.8526915601933994e-05,
      "loss": 1.1374,
      "step": 2138
    },
    {
      "epoch": 19.999705275567344,
      "grad_norm": 0.39241746068000793,
      "learning_rate": 1.8525333927849175e-05,
      "loss": 1.0894,
      "step": 2139
    },
    {
      "epoch": 20.0,
      "grad_norm": 2.1577234268188477,
      "learning_rate": 1.8523751472676077e-05,
      "loss": 0.6339,
      "step": 2140
    },
    {
      "epoch": 20.009431181844974,
      "grad_norm": 0.3921181261539459,
      "learning_rate": 1.8522168236559693e-05,
      "loss": 1.11,
      "step": 2141
    },
    {
      "epoch": 20.01886236368995,
      "grad_norm": 0.42802202701568604,
      "learning_rate": 1.852058421964508e-05,
      "loss": 1.0807,
      "step": 2142
    },
    {
      "epoch": 20.028293545534925,
      "grad_norm": 0.41281262040138245,
      "learning_rate": 1.851899942207736e-05,
      "loss": 1.1426,
      "step": 2143
    },
    {
      "epoch": 20.0377247273799,
      "grad_norm": 0.3966122567653656,
      "learning_rate": 1.851741384400173e-05,
      "loss": 1.0959,
      "step": 2144
    },
    {
      "epoch": 20.047155909224873,
      "grad_norm": 0.44993966817855835,
      "learning_rate": 1.8515827485563464e-05,
      "loss": 1.1413,
      "step": 2145
    },
    {
      "epoch": 20.05658709106985,
      "grad_norm": 0.4412993788719177,
      "learning_rate": 1.8514240346907906e-05,
      "loss": 1.1014,
      "step": 2146
    },
    {
      "epoch": 20.066018272914825,
      "grad_norm": 0.4146490693092346,
      "learning_rate": 1.8512652428180466e-05,
      "loss": 1.0858,
      "step": 2147
    },
    {
      "epoch": 20.0754494547598,
      "grad_norm": 0.39934802055358887,
      "learning_rate": 1.8511063729526626e-05,
      "loss": 1.144,
      "step": 2148
    },
    {
      "epoch": 20.084880636604776,
      "grad_norm": 0.4001694917678833,
      "learning_rate": 1.8509474251091948e-05,
      "loss": 1.0797,
      "step": 2149
    },
    {
      "epoch": 20.09431181844975,
      "grad_norm": 0.40774962306022644,
      "learning_rate": 1.8507883993022053e-05,
      "loss": 1.0971,
      "step": 2150
    },
    {
      "epoch": 20.103743000294724,
      "grad_norm": 0.40134865045547485,
      "learning_rate": 1.8506292955462645e-05,
      "loss": 1.1203,
      "step": 2151
    },
    {
      "epoch": 20.113174182139698,
      "grad_norm": 0.416378915309906,
      "learning_rate": 1.8504701138559495e-05,
      "loss": 1.0673,
      "step": 2152
    },
    {
      "epoch": 20.122605363984675,
      "grad_norm": 0.41750699281692505,
      "learning_rate": 1.8503108542458433e-05,
      "loss": 1.1156,
      "step": 2153
    },
    {
      "epoch": 20.13203654582965,
      "grad_norm": 0.4185147285461426,
      "learning_rate": 1.8501515167305392e-05,
      "loss": 1.1025,
      "step": 2154
    },
    {
      "epoch": 20.141467727674623,
      "grad_norm": 0.4072716534137726,
      "learning_rate": 1.849992101324634e-05,
      "loss": 1.0961,
      "step": 2155
    },
    {
      "epoch": 20.1508989095196,
      "grad_norm": 0.417705774307251,
      "learning_rate": 1.849832608042734e-05,
      "loss": 1.0615,
      "step": 2156
    },
    {
      "epoch": 20.160330091364575,
      "grad_norm": 0.4076956808567047,
      "learning_rate": 1.849673036899452e-05,
      "loss": 1.089,
      "step": 2157
    },
    {
      "epoch": 20.16976127320955,
      "grad_norm": 0.4277159571647644,
      "learning_rate": 1.8495133879094075e-05,
      "loss": 1.0984,
      "step": 2158
    },
    {
      "epoch": 20.179192455054523,
      "grad_norm": 0.436786025762558,
      "learning_rate": 1.8493536610872274e-05,
      "loss": 1.0861,
      "step": 2159
    },
    {
      "epoch": 20.1886236368995,
      "grad_norm": 0.3903051018714905,
      "learning_rate": 1.8491938564475464e-05,
      "loss": 1.0948,
      "step": 2160
    },
    {
      "epoch": 20.198054818744474,
      "grad_norm": 0.40877220034599304,
      "learning_rate": 1.849033974005005e-05,
      "loss": 1.1014,
      "step": 2161
    },
    {
      "epoch": 20.207486000589448,
      "grad_norm": 0.4017846882343292,
      "learning_rate": 1.8488740137742523e-05,
      "loss": 1.1181,
      "step": 2162
    },
    {
      "epoch": 20.216917182434425,
      "grad_norm": 0.4053809642791748,
      "learning_rate": 1.8487139757699436e-05,
      "loss": 1.1358,
      "step": 2163
    },
    {
      "epoch": 20.2263483642794,
      "grad_norm": 0.37511906027793884,
      "learning_rate": 1.8485538600067408e-05,
      "loss": 1.0915,
      "step": 2164
    },
    {
      "epoch": 20.235779546124373,
      "grad_norm": 0.396940141916275,
      "learning_rate": 1.8483936664993152e-05,
      "loss": 1.0379,
      "step": 2165
    },
    {
      "epoch": 20.245210727969347,
      "grad_norm": 0.4073165953159332,
      "learning_rate": 1.8482333952623422e-05,
      "loss": 1.1113,
      "step": 2166
    },
    {
      "epoch": 20.254641909814325,
      "grad_norm": 0.40296411514282227,
      "learning_rate": 1.8480730463105062e-05,
      "loss": 1.1027,
      "step": 2167
    },
    {
      "epoch": 20.2640730916593,
      "grad_norm": 0.40869635343551636,
      "learning_rate": 1.847912619658499e-05,
      "loss": 1.0798,
      "step": 2168
    },
    {
      "epoch": 20.273504273504273,
      "grad_norm": 0.4235785901546478,
      "learning_rate": 1.8477521153210176e-05,
      "loss": 1.0949,
      "step": 2169
    },
    {
      "epoch": 20.28293545534925,
      "grad_norm": 0.43891990184783936,
      "learning_rate": 1.8475915333127685e-05,
      "loss": 1.1201,
      "step": 2170
    },
    {
      "epoch": 20.292366637194224,
      "grad_norm": 0.42921513319015503,
      "learning_rate": 1.8474308736484636e-05,
      "loss": 1.1242,
      "step": 2171
    },
    {
      "epoch": 20.301797819039198,
      "grad_norm": 0.41984811425209045,
      "learning_rate": 1.8472701363428224e-05,
      "loss": 1.0756,
      "step": 2172
    },
    {
      "epoch": 20.311229000884172,
      "grad_norm": 0.4300342798233032,
      "learning_rate": 1.8471093214105722e-05,
      "loss": 1.1107,
      "step": 2173
    },
    {
      "epoch": 20.32066018272915,
      "grad_norm": 0.4024370014667511,
      "learning_rate": 1.8469484288664464e-05,
      "loss": 1.1299,
      "step": 2174
    },
    {
      "epoch": 20.330091364574123,
      "grad_norm": 0.3997952342033386,
      "learning_rate": 1.8467874587251854e-05,
      "loss": 1.0615,
      "step": 2175
    },
    {
      "epoch": 20.339522546419097,
      "grad_norm": 0.4026985764503479,
      "learning_rate": 1.8466264110015385e-05,
      "loss": 1.0971,
      "step": 2176
    },
    {
      "epoch": 20.348953728264075,
      "grad_norm": 0.407051146030426,
      "learning_rate": 1.8464652857102595e-05,
      "loss": 1.1184,
      "step": 2177
    },
    {
      "epoch": 20.35838491010905,
      "grad_norm": 0.4140477478504181,
      "learning_rate": 1.8463040828661116e-05,
      "loss": 1.077,
      "step": 2178
    },
    {
      "epoch": 20.367816091954023,
      "grad_norm": 0.43031588196754456,
      "learning_rate": 1.8461428024838637e-05,
      "loss": 1.1122,
      "step": 2179
    },
    {
      "epoch": 20.377247273798996,
      "grad_norm": 0.41276055574417114,
      "learning_rate": 1.845981444578292e-05,
      "loss": 1.104,
      "step": 2180
    },
    {
      "epoch": 20.386678455643974,
      "grad_norm": 0.40823468565940857,
      "learning_rate": 1.8458200091641808e-05,
      "loss": 1.1293,
      "step": 2181
    },
    {
      "epoch": 20.396109637488948,
      "grad_norm": 0.4165497124195099,
      "learning_rate": 1.8456584962563208e-05,
      "loss": 1.0959,
      "step": 2182
    },
    {
      "epoch": 20.405540819333922,
      "grad_norm": 0.3988529443740845,
      "learning_rate": 1.8454969058695088e-05,
      "loss": 1.0756,
      "step": 2183
    },
    {
      "epoch": 20.4149720011789,
      "grad_norm": 0.40105146169662476,
      "learning_rate": 1.8453352380185504e-05,
      "loss": 1.0811,
      "step": 2184
    },
    {
      "epoch": 20.424403183023873,
      "grad_norm": 0.40978142619132996,
      "learning_rate": 1.845173492718257e-05,
      "loss": 1.184,
      "step": 2185
    },
    {
      "epoch": 20.433834364868847,
      "grad_norm": 0.39861661195755005,
      "learning_rate": 1.8450116699834483e-05,
      "loss": 1.08,
      "step": 2186
    },
    {
      "epoch": 20.44326554671382,
      "grad_norm": 0.45959705114364624,
      "learning_rate": 1.84484976982895e-05,
      "loss": 1.0768,
      "step": 2187
    },
    {
      "epoch": 20.4526967285588,
      "grad_norm": 0.43543872237205505,
      "learning_rate": 1.8446877922695952e-05,
      "loss": 1.1149,
      "step": 2188
    },
    {
      "epoch": 20.462127910403773,
      "grad_norm": 0.42980989813804626,
      "learning_rate": 1.8445257373202252e-05,
      "loss": 1.0588,
      "step": 2189
    },
    {
      "epoch": 20.471559092248746,
      "grad_norm": 0.4185789227485657,
      "learning_rate": 1.8443636049956864e-05,
      "loss": 1.1228,
      "step": 2190
    },
    {
      "epoch": 20.480990274093724,
      "grad_norm": 0.4175684154033661,
      "learning_rate": 1.8442013953108336e-05,
      "loss": 1.1043,
      "step": 2191
    },
    {
      "epoch": 20.490421455938698,
      "grad_norm": 0.41095981001853943,
      "learning_rate": 1.8440391082805286e-05,
      "loss": 1.0794,
      "step": 2192
    },
    {
      "epoch": 20.499852637783672,
      "grad_norm": 0.42530301213264465,
      "learning_rate": 1.8438767439196395e-05,
      "loss": 1.1242,
      "step": 2193
    },
    {
      "epoch": 20.509283819628646,
      "grad_norm": 0.42782023549079895,
      "learning_rate": 1.843714302243043e-05,
      "loss": 1.1329,
      "step": 2194
    },
    {
      "epoch": 20.518715001473623,
      "grad_norm": 0.4006049633026123,
      "learning_rate": 1.8435517832656213e-05,
      "loss": 1.1111,
      "step": 2195
    },
    {
      "epoch": 20.528146183318597,
      "grad_norm": 0.4300988018512726,
      "learning_rate": 1.8433891870022643e-05,
      "loss": 1.0939,
      "step": 2196
    },
    {
      "epoch": 20.53757736516357,
      "grad_norm": 0.37976542115211487,
      "learning_rate": 1.8432265134678693e-05,
      "loss": 1.0621,
      "step": 2197
    },
    {
      "epoch": 20.54700854700855,
      "grad_norm": 0.4041669964790344,
      "learning_rate": 1.8430637626773404e-05,
      "loss": 1.0918,
      "step": 2198
    },
    {
      "epoch": 20.556439728853523,
      "grad_norm": 0.4082833230495453,
      "learning_rate": 1.8429009346455883e-05,
      "loss": 1.1011,
      "step": 2199
    },
    {
      "epoch": 20.565870910698496,
      "grad_norm": 0.41581353545188904,
      "learning_rate": 1.8427380293875317e-05,
      "loss": 1.1029,
      "step": 2200
    },
    {
      "epoch": 20.57530209254347,
      "grad_norm": 0.42488741874694824,
      "learning_rate": 1.8425750469180964e-05,
      "loss": 1.071,
      "step": 2201
    },
    {
      "epoch": 20.584733274388448,
      "grad_norm": 0.435282438993454,
      "learning_rate": 1.8424119872522138e-05,
      "loss": 1.1175,
      "step": 2202
    },
    {
      "epoch": 20.594164456233422,
      "grad_norm": 0.41526031494140625,
      "learning_rate": 1.842248850404824e-05,
      "loss": 1.102,
      "step": 2203
    },
    {
      "epoch": 20.603595638078396,
      "grad_norm": 0.4166717827320099,
      "learning_rate": 1.842085636390873e-05,
      "loss": 1.1005,
      "step": 2204
    },
    {
      "epoch": 20.613026819923373,
      "grad_norm": 0.42524778842926025,
      "learning_rate": 1.841922345225315e-05,
      "loss": 1.105,
      "step": 2205
    },
    {
      "epoch": 20.622458001768347,
      "grad_norm": 0.40436142683029175,
      "learning_rate": 1.8417589769231105e-05,
      "loss": 1.0908,
      "step": 2206
    },
    {
      "epoch": 20.63188918361332,
      "grad_norm": 0.43124300241470337,
      "learning_rate": 1.8415955314992273e-05,
      "loss": 1.1454,
      "step": 2207
    },
    {
      "epoch": 20.641320365458295,
      "grad_norm": 0.3968854248523712,
      "learning_rate": 1.84143200896864e-05,
      "loss": 1.0835,
      "step": 2208
    },
    {
      "epoch": 20.650751547303273,
      "grad_norm": 0.3924274742603302,
      "learning_rate": 1.8412684093463306e-05,
      "loss": 1.1105,
      "step": 2209
    },
    {
      "epoch": 20.660182729148246,
      "grad_norm": 0.4546213746070862,
      "learning_rate": 1.841104732647288e-05,
      "loss": 1.1119,
      "step": 2210
    },
    {
      "epoch": 20.66961391099322,
      "grad_norm": 0.4237407147884369,
      "learning_rate": 1.8409409788865082e-05,
      "loss": 1.0792,
      "step": 2211
    },
    {
      "epoch": 20.679045092838198,
      "grad_norm": 0.39635103940963745,
      "learning_rate": 1.8407771480789946e-05,
      "loss": 1.0979,
      "step": 2212
    },
    {
      "epoch": 20.688476274683172,
      "grad_norm": 0.41621774435043335,
      "learning_rate": 1.8406132402397568e-05,
      "loss": 1.105,
      "step": 2213
    },
    {
      "epoch": 20.697907456528146,
      "grad_norm": 0.4259223937988281,
      "learning_rate": 1.840449255383812e-05,
      "loss": 1.1035,
      "step": 2214
    },
    {
      "epoch": 20.70733863837312,
      "grad_norm": 0.37355175614356995,
      "learning_rate": 1.8402851935261853e-05,
      "loss": 1.0993,
      "step": 2215
    },
    {
      "epoch": 20.716769820218097,
      "grad_norm": 0.4321885108947754,
      "learning_rate": 1.840121054681907e-05,
      "loss": 1.0612,
      "step": 2216
    },
    {
      "epoch": 20.72620100206307,
      "grad_norm": 0.4453926384449005,
      "learning_rate": 1.8399568388660155e-05,
      "loss": 1.0899,
      "step": 2217
    },
    {
      "epoch": 20.735632183908045,
      "grad_norm": 0.41436532139778137,
      "learning_rate": 1.8397925460935567e-05,
      "loss": 1.1109,
      "step": 2218
    },
    {
      "epoch": 20.745063365753023,
      "grad_norm": 0.4029387831687927,
      "learning_rate": 1.839628176379583e-05,
      "loss": 1.1229,
      "step": 2219
    },
    {
      "epoch": 20.754494547597997,
      "grad_norm": 0.4226073622703552,
      "learning_rate": 1.8394637297391536e-05,
      "loss": 1.1262,
      "step": 2220
    },
    {
      "epoch": 20.76392572944297,
      "grad_norm": 0.42041194438934326,
      "learning_rate": 1.8392992061873354e-05,
      "loss": 1.1249,
      "step": 2221
    },
    {
      "epoch": 20.773356911287944,
      "grad_norm": 0.4089219272136688,
      "learning_rate": 1.8391346057392015e-05,
      "loss": 1.0623,
      "step": 2222
    },
    {
      "epoch": 20.782788093132922,
      "grad_norm": 0.392109215259552,
      "learning_rate": 1.8389699284098332e-05,
      "loss": 1.1219,
      "step": 2223
    },
    {
      "epoch": 20.792219274977896,
      "grad_norm": 0.426237016916275,
      "learning_rate": 1.8388051742143174e-05,
      "loss": 1.1158,
      "step": 2224
    },
    {
      "epoch": 20.80165045682287,
      "grad_norm": 0.38513606786727905,
      "learning_rate": 1.8386403431677493e-05,
      "loss": 1.0797,
      "step": 2225
    },
    {
      "epoch": 20.811081638667847,
      "grad_norm": 0.3922674357891083,
      "learning_rate": 1.8384754352852308e-05,
      "loss": 1.1014,
      "step": 2226
    },
    {
      "epoch": 20.82051282051282,
      "grad_norm": 0.40631958842277527,
      "learning_rate": 1.8383104505818706e-05,
      "loss": 1.1132,
      "step": 2227
    },
    {
      "epoch": 20.829944002357795,
      "grad_norm": 0.43324777483940125,
      "learning_rate": 1.8381453890727838e-05,
      "loss": 1.0931,
      "step": 2228
    },
    {
      "epoch": 20.83937518420277,
      "grad_norm": 0.43089595437049866,
      "learning_rate": 1.8379802507730948e-05,
      "loss": 1.0566,
      "step": 2229
    },
    {
      "epoch": 20.848806366047747,
      "grad_norm": 0.4166528582572937,
      "learning_rate": 1.8378150356979318e-05,
      "loss": 1.0685,
      "step": 2230
    },
    {
      "epoch": 20.85823754789272,
      "grad_norm": 0.4320712387561798,
      "learning_rate": 1.837649743862433e-05,
      "loss": 1.0926,
      "step": 2231
    },
    {
      "epoch": 20.867668729737694,
      "grad_norm": 0.3839297890663147,
      "learning_rate": 1.837484375281742e-05,
      "loss": 1.0987,
      "step": 2232
    },
    {
      "epoch": 20.877099911582672,
      "grad_norm": 0.44383475184440613,
      "learning_rate": 1.8373189299710098e-05,
      "loss": 1.0532,
      "step": 2233
    },
    {
      "epoch": 20.886531093427646,
      "grad_norm": 0.40060827136039734,
      "learning_rate": 1.8371534079453945e-05,
      "loss": 1.0913,
      "step": 2234
    },
    {
      "epoch": 20.89596227527262,
      "grad_norm": 0.40529948472976685,
      "learning_rate": 1.8369878092200612e-05,
      "loss": 1.0807,
      "step": 2235
    },
    {
      "epoch": 20.905393457117594,
      "grad_norm": 0.4191743731498718,
      "learning_rate": 1.8368221338101816e-05,
      "loss": 1.0994,
      "step": 2236
    },
    {
      "epoch": 20.91482463896257,
      "grad_norm": 0.4719661772251129,
      "learning_rate": 1.836656381730935e-05,
      "loss": 1.1215,
      "step": 2237
    },
    {
      "epoch": 20.924255820807545,
      "grad_norm": 0.40624138712882996,
      "learning_rate": 1.8364905529975082e-05,
      "loss": 1.1267,
      "step": 2238
    },
    {
      "epoch": 20.93368700265252,
      "grad_norm": 0.3930095136165619,
      "learning_rate": 1.8363246476250935e-05,
      "loss": 1.0696,
      "step": 2239
    },
    {
      "epoch": 20.943118184497497,
      "grad_norm": 0.405603289604187,
      "learning_rate": 1.836158665628892e-05,
      "loss": 1.1152,
      "step": 2240
    },
    {
      "epoch": 20.95254936634247,
      "grad_norm": 0.391309529542923,
      "learning_rate": 1.8359926070241096e-05,
      "loss": 1.1048,
      "step": 2241
    },
    {
      "epoch": 20.961980548187444,
      "grad_norm": 0.42552483081817627,
      "learning_rate": 1.8358264718259614e-05,
      "loss": 1.0844,
      "step": 2242
    },
    {
      "epoch": 20.97141173003242,
      "grad_norm": 0.412449449300766,
      "learning_rate": 1.835660260049669e-05,
      "loss": 1.081,
      "step": 2243
    },
    {
      "epoch": 20.980842911877396,
      "grad_norm": 0.40597477555274963,
      "learning_rate": 1.8354939717104605e-05,
      "loss": 1.0854,
      "step": 2244
    },
    {
      "epoch": 20.99027409372237,
      "grad_norm": 0.38151246309280396,
      "learning_rate": 1.83532760682357e-05,
      "loss": 1.1224,
      "step": 2245
    },
    {
      "epoch": 20.999705275567344,
      "grad_norm": 0.4573107361793518,
      "learning_rate": 1.835161165404241e-05,
      "loss": 1.0831,
      "step": 2246
    },
    {
      "epoch": 21.0,
      "grad_norm": 1.9876729249954224,
      "learning_rate": 1.8349946474677227e-05,
      "loss": 0.7429,
      "step": 2247
    },
    {
      "epoch": 21.009431181844974,
      "grad_norm": 0.41081947088241577,
      "learning_rate": 1.8348280530292712e-05,
      "loss": 1.0795,
      "step": 2248
    },
    {
      "epoch": 21.01886236368995,
      "grad_norm": 0.44606950879096985,
      "learning_rate": 1.8346613821041498e-05,
      "loss": 1.0894,
      "step": 2249
    },
    {
      "epoch": 21.028293545534925,
      "grad_norm": 0.38868728280067444,
      "learning_rate": 1.834494634707629e-05,
      "loss": 1.1128,
      "step": 2250
    },
    {
      "epoch": 21.0377247273799,
      "grad_norm": 0.37410688400268555,
      "learning_rate": 1.834327810854986e-05,
      "loss": 1.1017,
      "step": 2251
    },
    {
      "epoch": 21.047155909224873,
      "grad_norm": 0.40184786915779114,
      "learning_rate": 1.8341609105615053e-05,
      "loss": 1.0944,
      "step": 2252
    },
    {
      "epoch": 21.05658709106985,
      "grad_norm": 0.40939056873321533,
      "learning_rate": 1.833993933842478e-05,
      "loss": 1.1091,
      "step": 2253
    },
    {
      "epoch": 21.066018272914825,
      "grad_norm": 0.3875313401222229,
      "learning_rate": 1.8338268807132023e-05,
      "loss": 1.0779,
      "step": 2254
    },
    {
      "epoch": 21.0754494547598,
      "grad_norm": 0.4238894581794739,
      "learning_rate": 1.8336597511889842e-05,
      "loss": 1.0726,
      "step": 2255
    },
    {
      "epoch": 21.084880636604776,
      "grad_norm": 0.3873146176338196,
      "learning_rate": 1.8334925452851357e-05,
      "loss": 1.1067,
      "step": 2256
    },
    {
      "epoch": 21.09431181844975,
      "grad_norm": 0.38704079389572144,
      "learning_rate": 1.8333252630169756e-05,
      "loss": 1.0711,
      "step": 2257
    },
    {
      "epoch": 21.103743000294724,
      "grad_norm": 0.39286547899246216,
      "learning_rate": 1.8331579043998315e-05,
      "loss": 1.1391,
      "step": 2258
    },
    {
      "epoch": 21.113174182139698,
      "grad_norm": 0.41796305775642395,
      "learning_rate": 1.8329904694490357e-05,
      "loss": 1.101,
      "step": 2259
    },
    {
      "epoch": 21.122605363984675,
      "grad_norm": 0.46423372626304626,
      "learning_rate": 1.832822958179929e-05,
      "loss": 1.0691,
      "step": 2260
    },
    {
      "epoch": 21.13203654582965,
      "grad_norm": 0.4159824550151825,
      "learning_rate": 1.8326553706078584e-05,
      "loss": 1.1002,
      "step": 2261
    },
    {
      "epoch": 21.141467727674623,
      "grad_norm": 0.4019080400466919,
      "learning_rate": 1.8324877067481782e-05,
      "loss": 1.0898,
      "step": 2262
    },
    {
      "epoch": 21.1508989095196,
      "grad_norm": 0.4225274920463562,
      "learning_rate": 1.8323199666162504e-05,
      "loss": 1.0908,
      "step": 2263
    },
    {
      "epoch": 21.160330091364575,
      "grad_norm": 0.3989306092262268,
      "learning_rate": 1.8321521502274425e-05,
      "loss": 1.095,
      "step": 2264
    },
    {
      "epoch": 21.16976127320955,
      "grad_norm": 0.39025261998176575,
      "learning_rate": 1.8319842575971303e-05,
      "loss": 1.1227,
      "step": 2265
    },
    {
      "epoch": 21.179192455054523,
      "grad_norm": 0.42007869482040405,
      "learning_rate": 1.8318162887406955e-05,
      "loss": 1.0912,
      "step": 2266
    },
    {
      "epoch": 21.1886236368995,
      "grad_norm": 0.41856780648231506,
      "learning_rate": 1.831648243673528e-05,
      "loss": 1.0992,
      "step": 2267
    },
    {
      "epoch": 21.198054818744474,
      "grad_norm": 0.4659782946109772,
      "learning_rate": 1.8314801224110236e-05,
      "loss": 1.1363,
      "step": 2268
    },
    {
      "epoch": 21.207486000589448,
      "grad_norm": 0.4543668031692505,
      "learning_rate": 1.831311924968586e-05,
      "loss": 1.1104,
      "step": 2269
    },
    {
      "epoch": 21.216917182434425,
      "grad_norm": 0.4232463240623474,
      "learning_rate": 1.8311436513616243e-05,
      "loss": 1.1012,
      "step": 2270
    },
    {
      "epoch": 21.2263483642794,
      "grad_norm": 0.42714738845825195,
      "learning_rate": 1.830975301605557e-05,
      "loss": 1.1122,
      "step": 2271
    },
    {
      "epoch": 21.235779546124373,
      "grad_norm": 0.38645946979522705,
      "learning_rate": 1.8308068757158073e-05,
      "loss": 1.0842,
      "step": 2272
    },
    {
      "epoch": 21.245210727969347,
      "grad_norm": 0.4353278875350952,
      "learning_rate": 1.8306383737078074e-05,
      "loss": 1.0976,
      "step": 2273
    },
    {
      "epoch": 21.254641909814325,
      "grad_norm": 0.3956269323825836,
      "learning_rate": 1.8304697955969942e-05,
      "loss": 1.1089,
      "step": 2274
    },
    {
      "epoch": 21.2640730916593,
      "grad_norm": 0.4504848122596741,
      "learning_rate": 1.8303011413988133e-05,
      "loss": 1.0448,
      "step": 2275
    },
    {
      "epoch": 21.273504273504273,
      "grad_norm": 0.43079930543899536,
      "learning_rate": 1.8301324111287166e-05,
      "loss": 1.0931,
      "step": 2276
    },
    {
      "epoch": 21.28293545534925,
      "grad_norm": 0.45898550748825073,
      "learning_rate": 1.829963604802163e-05,
      "loss": 1.0462,
      "step": 2277
    },
    {
      "epoch": 21.292366637194224,
      "grad_norm": 0.4358067810535431,
      "learning_rate": 1.829794722434619e-05,
      "loss": 1.0854,
      "step": 2278
    },
    {
      "epoch": 21.301797819039198,
      "grad_norm": 0.4379771649837494,
      "learning_rate": 1.8296257640415572e-05,
      "loss": 1.0999,
      "step": 2279
    },
    {
      "epoch": 21.311229000884172,
      "grad_norm": 0.39639413356781006,
      "learning_rate": 1.829456729638458e-05,
      "loss": 1.0944,
      "step": 2280
    },
    {
      "epoch": 21.32066018272915,
      "grad_norm": 0.41376474499702454,
      "learning_rate": 1.829287619240807e-05,
      "loss": 1.0666,
      "step": 2281
    },
    {
      "epoch": 21.330091364574123,
      "grad_norm": 0.39768320322036743,
      "learning_rate": 1.8291184328640992e-05,
      "loss": 1.1008,
      "step": 2282
    },
    {
      "epoch": 21.339522546419097,
      "grad_norm": 0.4643915891647339,
      "learning_rate": 1.8289491705238355e-05,
      "loss": 1.0794,
      "step": 2283
    },
    {
      "epoch": 21.348953728264075,
      "grad_norm": 0.4460948705673218,
      "learning_rate": 1.828779832235523e-05,
      "loss": 1.1344,
      "step": 2284
    },
    {
      "epoch": 21.35838491010905,
      "grad_norm": 0.400535523891449,
      "learning_rate": 1.8286104180146762e-05,
      "loss": 1.107,
      "step": 2285
    },
    {
      "epoch": 21.367816091954023,
      "grad_norm": 0.4171527624130249,
      "learning_rate": 1.8284409278768177e-05,
      "loss": 1.1122,
      "step": 2286
    },
    {
      "epoch": 21.377247273798996,
      "grad_norm": 0.40939605236053467,
      "learning_rate": 1.8282713618374752e-05,
      "loss": 1.0906,
      "step": 2287
    },
    {
      "epoch": 21.386678455643974,
      "grad_norm": 0.3962380588054657,
      "learning_rate": 1.828101719912185e-05,
      "loss": 1.0954,
      "step": 2288
    },
    {
      "epoch": 21.396109637488948,
      "grad_norm": 0.405676007270813,
      "learning_rate": 1.8279320021164895e-05,
      "loss": 1.0935,
      "step": 2289
    },
    {
      "epoch": 21.405540819333922,
      "grad_norm": 0.4152052104473114,
      "learning_rate": 1.8277622084659377e-05,
      "loss": 1.0527,
      "step": 2290
    },
    {
      "epoch": 21.4149720011789,
      "grad_norm": 0.41046130657196045,
      "learning_rate": 1.827592338976087e-05,
      "loss": 1.0997,
      "step": 2291
    },
    {
      "epoch": 21.424403183023873,
      "grad_norm": 0.41100287437438965,
      "learning_rate": 1.8274223936624996e-05,
      "loss": 1.1103,
      "step": 2292
    },
    {
      "epoch": 21.433834364868847,
      "grad_norm": 0.44341734051704407,
      "learning_rate": 1.8272523725407467e-05,
      "loss": 1.1255,
      "step": 2293
    },
    {
      "epoch": 21.44326554671382,
      "grad_norm": 0.4264436662197113,
      "learning_rate": 1.827082275626405e-05,
      "loss": 1.1203,
      "step": 2294
    },
    {
      "epoch": 21.4526967285588,
      "grad_norm": 0.4207381010055542,
      "learning_rate": 1.8269121029350595e-05,
      "loss": 1.1004,
      "step": 2295
    },
    {
      "epoch": 21.462127910403773,
      "grad_norm": 0.46016472578048706,
      "learning_rate": 1.826741854482301e-05,
      "loss": 1.0894,
      "step": 2296
    },
    {
      "epoch": 21.471559092248746,
      "grad_norm": 0.44545215368270874,
      "learning_rate": 1.826571530283727e-05,
      "loss": 1.0734,
      "step": 2297
    },
    {
      "epoch": 21.480990274093724,
      "grad_norm": 0.44951727986335754,
      "learning_rate": 1.826401130354943e-05,
      "loss": 1.0803,
      "step": 2298
    },
    {
      "epoch": 21.490421455938698,
      "grad_norm": 0.4449913203716278,
      "learning_rate": 1.8262306547115612e-05,
      "loss": 1.0874,
      "step": 2299
    },
    {
      "epoch": 21.499852637783672,
      "grad_norm": 0.4733768105506897,
      "learning_rate": 1.8260601033692e-05,
      "loss": 1.0916,
      "step": 2300
    },
    {
      "epoch": 21.509283819628646,
      "grad_norm": 0.4377433657646179,
      "learning_rate": 1.8258894763434864e-05,
      "loss": 1.0853,
      "step": 2301
    },
    {
      "epoch": 21.518715001473623,
      "grad_norm": 0.3953649699687958,
      "learning_rate": 1.8257187736500518e-05,
      "loss": 1.1179,
      "step": 2302
    },
    {
      "epoch": 21.528146183318597,
      "grad_norm": 0.4367642402648926,
      "learning_rate": 1.8255479953045366e-05,
      "loss": 1.0638,
      "step": 2303
    },
    {
      "epoch": 21.53757736516357,
      "grad_norm": 0.394901305437088,
      "learning_rate": 1.8253771413225872e-05,
      "loss": 1.0607,
      "step": 2304
    },
    {
      "epoch": 21.54700854700855,
      "grad_norm": 0.430342435836792,
      "learning_rate": 1.825206211719858e-05,
      "loss": 1.1107,
      "step": 2305
    },
    {
      "epoch": 21.556439728853523,
      "grad_norm": 0.41203126311302185,
      "learning_rate": 1.8250352065120082e-05,
      "loss": 1.1251,
      "step": 2306
    },
    {
      "epoch": 21.565870910698496,
      "grad_norm": 0.4394165277481079,
      "learning_rate": 1.8248641257147062e-05,
      "loss": 1.0949,
      "step": 2307
    },
    {
      "epoch": 21.57530209254347,
      "grad_norm": 0.4509755074977875,
      "learning_rate": 1.824692969343626e-05,
      "loss": 1.1133,
      "step": 2308
    },
    {
      "epoch": 21.584733274388448,
      "grad_norm": 0.45029813051223755,
      "learning_rate": 1.824521737414449e-05,
      "loss": 1.1029,
      "step": 2309
    },
    {
      "epoch": 21.594164456233422,
      "grad_norm": 0.40469890832901,
      "learning_rate": 1.8243504299428634e-05,
      "loss": 1.0702,
      "step": 2310
    },
    {
      "epoch": 21.603595638078396,
      "grad_norm": 0.42972710728645325,
      "learning_rate": 1.8241790469445644e-05,
      "loss": 1.0678,
      "step": 2311
    },
    {
      "epoch": 21.613026819923373,
      "grad_norm": 0.4257171154022217,
      "learning_rate": 1.8240075884352538e-05,
      "loss": 1.119,
      "step": 2312
    },
    {
      "epoch": 21.622458001768347,
      "grad_norm": 0.39709803462028503,
      "learning_rate": 1.823836054430641e-05,
      "loss": 1.1025,
      "step": 2313
    },
    {
      "epoch": 21.63188918361332,
      "grad_norm": 0.40291038155555725,
      "learning_rate": 1.823664444946441e-05,
      "loss": 1.1469,
      "step": 2314
    },
    {
      "epoch": 21.641320365458295,
      "grad_norm": 0.4650978446006775,
      "learning_rate": 1.8234927599983782e-05,
      "loss": 1.0871,
      "step": 2315
    },
    {
      "epoch": 21.650751547303273,
      "grad_norm": 0.41283664107322693,
      "learning_rate": 1.8233209996021806e-05,
      "loss": 1.1145,
      "step": 2316
    },
    {
      "epoch": 21.660182729148246,
      "grad_norm": 0.4385949671268463,
      "learning_rate": 1.8231491637735858e-05,
      "loss": 1.0942,
      "step": 2317
    },
    {
      "epoch": 21.66961391099322,
      "grad_norm": 0.39438486099243164,
      "learning_rate": 1.8229772525283372e-05,
      "loss": 1.0635,
      "step": 2318
    },
    {
      "epoch": 21.679045092838198,
      "grad_norm": 0.38201549649238586,
      "learning_rate": 1.822805265882185e-05,
      "loss": 1.1147,
      "step": 2319
    },
    {
      "epoch": 21.688476274683172,
      "grad_norm": 0.5142001509666443,
      "learning_rate": 1.822633203850887e-05,
      "loss": 1.0705,
      "step": 2320
    },
    {
      "epoch": 21.697907456528146,
      "grad_norm": 0.40412279963493347,
      "learning_rate": 1.822461066450207e-05,
      "loss": 1.0749,
      "step": 2321
    },
    {
      "epoch": 21.70733863837312,
      "grad_norm": 0.3908655643463135,
      "learning_rate": 1.8222888536959164e-05,
      "loss": 1.0986,
      "step": 2322
    },
    {
      "epoch": 21.716769820218097,
      "grad_norm": 0.38712385296821594,
      "learning_rate": 1.8221165656037933e-05,
      "loss": 1.1174,
      "step": 2323
    },
    {
      "epoch": 21.72620100206307,
      "grad_norm": 0.4303358793258667,
      "learning_rate": 1.8219442021896224e-05,
      "loss": 1.0572,
      "step": 2324
    },
    {
      "epoch": 21.735632183908045,
      "grad_norm": 0.4467642903327942,
      "learning_rate": 1.8217717634691962e-05,
      "loss": 1.0767,
      "step": 2325
    },
    {
      "epoch": 21.745063365753023,
      "grad_norm": 0.4099315404891968,
      "learning_rate": 1.821599249458313e-05,
      "loss": 1.0768,
      "step": 2326
    },
    {
      "epoch": 21.754494547597997,
      "grad_norm": 0.4257630705833435,
      "learning_rate": 1.8214266601727784e-05,
      "loss": 1.0555,
      "step": 2327
    },
    {
      "epoch": 21.76392572944297,
      "grad_norm": 0.4229999780654907,
      "learning_rate": 1.8212539956284052e-05,
      "loss": 1.1039,
      "step": 2328
    },
    {
      "epoch": 21.773356911287944,
      "grad_norm": 0.4376743733882904,
      "learning_rate": 1.8210812558410128e-05,
      "loss": 1.0713,
      "step": 2329
    },
    {
      "epoch": 21.782788093132922,
      "grad_norm": 0.4251416027545929,
      "learning_rate": 1.8209084408264274e-05,
      "loss": 1.121,
      "step": 2330
    },
    {
      "epoch": 21.792219274977896,
      "grad_norm": 0.35633543133735657,
      "learning_rate": 1.8207355506004826e-05,
      "loss": 1.1078,
      "step": 2331
    },
    {
      "epoch": 21.80165045682287,
      "grad_norm": 0.44347137212753296,
      "learning_rate": 1.8205625851790185e-05,
      "loss": 1.107,
      "step": 2332
    },
    {
      "epoch": 21.811081638667847,
      "grad_norm": 0.4388163685798645,
      "learning_rate": 1.8203895445778817e-05,
      "loss": 1.0991,
      "step": 2333
    },
    {
      "epoch": 21.82051282051282,
      "grad_norm": 0.42294058203697205,
      "learning_rate": 1.8202164288129263e-05,
      "loss": 1.0888,
      "step": 2334
    },
    {
      "epoch": 21.829944002357795,
      "grad_norm": 0.4176185131072998,
      "learning_rate": 1.8200432379000136e-05,
      "loss": 1.1212,
      "step": 2335
    },
    {
      "epoch": 21.83937518420277,
      "grad_norm": 0.4108119010925293,
      "learning_rate": 1.819869971855011e-05,
      "loss": 1.1062,
      "step": 2336
    },
    {
      "epoch": 21.848806366047747,
      "grad_norm": 0.42825597524642944,
      "learning_rate": 1.819696630693793e-05,
      "loss": 1.1284,
      "step": 2337
    },
    {
      "epoch": 21.85823754789272,
      "grad_norm": 0.44872474670410156,
      "learning_rate": 1.819523214432241e-05,
      "loss": 1.1037,
      "step": 2338
    },
    {
      "epoch": 21.867668729737694,
      "grad_norm": 0.4117467999458313,
      "learning_rate": 1.8193497230862428e-05,
      "loss": 1.1047,
      "step": 2339
    },
    {
      "epoch": 21.877099911582672,
      "grad_norm": 0.40190720558166504,
      "learning_rate": 1.819176156671695e-05,
      "loss": 1.0867,
      "step": 2340
    },
    {
      "epoch": 21.886531093427646,
      "grad_norm": 0.4338245987892151,
      "learning_rate": 1.8190025152044984e-05,
      "loss": 1.1274,
      "step": 2341
    },
    {
      "epoch": 21.89596227527262,
      "grad_norm": 0.4126332402229309,
      "learning_rate": 1.8188287987005627e-05,
      "loss": 1.0818,
      "step": 2342
    },
    {
      "epoch": 21.905393457117594,
      "grad_norm": 0.41572508215904236,
      "learning_rate": 1.818655007175804e-05,
      "loss": 1.0793,
      "step": 2343
    },
    {
      "epoch": 21.91482463896257,
      "grad_norm": 0.3871474862098694,
      "learning_rate": 1.818481140646144e-05,
      "loss": 1.1119,
      "step": 2344
    },
    {
      "epoch": 21.924255820807545,
      "grad_norm": 0.44457775354385376,
      "learning_rate": 1.8183071991275126e-05,
      "loss": 1.1634,
      "step": 2345
    },
    {
      "epoch": 21.93368700265252,
      "grad_norm": 0.4201756715774536,
      "learning_rate": 1.8181331826358472e-05,
      "loss": 1.1117,
      "step": 2346
    },
    {
      "epoch": 21.943118184497497,
      "grad_norm": 0.44128572940826416,
      "learning_rate": 1.81795909118709e-05,
      "loss": 1.0904,
      "step": 2347
    },
    {
      "epoch": 21.95254936634247,
      "grad_norm": 0.42756348848342896,
      "learning_rate": 1.817784924797192e-05,
      "loss": 1.111,
      "step": 2348
    },
    {
      "epoch": 21.961980548187444,
      "grad_norm": 0.42837226390838623,
      "learning_rate": 1.8176106834821097e-05,
      "loss": 1.1101,
      "step": 2349
    },
    {
      "epoch": 21.97141173003242,
      "grad_norm": 0.4226764440536499,
      "learning_rate": 1.8174363672578074e-05,
      "loss": 1.0672,
      "step": 2350
    },
    {
      "epoch": 21.980842911877396,
      "grad_norm": 0.39422890543937683,
      "learning_rate": 1.8172619761402556e-05,
      "loss": 1.0871,
      "step": 2351
    },
    {
      "epoch": 21.99027409372237,
      "grad_norm": 0.41878482699394226,
      "learning_rate": 1.817087510145432e-05,
      "loss": 1.0997,
      "step": 2352
    },
    {
      "epoch": 21.999705275567344,
      "grad_norm": 0.4027596116065979,
      "learning_rate": 1.8169129692893213e-05,
      "loss": 1.0863,
      "step": 2353
    },
    {
      "epoch": 22.0,
      "grad_norm": 2.0625357627868652,
      "learning_rate": 1.816738353587915e-05,
      "loss": 0.7073,
      "step": 2354
    },
    {
      "epoch": 22.009431181844974,
      "grad_norm": 0.39088305830955505,
      "learning_rate": 1.816563663057211e-05,
      "loss": 1.0818,
      "step": 2355
    },
    {
      "epoch": 22.01886236368995,
      "grad_norm": 0.42028963565826416,
      "learning_rate": 1.8163888977132144e-05,
      "loss": 1.1023,
      "step": 2356
    },
    {
      "epoch": 22.028293545534925,
      "grad_norm": 0.4144330322742462,
      "learning_rate": 1.8162140575719376e-05,
      "loss": 1.0856,
      "step": 2357
    },
    {
      "epoch": 22.0377247273799,
      "grad_norm": 0.39582374691963196,
      "learning_rate": 1.816039142649399e-05,
      "loss": 1.0834,
      "step": 2358
    },
    {
      "epoch": 22.047155909224873,
      "grad_norm": 0.3986172080039978,
      "learning_rate": 1.815864152961624e-05,
      "loss": 1.072,
      "step": 2359
    },
    {
      "epoch": 22.05658709106985,
      "grad_norm": 0.4590773582458496,
      "learning_rate": 1.8156890885246454e-05,
      "loss": 1.1021,
      "step": 2360
    },
    {
      "epoch": 22.066018272914825,
      "grad_norm": 0.4382425844669342,
      "learning_rate": 1.815513949354503e-05,
      "loss": 1.0605,
      "step": 2361
    },
    {
      "epoch": 22.0754494547598,
      "grad_norm": 0.4127212166786194,
      "learning_rate": 1.815338735467242e-05,
      "loss": 1.122,
      "step": 2362
    },
    {
      "epoch": 22.084880636604776,
      "grad_norm": 0.4149150848388672,
      "learning_rate": 1.8151634468789162e-05,
      "loss": 1.1114,
      "step": 2363
    },
    {
      "epoch": 22.09431181844975,
      "grad_norm": 0.4103235602378845,
      "learning_rate": 1.8149880836055853e-05,
      "loss": 1.1247,
      "step": 2364
    },
    {
      "epoch": 22.103743000294724,
      "grad_norm": 0.419278085231781,
      "learning_rate": 1.8148126456633162e-05,
      "loss": 1.0922,
      "step": 2365
    },
    {
      "epoch": 22.113174182139698,
      "grad_norm": 0.427690714597702,
      "learning_rate": 1.814637133068182e-05,
      "loss": 1.0742,
      "step": 2366
    },
    {
      "epoch": 22.122605363984675,
      "grad_norm": 0.41156861186027527,
      "learning_rate": 1.8144615458362633e-05,
      "loss": 1.0886,
      "step": 2367
    },
    {
      "epoch": 22.13203654582965,
      "grad_norm": 0.41552093625068665,
      "learning_rate": 1.8142858839836478e-05,
      "loss": 1.0957,
      "step": 2368
    },
    {
      "epoch": 22.141467727674623,
      "grad_norm": 0.39224228262901306,
      "learning_rate": 1.8141101475264285e-05,
      "loss": 1.0901,
      "step": 2369
    },
    {
      "epoch": 22.1508989095196,
      "grad_norm": 0.3920116722583771,
      "learning_rate": 1.8139343364807077e-05,
      "loss": 1.0962,
      "step": 2370
    },
    {
      "epoch": 22.160330091364575,
      "grad_norm": 0.4140411913394928,
      "learning_rate": 1.8137584508625917e-05,
      "loss": 1.0868,
      "step": 2371
    },
    {
      "epoch": 22.16976127320955,
      "grad_norm": 0.41561171412467957,
      "learning_rate": 1.8135824906881967e-05,
      "loss": 1.1309,
      "step": 2372
    },
    {
      "epoch": 22.179192455054523,
      "grad_norm": 0.4204477369785309,
      "learning_rate": 1.8134064559736425e-05,
      "loss": 1.0952,
      "step": 2373
    },
    {
      "epoch": 22.1886236368995,
      "grad_norm": 0.44500356912612915,
      "learning_rate": 1.8132303467350585e-05,
      "loss": 1.1138,
      "step": 2374
    },
    {
      "epoch": 22.198054818744474,
      "grad_norm": 0.39948970079421997,
      "learning_rate": 1.8130541629885793e-05,
      "loss": 1.1043,
      "step": 2375
    },
    {
      "epoch": 22.207486000589448,
      "grad_norm": 0.43712112307548523,
      "learning_rate": 1.812877904750347e-05,
      "loss": 1.0318,
      "step": 2376
    },
    {
      "epoch": 22.216917182434425,
      "grad_norm": 0.40490081906318665,
      "learning_rate": 1.8127015720365098e-05,
      "loss": 1.0918,
      "step": 2377
    },
    {
      "epoch": 22.2263483642794,
      "grad_norm": 0.42532214522361755,
      "learning_rate": 1.8125251648632237e-05,
      "loss": 1.1194,
      "step": 2378
    },
    {
      "epoch": 22.235779546124373,
      "grad_norm": 0.4014604389667511,
      "learning_rate": 1.8123486832466513e-05,
      "loss": 1.102,
      "step": 2379
    },
    {
      "epoch": 22.245210727969347,
      "grad_norm": 0.4272404909133911,
      "learning_rate": 1.8121721272029612e-05,
      "loss": 1.0868,
      "step": 2380
    },
    {
      "epoch": 22.254641909814325,
      "grad_norm": 0.44225871562957764,
      "learning_rate": 1.81199549674833e-05,
      "loss": 1.0835,
      "step": 2381
    },
    {
      "epoch": 22.2640730916593,
      "grad_norm": 0.4243718981742859,
      "learning_rate": 1.8118187918989395e-05,
      "loss": 1.0937,
      "step": 2382
    },
    {
      "epoch": 22.273504273504273,
      "grad_norm": 0.43613579869270325,
      "learning_rate": 1.811642012670981e-05,
      "loss": 1.1011,
      "step": 2383
    },
    {
      "epoch": 22.28293545534925,
      "grad_norm": 0.4330958127975464,
      "learning_rate": 1.8114651590806494e-05,
      "loss": 1.0746,
      "step": 2384
    },
    {
      "epoch": 22.292366637194224,
      "grad_norm": 0.4018012285232544,
      "learning_rate": 1.8112882311441487e-05,
      "loss": 1.1094,
      "step": 2385
    },
    {
      "epoch": 22.301797819039198,
      "grad_norm": 0.413553386926651,
      "learning_rate": 1.8111112288776886e-05,
      "loss": 1.0904,
      "step": 2386
    },
    {
      "epoch": 22.311229000884172,
      "grad_norm": 0.4407220184803009,
      "learning_rate": 1.8109341522974864e-05,
      "loss": 1.0686,
      "step": 2387
    },
    {
      "epoch": 22.32066018272915,
      "grad_norm": 0.3905607759952545,
      "learning_rate": 1.810757001419766e-05,
      "loss": 1.1052,
      "step": 2388
    },
    {
      "epoch": 22.330091364574123,
      "grad_norm": 0.4112051725387573,
      "learning_rate": 1.8105797762607568e-05,
      "loss": 1.0861,
      "step": 2389
    },
    {
      "epoch": 22.339522546419097,
      "grad_norm": 0.4202785789966583,
      "learning_rate": 1.8104024768366975e-05,
      "loss": 1.0962,
      "step": 2390
    },
    {
      "epoch": 22.348953728264075,
      "grad_norm": 0.40321311354637146,
      "learning_rate": 1.8102251031638314e-05,
      "loss": 1.0893,
      "step": 2391
    },
    {
      "epoch": 22.35838491010905,
      "grad_norm": 0.43101710081100464,
      "learning_rate": 1.8100476552584093e-05,
      "loss": 1.0687,
      "step": 2392
    },
    {
      "epoch": 22.367816091954023,
      "grad_norm": 0.43609628081321716,
      "learning_rate": 1.8098701331366895e-05,
      "loss": 1.1038,
      "step": 2393
    },
    {
      "epoch": 22.377247273798996,
      "grad_norm": 0.4585578739643097,
      "learning_rate": 1.809692536814936e-05,
      "loss": 1.0917,
      "step": 2394
    },
    {
      "epoch": 22.386678455643974,
      "grad_norm": 0.4074287712574005,
      "learning_rate": 1.8095148663094203e-05,
      "loss": 1.0847,
      "step": 2395
    },
    {
      "epoch": 22.396109637488948,
      "grad_norm": 0.39621803164482117,
      "learning_rate": 1.8093371216364208e-05,
      "loss": 1.0974,
      "step": 2396
    },
    {
      "epoch": 22.405540819333922,
      "grad_norm": 0.45358923077583313,
      "learning_rate": 1.8091593028122225e-05,
      "loss": 1.0646,
      "step": 2397
    },
    {
      "epoch": 22.4149720011789,
      "grad_norm": 0.4148038625717163,
      "learning_rate": 1.808981409853116e-05,
      "loss": 1.0922,
      "step": 2398
    },
    {
      "epoch": 22.424403183023873,
      "grad_norm": 0.4099191725254059,
      "learning_rate": 1.8088034427754007e-05,
      "loss": 1.0949,
      "step": 2399
    },
    {
      "epoch": 22.433834364868847,
      "grad_norm": 0.40969040989875793,
      "learning_rate": 1.8086254015953822e-05,
      "loss": 1.1427,
      "step": 2400
    },
    {
      "epoch": 22.44326554671382,
      "grad_norm": 0.39690127968788147,
      "learning_rate": 1.8084472863293716e-05,
      "loss": 1.1359,
      "step": 2401
    },
    {
      "epoch": 22.4526967285588,
      "grad_norm": 0.48235657811164856,
      "learning_rate": 1.8082690969936884e-05,
      "loss": 1.1021,
      "step": 2402
    },
    {
      "epoch": 22.462127910403773,
      "grad_norm": 0.3931909501552582,
      "learning_rate": 1.8080908336046583e-05,
      "loss": 1.1228,
      "step": 2403
    },
    {
      "epoch": 22.471559092248746,
      "grad_norm": 0.41655227541923523,
      "learning_rate": 1.8079124961786135e-05,
      "loss": 1.0928,
      "step": 2404
    },
    {
      "epoch": 22.480990274093724,
      "grad_norm": 0.4170392155647278,
      "learning_rate": 1.8077340847318933e-05,
      "loss": 1.0867,
      "step": 2405
    },
    {
      "epoch": 22.490421455938698,
      "grad_norm": 0.442888081073761,
      "learning_rate": 1.8075555992808433e-05,
      "loss": 1.1008,
      "step": 2406
    },
    {
      "epoch": 22.499852637783672,
      "grad_norm": 0.4364156126976013,
      "learning_rate": 1.807377039841817e-05,
      "loss": 1.1329,
      "step": 2407
    },
    {
      "epoch": 22.509283819628646,
      "grad_norm": 0.43346107006073,
      "learning_rate": 1.8071984064311734e-05,
      "loss": 1.0957,
      "step": 2408
    },
    {
      "epoch": 22.518715001473623,
      "grad_norm": 0.4234200119972229,
      "learning_rate": 1.8070196990652792e-05,
      "loss": 1.0677,
      "step": 2409
    },
    {
      "epoch": 22.528146183318597,
      "grad_norm": 0.402593195438385,
      "learning_rate": 1.806840917760507e-05,
      "loss": 1.0771,
      "step": 2410
    },
    {
      "epoch": 22.53757736516357,
      "grad_norm": 0.44023147225379944,
      "learning_rate": 1.8066620625332375e-05,
      "loss": 1.1232,
      "step": 2411
    },
    {
      "epoch": 22.54700854700855,
      "grad_norm": 0.4439500868320465,
      "learning_rate": 1.8064831333998567e-05,
      "loss": 1.1047,
      "step": 2412
    },
    {
      "epoch": 22.556439728853523,
      "grad_norm": 0.4504784643650055,
      "learning_rate": 1.8063041303767582e-05,
      "loss": 1.0988,
      "step": 2413
    },
    {
      "epoch": 22.565870910698496,
      "grad_norm": 0.3819068670272827,
      "learning_rate": 1.8061250534803422e-05,
      "loss": 1.1248,
      "step": 2414
    },
    {
      "epoch": 22.57530209254347,
      "grad_norm": 0.44599130749702454,
      "learning_rate": 1.8059459027270158e-05,
      "loss": 1.0802,
      "step": 2415
    },
    {
      "epoch": 22.584733274388448,
      "grad_norm": 0.45872876048088074,
      "learning_rate": 1.805766678133192e-05,
      "loss": 1.0966,
      "step": 2416
    },
    {
      "epoch": 22.594164456233422,
      "grad_norm": 0.40735921263694763,
      "learning_rate": 1.8055873797152926e-05,
      "loss": 1.0776,
      "step": 2417
    },
    {
      "epoch": 22.603595638078396,
      "grad_norm": 0.42487505078315735,
      "learning_rate": 1.805408007489744e-05,
      "loss": 1.133,
      "step": 2418
    },
    {
      "epoch": 22.613026819923373,
      "grad_norm": 0.42346715927124023,
      "learning_rate": 1.8052285614729803e-05,
      "loss": 1.0743,
      "step": 2419
    },
    {
      "epoch": 22.622458001768347,
      "grad_norm": 0.407976895570755,
      "learning_rate": 1.8050490416814422e-05,
      "loss": 1.0628,
      "step": 2420
    },
    {
      "epoch": 22.63188918361332,
      "grad_norm": 0.4145120680332184,
      "learning_rate": 1.804869448131578e-05,
      "loss": 1.0664,
      "step": 2421
    },
    {
      "epoch": 22.641320365458295,
      "grad_norm": 0.3816611170768738,
      "learning_rate": 1.804689780839841e-05,
      "loss": 1.1028,
      "step": 2422
    },
    {
      "epoch": 22.650751547303273,
      "grad_norm": 0.454684853553772,
      "learning_rate": 1.8045100398226924e-05,
      "loss": 1.0973,
      "step": 2423
    },
    {
      "epoch": 22.660182729148246,
      "grad_norm": 0.4221593141555786,
      "learning_rate": 1.804330225096601e-05,
      "loss": 1.1169,
      "step": 2424
    },
    {
      "epoch": 22.66961391099322,
      "grad_norm": 0.4355598986148834,
      "learning_rate": 1.80415033667804e-05,
      "loss": 1.1153,
      "step": 2425
    },
    {
      "epoch": 22.679045092838198,
      "grad_norm": 0.4303300082683563,
      "learning_rate": 1.8039703745834917e-05,
      "loss": 1.1385,
      "step": 2426
    },
    {
      "epoch": 22.688476274683172,
      "grad_norm": 0.45220595598220825,
      "learning_rate": 1.803790338829444e-05,
      "loss": 1.0748,
      "step": 2427
    },
    {
      "epoch": 22.697907456528146,
      "grad_norm": 0.4140205979347229,
      "learning_rate": 1.803610229432391e-05,
      "loss": 1.1114,
      "step": 2428
    },
    {
      "epoch": 22.70733863837312,
      "grad_norm": 0.4465577304363251,
      "learning_rate": 1.8034300464088353e-05,
      "loss": 1.1038,
      "step": 2429
    },
    {
      "epoch": 22.716769820218097,
      "grad_norm": 0.4258706271648407,
      "learning_rate": 1.8032497897752845e-05,
      "loss": 1.095,
      "step": 2430
    },
    {
      "epoch": 22.72620100206307,
      "grad_norm": 0.4391726851463318,
      "learning_rate": 1.803069459548254e-05,
      "loss": 1.056,
      "step": 2431
    },
    {
      "epoch": 22.735632183908045,
      "grad_norm": 0.43160462379455566,
      "learning_rate": 1.8028890557442654e-05,
      "loss": 1.1065,
      "step": 2432
    },
    {
      "epoch": 22.745063365753023,
      "grad_norm": 0.4872667193412781,
      "learning_rate": 1.8027085783798476e-05,
      "loss": 1.0853,
      "step": 2433
    },
    {
      "epoch": 22.754494547597997,
      "grad_norm": 0.3846692442893982,
      "learning_rate": 1.802528027471535e-05,
      "loss": 1.1083,
      "step": 2434
    },
    {
      "epoch": 22.76392572944297,
      "grad_norm": 0.4424512982368469,
      "learning_rate": 1.8023474030358708e-05,
      "loss": 1.0746,
      "step": 2435
    },
    {
      "epoch": 22.773356911287944,
      "grad_norm": 0.414493590593338,
      "learning_rate": 1.802166705089403e-05,
      "loss": 1.1038,
      "step": 2436
    },
    {
      "epoch": 22.782788093132922,
      "grad_norm": 0.4111742079257965,
      "learning_rate": 1.8019859336486872e-05,
      "loss": 1.0603,
      "step": 2437
    },
    {
      "epoch": 22.792219274977896,
      "grad_norm": 0.42926397919654846,
      "learning_rate": 1.8018050887302855e-05,
      "loss": 1.0929,
      "step": 2438
    },
    {
      "epoch": 22.80165045682287,
      "grad_norm": 0.40493693947792053,
      "learning_rate": 1.801624170350767e-05,
      "loss": 1.0961,
      "step": 2439
    },
    {
      "epoch": 22.811081638667847,
      "grad_norm": 0.41980740427970886,
      "learning_rate": 1.8014431785267077e-05,
      "loss": 1.1046,
      "step": 2440
    },
    {
      "epoch": 22.82051282051282,
      "grad_norm": 0.45362216234207153,
      "learning_rate": 1.8012621132746893e-05,
      "loss": 1.1175,
      "step": 2441
    },
    {
      "epoch": 22.829944002357795,
      "grad_norm": 0.457190603017807,
      "learning_rate": 1.801080974611302e-05,
      "loss": 1.1116,
      "step": 2442
    },
    {
      "epoch": 22.83937518420277,
      "grad_norm": 0.44054079055786133,
      "learning_rate": 1.8008997625531404e-05,
      "loss": 1.0789,
      "step": 2443
    },
    {
      "epoch": 22.848806366047747,
      "grad_norm": 0.42898106575012207,
      "learning_rate": 1.8007184771168078e-05,
      "loss": 1.0581,
      "step": 2444
    },
    {
      "epoch": 22.85823754789272,
      "grad_norm": 0.41582003235816956,
      "learning_rate": 1.8005371183189136e-05,
      "loss": 1.1349,
      "step": 2445
    },
    {
      "epoch": 22.867668729737694,
      "grad_norm": 0.4569457471370697,
      "learning_rate": 1.8003556861760737e-05,
      "loss": 1.1071,
      "step": 2446
    },
    {
      "epoch": 22.877099911582672,
      "grad_norm": 0.402172327041626,
      "learning_rate": 1.8001741807049106e-05,
      "loss": 1.0488,
      "step": 2447
    },
    {
      "epoch": 22.886531093427646,
      "grad_norm": 0.44194018840789795,
      "learning_rate": 1.7999926019220538e-05,
      "loss": 1.0802,
      "step": 2448
    },
    {
      "epoch": 22.89596227527262,
      "grad_norm": 0.40973347425460815,
      "learning_rate": 1.7998109498441397e-05,
      "loss": 1.1319,
      "step": 2449
    },
    {
      "epoch": 22.905393457117594,
      "grad_norm": 0.4441991448402405,
      "learning_rate": 1.7996292244878117e-05,
      "loss": 1.0811,
      "step": 2450
    },
    {
      "epoch": 22.91482463896257,
      "grad_norm": 0.4365827739238739,
      "learning_rate": 1.799447425869718e-05,
      "loss": 1.1129,
      "step": 2451
    },
    {
      "epoch": 22.924255820807545,
      "grad_norm": 0.4125273823738098,
      "learning_rate": 1.7992655540065166e-05,
      "loss": 1.0606,
      "step": 2452
    },
    {
      "epoch": 22.93368700265252,
      "grad_norm": 0.422199010848999,
      "learning_rate": 1.7990836089148692e-05,
      "loss": 1.1082,
      "step": 2453
    },
    {
      "epoch": 22.943118184497497,
      "grad_norm": 0.41679275035858154,
      "learning_rate": 1.7989015906114465e-05,
      "loss": 1.0709,
      "step": 2454
    },
    {
      "epoch": 22.95254936634247,
      "grad_norm": 0.45695269107818604,
      "learning_rate": 1.7987194991129243e-05,
      "loss": 1.1081,
      "step": 2455
    },
    {
      "epoch": 22.961980548187444,
      "grad_norm": 0.47956719994544983,
      "learning_rate": 1.798537334435986e-05,
      "loss": 1.0849,
      "step": 2456
    },
    {
      "epoch": 22.97141173003242,
      "grad_norm": 0.4106736183166504,
      "learning_rate": 1.7983550965973217e-05,
      "loss": 1.0782,
      "step": 2457
    },
    {
      "epoch": 22.980842911877396,
      "grad_norm": 0.42998576164245605,
      "learning_rate": 1.7981727856136276e-05,
      "loss": 1.1303,
      "step": 2458
    },
    {
      "epoch": 22.99027409372237,
      "grad_norm": 0.46406441926956177,
      "learning_rate": 1.7979904015016073e-05,
      "loss": 1.0581,
      "step": 2459
    },
    {
      "epoch": 22.999705275567344,
      "grad_norm": 0.4210427403450012,
      "learning_rate": 1.7978079442779706e-05,
      "loss": 1.1333,
      "step": 2460
    },
    {
      "epoch": 23.0,
      "grad_norm": 1.883737325668335,
      "learning_rate": 1.797625413959434e-05,
      "loss": 1.0854,
      "step": 2461
    },
    {
      "epoch": 23.009431181844974,
      "grad_norm": 0.4319457411766052,
      "learning_rate": 1.797442810562721e-05,
      "loss": 1.0541,
      "step": 2462
    },
    {
      "epoch": 23.01886236368995,
      "grad_norm": 0.3938503563404083,
      "learning_rate": 1.7972601341045616e-05,
      "loss": 1.0983,
      "step": 2463
    },
    {
      "epoch": 23.028293545534925,
      "grad_norm": 0.3876650631427765,
      "learning_rate": 1.7970773846016928e-05,
      "loss": 1.0746,
      "step": 2464
    },
    {
      "epoch": 23.0377247273799,
      "grad_norm": 0.424258291721344,
      "learning_rate": 1.796894562070858e-05,
      "loss": 1.0887,
      "step": 2465
    },
    {
      "epoch": 23.047155909224873,
      "grad_norm": 0.40580543875694275,
      "learning_rate": 1.7967116665288073e-05,
      "loss": 1.0944,
      "step": 2466
    },
    {
      "epoch": 23.05658709106985,
      "grad_norm": 0.4275672137737274,
      "learning_rate": 1.7965286979922975e-05,
      "loss": 1.1284,
      "step": 2467
    },
    {
      "epoch": 23.066018272914825,
      "grad_norm": 0.4164577126502991,
      "learning_rate": 1.7963456564780917e-05,
      "loss": 1.1072,
      "step": 2468
    },
    {
      "epoch": 23.0754494547598,
      "grad_norm": 0.395624041557312,
      "learning_rate": 1.7961625420029608e-05,
      "loss": 1.0961,
      "step": 2469
    },
    {
      "epoch": 23.084880636604776,
      "grad_norm": 0.4132235050201416,
      "learning_rate": 1.795979354583681e-05,
      "loss": 1.0893,
      "step": 2470
    },
    {
      "epoch": 23.09431181844975,
      "grad_norm": 0.41971585154533386,
      "learning_rate": 1.7957960942370365e-05,
      "loss": 1.1037,
      "step": 2471
    },
    {
      "epoch": 23.103743000294724,
      "grad_norm": 0.39431971311569214,
      "learning_rate": 1.795612760979817e-05,
      "loss": 1.0911,
      "step": 2472
    },
    {
      "epoch": 23.113174182139698,
      "grad_norm": 0.4157923460006714,
      "learning_rate": 1.7954293548288204e-05,
      "loss": 1.1145,
      "step": 2473
    },
    {
      "epoch": 23.122605363984675,
      "grad_norm": 0.41018325090408325,
      "learning_rate": 1.7952458758008493e-05,
      "loss": 1.1076,
      "step": 2474
    },
    {
      "epoch": 23.13203654582965,
      "grad_norm": 0.4541797637939453,
      "learning_rate": 1.7950623239127142e-05,
      "loss": 1.0562,
      "step": 2475
    },
    {
      "epoch": 23.141467727674623,
      "grad_norm": 0.4501133859157562,
      "learning_rate": 1.7948786991812323e-05,
      "loss": 1.0978,
      "step": 2476
    },
    {
      "epoch": 23.1508989095196,
      "grad_norm": 0.44811397790908813,
      "learning_rate": 1.7946950016232274e-05,
      "loss": 1.0957,
      "step": 2477
    },
    {
      "epoch": 23.160330091364575,
      "grad_norm": 0.42679446935653687,
      "learning_rate": 1.794511231255529e-05,
      "loss": 1.0714,
      "step": 2478
    },
    {
      "epoch": 23.16976127320955,
      "grad_norm": 0.4484318494796753,
      "learning_rate": 1.794327388094975e-05,
      "loss": 1.0426,
      "step": 2479
    },
    {
      "epoch": 23.179192455054523,
      "grad_norm": 0.4110547602176666,
      "learning_rate": 1.7941434721584085e-05,
      "loss": 1.0885,
      "step": 2480
    },
    {
      "epoch": 23.1886236368995,
      "grad_norm": 0.43427062034606934,
      "learning_rate": 1.79395948346268e-05,
      "loss": 1.0948,
      "step": 2481
    },
    {
      "epoch": 23.198054818744474,
      "grad_norm": 0.4190041422843933,
      "learning_rate": 1.7937754220246466e-05,
      "loss": 1.1119,
      "step": 2482
    },
    {
      "epoch": 23.207486000589448,
      "grad_norm": 0.4035089612007141,
      "learning_rate": 1.793591287861172e-05,
      "loss": 1.0931,
      "step": 2483
    },
    {
      "epoch": 23.216917182434425,
      "grad_norm": 0.45688483119010925,
      "learning_rate": 1.793407080989126e-05,
      "loss": 1.0855,
      "step": 2484
    },
    {
      "epoch": 23.2263483642794,
      "grad_norm": 0.4113478660583496,
      "learning_rate": 1.7932228014253858e-05,
      "loss": 1.0903,
      "step": 2485
    },
    {
      "epoch": 23.235779546124373,
      "grad_norm": 0.43933776021003723,
      "learning_rate": 1.7930384491868356e-05,
      "loss": 1.052,
      "step": 2486
    },
    {
      "epoch": 23.245210727969347,
      "grad_norm": 0.4119982421398163,
      "learning_rate": 1.7928540242903647e-05,
      "loss": 1.0885,
      "step": 2487
    },
    {
      "epoch": 23.254641909814325,
      "grad_norm": 0.450251966714859,
      "learning_rate": 1.7926695267528707e-05,
      "loss": 1.0658,
      "step": 2488
    },
    {
      "epoch": 23.2640730916593,
      "grad_norm": 0.46191754937171936,
      "learning_rate": 1.7924849565912574e-05,
      "loss": 1.057,
      "step": 2489
    },
    {
      "epoch": 23.273504273504273,
      "grad_norm": 0.44709596037864685,
      "learning_rate": 1.7923003138224345e-05,
      "loss": 1.104,
      "step": 2490
    },
    {
      "epoch": 23.28293545534925,
      "grad_norm": 0.41643020510673523,
      "learning_rate": 1.7921155984633193e-05,
      "loss": 1.0583,
      "step": 2491
    },
    {
      "epoch": 23.292366637194224,
      "grad_norm": 0.4363381862640381,
      "learning_rate": 1.791930810530835e-05,
      "loss": 1.0843,
      "step": 2492
    },
    {
      "epoch": 23.301797819039198,
      "grad_norm": 0.4398075342178345,
      "learning_rate": 1.791745950041912e-05,
      "loss": 1.0995,
      "step": 2493
    },
    {
      "epoch": 23.311229000884172,
      "grad_norm": 0.41694003343582153,
      "learning_rate": 1.7915610170134874e-05,
      "loss": 1.1117,
      "step": 2494
    },
    {
      "epoch": 23.32066018272915,
      "grad_norm": 0.40987375378608704,
      "learning_rate": 1.7913760114625044e-05,
      "loss": 1.1015,
      "step": 2495
    },
    {
      "epoch": 23.330091364574123,
      "grad_norm": 0.43973252177238464,
      "learning_rate": 1.791190933405913e-05,
      "loss": 1.0728,
      "step": 2496
    },
    {
      "epoch": 23.339522546419097,
      "grad_norm": 0.4297792911529541,
      "learning_rate": 1.7910057828606703e-05,
      "loss": 1.1081,
      "step": 2497
    },
    {
      "epoch": 23.348953728264075,
      "grad_norm": 0.39959704875946045,
      "learning_rate": 1.79082055984374e-05,
      "loss": 1.0727,
      "step": 2498
    },
    {
      "epoch": 23.35838491010905,
      "grad_norm": 0.4149531424045563,
      "learning_rate": 1.7906352643720914e-05,
      "loss": 1.1571,
      "step": 2499
    },
    {
      "epoch": 23.367816091954023,
      "grad_norm": 0.42238807678222656,
      "learning_rate": 1.7904498964627018e-05,
      "loss": 1.0978,
      "step": 2500
    },
    {
      "epoch": 23.377247273798996,
      "grad_norm": 0.5047217011451721,
      "learning_rate": 1.7902644561325543e-05,
      "loss": 1.0814,
      "step": 2501
    },
    {
      "epoch": 23.386678455643974,
      "grad_norm": 0.41733771562576294,
      "learning_rate": 1.790078943398639e-05,
      "loss": 1.1114,
      "step": 2502
    },
    {
      "epoch": 23.396109637488948,
      "grad_norm": 0.41608425974845886,
      "learning_rate": 1.7898933582779524e-05,
      "loss": 1.0782,
      "step": 2503
    },
    {
      "epoch": 23.405540819333922,
      "grad_norm": 0.44606584310531616,
      "learning_rate": 1.789707700787498e-05,
      "loss": 1.1047,
      "step": 2504
    },
    {
      "epoch": 23.4149720011789,
      "grad_norm": 0.4372192323207855,
      "learning_rate": 1.7895219709442853e-05,
      "loss": 1.0627,
      "step": 2505
    },
    {
      "epoch": 23.424403183023873,
      "grad_norm": 0.43084457516670227,
      "learning_rate": 1.7893361687653307e-05,
      "loss": 1.103,
      "step": 2506
    },
    {
      "epoch": 23.433834364868847,
      "grad_norm": 0.44406747817993164,
      "learning_rate": 1.7891502942676582e-05,
      "loss": 1.0783,
      "step": 2507
    },
    {
      "epoch": 23.44326554671382,
      "grad_norm": 0.4110109210014343,
      "learning_rate": 1.7889643474682966e-05,
      "loss": 1.1019,
      "step": 2508
    },
    {
      "epoch": 23.4526967285588,
      "grad_norm": 0.39150381088256836,
      "learning_rate": 1.788778328384283e-05,
      "loss": 1.1201,
      "step": 2509
    },
    {
      "epoch": 23.462127910403773,
      "grad_norm": 0.44419172406196594,
      "learning_rate": 1.7885922370326597e-05,
      "loss": 1.127,
      "step": 2510
    },
    {
      "epoch": 23.471559092248746,
      "grad_norm": 0.49075835943222046,
      "learning_rate": 1.7884060734304772e-05,
      "loss": 1.0655,
      "step": 2511
    },
    {
      "epoch": 23.480990274093724,
      "grad_norm": 0.41363924741744995,
      "learning_rate": 1.788219837594791e-05,
      "loss": 1.1022,
      "step": 2512
    },
    {
      "epoch": 23.490421455938698,
      "grad_norm": 0.40415674448013306,
      "learning_rate": 1.788033529542664e-05,
      "loss": 1.085,
      "step": 2513
    },
    {
      "epoch": 23.499852637783672,
      "grad_norm": 0.430123507976532,
      "learning_rate": 1.7878471492911663e-05,
      "loss": 1.0925,
      "step": 2514
    },
    {
      "epoch": 23.509283819628646,
      "grad_norm": 0.4381890892982483,
      "learning_rate": 1.7876606968573736e-05,
      "loss": 1.131,
      "step": 2515
    },
    {
      "epoch": 23.518715001473623,
      "grad_norm": 0.42928388714790344,
      "learning_rate": 1.7874741722583684e-05,
      "loss": 1.0944,
      "step": 2516
    },
    {
      "epoch": 23.528146183318597,
      "grad_norm": 0.4711311459541321,
      "learning_rate": 1.78728757551124e-05,
      "loss": 1.0955,
      "step": 2517
    },
    {
      "epoch": 23.53757736516357,
      "grad_norm": 0.4179683327674866,
      "learning_rate": 1.7871009066330852e-05,
      "loss": 1.1169,
      "step": 2518
    },
    {
      "epoch": 23.54700854700855,
      "grad_norm": 0.44729137420654297,
      "learning_rate": 1.7869141656410053e-05,
      "loss": 1.0817,
      "step": 2519
    },
    {
      "epoch": 23.556439728853523,
      "grad_norm": 0.43045347929000854,
      "learning_rate": 1.7867273525521104e-05,
      "loss": 1.0851,
      "step": 2520
    },
    {
      "epoch": 23.565870910698496,
      "grad_norm": 0.4128727912902832,
      "learning_rate": 1.786540467383516e-05,
      "loss": 1.1026,
      "step": 2521
    },
    {
      "epoch": 23.57530209254347,
      "grad_norm": 0.4361482560634613,
      "learning_rate": 1.7863535101523443e-05,
      "loss": 1.1269,
      "step": 2522
    },
    {
      "epoch": 23.584733274388448,
      "grad_norm": 0.40297847986221313,
      "learning_rate": 1.786166480875724e-05,
      "loss": 1.0544,
      "step": 2523
    },
    {
      "epoch": 23.594164456233422,
      "grad_norm": 0.43807917833328247,
      "learning_rate": 1.7859793795707913e-05,
      "loss": 1.0931,
      "step": 2524
    },
    {
      "epoch": 23.603595638078396,
      "grad_norm": 0.4365907609462738,
      "learning_rate": 1.785792206254688e-05,
      "loss": 1.0681,
      "step": 2525
    },
    {
      "epoch": 23.613026819923373,
      "grad_norm": 0.41198936104774475,
      "learning_rate": 1.785604960944563e-05,
      "loss": 1.1143,
      "step": 2526
    },
    {
      "epoch": 23.622458001768347,
      "grad_norm": 0.4435901343822479,
      "learning_rate": 1.7854176436575713e-05,
      "loss": 1.0448,
      "step": 2527
    },
    {
      "epoch": 23.63188918361332,
      "grad_norm": 0.4272862374782562,
      "learning_rate": 1.785230254410875e-05,
      "loss": 1.1034,
      "step": 2528
    },
    {
      "epoch": 23.641320365458295,
      "grad_norm": 0.4376199543476105,
      "learning_rate": 1.7850427932216433e-05,
      "loss": 1.0768,
      "step": 2529
    },
    {
      "epoch": 23.650751547303273,
      "grad_norm": 0.46814724802970886,
      "learning_rate": 1.7848552601070506e-05,
      "loss": 1.1346,
      "step": 2530
    },
    {
      "epoch": 23.660182729148246,
      "grad_norm": 0.43692639470100403,
      "learning_rate": 1.7846676550842786e-05,
      "loss": 1.1255,
      "step": 2531
    },
    {
      "epoch": 23.66961391099322,
      "grad_norm": 0.4158974289894104,
      "learning_rate": 1.7844799781705158e-05,
      "loss": 1.076,
      "step": 2532
    },
    {
      "epoch": 23.679045092838198,
      "grad_norm": 0.4631292521953583,
      "learning_rate": 1.784292229382957e-05,
      "loss": 1.0954,
      "step": 2533
    },
    {
      "epoch": 23.688476274683172,
      "grad_norm": 0.45946580171585083,
      "learning_rate": 1.784104408738804e-05,
      "loss": 1.0784,
      "step": 2534
    },
    {
      "epoch": 23.697907456528146,
      "grad_norm": 0.46742022037506104,
      "learning_rate": 1.783916516255265e-05,
      "loss": 1.1115,
      "step": 2535
    },
    {
      "epoch": 23.70733863837312,
      "grad_norm": 0.4454789161682129,
      "learning_rate": 1.7837285519495537e-05,
      "loss": 1.0939,
      "step": 2536
    },
    {
      "epoch": 23.716769820218097,
      "grad_norm": 0.43237873911857605,
      "learning_rate": 1.7835405158388923e-05,
      "loss": 1.0721,
      "step": 2537
    },
    {
      "epoch": 23.72620100206307,
      "grad_norm": 0.44761401414871216,
      "learning_rate": 1.783352407940508e-05,
      "loss": 1.1115,
      "step": 2538
    },
    {
      "epoch": 23.735632183908045,
      "grad_norm": 0.44925665855407715,
      "learning_rate": 1.7831642282716354e-05,
      "loss": 1.0506,
      "step": 2539
    },
    {
      "epoch": 23.745063365753023,
      "grad_norm": 0.45399272441864014,
      "learning_rate": 1.7829759768495156e-05,
      "loss": 1.0808,
      "step": 2540
    },
    {
      "epoch": 23.754494547597997,
      "grad_norm": 0.42191922664642334,
      "learning_rate": 1.7827876536913962e-05,
      "loss": 1.1237,
      "step": 2541
    },
    {
      "epoch": 23.76392572944297,
      "grad_norm": 0.4618360996246338,
      "learning_rate": 1.782599258814531e-05,
      "loss": 1.0897,
      "step": 2542
    },
    {
      "epoch": 23.773356911287944,
      "grad_norm": 0.4266512989997864,
      "learning_rate": 1.7824107922361803e-05,
      "loss": 1.0716,
      "step": 2543
    },
    {
      "epoch": 23.782788093132922,
      "grad_norm": 0.47283756732940674,
      "learning_rate": 1.7822222539736123e-05,
      "loss": 1.0999,
      "step": 2544
    },
    {
      "epoch": 23.792219274977896,
      "grad_norm": 0.38165315985679626,
      "learning_rate": 1.7820336440441006e-05,
      "loss": 1.0546,
      "step": 2545
    },
    {
      "epoch": 23.80165045682287,
      "grad_norm": 0.4938074052333832,
      "learning_rate": 1.781844962464925e-05,
      "loss": 1.1109,
      "step": 2546
    },
    {
      "epoch": 23.811081638667847,
      "grad_norm": 0.39623361825942993,
      "learning_rate": 1.781656209253373e-05,
      "loss": 1.0981,
      "step": 2547
    },
    {
      "epoch": 23.82051282051282,
      "grad_norm": 0.4123920798301697,
      "learning_rate": 1.7814673844267375e-05,
      "loss": 1.1406,
      "step": 2548
    },
    {
      "epoch": 23.829944002357795,
      "grad_norm": 0.4431108832359314,
      "learning_rate": 1.781278488002319e-05,
      "loss": 1.0704,
      "step": 2549
    },
    {
      "epoch": 23.83937518420277,
      "grad_norm": 0.44993630051612854,
      "learning_rate": 1.7810895199974242e-05,
      "loss": 1.0845,
      "step": 2550
    },
    {
      "epoch": 23.848806366047747,
      "grad_norm": 0.4349290132522583,
      "learning_rate": 1.7809004804293663e-05,
      "loss": 1.0774,
      "step": 2551
    },
    {
      "epoch": 23.85823754789272,
      "grad_norm": 0.4091801047325134,
      "learning_rate": 1.7807113693154647e-05,
      "loss": 1.0867,
      "step": 2552
    },
    {
      "epoch": 23.867668729737694,
      "grad_norm": 0.43903058767318726,
      "learning_rate": 1.780522186673046e-05,
      "loss": 1.1242,
      "step": 2553
    },
    {
      "epoch": 23.877099911582672,
      "grad_norm": 0.41174980998039246,
      "learning_rate": 1.780332932519443e-05,
      "loss": 1.127,
      "step": 2554
    },
    {
      "epoch": 23.886531093427646,
      "grad_norm": 0.4350796937942505,
      "learning_rate": 1.780143606871995e-05,
      "loss": 1.1062,
      "step": 2555
    },
    {
      "epoch": 23.89596227527262,
      "grad_norm": 0.4331618547439575,
      "learning_rate": 1.779954209748048e-05,
      "loss": 1.1121,
      "step": 2556
    },
    {
      "epoch": 23.905393457117594,
      "grad_norm": 0.44442251324653625,
      "learning_rate": 1.7797647411649544e-05,
      "loss": 1.069,
      "step": 2557
    },
    {
      "epoch": 23.91482463896257,
      "grad_norm": 0.4112626314163208,
      "learning_rate": 1.7795752011400737e-05,
      "loss": 1.0704,
      "step": 2558
    },
    {
      "epoch": 23.924255820807545,
      "grad_norm": 0.4186033308506012,
      "learning_rate": 1.779385589690771e-05,
      "loss": 1.0873,
      "step": 2559
    },
    {
      "epoch": 23.93368700265252,
      "grad_norm": 0.4728970527648926,
      "learning_rate": 1.7791959068344184e-05,
      "loss": 1.0872,
      "step": 2560
    },
    {
      "epoch": 23.943118184497497,
      "grad_norm": 0.4278203845024109,
      "learning_rate": 1.779006152588395e-05,
      "loss": 1.0707,
      "step": 2561
    },
    {
      "epoch": 23.95254936634247,
      "grad_norm": 0.43681392073631287,
      "learning_rate": 1.7788163269700856e-05,
      "loss": 1.1128,
      "step": 2562
    },
    {
      "epoch": 23.961980548187444,
      "grad_norm": 0.4268186092376709,
      "learning_rate": 1.778626429996882e-05,
      "loss": 1.0596,
      "step": 2563
    },
    {
      "epoch": 23.97141173003242,
      "grad_norm": 0.4266444444656372,
      "learning_rate": 1.7784364616861834e-05,
      "loss": 1.1024,
      "step": 2564
    },
    {
      "epoch": 23.980842911877396,
      "grad_norm": 0.4285402297973633,
      "learning_rate": 1.778246422055393e-05,
      "loss": 1.1275,
      "step": 2565
    },
    {
      "epoch": 23.99027409372237,
      "grad_norm": 0.46900275349617004,
      "learning_rate": 1.7780563111219237e-05,
      "loss": 1.0559,
      "step": 2566
    },
    {
      "epoch": 23.999705275567344,
      "grad_norm": 0.5024701952934265,
      "learning_rate": 1.7778661289031927e-05,
      "loss": 1.1432,
      "step": 2567
    },
    {
      "epoch": 24.0,
      "grad_norm": 1.898332118988037,
      "learning_rate": 1.777675875416624e-05,
      "loss": 0.4399,
      "step": 2568
    },
    {
      "epoch": 24.009431181844974,
      "grad_norm": 0.4234868884086609,
      "learning_rate": 1.7774855506796497e-05,
      "loss": 1.1088,
      "step": 2569
    },
    {
      "epoch": 24.01886236368995,
      "grad_norm": 0.4632627069950104,
      "learning_rate": 1.7772951547097063e-05,
      "loss": 1.099,
      "step": 2570
    },
    {
      "epoch": 24.028293545534925,
      "grad_norm": 0.461421400308609,
      "learning_rate": 1.777104687524238e-05,
      "loss": 1.0923,
      "step": 2571
    },
    {
      "epoch": 24.0377247273799,
      "grad_norm": 0.4328020513057709,
      "learning_rate": 1.7769141491406955e-05,
      "loss": 1.0811,
      "step": 2572
    },
    {
      "epoch": 24.047155909224873,
      "grad_norm": 0.41490936279296875,
      "learning_rate": 1.776723539576536e-05,
      "loss": 1.0543,
      "step": 2573
    },
    {
      "epoch": 24.05658709106985,
      "grad_norm": 0.44243013858795166,
      "learning_rate": 1.776532858849223e-05,
      "loss": 1.044,
      "step": 2574
    },
    {
      "epoch": 24.066018272914825,
      "grad_norm": 0.4030907452106476,
      "learning_rate": 1.7763421069762265e-05,
      "loss": 1.105,
      "step": 2575
    },
    {
      "epoch": 24.0754494547598,
      "grad_norm": 0.45854994654655457,
      "learning_rate": 1.776151283975023e-05,
      "loss": 1.0971,
      "step": 2576
    },
    {
      "epoch": 24.084880636604776,
      "grad_norm": 0.44845664501190186,
      "learning_rate": 1.7759603898630962e-05,
      "loss": 1.0999,
      "step": 2577
    },
    {
      "epoch": 24.09431181844975,
      "grad_norm": 0.4460923969745636,
      "learning_rate": 1.7757694246579353e-05,
      "loss": 1.0972,
      "step": 2578
    },
    {
      "epoch": 24.103743000294724,
      "grad_norm": 0.44381168484687805,
      "learning_rate": 1.7755783883770362e-05,
      "loss": 1.0928,
      "step": 2579
    },
    {
      "epoch": 24.113174182139698,
      "grad_norm": 0.41647303104400635,
      "learning_rate": 1.775387281037902e-05,
      "loss": 1.0799,
      "step": 2580
    },
    {
      "epoch": 24.122605363984675,
      "grad_norm": 0.42380475997924805,
      "learning_rate": 1.775196102658042e-05,
      "loss": 1.0961,
      "step": 2581
    },
    {
      "epoch": 24.13203654582965,
      "grad_norm": 0.4264562726020813,
      "learning_rate": 1.7750048532549718e-05,
      "loss": 1.0755,
      "step": 2582
    },
    {
      "epoch": 24.141467727674623,
      "grad_norm": 0.4418753981590271,
      "learning_rate": 1.774813532846213e-05,
      "loss": 1.1225,
      "step": 2583
    },
    {
      "epoch": 24.1508989095196,
      "grad_norm": 0.4112510681152344,
      "learning_rate": 1.7746221414492954e-05,
      "loss": 1.0951,
      "step": 2584
    },
    {
      "epoch": 24.160330091364575,
      "grad_norm": 0.41646137833595276,
      "learning_rate": 1.7744306790817534e-05,
      "loss": 1.0719,
      "step": 2585
    },
    {
      "epoch": 24.16976127320955,
      "grad_norm": 0.38029417395591736,
      "learning_rate": 1.7742391457611288e-05,
      "loss": 1.0985,
      "step": 2586
    },
    {
      "epoch": 24.179192455054523,
      "grad_norm": 0.4691915810108185,
      "learning_rate": 1.7740475415049704e-05,
      "loss": 1.1019,
      "step": 2587
    },
    {
      "epoch": 24.1886236368995,
      "grad_norm": 0.4702071249485016,
      "learning_rate": 1.773855866330832e-05,
      "loss": 1.0832,
      "step": 2588
    },
    {
      "epoch": 24.198054818744474,
      "grad_norm": 0.41607150435447693,
      "learning_rate": 1.7736641202562757e-05,
      "loss": 1.0588,
      "step": 2589
    },
    {
      "epoch": 24.207486000589448,
      "grad_norm": 0.42023417353630066,
      "learning_rate": 1.773472303298869e-05,
      "loss": 1.0753,
      "step": 2590
    },
    {
      "epoch": 24.216917182434425,
      "grad_norm": 0.42304491996765137,
      "learning_rate": 1.7732804154761852e-05,
      "loss": 1.061,
      "step": 2591
    },
    {
      "epoch": 24.2263483642794,
      "grad_norm": 0.41511327028274536,
      "learning_rate": 1.773088456805806e-05,
      "loss": 1.1097,
      "step": 2592
    },
    {
      "epoch": 24.235779546124373,
      "grad_norm": 0.43737849593162537,
      "learning_rate": 1.7728964273053187e-05,
      "loss": 1.0589,
      "step": 2593
    },
    {
      "epoch": 24.245210727969347,
      "grad_norm": 0.45987415313720703,
      "learning_rate": 1.7727043269923165e-05,
      "loss": 1.1176,
      "step": 2594
    },
    {
      "epoch": 24.254641909814325,
      "grad_norm": 0.4414021670818329,
      "learning_rate": 1.7725121558843995e-05,
      "loss": 1.1071,
      "step": 2595
    },
    {
      "epoch": 24.2640730916593,
      "grad_norm": 0.4174228012561798,
      "learning_rate": 1.7723199139991748e-05,
      "loss": 1.0803,
      "step": 2596
    },
    {
      "epoch": 24.273504273504273,
      "grad_norm": 0.4276464283466339,
      "learning_rate": 1.772127601354255e-05,
      "loss": 1.1262,
      "step": 2597
    },
    {
      "epoch": 24.28293545534925,
      "grad_norm": 0.4128017723560333,
      "learning_rate": 1.77193521796726e-05,
      "loss": 1.0817,
      "step": 2598
    },
    {
      "epoch": 24.292366637194224,
      "grad_norm": 0.40961575508117676,
      "learning_rate": 1.7717427638558157e-05,
      "loss": 1.0983,
      "step": 2599
    },
    {
      "epoch": 24.301797819039198,
      "grad_norm": 0.41219741106033325,
      "learning_rate": 1.7715502390375553e-05,
      "loss": 1.1107,
      "step": 2600
    },
    {
      "epoch": 24.311229000884172,
      "grad_norm": 0.44898372888565063,
      "learning_rate": 1.771357643530117e-05,
      "loss": 1.0988,
      "step": 2601
    },
    {
      "epoch": 24.32066018272915,
      "grad_norm": 0.44365912675857544,
      "learning_rate": 1.771164977351147e-05,
      "loss": 1.0682,
      "step": 2602
    },
    {
      "epoch": 24.330091364574123,
      "grad_norm": 0.4394334852695465,
      "learning_rate": 1.770972240518297e-05,
      "loss": 1.0614,
      "step": 2603
    },
    {
      "epoch": 24.339522546419097,
      "grad_norm": 0.4146938920021057,
      "learning_rate": 1.7707794330492256e-05,
      "loss": 1.0819,
      "step": 2604
    },
    {
      "epoch": 24.348953728264075,
      "grad_norm": 0.40231963992118835,
      "learning_rate": 1.770586554961598e-05,
      "loss": 1.0716,
      "step": 2605
    },
    {
      "epoch": 24.35838491010905,
      "grad_norm": 0.5275352001190186,
      "learning_rate": 1.770393606273085e-05,
      "loss": 1.1014,
      "step": 2606
    },
    {
      "epoch": 24.367816091954023,
      "grad_norm": 0.4254339933395386,
      "learning_rate": 1.7702005870013647e-05,
      "loss": 1.1129,
      "step": 2607
    },
    {
      "epoch": 24.377247273798996,
      "grad_norm": 0.408193439245224,
      "learning_rate": 1.770007497164122e-05,
      "loss": 1.102,
      "step": 2608
    },
    {
      "epoch": 24.386678455643974,
      "grad_norm": 0.4188356101512909,
      "learning_rate": 1.769814336779047e-05,
      "loss": 1.0996,
      "step": 2609
    },
    {
      "epoch": 24.396109637488948,
      "grad_norm": 0.44614288210868835,
      "learning_rate": 1.7696211058638375e-05,
      "loss": 1.0818,
      "step": 2610
    },
    {
      "epoch": 24.405540819333922,
      "grad_norm": 0.434719055891037,
      "learning_rate": 1.769427804436197e-05,
      "loss": 1.0909,
      "step": 2611
    },
    {
      "epoch": 24.4149720011789,
      "grad_norm": 0.4604170620441437,
      "learning_rate": 1.7692344325138357e-05,
      "loss": 1.1126,
      "step": 2612
    },
    {
      "epoch": 24.424403183023873,
      "grad_norm": 0.3863292336463928,
      "learning_rate": 1.7690409901144706e-05,
      "loss": 1.0872,
      "step": 2613
    },
    {
      "epoch": 24.433834364868847,
      "grad_norm": 0.44421643018722534,
      "learning_rate": 1.7688474772558244e-05,
      "loss": 1.13,
      "step": 2614
    },
    {
      "epoch": 24.44326554671382,
      "grad_norm": 0.4534836411476135,
      "learning_rate": 1.7686538939556272e-05,
      "loss": 1.0781,
      "step": 2615
    },
    {
      "epoch": 24.4526967285588,
      "grad_norm": 0.41096988320350647,
      "learning_rate": 1.768460240231614e-05,
      "loss": 1.07,
      "step": 2616
    },
    {
      "epoch": 24.462127910403773,
      "grad_norm": 0.4313930571079254,
      "learning_rate": 1.7682665161015285e-05,
      "loss": 1.0827,
      "step": 2617
    },
    {
      "epoch": 24.471559092248746,
      "grad_norm": 0.44461047649383545,
      "learning_rate": 1.768072721583119e-05,
      "loss": 1.0877,
      "step": 2618
    },
    {
      "epoch": 24.480990274093724,
      "grad_norm": 0.44933974742889404,
      "learning_rate": 1.7678788566941413e-05,
      "loss": 1.1233,
      "step": 2619
    },
    {
      "epoch": 24.490421455938698,
      "grad_norm": 0.4282134473323822,
      "learning_rate": 1.7676849214523563e-05,
      "loss": 1.0482,
      "step": 2620
    },
    {
      "epoch": 24.499852637783672,
      "grad_norm": 0.4237600564956665,
      "learning_rate": 1.7674909158755337e-05,
      "loss": 1.0583,
      "step": 2621
    },
    {
      "epoch": 24.509283819628646,
      "grad_norm": 0.44446948170661926,
      "learning_rate": 1.767296839981447e-05,
      "loss": 1.1062,
      "step": 2622
    },
    {
      "epoch": 24.518715001473623,
      "grad_norm": 0.4777706265449524,
      "learning_rate": 1.7671026937878778e-05,
      "loss": 1.0767,
      "step": 2623
    },
    {
      "epoch": 24.528146183318597,
      "grad_norm": 0.45373404026031494,
      "learning_rate": 1.766908477312614e-05,
      "loss": 1.1312,
      "step": 2624
    },
    {
      "epoch": 24.53757736516357,
      "grad_norm": 0.446925550699234,
      "learning_rate": 1.7667141905734497e-05,
      "loss": 1.0806,
      "step": 2625
    },
    {
      "epoch": 24.54700854700855,
      "grad_norm": 0.43044495582580566,
      "learning_rate": 1.7665198335881845e-05,
      "loss": 1.1162,
      "step": 2626
    },
    {
      "epoch": 24.556439728853523,
      "grad_norm": 0.43525972962379456,
      "learning_rate": 1.766325406374626e-05,
      "loss": 1.1169,
      "step": 2627
    },
    {
      "epoch": 24.565870910698496,
      "grad_norm": 0.4079713523387909,
      "learning_rate": 1.7661309089505878e-05,
      "loss": 1.1609,
      "step": 2628
    },
    {
      "epoch": 24.57530209254347,
      "grad_norm": 0.4307803809642792,
      "learning_rate": 1.765936341333889e-05,
      "loss": 1.0962,
      "step": 2629
    },
    {
      "epoch": 24.584733274388448,
      "grad_norm": 0.43479809165000916,
      "learning_rate": 1.7657417035423563e-05,
      "loss": 1.0823,
      "step": 2630
    },
    {
      "epoch": 24.594164456233422,
      "grad_norm": 0.43221044540405273,
      "learning_rate": 1.7655469955938222e-05,
      "loss": 1.0664,
      "step": 2631
    },
    {
      "epoch": 24.603595638078396,
      "grad_norm": 0.40740707516670227,
      "learning_rate": 1.765352217506126e-05,
      "loss": 1.0841,
      "step": 2632
    },
    {
      "epoch": 24.613026819923373,
      "grad_norm": 0.44020628929138184,
      "learning_rate": 1.7651573692971127e-05,
      "loss": 1.1058,
      "step": 2633
    },
    {
      "epoch": 24.622458001768347,
      "grad_norm": 0.43383073806762695,
      "learning_rate": 1.764962450984635e-05,
      "loss": 1.0699,
      "step": 2634
    },
    {
      "epoch": 24.63188918361332,
      "grad_norm": 0.42677807807922363,
      "learning_rate": 1.7647674625865502e-05,
      "loss": 1.1067,
      "step": 2635
    },
    {
      "epoch": 24.641320365458295,
      "grad_norm": 0.4468882977962494,
      "learning_rate": 1.764572404120724e-05,
      "loss": 1.086,
      "step": 2636
    },
    {
      "epoch": 24.650751547303273,
      "grad_norm": 0.40722212195396423,
      "learning_rate": 1.764377275605027e-05,
      "loss": 1.0976,
      "step": 2637
    },
    {
      "epoch": 24.660182729148246,
      "grad_norm": 0.44207674264907837,
      "learning_rate": 1.7641820770573376e-05,
      "loss": 1.103,
      "step": 2638
    },
    {
      "epoch": 24.66961391099322,
      "grad_norm": 0.4418982267379761,
      "learning_rate": 1.7639868084955388e-05,
      "loss": 1.0402,
      "step": 2639
    },
    {
      "epoch": 24.679045092838198,
      "grad_norm": 0.4388664662837982,
      "learning_rate": 1.7637914699375217e-05,
      "loss": 1.0356,
      "step": 2640
    },
    {
      "epoch": 24.688476274683172,
      "grad_norm": 0.4126407504081726,
      "learning_rate": 1.7635960614011828e-05,
      "loss": 1.1082,
      "step": 2641
    },
    {
      "epoch": 24.697907456528146,
      "grad_norm": 0.42642977833747864,
      "learning_rate": 1.7634005829044256e-05,
      "loss": 1.1186,
      "step": 2642
    },
    {
      "epoch": 24.70733863837312,
      "grad_norm": 0.47781163454055786,
      "learning_rate": 1.7632050344651597e-05,
      "loss": 1.1055,
      "step": 2643
    },
    {
      "epoch": 24.716769820218097,
      "grad_norm": 0.4176471531391144,
      "learning_rate": 1.7630094161013017e-05,
      "loss": 1.1141,
      "step": 2644
    },
    {
      "epoch": 24.72620100206307,
      "grad_norm": 0.46081021428108215,
      "learning_rate": 1.762813727830773e-05,
      "loss": 1.1357,
      "step": 2645
    },
    {
      "epoch": 24.735632183908045,
      "grad_norm": 0.4199695587158203,
      "learning_rate": 1.7626179696715035e-05,
      "loss": 1.0651,
      "step": 2646
    },
    {
      "epoch": 24.745063365753023,
      "grad_norm": 0.4452597200870514,
      "learning_rate": 1.762422141641428e-05,
      "loss": 1.0965,
      "step": 2647
    },
    {
      "epoch": 24.754494547597997,
      "grad_norm": 0.4450240135192871,
      "learning_rate": 1.762226243758488e-05,
      "loss": 1.1287,
      "step": 2648
    },
    {
      "epoch": 24.76392572944297,
      "grad_norm": 0.4887973368167877,
      "learning_rate": 1.762030276040632e-05,
      "loss": 1.0734,
      "step": 2649
    },
    {
      "epoch": 24.773356911287944,
      "grad_norm": 0.45652514696121216,
      "learning_rate": 1.7618342385058147e-05,
      "loss": 1.0649,
      "step": 2650
    },
    {
      "epoch": 24.782788093132922,
      "grad_norm": 0.4524941146373749,
      "learning_rate": 1.761638131171996e-05,
      "loss": 1.07,
      "step": 2651
    },
    {
      "epoch": 24.792219274977896,
      "grad_norm": 0.4132077991962433,
      "learning_rate": 1.7614419540571447e-05,
      "loss": 1.0865,
      "step": 2652
    },
    {
      "epoch": 24.80165045682287,
      "grad_norm": 0.42664775252342224,
      "learning_rate": 1.7612457071792334e-05,
      "loss": 1.085,
      "step": 2653
    },
    {
      "epoch": 24.811081638667847,
      "grad_norm": 0.4141773581504822,
      "learning_rate": 1.761049390556242e-05,
      "loss": 1.1134,
      "step": 2654
    },
    {
      "epoch": 24.82051282051282,
      "grad_norm": 0.4217175841331482,
      "learning_rate": 1.7608530042061577e-05,
      "loss": 1.1011,
      "step": 2655
    },
    {
      "epoch": 24.829944002357795,
      "grad_norm": 0.4527423679828644,
      "learning_rate": 1.7606565481469732e-05,
      "loss": 1.1143,
      "step": 2656
    },
    {
      "epoch": 24.83937518420277,
      "grad_norm": 0.43384939432144165,
      "learning_rate": 1.7604600223966872e-05,
      "loss": 1.1035,
      "step": 2657
    },
    {
      "epoch": 24.848806366047747,
      "grad_norm": 0.5121461153030396,
      "learning_rate": 1.7602634269733057e-05,
      "loss": 1.0735,
      "step": 2658
    },
    {
      "epoch": 24.85823754789272,
      "grad_norm": 0.4253827631473541,
      "learning_rate": 1.7600667618948413e-05,
      "loss": 1.0445,
      "step": 2659
    },
    {
      "epoch": 24.867668729737694,
      "grad_norm": 0.4082289934158325,
      "learning_rate": 1.759870027179311e-05,
      "loss": 1.107,
      "step": 2660
    },
    {
      "epoch": 24.877099911582672,
      "grad_norm": 0.4042803943157196,
      "learning_rate": 1.759673222844741e-05,
      "loss": 1.0892,
      "step": 2661
    },
    {
      "epoch": 24.886531093427646,
      "grad_norm": 0.4364112913608551,
      "learning_rate": 1.7594763489091615e-05,
      "loss": 1.0787,
      "step": 2662
    },
    {
      "epoch": 24.89596227527262,
      "grad_norm": 0.4086099863052368,
      "learning_rate": 1.7592794053906107e-05,
      "loss": 1.0871,
      "step": 2663
    },
    {
      "epoch": 24.905393457117594,
      "grad_norm": 0.44572892785072327,
      "learning_rate": 1.7590823923071316e-05,
      "loss": 1.0964,
      "step": 2664
    },
    {
      "epoch": 24.91482463896257,
      "grad_norm": 0.4148660898208618,
      "learning_rate": 1.7588853096767753e-05,
      "loss": 1.1301,
      "step": 2665
    },
    {
      "epoch": 24.924255820807545,
      "grad_norm": 0.436757355928421,
      "learning_rate": 1.7586881575175985e-05,
      "loss": 1.0973,
      "step": 2666
    },
    {
      "epoch": 24.93368700265252,
      "grad_norm": 0.40171557664871216,
      "learning_rate": 1.7584909358476635e-05,
      "loss": 1.1232,
      "step": 2667
    },
    {
      "epoch": 24.943118184497497,
      "grad_norm": 0.42417222261428833,
      "learning_rate": 1.7582936446850397e-05,
      "loss": 1.0865,
      "step": 2668
    },
    {
      "epoch": 24.95254936634247,
      "grad_norm": 0.4230307340621948,
      "learning_rate": 1.7580962840478036e-05,
      "loss": 1.0843,
      "step": 2669
    },
    {
      "epoch": 24.961980548187444,
      "grad_norm": 0.43068447709083557,
      "learning_rate": 1.7578988539540373e-05,
      "loss": 1.0905,
      "step": 2670
    },
    {
      "epoch": 24.97141173003242,
      "grad_norm": 0.4819924235343933,
      "learning_rate": 1.7577013544218286e-05,
      "loss": 1.0573,
      "step": 2671
    },
    {
      "epoch": 24.980842911877396,
      "grad_norm": 0.4300670623779297,
      "learning_rate": 1.7575037854692725e-05,
      "loss": 1.1186,
      "step": 2672
    },
    {
      "epoch": 24.99027409372237,
      "grad_norm": 0.43844470381736755,
      "learning_rate": 1.7573061471144702e-05,
      "loss": 1.1163,
      "step": 2673
    },
    {
      "epoch": 24.999705275567344,
      "grad_norm": 0.42841821908950806,
      "learning_rate": 1.75710843937553e-05,
      "loss": 1.0816,
      "step": 2674
    },
    {
      "epoch": 25.0,
      "grad_norm": 1.6845459938049316,
      "learning_rate": 1.7569106622705647e-05,
      "loss": 1.0136,
      "step": 2675
    },
    {
      "epoch": 25.009431181844974,
      "grad_norm": 0.42893368005752563,
      "learning_rate": 1.7567128158176955e-05,
      "loss": 1.1,
      "step": 2676
    },
    {
      "epoch": 25.01886236368995,
      "grad_norm": 0.41547784209251404,
      "learning_rate": 1.756514900035048e-05,
      "loss": 1.0737,
      "step": 2677
    },
    {
      "epoch": 25.028293545534925,
      "grad_norm": 0.4852856397628784,
      "learning_rate": 1.7563169149407566e-05,
      "loss": 1.0341,
      "step": 2678
    },
    {
      "epoch": 25.0377247273799,
      "grad_norm": 0.4161720275878906,
      "learning_rate": 1.7561188605529594e-05,
      "loss": 1.0546,
      "step": 2679
    },
    {
      "epoch": 25.047155909224873,
      "grad_norm": 0.45883476734161377,
      "learning_rate": 1.755920736889803e-05,
      "loss": 1.0662,
      "step": 2680
    },
    {
      "epoch": 25.05658709106985,
      "grad_norm": 0.39774617552757263,
      "learning_rate": 1.755722543969438e-05,
      "loss": 1.0596,
      "step": 2681
    },
    {
      "epoch": 25.066018272914825,
      "grad_norm": 0.4252486228942871,
      "learning_rate": 1.7555242818100246e-05,
      "loss": 1.0741,
      "step": 2682
    },
    {
      "epoch": 25.0754494547598,
      "grad_norm": 0.4198470413684845,
      "learning_rate": 1.755325950429726e-05,
      "loss": 1.0488,
      "step": 2683
    },
    {
      "epoch": 25.084880636604776,
      "grad_norm": 0.4726211726665497,
      "learning_rate": 1.755127549846714e-05,
      "loss": 1.0846,
      "step": 2684
    },
    {
      "epoch": 25.09431181844975,
      "grad_norm": 0.42874687910079956,
      "learning_rate": 1.7549290800791664e-05,
      "loss": 1.0738,
      "step": 2685
    },
    {
      "epoch": 25.103743000294724,
      "grad_norm": 0.4412015974521637,
      "learning_rate": 1.754730541145266e-05,
      "loss": 1.1053,
      "step": 2686
    },
    {
      "epoch": 25.113174182139698,
      "grad_norm": 0.45893678069114685,
      "learning_rate": 1.754531933063203e-05,
      "loss": 1.0715,
      "step": 2687
    },
    {
      "epoch": 25.122605363984675,
      "grad_norm": 0.4258037805557251,
      "learning_rate": 1.7543332558511746e-05,
      "loss": 1.0908,
      "step": 2688
    },
    {
      "epoch": 25.13203654582965,
      "grad_norm": 0.4124298691749573,
      "learning_rate": 1.7541345095273826e-05,
      "loss": 1.0534,
      "step": 2689
    },
    {
      "epoch": 25.141467727674623,
      "grad_norm": 0.4237181842327118,
      "learning_rate": 1.753935694110037e-05,
      "loss": 1.1158,
      "step": 2690
    },
    {
      "epoch": 25.1508989095196,
      "grad_norm": 0.44761112332344055,
      "learning_rate": 1.7537368096173525e-05,
      "loss": 1.12,
      "step": 2691
    },
    {
      "epoch": 25.160330091364575,
      "grad_norm": 0.4282969534397125,
      "learning_rate": 1.7535378560675507e-05,
      "loss": 1.1325,
      "step": 2692
    },
    {
      "epoch": 25.16976127320955,
      "grad_norm": 0.46667739748954773,
      "learning_rate": 1.75333883347886e-05,
      "loss": 1.0667,
      "step": 2693
    },
    {
      "epoch": 25.179192455054523,
      "grad_norm": 0.42874616384506226,
      "learning_rate": 1.7531397418695153e-05,
      "loss": 1.0538,
      "step": 2694
    },
    {
      "epoch": 25.1886236368995,
      "grad_norm": 0.4472006559371948,
      "learning_rate": 1.7529405812577564e-05,
      "loss": 1.0774,
      "step": 2695
    },
    {
      "epoch": 25.198054818744474,
      "grad_norm": 0.45329323410987854,
      "learning_rate": 1.7527413516618307e-05,
      "loss": 1.0611,
      "step": 2696
    },
    {
      "epoch": 25.207486000589448,
      "grad_norm": 0.42968297004699707,
      "learning_rate": 1.7525420530999918e-05,
      "loss": 1.0741,
      "step": 2697
    },
    {
      "epoch": 25.216917182434425,
      "grad_norm": 0.3958769142627716,
      "learning_rate": 1.752342685590499e-05,
      "loss": 1.1331,
      "step": 2698
    },
    {
      "epoch": 25.2263483642794,
      "grad_norm": 0.4364062249660492,
      "learning_rate": 1.7521432491516183e-05,
      "loss": 1.0977,
      "step": 2699
    },
    {
      "epoch": 25.235779546124373,
      "grad_norm": 0.42780157923698425,
      "learning_rate": 1.7519437438016222e-05,
      "loss": 1.1023,
      "step": 2700
    },
    {
      "epoch": 25.245210727969347,
      "grad_norm": 0.4440351128578186,
      "learning_rate": 1.7517441695587893e-05,
      "loss": 1.067,
      "step": 2701
    },
    {
      "epoch": 25.254641909814325,
      "grad_norm": 0.42884576320648193,
      "learning_rate": 1.7515445264414045e-05,
      "loss": 1.0959,
      "step": 2702
    },
    {
      "epoch": 25.2640730916593,
      "grad_norm": 0.4017365574836731,
      "learning_rate": 1.751344814467759e-05,
      "loss": 1.116,
      "step": 2703
    },
    {
      "epoch": 25.273504273504273,
      "grad_norm": 0.4342891275882721,
      "learning_rate": 1.75114503365615e-05,
      "loss": 1.1,
      "step": 2704
    },
    {
      "epoch": 25.28293545534925,
      "grad_norm": 0.432204008102417,
      "learning_rate": 1.7509451840248818e-05,
      "loss": 1.075,
      "step": 2705
    },
    {
      "epoch": 25.292366637194224,
      "grad_norm": 0.39962542057037354,
      "learning_rate": 1.750745265592265e-05,
      "loss": 1.1393,
      "step": 2706
    },
    {
      "epoch": 25.301797819039198,
      "grad_norm": 0.449054092168808,
      "learning_rate": 1.750545278376615e-05,
      "loss": 1.1178,
      "step": 2707
    },
    {
      "epoch": 25.311229000884172,
      "grad_norm": 0.3991290330886841,
      "learning_rate": 1.750345222396255e-05,
      "loss": 1.0897,
      "step": 2708
    },
    {
      "epoch": 25.32066018272915,
      "grad_norm": 0.4322715103626251,
      "learning_rate": 1.7501450976695143e-05,
      "loss": 1.0625,
      "step": 2709
    },
    {
      "epoch": 25.330091364574123,
      "grad_norm": 0.4751984179019928,
      "learning_rate": 1.7499449042147283e-05,
      "loss": 1.0722,
      "step": 2710
    },
    {
      "epoch": 25.339522546419097,
      "grad_norm": 0.4064614176750183,
      "learning_rate": 1.7497446420502384e-05,
      "loss": 1.0881,
      "step": 2711
    },
    {
      "epoch": 25.348953728264075,
      "grad_norm": 0.43185997009277344,
      "learning_rate": 1.7495443111943918e-05,
      "loss": 1.066,
      "step": 2712
    },
    {
      "epoch": 25.35838491010905,
      "grad_norm": 0.4569876790046692,
      "learning_rate": 1.7493439116655445e-05,
      "loss": 1.0412,
      "step": 2713
    },
    {
      "epoch": 25.367816091954023,
      "grad_norm": 0.42248767614364624,
      "learning_rate": 1.7491434434820558e-05,
      "loss": 1.1143,
      "step": 2714
    },
    {
      "epoch": 25.377247273798996,
      "grad_norm": 0.41339266300201416,
      "learning_rate": 1.7489429066622924e-05,
      "loss": 1.0992,
      "step": 2715
    },
    {
      "epoch": 25.386678455643974,
      "grad_norm": 0.41348007321357727,
      "learning_rate": 1.7487423012246285e-05,
      "loss": 1.0602,
      "step": 2716
    },
    {
      "epoch": 25.396109637488948,
      "grad_norm": 0.4323618412017822,
      "learning_rate": 1.7485416271874426e-05,
      "loss": 1.119,
      "step": 2717
    },
    {
      "epoch": 25.405540819333922,
      "grad_norm": 0.43285349011421204,
      "learning_rate": 1.7483408845691204e-05,
      "loss": 1.0879,
      "step": 2718
    },
    {
      "epoch": 25.4149720011789,
      "grad_norm": 0.4781443774700165,
      "learning_rate": 1.748140073388054e-05,
      "loss": 1.0947,
      "step": 2719
    },
    {
      "epoch": 25.424403183023873,
      "grad_norm": 0.47338753938674927,
      "learning_rate": 1.7479391936626423e-05,
      "loss": 1.0922,
      "step": 2720
    },
    {
      "epoch": 25.433834364868847,
      "grad_norm": 0.41709405183792114,
      "learning_rate": 1.7477382454112893e-05,
      "loss": 1.0754,
      "step": 2721
    },
    {
      "epoch": 25.44326554671382,
      "grad_norm": 0.4125816524028778,
      "learning_rate": 1.7475372286524055e-05,
      "loss": 1.1009,
      "step": 2722
    },
    {
      "epoch": 25.4526967285588,
      "grad_norm": 0.44779831171035767,
      "learning_rate": 1.747336143404408e-05,
      "loss": 1.0613,
      "step": 2723
    },
    {
      "epoch": 25.462127910403773,
      "grad_norm": 0.45564350485801697,
      "learning_rate": 1.747134989685721e-05,
      "loss": 1.1173,
      "step": 2724
    },
    {
      "epoch": 25.471559092248746,
      "grad_norm": 0.4191732704639435,
      "learning_rate": 1.7469337675147733e-05,
      "loss": 1.1071,
      "step": 2725
    },
    {
      "epoch": 25.480990274093724,
      "grad_norm": 0.432192325592041,
      "learning_rate": 1.746732476910001e-05,
      "loss": 1.0857,
      "step": 2726
    },
    {
      "epoch": 25.490421455938698,
      "grad_norm": 0.4444952607154846,
      "learning_rate": 1.7465311178898468e-05,
      "loss": 1.0995,
      "step": 2727
    },
    {
      "epoch": 25.499852637783672,
      "grad_norm": 0.4297420084476471,
      "learning_rate": 1.7463296904727585e-05,
      "loss": 1.0827,
      "step": 2728
    },
    {
      "epoch": 25.509283819628646,
      "grad_norm": 0.43319350481033325,
      "learning_rate": 1.7461281946771907e-05,
      "loss": 1.1361,
      "step": 2729
    },
    {
      "epoch": 25.518715001473623,
      "grad_norm": 0.4537355899810791,
      "learning_rate": 1.7459266305216052e-05,
      "loss": 1.0431,
      "step": 2730
    },
    {
      "epoch": 25.528146183318597,
      "grad_norm": 0.44838184118270874,
      "learning_rate": 1.7457249980244687e-05,
      "loss": 1.0961,
      "step": 2731
    },
    {
      "epoch": 25.53757736516357,
      "grad_norm": 0.43851354718208313,
      "learning_rate": 1.7455232972042545e-05,
      "loss": 1.0931,
      "step": 2732
    },
    {
      "epoch": 25.54700854700855,
      "grad_norm": 0.4585001766681671,
      "learning_rate": 1.7453215280794426e-05,
      "loss": 1.0758,
      "step": 2733
    },
    {
      "epoch": 25.556439728853523,
      "grad_norm": 0.4156813323497772,
      "learning_rate": 1.7451196906685194e-05,
      "loss": 1.0694,
      "step": 2734
    },
    {
      "epoch": 25.565870910698496,
      "grad_norm": 0.41604962944984436,
      "learning_rate": 1.7449177849899763e-05,
      "loss": 1.0713,
      "step": 2735
    },
    {
      "epoch": 25.57530209254347,
      "grad_norm": 0.4349886178970337,
      "learning_rate": 1.744715811062313e-05,
      "loss": 1.1096,
      "step": 2736
    },
    {
      "epoch": 25.584733274388448,
      "grad_norm": 0.463765949010849,
      "learning_rate": 1.744513768904033e-05,
      "loss": 1.1015,
      "step": 2737
    },
    {
      "epoch": 25.594164456233422,
      "grad_norm": 0.40161117911338806,
      "learning_rate": 1.7443116585336483e-05,
      "loss": 1.0772,
      "step": 2738
    },
    {
      "epoch": 25.603595638078396,
      "grad_norm": 0.41167959570884705,
      "learning_rate": 1.7441094799696757e-05,
      "loss": 1.0749,
      "step": 2739
    },
    {
      "epoch": 25.613026819923373,
      "grad_norm": 0.46062734723091125,
      "learning_rate": 1.743907233230639e-05,
      "loss": 1.0912,
      "step": 2740
    },
    {
      "epoch": 25.622458001768347,
      "grad_norm": 0.4645545184612274,
      "learning_rate": 1.743704918335068e-05,
      "loss": 1.0897,
      "step": 2741
    },
    {
      "epoch": 25.63188918361332,
      "grad_norm": 0.44253432750701904,
      "learning_rate": 1.7435025353014984e-05,
      "loss": 1.1407,
      "step": 2742
    },
    {
      "epoch": 25.641320365458295,
      "grad_norm": 0.4384362995624542,
      "learning_rate": 1.7433000841484726e-05,
      "loss": 1.1072,
      "step": 2743
    },
    {
      "epoch": 25.650751547303273,
      "grad_norm": 0.42359310388565063,
      "learning_rate": 1.7430975648945393e-05,
      "loss": 1.105,
      "step": 2744
    },
    {
      "epoch": 25.660182729148246,
      "grad_norm": 0.42153748869895935,
      "learning_rate": 1.7428949775582532e-05,
      "loss": 1.0609,
      "step": 2745
    },
    {
      "epoch": 25.66961391099322,
      "grad_norm": 0.4534006118774414,
      "learning_rate": 1.742692322158175e-05,
      "loss": 1.1498,
      "step": 2746
    },
    {
      "epoch": 25.679045092838198,
      "grad_norm": 0.4367327392101288,
      "learning_rate": 1.7424895987128723e-05,
      "loss": 1.0924,
      "step": 2747
    },
    {
      "epoch": 25.688476274683172,
      "grad_norm": 0.43222999572753906,
      "learning_rate": 1.7422868072409182e-05,
      "loss": 1.1227,
      "step": 2748
    },
    {
      "epoch": 25.697907456528146,
      "grad_norm": 0.44721946120262146,
      "learning_rate": 1.7420839477608927e-05,
      "loss": 1.1038,
      "step": 2749
    },
    {
      "epoch": 25.70733863837312,
      "grad_norm": 0.4154534339904785,
      "learning_rate": 1.7418810202913814e-05,
      "loss": 1.0998,
      "step": 2750
    },
    {
      "epoch": 25.716769820218097,
      "grad_norm": 0.4304070770740509,
      "learning_rate": 1.7416780248509767e-05,
      "loss": 1.1105,
      "step": 2751
    },
    {
      "epoch": 25.72620100206307,
      "grad_norm": 0.44865110516548157,
      "learning_rate": 1.7414749614582768e-05,
      "loss": 1.0479,
      "step": 2752
    },
    {
      "epoch": 25.735632183908045,
      "grad_norm": 0.4183395504951477,
      "learning_rate": 1.741271830131887e-05,
      "loss": 1.1177,
      "step": 2753
    },
    {
      "epoch": 25.745063365753023,
      "grad_norm": 0.44195541739463806,
      "learning_rate": 1.741068630890417e-05,
      "loss": 1.0738,
      "step": 2754
    },
    {
      "epoch": 25.754494547597997,
      "grad_norm": 0.39044487476348877,
      "learning_rate": 1.7408653637524843e-05,
      "loss": 1.117,
      "step": 2755
    },
    {
      "epoch": 25.76392572944297,
      "grad_norm": 0.4407348036766052,
      "learning_rate": 1.7406620287367123e-05,
      "loss": 1.0724,
      "step": 2756
    },
    {
      "epoch": 25.773356911287944,
      "grad_norm": 0.4436168968677521,
      "learning_rate": 1.7404586258617303e-05,
      "loss": 1.104,
      "step": 2757
    },
    {
      "epoch": 25.782788093132922,
      "grad_norm": 0.44222113490104675,
      "learning_rate": 1.7402551551461747e-05,
      "loss": 1.1066,
      "step": 2758
    },
    {
      "epoch": 25.792219274977896,
      "grad_norm": 0.4498784840106964,
      "learning_rate": 1.740051616608686e-05,
      "loss": 1.121,
      "step": 2759
    },
    {
      "epoch": 25.80165045682287,
      "grad_norm": 0.43908533453941345,
      "learning_rate": 1.739848010267914e-05,
      "loss": 1.1403,
      "step": 2760
    },
    {
      "epoch": 25.811081638667847,
      "grad_norm": 0.45209547877311707,
      "learning_rate": 1.739644336142512e-05,
      "loss": 1.0704,
      "step": 2761
    },
    {
      "epoch": 25.82051282051282,
      "grad_norm": 0.43593132495880127,
      "learning_rate": 1.7394405942511407e-05,
      "loss": 1.0921,
      "step": 2762
    },
    {
      "epoch": 25.829944002357795,
      "grad_norm": 0.44879212975502014,
      "learning_rate": 1.739236784612467e-05,
      "loss": 1.1158,
      "step": 2763
    },
    {
      "epoch": 25.83937518420277,
      "grad_norm": 0.5200842022895813,
      "learning_rate": 1.7390329072451635e-05,
      "loss": 1.1092,
      "step": 2764
    },
    {
      "epoch": 25.848806366047747,
      "grad_norm": 0.41939428448677063,
      "learning_rate": 1.73882896216791e-05,
      "loss": 1.0744,
      "step": 2765
    },
    {
      "epoch": 25.85823754789272,
      "grad_norm": 0.4135284125804901,
      "learning_rate": 1.7386249493993915e-05,
      "loss": 1.0877,
      "step": 2766
    },
    {
      "epoch": 25.867668729737694,
      "grad_norm": 0.4316829741001129,
      "learning_rate": 1.7384208689582998e-05,
      "loss": 1.0474,
      "step": 2767
    },
    {
      "epoch": 25.877099911582672,
      "grad_norm": 0.44887951016426086,
      "learning_rate": 1.7382167208633326e-05,
      "loss": 1.1001,
      "step": 2768
    },
    {
      "epoch": 25.886531093427646,
      "grad_norm": 0.46615564823150635,
      "learning_rate": 1.7380125051331936e-05,
      "loss": 1.065,
      "step": 2769
    },
    {
      "epoch": 25.89596227527262,
      "grad_norm": 0.46082961559295654,
      "learning_rate": 1.7378082217865934e-05,
      "loss": 1.0544,
      "step": 2770
    },
    {
      "epoch": 25.905393457117594,
      "grad_norm": 0.44357338547706604,
      "learning_rate": 1.7376038708422482e-05,
      "loss": 1.0979,
      "step": 2771
    },
    {
      "epoch": 25.91482463896257,
      "grad_norm": 0.4154505729675293,
      "learning_rate": 1.7373994523188803e-05,
      "loss": 1.0796,
      "step": 2772
    },
    {
      "epoch": 25.924255820807545,
      "grad_norm": 0.43552374839782715,
      "learning_rate": 1.7371949662352188e-05,
      "loss": 1.109,
      "step": 2773
    },
    {
      "epoch": 25.93368700265252,
      "grad_norm": 0.4677012860774994,
      "learning_rate": 1.7369904126099982e-05,
      "loss": 1.0503,
      "step": 2774
    },
    {
      "epoch": 25.943118184497497,
      "grad_norm": 0.4954793453216553,
      "learning_rate": 1.736785791461961e-05,
      "loss": 1.1408,
      "step": 2775
    },
    {
      "epoch": 25.95254936634247,
      "grad_norm": 0.424351304769516,
      "learning_rate": 1.7365811028098525e-05,
      "loss": 1.0894,
      "step": 2776
    },
    {
      "epoch": 25.961980548187444,
      "grad_norm": 0.4570266306400299,
      "learning_rate": 1.736376346672428e-05,
      "loss": 1.0317,
      "step": 2777
    },
    {
      "epoch": 25.97141173003242,
      "grad_norm": 0.4678514003753662,
      "learning_rate": 1.7361715230684454e-05,
      "loss": 1.074,
      "step": 2778
    },
    {
      "epoch": 25.980842911877396,
      "grad_norm": 0.47983384132385254,
      "learning_rate": 1.7359666320166724e-05,
      "loss": 1.1222,
      "step": 2779
    },
    {
      "epoch": 25.99027409372237,
      "grad_norm": 0.4420861601829529,
      "learning_rate": 1.7357616735358797e-05,
      "loss": 1.0983,
      "step": 2780
    },
    {
      "epoch": 25.999705275567344,
      "grad_norm": 0.43571531772613525,
      "learning_rate": 1.7355566476448464e-05,
      "loss": 1.0844,
      "step": 2781
    },
    {
      "epoch": 26.0,
      "grad_norm": 2.2939934730529785,
      "learning_rate": 1.7353515543623564e-05,
      "loss": 0.9205,
      "step": 2782
    },
    {
      "epoch": 26.009431181844974,
      "grad_norm": 0.4376967251300812,
      "learning_rate": 1.7351463937072008e-05,
      "loss": 1.0725,
      "step": 2783
    },
    {
      "epoch": 26.01886236368995,
      "grad_norm": 0.41604018211364746,
      "learning_rate": 1.7349411656981753e-05,
      "loss": 1.1474,
      "step": 2784
    },
    {
      "epoch": 26.028293545534925,
      "grad_norm": 0.4519060254096985,
      "learning_rate": 1.734735870354084e-05,
      "loss": 1.0962,
      "step": 2785
    },
    {
      "epoch": 26.0377247273799,
      "grad_norm": 0.4737785756587982,
      "learning_rate": 1.7345305076937353e-05,
      "loss": 1.1059,
      "step": 2786
    },
    {
      "epoch": 26.047155909224873,
      "grad_norm": 0.4589948356151581,
      "learning_rate": 1.734325077735945e-05,
      "loss": 1.0656,
      "step": 2787
    },
    {
      "epoch": 26.05658709106985,
      "grad_norm": 0.4488023817539215,
      "learning_rate": 1.7341195804995338e-05,
      "loss": 1.0907,
      "step": 2788
    },
    {
      "epoch": 26.066018272914825,
      "grad_norm": 0.4117906987667084,
      "learning_rate": 1.7339140160033297e-05,
      "loss": 1.1386,
      "step": 2789
    },
    {
      "epoch": 26.0754494547598,
      "grad_norm": 0.43543460965156555,
      "learning_rate": 1.7337083842661665e-05,
      "loss": 1.0991,
      "step": 2790
    },
    {
      "epoch": 26.084880636604776,
      "grad_norm": 0.4482973515987396,
      "learning_rate": 1.733502685306884e-05,
      "loss": 1.0687,
      "step": 2791
    },
    {
      "epoch": 26.09431181844975,
      "grad_norm": 0.42994409799575806,
      "learning_rate": 1.7332969191443287e-05,
      "loss": 1.0904,
      "step": 2792
    },
    {
      "epoch": 26.103743000294724,
      "grad_norm": 0.464523047208786,
      "learning_rate": 1.7330910857973517e-05,
      "loss": 1.0518,
      "step": 2793
    },
    {
      "epoch": 26.113174182139698,
      "grad_norm": 0.4654797315597534,
      "learning_rate": 1.7328851852848125e-05,
      "loss": 1.1004,
      "step": 2794
    },
    {
      "epoch": 26.122605363984675,
      "grad_norm": 0.4313465356826782,
      "learning_rate": 1.7326792176255752e-05,
      "loss": 1.082,
      "step": 2795
    },
    {
      "epoch": 26.13203654582965,
      "grad_norm": 0.4616045653820038,
      "learning_rate": 1.7324731828385105e-05,
      "loss": 1.0477,
      "step": 2796
    },
    {
      "epoch": 26.141467727674623,
      "grad_norm": 0.409769207239151,
      "learning_rate": 1.7322670809424956e-05,
      "loss": 1.0804,
      "step": 2797
    },
    {
      "epoch": 26.1508989095196,
      "grad_norm": 0.4624073803424835,
      "learning_rate": 1.7320609119564128e-05,
      "loss": 1.1111,
      "step": 2798
    },
    {
      "epoch": 26.160330091364575,
      "grad_norm": 0.4774531126022339,
      "learning_rate": 1.7318546758991515e-05,
      "loss": 1.0844,
      "step": 2799
    },
    {
      "epoch": 26.16976127320955,
      "grad_norm": 0.4211791157722473,
      "learning_rate": 1.7316483727896075e-05,
      "loss": 1.1036,
      "step": 2800
    },
    {
      "epoch": 26.179192455054523,
      "grad_norm": 0.4565899968147278,
      "learning_rate": 1.7314420026466814e-05,
      "loss": 1.0404,
      "step": 2801
    },
    {
      "epoch": 26.1886236368995,
      "grad_norm": 0.42515185475349426,
      "learning_rate": 1.7312355654892818e-05,
      "loss": 1.0974,
      "step": 2802
    },
    {
      "epoch": 26.198054818744474,
      "grad_norm": 0.42862147092819214,
      "learning_rate": 1.7310290613363213e-05,
      "loss": 1.0844,
      "step": 2803
    },
    {
      "epoch": 26.207486000589448,
      "grad_norm": 0.41599881649017334,
      "learning_rate": 1.7308224902067202e-05,
      "loss": 1.0772,
      "step": 2804
    },
    {
      "epoch": 26.216917182434425,
      "grad_norm": 0.4952484369277954,
      "learning_rate": 1.7306158521194045e-05,
      "loss": 1.0677,
      "step": 2805
    },
    {
      "epoch": 26.2263483642794,
      "grad_norm": 0.4663310945034027,
      "learning_rate": 1.7304091470933064e-05,
      "loss": 1.0683,
      "step": 2806
    },
    {
      "epoch": 26.235779546124373,
      "grad_norm": 0.4538877606391907,
      "learning_rate": 1.730202375147364e-05,
      "loss": 1.0869,
      "step": 2807
    },
    {
      "epoch": 26.245210727969347,
      "grad_norm": 0.4693233370780945,
      "learning_rate": 1.7299955363005216e-05,
      "loss": 1.1196,
      "step": 2808
    },
    {
      "epoch": 26.254641909814325,
      "grad_norm": 0.4346632957458496,
      "learning_rate": 1.7297886305717302e-05,
      "loss": 1.1007,
      "step": 2809
    },
    {
      "epoch": 26.2640730916593,
      "grad_norm": 0.43440595269203186,
      "learning_rate": 1.7295816579799458e-05,
      "loss": 1.0867,
      "step": 2810
    },
    {
      "epoch": 26.273504273504273,
      "grad_norm": 0.4560890197753906,
      "learning_rate": 1.7293746185441312e-05,
      "loss": 1.0918,
      "step": 2811
    },
    {
      "epoch": 26.28293545534925,
      "grad_norm": 0.45258039236068726,
      "learning_rate": 1.7291675122832558e-05,
      "loss": 1.1019,
      "step": 2812
    },
    {
      "epoch": 26.292366637194224,
      "grad_norm": 0.4500758647918701,
      "learning_rate": 1.7289603392162945e-05,
      "loss": 1.0832,
      "step": 2813
    },
    {
      "epoch": 26.301797819039198,
      "grad_norm": 0.43210065364837646,
      "learning_rate": 1.728753099362228e-05,
      "loss": 1.1011,
      "step": 2814
    },
    {
      "epoch": 26.311229000884172,
      "grad_norm": 0.4382438659667969,
      "learning_rate": 1.728545792740044e-05,
      "loss": 1.0695,
      "step": 2815
    },
    {
      "epoch": 26.32066018272915,
      "grad_norm": 0.4388991594314575,
      "learning_rate": 1.7283384193687352e-05,
      "loss": 1.1036,
      "step": 2816
    },
    {
      "epoch": 26.330091364574123,
      "grad_norm": 0.4691121578216553,
      "learning_rate": 1.7281309792673017e-05,
      "loss": 1.0918,
      "step": 2817
    },
    {
      "epoch": 26.339522546419097,
      "grad_norm": 0.4145047068595886,
      "learning_rate": 1.7279234724547492e-05,
      "loss": 1.0853,
      "step": 2818
    },
    {
      "epoch": 26.348953728264075,
      "grad_norm": 0.41782087087631226,
      "learning_rate": 1.727715898950089e-05,
      "loss": 1.0969,
      "step": 2819
    },
    {
      "epoch": 26.35838491010905,
      "grad_norm": 0.43736791610717773,
      "learning_rate": 1.7275082587723393e-05,
      "loss": 1.1058,
      "step": 2820
    },
    {
      "epoch": 26.367816091954023,
      "grad_norm": 0.4596380293369293,
      "learning_rate": 1.7273005519405234e-05,
      "loss": 1.0427,
      "step": 2821
    },
    {
      "epoch": 26.377247273798996,
      "grad_norm": 0.43191957473754883,
      "learning_rate": 1.7270927784736718e-05,
      "loss": 1.0728,
      "step": 2822
    },
    {
      "epoch": 26.386678455643974,
      "grad_norm": 0.432602196931839,
      "learning_rate": 1.7268849383908207e-05,
      "loss": 1.1271,
      "step": 2823
    },
    {
      "epoch": 26.396109637488948,
      "grad_norm": 0.47048383951187134,
      "learning_rate": 1.726677031711012e-05,
      "loss": 1.1306,
      "step": 2824
    },
    {
      "epoch": 26.405540819333922,
      "grad_norm": 0.43160200119018555,
      "learning_rate": 1.7264690584532944e-05,
      "loss": 1.069,
      "step": 2825
    },
    {
      "epoch": 26.4149720011789,
      "grad_norm": 0.46610453724861145,
      "learning_rate": 1.7262610186367222e-05,
      "loss": 1.0918,
      "step": 2826
    },
    {
      "epoch": 26.424403183023873,
      "grad_norm": 0.4424538016319275,
      "learning_rate": 1.7260529122803556e-05,
      "loss": 1.0807,
      "step": 2827
    },
    {
      "epoch": 26.433834364868847,
      "grad_norm": 0.4121975898742676,
      "learning_rate": 1.7258447394032618e-05,
      "loss": 1.1333,
      "step": 2828
    },
    {
      "epoch": 26.44326554671382,
      "grad_norm": 0.4222799837589264,
      "learning_rate": 1.7256365000245132e-05,
      "loss": 1.0899,
      "step": 2829
    },
    {
      "epoch": 26.4526967285588,
      "grad_norm": 0.5032564401626587,
      "learning_rate": 1.7254281941631887e-05,
      "loss": 1.1181,
      "step": 2830
    },
    {
      "epoch": 26.462127910403773,
      "grad_norm": 0.47030991315841675,
      "learning_rate": 1.725219821838373e-05,
      "loss": 1.0827,
      "step": 2831
    },
    {
      "epoch": 26.471559092248746,
      "grad_norm": 0.4527387022972107,
      "learning_rate": 1.7250113830691576e-05,
      "loss": 1.0846,
      "step": 2832
    },
    {
      "epoch": 26.480990274093724,
      "grad_norm": 0.4292869567871094,
      "learning_rate": 1.7248028778746393e-05,
      "loss": 1.0615,
      "step": 2833
    },
    {
      "epoch": 26.490421455938698,
      "grad_norm": 0.4607832431793213,
      "learning_rate": 1.7245943062739213e-05,
      "loss": 1.0608,
      "step": 2834
    },
    {
      "epoch": 26.499852637783672,
      "grad_norm": 0.4852883517742157,
      "learning_rate": 1.7243856682861125e-05,
      "loss": 1.0545,
      "step": 2835
    },
    {
      "epoch": 26.509283819628646,
      "grad_norm": 0.453586220741272,
      "learning_rate": 1.724176963930329e-05,
      "loss": 1.1003,
      "step": 2836
    },
    {
      "epoch": 26.518715001473623,
      "grad_norm": 0.43891987204551697,
      "learning_rate": 1.7239681932256918e-05,
      "loss": 1.0638,
      "step": 2837
    },
    {
      "epoch": 26.528146183318597,
      "grad_norm": 0.44608643651008606,
      "learning_rate": 1.723759356191328e-05,
      "loss": 1.0969,
      "step": 2838
    },
    {
      "epoch": 26.53757736516357,
      "grad_norm": 0.43990176916122437,
      "learning_rate": 1.723550452846372e-05,
      "loss": 1.0738,
      "step": 2839
    },
    {
      "epoch": 26.54700854700855,
      "grad_norm": 0.4591456353664398,
      "learning_rate": 1.723341483209963e-05,
      "loss": 1.0923,
      "step": 2840
    },
    {
      "epoch": 26.556439728853523,
      "grad_norm": 0.4708237648010254,
      "learning_rate": 1.7231324473012466e-05,
      "loss": 1.0873,
      "step": 2841
    },
    {
      "epoch": 26.565870910698496,
      "grad_norm": 0.439876914024353,
      "learning_rate": 1.722923345139375e-05,
      "loss": 1.0752,
      "step": 2842
    },
    {
      "epoch": 26.57530209254347,
      "grad_norm": 0.4594153165817261,
      "learning_rate": 1.7227141767435054e-05,
      "loss": 1.1233,
      "step": 2843
    },
    {
      "epoch": 26.584733274388448,
      "grad_norm": 0.45123764872550964,
      "learning_rate": 1.7225049421328024e-05,
      "loss": 1.0951,
      "step": 2844
    },
    {
      "epoch": 26.594164456233422,
      "grad_norm": 0.449336439371109,
      "learning_rate": 1.722295641326436e-05,
      "loss": 1.1202,
      "step": 2845
    },
    {
      "epoch": 26.603595638078396,
      "grad_norm": 0.40204185247421265,
      "learning_rate": 1.722086274343582e-05,
      "loss": 1.0868,
      "step": 2846
    },
    {
      "epoch": 26.613026819923373,
      "grad_norm": 0.4775031805038452,
      "learning_rate": 1.721876841203422e-05,
      "loss": 1.043,
      "step": 2847
    },
    {
      "epoch": 26.622458001768347,
      "grad_norm": 0.4135233461856842,
      "learning_rate": 1.7216673419251456e-05,
      "loss": 1.097,
      "step": 2848
    },
    {
      "epoch": 26.63188918361332,
      "grad_norm": 0.4458242356777191,
      "learning_rate": 1.721457776527946e-05,
      "loss": 1.1146,
      "step": 2849
    },
    {
      "epoch": 26.641320365458295,
      "grad_norm": 0.4481847584247589,
      "learning_rate": 1.7212481450310236e-05,
      "loss": 1.1288,
      "step": 2850
    },
    {
      "epoch": 26.650751547303273,
      "grad_norm": 0.44723862409591675,
      "learning_rate": 1.721038447453585e-05,
      "loss": 1.1149,
      "step": 2851
    },
    {
      "epoch": 26.660182729148246,
      "grad_norm": 0.44042620062828064,
      "learning_rate": 1.7208286838148426e-05,
      "loss": 1.1096,
      "step": 2852
    },
    {
      "epoch": 26.66961391099322,
      "grad_norm": 0.4177982807159424,
      "learning_rate": 1.720618854134015e-05,
      "loss": 1.0557,
      "step": 2853
    },
    {
      "epoch": 26.679045092838198,
      "grad_norm": 0.421972393989563,
      "learning_rate": 1.7204089584303263e-05,
      "loss": 1.1082,
      "step": 2854
    },
    {
      "epoch": 26.688476274683172,
      "grad_norm": 0.4759598672389984,
      "learning_rate": 1.7201989967230072e-05,
      "loss": 1.0881,
      "step": 2855
    },
    {
      "epoch": 26.697907456528146,
      "grad_norm": 0.45821622014045715,
      "learning_rate": 1.7199889690312947e-05,
      "loss": 1.0888,
      "step": 2856
    },
    {
      "epoch": 26.70733863837312,
      "grad_norm": 0.4648576080799103,
      "learning_rate": 1.7197788753744313e-05,
      "loss": 1.1158,
      "step": 2857
    },
    {
      "epoch": 26.716769820218097,
      "grad_norm": 0.42699721455574036,
      "learning_rate": 1.7195687157716654e-05,
      "loss": 1.076,
      "step": 2858
    },
    {
      "epoch": 26.72620100206307,
      "grad_norm": 0.4322170913219452,
      "learning_rate": 1.7193584902422523e-05,
      "loss": 1.0778,
      "step": 2859
    },
    {
      "epoch": 26.735632183908045,
      "grad_norm": 0.4197929799556732,
      "learning_rate": 1.719148198805452e-05,
      "loss": 1.0634,
      "step": 2860
    },
    {
      "epoch": 26.745063365753023,
      "grad_norm": 0.4609673023223877,
      "learning_rate": 1.718937841480532e-05,
      "loss": 1.0779,
      "step": 2861
    },
    {
      "epoch": 26.754494547597997,
      "grad_norm": 0.4461252689361572,
      "learning_rate": 1.718727418286765e-05,
      "loss": 1.0913,
      "step": 2862
    },
    {
      "epoch": 26.76392572944297,
      "grad_norm": 0.4351853132247925,
      "learning_rate": 1.71851692924343e-05,
      "loss": 1.0979,
      "step": 2863
    },
    {
      "epoch": 26.773356911287944,
      "grad_norm": 0.4454188942909241,
      "learning_rate": 1.7183063743698116e-05,
      "loss": 1.1034,
      "step": 2864
    },
    {
      "epoch": 26.782788093132922,
      "grad_norm": 0.4746839702129364,
      "learning_rate": 1.7180957536852012e-05,
      "loss": 1.0739,
      "step": 2865
    },
    {
      "epoch": 26.792219274977896,
      "grad_norm": 0.481336772441864,
      "learning_rate": 1.7178850672088953e-05,
      "loss": 1.0863,
      "step": 2866
    },
    {
      "epoch": 26.80165045682287,
      "grad_norm": 0.4283294975757599,
      "learning_rate": 1.7176743149601975e-05,
      "loss": 1.0971,
      "step": 2867
    },
    {
      "epoch": 26.811081638667847,
      "grad_norm": 0.45208504796028137,
      "learning_rate": 1.7174634969584164e-05,
      "loss": 1.0358,
      "step": 2868
    },
    {
      "epoch": 26.82051282051282,
      "grad_norm": 0.4618793725967407,
      "learning_rate": 1.7172526132228673e-05,
      "loss": 1.0808,
      "step": 2869
    },
    {
      "epoch": 26.829944002357795,
      "grad_norm": 0.4327504634857178,
      "learning_rate": 1.7170416637728712e-05,
      "loss": 1.0645,
      "step": 2870
    },
    {
      "epoch": 26.83937518420277,
      "grad_norm": 0.45395633578300476,
      "learning_rate": 1.7168306486277552e-05,
      "loss": 1.1061,
      "step": 2871
    },
    {
      "epoch": 26.848806366047747,
      "grad_norm": 0.47863203287124634,
      "learning_rate": 1.7166195678068524e-05,
      "loss": 1.0854,
      "step": 2872
    },
    {
      "epoch": 26.85823754789272,
      "grad_norm": 0.45311716198921204,
      "learning_rate": 1.716408421329502e-05,
      "loss": 1.0657,
      "step": 2873
    },
    {
      "epoch": 26.867668729737694,
      "grad_norm": 0.4472017288208008,
      "learning_rate": 1.7161972092150496e-05,
      "loss": 1.0959,
      "step": 2874
    },
    {
      "epoch": 26.877099911582672,
      "grad_norm": 0.4440765082836151,
      "learning_rate": 1.7159859314828454e-05,
      "loss": 1.0784,
      "step": 2875
    },
    {
      "epoch": 26.886531093427646,
      "grad_norm": 0.49066704511642456,
      "learning_rate": 1.7157745881522475e-05,
      "loss": 1.1034,
      "step": 2876
    },
    {
      "epoch": 26.89596227527262,
      "grad_norm": 0.4431796371936798,
      "learning_rate": 1.715563179242619e-05,
      "loss": 1.0706,
      "step": 2877
    },
    {
      "epoch": 26.905393457117594,
      "grad_norm": 0.45455074310302734,
      "learning_rate": 1.715351704773329e-05,
      "loss": 1.1024,
      "step": 2878
    },
    {
      "epoch": 26.91482463896257,
      "grad_norm": 0.44944971799850464,
      "learning_rate": 1.715140164763752e-05,
      "loss": 1.0628,
      "step": 2879
    },
    {
      "epoch": 26.924255820807545,
      "grad_norm": 0.45172983407974243,
      "learning_rate": 1.7149285592332706e-05,
      "loss": 1.062,
      "step": 2880
    },
    {
      "epoch": 26.93368700265252,
      "grad_norm": 0.4375927746295929,
      "learning_rate": 1.7147168882012708e-05,
      "loss": 1.078,
      "step": 2881
    },
    {
      "epoch": 26.943118184497497,
      "grad_norm": 0.4216648042201996,
      "learning_rate": 1.7145051516871463e-05,
      "loss": 1.0694,
      "step": 2882
    },
    {
      "epoch": 26.95254936634247,
      "grad_norm": 0.4340702295303345,
      "learning_rate": 1.7142933497102964e-05,
      "loss": 1.0822,
      "step": 2883
    },
    {
      "epoch": 26.961980548187444,
      "grad_norm": 0.4552995264530182,
      "learning_rate": 1.7140814822901265e-05,
      "loss": 1.1054,
      "step": 2884
    },
    {
      "epoch": 26.97141173003242,
      "grad_norm": 0.44449180364608765,
      "learning_rate": 1.7138695494460476e-05,
      "loss": 1.1327,
      "step": 2885
    },
    {
      "epoch": 26.980842911877396,
      "grad_norm": 0.41037362813949585,
      "learning_rate": 1.7136575511974767e-05,
      "loss": 1.0828,
      "step": 2886
    },
    {
      "epoch": 26.99027409372237,
      "grad_norm": 0.4380272626876831,
      "learning_rate": 1.7134454875638372e-05,
      "loss": 1.0557,
      "step": 2887
    },
    {
      "epoch": 26.999705275567344,
      "grad_norm": 0.46168744564056396,
      "learning_rate": 1.7132333585645585e-05,
      "loss": 1.1125,
      "step": 2888
    },
    {
      "epoch": 27.0,
      "grad_norm": 3.2779836654663086,
      "learning_rate": 1.7130211642190756e-05,
      "loss": 0.3203,
      "step": 2889
    },
    {
      "epoch": 27.009431181844974,
      "grad_norm": 0.427275151014328,
      "learning_rate": 1.7128089045468294e-05,
      "loss": 1.0906,
      "step": 2890
    },
    {
      "epoch": 27.01886236368995,
      "grad_norm": 0.4467940330505371,
      "learning_rate": 1.7125965795672677e-05,
      "loss": 1.0616,
      "step": 2891
    },
    {
      "epoch": 27.028293545534925,
      "grad_norm": 0.4527157247066498,
      "learning_rate": 1.712384189299843e-05,
      "loss": 1.1097,
      "step": 2892
    },
    {
      "epoch": 27.0377247273799,
      "grad_norm": 0.4770205318927765,
      "learning_rate": 1.7121717337640146e-05,
      "loss": 1.0645,
      "step": 2893
    },
    {
      "epoch": 27.047155909224873,
      "grad_norm": 0.4468354880809784,
      "learning_rate": 1.711959212979248e-05,
      "loss": 1.0637,
      "step": 2894
    },
    {
      "epoch": 27.05658709106985,
      "grad_norm": 0.42381736636161804,
      "learning_rate": 1.711746626965014e-05,
      "loss": 1.0932,
      "step": 2895
    },
    {
      "epoch": 27.066018272914825,
      "grad_norm": 0.44368797540664673,
      "learning_rate": 1.71153397574079e-05,
      "loss": 1.0864,
      "step": 2896
    },
    {
      "epoch": 27.0754494547598,
      "grad_norm": 0.45206350088119507,
      "learning_rate": 1.7113212593260582e-05,
      "loss": 1.0207,
      "step": 2897
    },
    {
      "epoch": 27.084880636604776,
      "grad_norm": 0.43364185094833374,
      "learning_rate": 1.711108477740308e-05,
      "loss": 1.0803,
      "step": 2898
    },
    {
      "epoch": 27.09431181844975,
      "grad_norm": 0.469156950712204,
      "learning_rate": 1.710895631003035e-05,
      "loss": 1.1259,
      "step": 2899
    },
    {
      "epoch": 27.103743000294724,
      "grad_norm": 0.45060500502586365,
      "learning_rate": 1.710682719133739e-05,
      "loss": 1.0523,
      "step": 2900
    },
    {
      "epoch": 27.113174182139698,
      "grad_norm": 0.46641767024993896,
      "learning_rate": 1.7104697421519283e-05,
      "loss": 1.0488,
      "step": 2901
    },
    {
      "epoch": 27.122605363984675,
      "grad_norm": 0.5166913866996765,
      "learning_rate": 1.7102567000771143e-05,
      "loss": 1.1047,
      "step": 2902
    },
    {
      "epoch": 27.13203654582965,
      "grad_norm": 0.47924211621284485,
      "learning_rate": 1.710043592928817e-05,
      "loss": 1.0876,
      "step": 2903
    },
    {
      "epoch": 27.141467727674623,
      "grad_norm": 0.4493969976902008,
      "learning_rate": 1.709830420726561e-05,
      "loss": 1.0986,
      "step": 2904
    },
    {
      "epoch": 27.1508989095196,
      "grad_norm": 0.4335653781890869,
      "learning_rate": 1.7096171834898765e-05,
      "loss": 1.088,
      "step": 2905
    },
    {
      "epoch": 27.160330091364575,
      "grad_norm": 0.4460194706916809,
      "learning_rate": 1.7094038812383008e-05,
      "loss": 1.0595,
      "step": 2906
    },
    {
      "epoch": 27.16976127320955,
      "grad_norm": 0.4530127942562103,
      "learning_rate": 1.7091905139913758e-05,
      "loss": 1.0765,
      "step": 2907
    },
    {
      "epoch": 27.179192455054523,
      "grad_norm": 0.4358995258808136,
      "learning_rate": 1.708977081768651e-05,
      "loss": 1.0732,
      "step": 2908
    },
    {
      "epoch": 27.1886236368995,
      "grad_norm": 0.46981707215309143,
      "learning_rate": 1.708763584589681e-05,
      "loss": 1.0669,
      "step": 2909
    },
    {
      "epoch": 27.198054818744474,
      "grad_norm": 0.4380417466163635,
      "learning_rate": 1.7085500224740258e-05,
      "loss": 1.0835,
      "step": 2910
    },
    {
      "epoch": 27.207486000589448,
      "grad_norm": 0.4405417740345001,
      "learning_rate": 1.708336395441252e-05,
      "loss": 1.0862,
      "step": 2911
    },
    {
      "epoch": 27.216917182434425,
      "grad_norm": 0.45450544357299805,
      "learning_rate": 1.708122703510932e-05,
      "loss": 1.1121,
      "step": 2912
    },
    {
      "epoch": 27.2263483642794,
      "grad_norm": 0.5037376284599304,
      "learning_rate": 1.7079089467026446e-05,
      "loss": 1.0959,
      "step": 2913
    },
    {
      "epoch": 27.235779546124373,
      "grad_norm": 0.4400935471057892,
      "learning_rate": 1.707695125035974e-05,
      "loss": 1.082,
      "step": 2914
    },
    {
      "epoch": 27.245210727969347,
      "grad_norm": 0.4258227050304413,
      "learning_rate": 1.70748123853051e-05,
      "loss": 1.1215,
      "step": 2915
    },
    {
      "epoch": 27.254641909814325,
      "grad_norm": 0.46152132749557495,
      "learning_rate": 1.7072672872058492e-05,
      "loss": 1.0847,
      "step": 2916
    },
    {
      "epoch": 27.2640730916593,
      "grad_norm": 0.4211271405220032,
      "learning_rate": 1.707053271081594e-05,
      "loss": 1.0966,
      "step": 2917
    },
    {
      "epoch": 27.273504273504273,
      "grad_norm": 0.4572596251964569,
      "learning_rate": 1.706839190177352e-05,
      "loss": 1.0528,
      "step": 2918
    },
    {
      "epoch": 27.28293545534925,
      "grad_norm": 0.4514962136745453,
      "learning_rate": 1.7066250445127374e-05,
      "loss": 1.0721,
      "step": 2919
    },
    {
      "epoch": 27.292366637194224,
      "grad_norm": 0.44950202107429504,
      "learning_rate": 1.70641083410737e-05,
      "loss": 1.0434,
      "step": 2920
    },
    {
      "epoch": 27.301797819039198,
      "grad_norm": 0.4362722635269165,
      "learning_rate": 1.706196558980876e-05,
      "loss": 1.1252,
      "step": 2921
    },
    {
      "epoch": 27.311229000884172,
      "grad_norm": 0.4491249620914459,
      "learning_rate": 1.7059822191528873e-05,
      "loss": 1.0843,
      "step": 2922
    },
    {
      "epoch": 27.32066018272915,
      "grad_norm": 0.42103275656700134,
      "learning_rate": 1.7057678146430408e-05,
      "loss": 1.0713,
      "step": 2923
    },
    {
      "epoch": 27.330091364574123,
      "grad_norm": 0.41290029883384705,
      "learning_rate": 1.7055533454709814e-05,
      "loss": 1.1132,
      "step": 2924
    },
    {
      "epoch": 27.339522546419097,
      "grad_norm": 0.4530859589576721,
      "learning_rate": 1.7053388116563577e-05,
      "loss": 1.0576,
      "step": 2925
    },
    {
      "epoch": 27.348953728264075,
      "grad_norm": 0.4011657238006592,
      "learning_rate": 1.7051242132188255e-05,
      "loss": 1.0723,
      "step": 2926
    },
    {
      "epoch": 27.35838491010905,
      "grad_norm": 0.43078410625457764,
      "learning_rate": 1.7049095501780467e-05,
      "loss": 1.0419,
      "step": 2927
    },
    {
      "epoch": 27.367816091954023,
      "grad_norm": 0.4386364221572876,
      "learning_rate": 1.704694822553688e-05,
      "loss": 1.0931,
      "step": 2928
    },
    {
      "epoch": 27.377247273798996,
      "grad_norm": 0.4175858199596405,
      "learning_rate": 1.704480030365423e-05,
      "loss": 1.109,
      "step": 2929
    },
    {
      "epoch": 27.386678455643974,
      "grad_norm": 0.4288935959339142,
      "learning_rate": 1.7042651736329307e-05,
      "loss": 1.0569,
      "step": 2930
    },
    {
      "epoch": 27.396109637488948,
      "grad_norm": 0.42916128039360046,
      "learning_rate": 1.7040502523758963e-05,
      "loss": 1.0763,
      "step": 2931
    },
    {
      "epoch": 27.405540819333922,
      "grad_norm": 0.4845292568206787,
      "learning_rate": 1.703835266614011e-05,
      "loss": 1.108,
      "step": 2932
    },
    {
      "epoch": 27.4149720011789,
      "grad_norm": 0.42522698640823364,
      "learning_rate": 1.703620216366972e-05,
      "loss": 1.0848,
      "step": 2933
    },
    {
      "epoch": 27.424403183023873,
      "grad_norm": 0.4407019019126892,
      "learning_rate": 1.703405101654481e-05,
      "loss": 1.0918,
      "step": 2934
    },
    {
      "epoch": 27.433834364868847,
      "grad_norm": 0.43541961908340454,
      "learning_rate": 1.703189922496248e-05,
      "loss": 1.1347,
      "step": 2935
    },
    {
      "epoch": 27.44326554671382,
      "grad_norm": 0.43830606341362,
      "learning_rate": 1.702974678911987e-05,
      "loss": 1.0984,
      "step": 2936
    },
    {
      "epoch": 27.4526967285588,
      "grad_norm": 0.4267257750034332,
      "learning_rate": 1.7027593709214185e-05,
      "loss": 1.0523,
      "step": 2937
    },
    {
      "epoch": 27.462127910403773,
      "grad_norm": 0.41807547211647034,
      "learning_rate": 1.7025439985442693e-05,
      "loss": 1.0545,
      "step": 2938
    },
    {
      "epoch": 27.471559092248746,
      "grad_norm": 0.4111427366733551,
      "learning_rate": 1.7023285618002708e-05,
      "loss": 1.1011,
      "step": 2939
    },
    {
      "epoch": 27.480990274093724,
      "grad_norm": 0.45707133412361145,
      "learning_rate": 1.7021130607091628e-05,
      "loss": 1.0703,
      "step": 2940
    },
    {
      "epoch": 27.490421455938698,
      "grad_norm": 0.4115637242794037,
      "learning_rate": 1.7018974952906885e-05,
      "loss": 1.0781,
      "step": 2941
    },
    {
      "epoch": 27.499852637783672,
      "grad_norm": 0.4411139190196991,
      "learning_rate": 1.7016818655645977e-05,
      "loss": 1.084,
      "step": 2942
    },
    {
      "epoch": 27.509283819628646,
      "grad_norm": 0.4575614929199219,
      "learning_rate": 1.7014661715506472e-05,
      "loss": 1.072,
      "step": 2943
    },
    {
      "epoch": 27.518715001473623,
      "grad_norm": 0.41956230998039246,
      "learning_rate": 1.7012504132685978e-05,
      "loss": 1.1219,
      "step": 2944
    },
    {
      "epoch": 27.528146183318597,
      "grad_norm": 0.43811213970184326,
      "learning_rate": 1.7010345907382176e-05,
      "loss": 1.0806,
      "step": 2945
    },
    {
      "epoch": 27.53757736516357,
      "grad_norm": 0.4338599741458893,
      "learning_rate": 1.7008187039792807e-05,
      "loss": 1.1065,
      "step": 2946
    },
    {
      "epoch": 27.54700854700855,
      "grad_norm": 0.4344461262226105,
      "learning_rate": 1.700602753011566e-05,
      "loss": 1.0916,
      "step": 2947
    },
    {
      "epoch": 27.556439728853523,
      "grad_norm": 0.506685197353363,
      "learning_rate": 1.700386737854859e-05,
      "loss": 1.109,
      "step": 2948
    },
    {
      "epoch": 27.565870910698496,
      "grad_norm": 0.4224735200405121,
      "learning_rate": 1.700170658528951e-05,
      "loss": 1.1113,
      "step": 2949
    },
    {
      "epoch": 27.57530209254347,
      "grad_norm": 0.4656005799770355,
      "learning_rate": 1.6999545150536388e-05,
      "loss": 1.0868,
      "step": 2950
    },
    {
      "epoch": 27.584733274388448,
      "grad_norm": 0.448873907327652,
      "learning_rate": 1.6997383074487262e-05,
      "loss": 1.0951,
      "step": 2951
    },
    {
      "epoch": 27.594164456233422,
      "grad_norm": 0.4306422173976898,
      "learning_rate": 1.6995220357340213e-05,
      "loss": 1.1052,
      "step": 2952
    },
    {
      "epoch": 27.603595638078396,
      "grad_norm": 0.4684416651725769,
      "learning_rate": 1.6993056999293385e-05,
      "loss": 1.0705,
      "step": 2953
    },
    {
      "epoch": 27.613026819923373,
      "grad_norm": 0.47526293992996216,
      "learning_rate": 1.6990893000544996e-05,
      "loss": 1.1178,
      "step": 2954
    },
    {
      "epoch": 27.622458001768347,
      "grad_norm": 0.4354169964790344,
      "learning_rate": 1.6988728361293303e-05,
      "loss": 1.0851,
      "step": 2955
    },
    {
      "epoch": 27.63188918361332,
      "grad_norm": 0.4174658954143524,
      "learning_rate": 1.698656308173663e-05,
      "loss": 1.0862,
      "step": 2956
    },
    {
      "epoch": 27.641320365458295,
      "grad_norm": 0.4580265283584595,
      "learning_rate": 1.6984397162073362e-05,
      "loss": 1.0293,
      "step": 2957
    },
    {
      "epoch": 27.650751547303273,
      "grad_norm": 0.45854881405830383,
      "learning_rate": 1.698223060250194e-05,
      "loss": 1.1295,
      "step": 2958
    },
    {
      "epoch": 27.660182729148246,
      "grad_norm": 0.461058646440506,
      "learning_rate": 1.698006340322086e-05,
      "loss": 1.1072,
      "step": 2959
    },
    {
      "epoch": 27.66961391099322,
      "grad_norm": 0.46474599838256836,
      "learning_rate": 1.697789556442868e-05,
      "loss": 1.0895,
      "step": 2960
    },
    {
      "epoch": 27.679045092838198,
      "grad_norm": 0.4160859286785126,
      "learning_rate": 1.697572708632402e-05,
      "loss": 1.0975,
      "step": 2961
    },
    {
      "epoch": 27.688476274683172,
      "grad_norm": 0.4229770600795746,
      "learning_rate": 1.6973557969105555e-05,
      "loss": 1.1036,
      "step": 2962
    },
    {
      "epoch": 27.697907456528146,
      "grad_norm": 0.4234057366847992,
      "learning_rate": 1.6971388212972013e-05,
      "loss": 1.085,
      "step": 2963
    },
    {
      "epoch": 27.70733863837312,
      "grad_norm": 0.48818933963775635,
      "learning_rate": 1.6969217818122195e-05,
      "loss": 1.0716,
      "step": 2964
    },
    {
      "epoch": 27.716769820218097,
      "grad_norm": 0.4542373716831207,
      "learning_rate": 1.6967046784754946e-05,
      "loss": 1.1136,
      "step": 2965
    },
    {
      "epoch": 27.72620100206307,
      "grad_norm": 0.41939452290534973,
      "learning_rate": 1.696487511306918e-05,
      "loss": 1.0675,
      "step": 2966
    },
    {
      "epoch": 27.735632183908045,
      "grad_norm": 0.441717267036438,
      "learning_rate": 1.696270280326386e-05,
      "loss": 1.1438,
      "step": 2967
    },
    {
      "epoch": 27.745063365753023,
      "grad_norm": 0.42266350984573364,
      "learning_rate": 1.6960529855538013e-05,
      "loss": 1.0643,
      "step": 2968
    },
    {
      "epoch": 27.754494547597997,
      "grad_norm": 0.4228062033653259,
      "learning_rate": 1.6958356270090726e-05,
      "loss": 1.0751,
      "step": 2969
    },
    {
      "epoch": 27.76392572944297,
      "grad_norm": 0.43948692083358765,
      "learning_rate": 1.695618204712114e-05,
      "loss": 1.0841,
      "step": 2970
    },
    {
      "epoch": 27.773356911287944,
      "grad_norm": 0.4822072386741638,
      "learning_rate": 1.6954007186828458e-05,
      "loss": 1.1033,
      "step": 2971
    },
    {
      "epoch": 27.782788093132922,
      "grad_norm": 0.4536893665790558,
      "learning_rate": 1.695183168941194e-05,
      "loss": 1.0971,
      "step": 2972
    },
    {
      "epoch": 27.792219274977896,
      "grad_norm": 0.44530680775642395,
      "learning_rate": 1.6949655555070904e-05,
      "loss": 1.1243,
      "step": 2973
    },
    {
      "epoch": 27.80165045682287,
      "grad_norm": 0.4502002000808716,
      "learning_rate": 1.694747878400473e-05,
      "loss": 1.0547,
      "step": 2974
    },
    {
      "epoch": 27.811081638667847,
      "grad_norm": 0.4351111948490143,
      "learning_rate": 1.6945301376412847e-05,
      "loss": 1.0888,
      "step": 2975
    },
    {
      "epoch": 27.82051282051282,
      "grad_norm": 0.46392858028411865,
      "learning_rate": 1.694312333249475e-05,
      "loss": 1.1061,
      "step": 2976
    },
    {
      "epoch": 27.829944002357795,
      "grad_norm": 0.4329262971878052,
      "learning_rate": 1.6940944652449994e-05,
      "loss": 1.0606,
      "step": 2977
    },
    {
      "epoch": 27.83937518420277,
      "grad_norm": 0.42428356409072876,
      "learning_rate": 1.6938765336478186e-05,
      "loss": 1.0836,
      "step": 2978
    },
    {
      "epoch": 27.848806366047747,
      "grad_norm": 0.42759740352630615,
      "learning_rate": 1.6936585384778993e-05,
      "loss": 1.0723,
      "step": 2979
    },
    {
      "epoch": 27.85823754789272,
      "grad_norm": 0.44092002511024475,
      "learning_rate": 1.6934404797552145e-05,
      "loss": 1.0546,
      "step": 2980
    },
    {
      "epoch": 27.867668729737694,
      "grad_norm": 0.4362010061740875,
      "learning_rate": 1.693222357499743e-05,
      "loss": 1.0847,
      "step": 2981
    },
    {
      "epoch": 27.877099911582672,
      "grad_norm": 0.4542732834815979,
      "learning_rate": 1.693004171731468e-05,
      "loss": 1.1185,
      "step": 2982
    },
    {
      "epoch": 27.886531093427646,
      "grad_norm": 0.46811777353286743,
      "learning_rate": 1.6927859224703803e-05,
      "loss": 1.1094,
      "step": 2983
    },
    {
      "epoch": 27.89596227527262,
      "grad_norm": 0.43305253982543945,
      "learning_rate": 1.6925676097364757e-05,
      "loss": 1.0629,
      "step": 2984
    },
    {
      "epoch": 27.905393457117594,
      "grad_norm": 0.4743993282318115,
      "learning_rate": 1.6923492335497563e-05,
      "loss": 1.0682,
      "step": 2985
    },
    {
      "epoch": 27.91482463896257,
      "grad_norm": 0.4215141832828522,
      "learning_rate": 1.692130793930229e-05,
      "loss": 1.109,
      "step": 2986
    },
    {
      "epoch": 27.924255820807545,
      "grad_norm": 0.4504697322845459,
      "learning_rate": 1.6919122908979074e-05,
      "loss": 1.0968,
      "step": 2987
    },
    {
      "epoch": 27.93368700265252,
      "grad_norm": 0.4592825472354889,
      "learning_rate": 1.6916937244728112e-05,
      "loss": 1.0749,
      "step": 2988
    },
    {
      "epoch": 27.943118184497497,
      "grad_norm": 0.46010997891426086,
      "learning_rate": 1.6914750946749646e-05,
      "loss": 1.1043,
      "step": 2989
    },
    {
      "epoch": 27.95254936634247,
      "grad_norm": 0.4599390923976898,
      "learning_rate": 1.6912564015243987e-05,
      "loss": 1.1471,
      "step": 2990
    },
    {
      "epoch": 27.961980548187444,
      "grad_norm": 0.4881290793418884,
      "learning_rate": 1.6910376450411505e-05,
      "loss": 1.084,
      "step": 2991
    },
    {
      "epoch": 27.97141173003242,
      "grad_norm": 0.45136502385139465,
      "learning_rate": 1.690818825245262e-05,
      "loss": 1.1009,
      "step": 2992
    },
    {
      "epoch": 27.980842911877396,
      "grad_norm": 0.4225202202796936,
      "learning_rate": 1.690599942156781e-05,
      "loss": 1.0988,
      "step": 2993
    },
    {
      "epoch": 27.99027409372237,
      "grad_norm": 0.44497576355934143,
      "learning_rate": 1.690380995795762e-05,
      "loss": 1.1089,
      "step": 2994
    },
    {
      "epoch": 27.999705275567344,
      "grad_norm": 0.4541633725166321,
      "learning_rate": 1.690161986182265e-05,
      "loss": 1.0659,
      "step": 2995
    },
    {
      "epoch": 28.0,
      "grad_norm": 1.9149706363677979,
      "learning_rate": 1.6899429133363554e-05,
      "loss": 0.905,
      "step": 2996
    },
    {
      "epoch": 28.009431181844974,
      "grad_norm": 0.4539135992527008,
      "learning_rate": 1.6897237772781046e-05,
      "loss": 1.0746,
      "step": 2997
    },
    {
      "epoch": 28.01886236368995,
      "grad_norm": 0.41074374318122864,
      "learning_rate": 1.6895045780275892e-05,
      "loss": 1.0807,
      "step": 2998
    },
    {
      "epoch": 28.028293545534925,
      "grad_norm": 0.4575376808643341,
      "learning_rate": 1.6892853156048928e-05,
      "loss": 1.0375,
      "step": 2999
    },
    {
      "epoch": 28.0377247273799,
      "grad_norm": 0.4505786597728729,
      "learning_rate": 1.6890659900301042e-05,
      "loss": 1.0972,
      "step": 3000
    },
    {
      "epoch": 28.047155909224873,
      "grad_norm": 0.439557284116745,
      "learning_rate": 1.6888466013233175e-05,
      "loss": 1.0965,
      "step": 3001
    },
    {
      "epoch": 28.05658709106985,
      "grad_norm": 0.4449111819267273,
      "learning_rate": 1.688627149504633e-05,
      "loss": 1.0767,
      "step": 3002
    },
    {
      "epoch": 28.066018272914825,
      "grad_norm": 0.454342246055603,
      "learning_rate": 1.6884076345941572e-05,
      "loss": 1.0471,
      "step": 3003
    },
    {
      "epoch": 28.0754494547598,
      "grad_norm": 0.46240800619125366,
      "learning_rate": 1.688188056612002e-05,
      "loss": 1.0681,
      "step": 3004
    },
    {
      "epoch": 28.084880636604776,
      "grad_norm": 0.43732401728630066,
      "learning_rate": 1.6879684155782847e-05,
      "loss": 1.0682,
      "step": 3005
    },
    {
      "epoch": 28.09431181844975,
      "grad_norm": 0.47115081548690796,
      "learning_rate": 1.687748711513129e-05,
      "loss": 1.0945,
      "step": 3006
    },
    {
      "epoch": 28.103743000294724,
      "grad_norm": 0.4576622247695923,
      "learning_rate": 1.687528944436664e-05,
      "loss": 1.063,
      "step": 3007
    },
    {
      "epoch": 28.113174182139698,
      "grad_norm": 0.4319034516811371,
      "learning_rate": 1.687309114369025e-05,
      "loss": 1.0861,
      "step": 3008
    },
    {
      "epoch": 28.122605363984675,
      "grad_norm": 0.4369412064552307,
      "learning_rate": 1.687089221330352e-05,
      "loss": 1.0719,
      "step": 3009
    },
    {
      "epoch": 28.13203654582965,
      "grad_norm": 0.4023958146572113,
      "learning_rate": 1.6868692653407922e-05,
      "loss": 1.0895,
      "step": 3010
    },
    {
      "epoch": 28.141467727674623,
      "grad_norm": 0.44093838334083557,
      "learning_rate": 1.686649246420498e-05,
      "loss": 1.1102,
      "step": 3011
    },
    {
      "epoch": 28.1508989095196,
      "grad_norm": 0.4175834059715271,
      "learning_rate": 1.6864291645896266e-05,
      "loss": 1.0864,
      "step": 3012
    },
    {
      "epoch": 28.160330091364575,
      "grad_norm": 0.4082290530204773,
      "learning_rate": 1.6862090198683428e-05,
      "loss": 1.098,
      "step": 3013
    },
    {
      "epoch": 28.16976127320955,
      "grad_norm": 0.4572993218898773,
      "learning_rate": 1.6859888122768157e-05,
      "loss": 1.0822,
      "step": 3014
    },
    {
      "epoch": 28.179192455054523,
      "grad_norm": 0.4694819450378418,
      "learning_rate": 1.6857685418352207e-05,
      "loss": 1.1137,
      "step": 3015
    },
    {
      "epoch": 28.1886236368995,
      "grad_norm": 0.4542582929134369,
      "learning_rate": 1.6855482085637388e-05,
      "loss": 1.0709,
      "step": 3016
    },
    {
      "epoch": 28.198054818744474,
      "grad_norm": 0.45556244254112244,
      "learning_rate": 1.685327812482557e-05,
      "loss": 1.0508,
      "step": 3017
    },
    {
      "epoch": 28.207486000589448,
      "grad_norm": 0.4484015703201294,
      "learning_rate": 1.6851073536118684e-05,
      "loss": 1.0768,
      "step": 3018
    },
    {
      "epoch": 28.216917182434425,
      "grad_norm": 0.4672759771347046,
      "learning_rate": 1.6848868319718705e-05,
      "loss": 1.0766,
      "step": 3019
    },
    {
      "epoch": 28.2263483642794,
      "grad_norm": 0.4342638850212097,
      "learning_rate": 1.6846662475827675e-05,
      "loss": 1.1216,
      "step": 3020
    },
    {
      "epoch": 28.235779546124373,
      "grad_norm": 0.4455719590187073,
      "learning_rate": 1.6844456004647698e-05,
      "loss": 1.1129,
      "step": 3021
    },
    {
      "epoch": 28.245210727969347,
      "grad_norm": 0.44769492745399475,
      "learning_rate": 1.6842248906380928e-05,
      "loss": 1.0463,
      "step": 3022
    },
    {
      "epoch": 28.254641909814325,
      "grad_norm": 0.4335850477218628,
      "learning_rate": 1.684004118122958e-05,
      "loss": 1.064,
      "step": 3023
    },
    {
      "epoch": 28.2640730916593,
      "grad_norm": 0.4709929823875427,
      "learning_rate": 1.6837832829395923e-05,
      "loss": 1.0831,
      "step": 3024
    },
    {
      "epoch": 28.273504273504273,
      "grad_norm": 0.4566993713378906,
      "learning_rate": 1.6835623851082285e-05,
      "loss": 1.1169,
      "step": 3025
    },
    {
      "epoch": 28.28293545534925,
      "grad_norm": 0.47649070620536804,
      "learning_rate": 1.6833414246491052e-05,
      "loss": 1.0698,
      "step": 3026
    },
    {
      "epoch": 28.292366637194224,
      "grad_norm": 0.4498426914215088,
      "learning_rate": 1.6831204015824666e-05,
      "loss": 1.0535,
      "step": 3027
    },
    {
      "epoch": 28.301797819039198,
      "grad_norm": 0.43413209915161133,
      "learning_rate": 1.6828993159285635e-05,
      "loss": 1.1257,
      "step": 3028
    },
    {
      "epoch": 28.311229000884172,
      "grad_norm": 0.4614328145980835,
      "learning_rate": 1.6826781677076506e-05,
      "loss": 1.0439,
      "step": 3029
    },
    {
      "epoch": 28.32066018272915,
      "grad_norm": 0.4692303538322449,
      "learning_rate": 1.68245695693999e-05,
      "loss": 1.1276,
      "step": 3030
    },
    {
      "epoch": 28.330091364574123,
      "grad_norm": 0.4358323812484741,
      "learning_rate": 1.6822356836458493e-05,
      "loss": 1.0629,
      "step": 3031
    },
    {
      "epoch": 28.339522546419097,
      "grad_norm": 0.4402109980583191,
      "learning_rate": 1.682014347845501e-05,
      "loss": 1.0558,
      "step": 3032
    },
    {
      "epoch": 28.348953728264075,
      "grad_norm": 0.4626575708389282,
      "learning_rate": 1.681792949559224e-05,
      "loss": 1.0642,
      "step": 3033
    },
    {
      "epoch": 28.35838491010905,
      "grad_norm": 0.49427422881126404,
      "learning_rate": 1.6815714888073026e-05,
      "loss": 1.0815,
      "step": 3034
    },
    {
      "epoch": 28.367816091954023,
      "grad_norm": 0.4692770838737488,
      "learning_rate": 1.681349965610027e-05,
      "loss": 1.0593,
      "step": 3035
    },
    {
      "epoch": 28.377247273798996,
      "grad_norm": 0.4576817452907562,
      "learning_rate": 1.6811283799876932e-05,
      "loss": 1.0874,
      "step": 3036
    },
    {
      "epoch": 28.386678455643974,
      "grad_norm": 0.49619707465171814,
      "learning_rate": 1.6809067319606027e-05,
      "loss": 1.1147,
      "step": 3037
    },
    {
      "epoch": 28.396109637488948,
      "grad_norm": 0.45405521988868713,
      "learning_rate": 1.680685021549063e-05,
      "loss": 1.0892,
      "step": 3038
    },
    {
      "epoch": 28.405540819333922,
      "grad_norm": 0.4665006399154663,
      "learning_rate": 1.6804632487733866e-05,
      "loss": 1.0681,
      "step": 3039
    },
    {
      "epoch": 28.4149720011789,
      "grad_norm": 0.4710666835308075,
      "learning_rate": 1.6802414136538928e-05,
      "loss": 1.0892,
      "step": 3040
    },
    {
      "epoch": 28.424403183023873,
      "grad_norm": 0.5049771666526794,
      "learning_rate": 1.680019516210906e-05,
      "loss": 1.0653,
      "step": 3041
    },
    {
      "epoch": 28.433834364868847,
      "grad_norm": 0.43508413434028625,
      "learning_rate": 1.679797556464756e-05,
      "loss": 1.107,
      "step": 3042
    },
    {
      "epoch": 28.44326554671382,
      "grad_norm": 0.44741642475128174,
      "learning_rate": 1.6795755344357792e-05,
      "loss": 1.1078,
      "step": 3043
    },
    {
      "epoch": 28.4526967285588,
      "grad_norm": 0.47427982091903687,
      "learning_rate": 1.679353450144317e-05,
      "loss": 1.0531,
      "step": 3044
    },
    {
      "epoch": 28.462127910403773,
      "grad_norm": 0.41620486974716187,
      "learning_rate": 1.6791313036107163e-05,
      "loss": 1.0773,
      "step": 3045
    },
    {
      "epoch": 28.471559092248746,
      "grad_norm": 0.45630884170532227,
      "learning_rate": 1.6789090948553303e-05,
      "loss": 1.1048,
      "step": 3046
    },
    {
      "epoch": 28.480990274093724,
      "grad_norm": 0.4093480706214905,
      "learning_rate": 1.678686823898518e-05,
      "loss": 1.087,
      "step": 3047
    },
    {
      "epoch": 28.490421455938698,
      "grad_norm": 0.4546709954738617,
      "learning_rate": 1.6784644907606434e-05,
      "loss": 1.0996,
      "step": 3048
    },
    {
      "epoch": 28.499852637783672,
      "grad_norm": 0.46190145611763,
      "learning_rate": 1.678242095462077e-05,
      "loss": 1.124,
      "step": 3049
    },
    {
      "epoch": 28.509283819628646,
      "grad_norm": 0.4558461308479309,
      "learning_rate": 1.6780196380231943e-05,
      "loss": 1.1152,
      "step": 3050
    },
    {
      "epoch": 28.518715001473623,
      "grad_norm": 0.47019776701927185,
      "learning_rate": 1.6777971184643765e-05,
      "loss": 1.0637,
      "step": 3051
    },
    {
      "epoch": 28.528146183318597,
      "grad_norm": 0.42337557673454285,
      "learning_rate": 1.6775745368060118e-05,
      "loss": 1.0828,
      "step": 3052
    },
    {
      "epoch": 28.53757736516357,
      "grad_norm": 0.46373680233955383,
      "learning_rate": 1.677351893068492e-05,
      "loss": 1.0901,
      "step": 3053
    },
    {
      "epoch": 28.54700854700855,
      "grad_norm": 0.4310053884983063,
      "learning_rate": 1.6771291872722157e-05,
      "loss": 1.0725,
      "step": 3054
    },
    {
      "epoch": 28.556439728853523,
      "grad_norm": 0.4555477499961853,
      "learning_rate": 1.6769064194375877e-05,
      "loss": 1.0826,
      "step": 3055
    },
    {
      "epoch": 28.565870910698496,
      "grad_norm": 0.4636659026145935,
      "learning_rate": 1.6766835895850176e-05,
      "loss": 1.0974,
      "step": 3056
    },
    {
      "epoch": 28.57530209254347,
      "grad_norm": 0.45733416080474854,
      "learning_rate": 1.6764606977349212e-05,
      "loss": 1.1058,
      "step": 3057
    },
    {
      "epoch": 28.584733274388448,
      "grad_norm": 0.4414634108543396,
      "learning_rate": 1.676237743907719e-05,
      "loss": 1.0774,
      "step": 3058
    },
    {
      "epoch": 28.594164456233422,
      "grad_norm": 0.4664212167263031,
      "learning_rate": 1.6760147281238395e-05,
      "loss": 1.1123,
      "step": 3059
    },
    {
      "epoch": 28.603595638078396,
      "grad_norm": 0.5199339389801025,
      "learning_rate": 1.6757916504037138e-05,
      "loss": 1.1113,
      "step": 3060
    },
    {
      "epoch": 28.613026819923373,
      "grad_norm": 0.442396879196167,
      "learning_rate": 1.6755685107677808e-05,
      "loss": 1.0648,
      "step": 3061
    },
    {
      "epoch": 28.622458001768347,
      "grad_norm": 0.43245092034339905,
      "learning_rate": 1.675345309236485e-05,
      "loss": 1.0659,
      "step": 3062
    },
    {
      "epoch": 28.63188918361332,
      "grad_norm": 0.4364834427833557,
      "learning_rate": 1.6751220458302753e-05,
      "loss": 1.0758,
      "step": 3063
    },
    {
      "epoch": 28.641320365458295,
      "grad_norm": 0.48329323530197144,
      "learning_rate": 1.674898720569607e-05,
      "loss": 1.1002,
      "step": 3064
    },
    {
      "epoch": 28.650751547303273,
      "grad_norm": 0.44949278235435486,
      "learning_rate": 1.6746753334749415e-05,
      "loss": 1.0967,
      "step": 3065
    },
    {
      "epoch": 28.660182729148246,
      "grad_norm": 0.43496885895729065,
      "learning_rate": 1.6744518845667454e-05,
      "loss": 1.0977,
      "step": 3066
    },
    {
      "epoch": 28.66961391099322,
      "grad_norm": 0.4362998306751251,
      "learning_rate": 1.674228373865491e-05,
      "loss": 1.0824,
      "step": 3067
    },
    {
      "epoch": 28.679045092838198,
      "grad_norm": 0.4455733299255371,
      "learning_rate": 1.674004801391656e-05,
      "loss": 1.0839,
      "step": 3068
    },
    {
      "epoch": 28.688476274683172,
      "grad_norm": 0.47701495885849,
      "learning_rate": 1.673781167165724e-05,
      "loss": 1.0547,
      "step": 3069
    },
    {
      "epoch": 28.697907456528146,
      "grad_norm": 0.46824002265930176,
      "learning_rate": 1.673557471208185e-05,
      "loss": 1.0605,
      "step": 3070
    },
    {
      "epoch": 28.70733863837312,
      "grad_norm": 0.4527682363986969,
      "learning_rate": 1.6733337135395332e-05,
      "loss": 1.1163,
      "step": 3071
    },
    {
      "epoch": 28.716769820218097,
      "grad_norm": 0.4294699728488922,
      "learning_rate": 1.6731098941802698e-05,
      "loss": 1.0461,
      "step": 3072
    },
    {
      "epoch": 28.72620100206307,
      "grad_norm": 0.4443267583847046,
      "learning_rate": 1.6728860131509003e-05,
      "loss": 1.1044,
      "step": 3073
    },
    {
      "epoch": 28.735632183908045,
      "grad_norm": 0.4537442922592163,
      "learning_rate": 1.6726620704719372e-05,
      "loss": 1.0941,
      "step": 3074
    },
    {
      "epoch": 28.745063365753023,
      "grad_norm": 0.42105722427368164,
      "learning_rate": 1.672438066163898e-05,
      "loss": 1.1287,
      "step": 3075
    },
    {
      "epoch": 28.754494547597997,
      "grad_norm": 0.44960883259773254,
      "learning_rate": 1.6722140002473058e-05,
      "loss": 1.0937,
      "step": 3076
    },
    {
      "epoch": 28.76392572944297,
      "grad_norm": 0.4262641370296478,
      "learning_rate": 1.6719898727426894e-05,
      "loss": 1.0708,
      "step": 3077
    },
    {
      "epoch": 28.773356911287944,
      "grad_norm": 0.43198537826538086,
      "learning_rate": 1.671765683670583e-05,
      "loss": 1.0753,
      "step": 3078
    },
    {
      "epoch": 28.782788093132922,
      "grad_norm": 0.46210551261901855,
      "learning_rate": 1.6715414330515276e-05,
      "loss": 1.0785,
      "step": 3079
    },
    {
      "epoch": 28.792219274977896,
      "grad_norm": 0.44057998061180115,
      "learning_rate": 1.6713171209060683e-05,
      "loss": 1.0959,
      "step": 3080
    },
    {
      "epoch": 28.80165045682287,
      "grad_norm": 0.4422953128814697,
      "learning_rate": 1.6710927472547562e-05,
      "loss": 1.0964,
      "step": 3081
    },
    {
      "epoch": 28.811081638667847,
      "grad_norm": 0.49211686849594116,
      "learning_rate": 1.6708683121181492e-05,
      "loss": 1.089,
      "step": 3082
    },
    {
      "epoch": 28.82051282051282,
      "grad_norm": 0.4517574906349182,
      "learning_rate": 1.670643815516809e-05,
      "loss": 1.1039,
      "step": 3083
    },
    {
      "epoch": 28.829944002357795,
      "grad_norm": 0.44101762771606445,
      "learning_rate": 1.670419257471305e-05,
      "loss": 1.0465,
      "step": 3084
    },
    {
      "epoch": 28.83937518420277,
      "grad_norm": 0.44750767946243286,
      "learning_rate": 1.6701946380022104e-05,
      "loss": 1.0738,
      "step": 3085
    },
    {
      "epoch": 28.848806366047747,
      "grad_norm": 0.4650917053222656,
      "learning_rate": 1.6699699571301052e-05,
      "loss": 1.1513,
      "step": 3086
    },
    {
      "epoch": 28.85823754789272,
      "grad_norm": 0.4576112926006317,
      "learning_rate": 1.6697452148755736e-05,
      "loss": 1.1052,
      "step": 3087
    },
    {
      "epoch": 28.867668729737694,
      "grad_norm": 0.49540799856185913,
      "learning_rate": 1.6695204112592078e-05,
      "loss": 1.0702,
      "step": 3088
    },
    {
      "epoch": 28.877099911582672,
      "grad_norm": 0.45060110092163086,
      "learning_rate": 1.6692955463016037e-05,
      "loss": 1.1077,
      "step": 3089
    },
    {
      "epoch": 28.886531093427646,
      "grad_norm": 0.4727064371109009,
      "learning_rate": 1.6690706200233628e-05,
      "loss": 1.0634,
      "step": 3090
    },
    {
      "epoch": 28.89596227527262,
      "grad_norm": 0.4698019325733185,
      "learning_rate": 1.668845632445093e-05,
      "loss": 1.0935,
      "step": 3091
    },
    {
      "epoch": 28.905393457117594,
      "grad_norm": 0.4555988907814026,
      "learning_rate": 1.6686205835874084e-05,
      "loss": 1.054,
      "step": 3092
    },
    {
      "epoch": 28.91482463896257,
      "grad_norm": 0.4632149040699005,
      "learning_rate": 1.668395473470927e-05,
      "loss": 1.0689,
      "step": 3093
    },
    {
      "epoch": 28.924255820807545,
      "grad_norm": 0.4548781216144562,
      "learning_rate": 1.6681703021162733e-05,
      "loss": 1.0893,
      "step": 3094
    },
    {
      "epoch": 28.93368700265252,
      "grad_norm": 0.4462908208370209,
      "learning_rate": 1.667945069544078e-05,
      "loss": 1.1136,
      "step": 3095
    },
    {
      "epoch": 28.943118184497497,
      "grad_norm": 0.4732154905796051,
      "learning_rate": 1.6677197757749768e-05,
      "loss": 1.0579,
      "step": 3096
    },
    {
      "epoch": 28.95254936634247,
      "grad_norm": 0.4402284324169159,
      "learning_rate": 1.6674944208296104e-05,
      "loss": 1.0612,
      "step": 3097
    },
    {
      "epoch": 28.961980548187444,
      "grad_norm": 0.45028936862945557,
      "learning_rate": 1.667269004728626e-05,
      "loss": 1.059,
      "step": 3098
    },
    {
      "epoch": 28.97141173003242,
      "grad_norm": 0.4519365727901459,
      "learning_rate": 1.6670435274926767e-05,
      "loss": 1.1109,
      "step": 3099
    },
    {
      "epoch": 28.980842911877396,
      "grad_norm": 0.4226590394973755,
      "learning_rate": 1.6668179891424203e-05,
      "loss": 1.0735,
      "step": 3100
    },
    {
      "epoch": 28.99027409372237,
      "grad_norm": 0.4468157887458801,
      "learning_rate": 1.6665923896985203e-05,
      "loss": 1.1154,
      "step": 3101
    },
    {
      "epoch": 28.999705275567344,
      "grad_norm": 0.437430739402771,
      "learning_rate": 1.6663667291816465e-05,
      "loss": 1.0924,
      "step": 3102
    },
    {
      "epoch": 29.0,
      "grad_norm": 2.5802440643310547,
      "learning_rate": 1.666141007612473e-05,
      "loss": 0.5608,
      "step": 3103
    },
    {
      "epoch": 29.009431181844974,
      "grad_norm": 0.44149404764175415,
      "learning_rate": 1.665915225011681e-05,
      "loss": 1.1254,
      "step": 3104
    },
    {
      "epoch": 29.01886236368995,
      "grad_norm": 0.429568886756897,
      "learning_rate": 1.665689381399957e-05,
      "loss": 1.0665,
      "step": 3105
    },
    {
      "epoch": 29.028293545534925,
      "grad_norm": 0.4741397202014923,
      "learning_rate": 1.665463476797992e-05,
      "loss": 1.0884,
      "step": 3106
    },
    {
      "epoch": 29.0377247273799,
      "grad_norm": 0.4519481956958771,
      "learning_rate": 1.6652375112264837e-05,
      "loss": 1.0852,
      "step": 3107
    },
    {
      "epoch": 29.047155909224873,
      "grad_norm": 0.4711868166923523,
      "learning_rate": 1.6650114847061347e-05,
      "loss": 1.1065,
      "step": 3108
    },
    {
      "epoch": 29.05658709106985,
      "grad_norm": 0.4428175985813141,
      "learning_rate": 1.6647853972576537e-05,
      "loss": 1.077,
      "step": 3109
    },
    {
      "epoch": 29.066018272914825,
      "grad_norm": 0.441279798746109,
      "learning_rate": 1.6645592489017545e-05,
      "loss": 1.0863,
      "step": 3110
    },
    {
      "epoch": 29.0754494547598,
      "grad_norm": 0.45929768681526184,
      "learning_rate": 1.6643330396591572e-05,
      "loss": 1.0424,
      "step": 3111
    },
    {
      "epoch": 29.084880636604776,
      "grad_norm": 0.4613528549671173,
      "learning_rate": 1.6641067695505867e-05,
      "loss": 1.0671,
      "step": 3112
    },
    {
      "epoch": 29.09431181844975,
      "grad_norm": 0.4152880609035492,
      "learning_rate": 1.6638804385967735e-05,
      "loss": 1.0899,
      "step": 3113
    },
    {
      "epoch": 29.103743000294724,
      "grad_norm": 0.45481443405151367,
      "learning_rate": 1.6636540468184548e-05,
      "loss": 1.099,
      "step": 3114
    },
    {
      "epoch": 29.113174182139698,
      "grad_norm": 0.45856425166130066,
      "learning_rate": 1.6634275942363716e-05,
      "loss": 1.0853,
      "step": 3115
    },
    {
      "epoch": 29.122605363984675,
      "grad_norm": 0.44412916898727417,
      "learning_rate": 1.6632010808712723e-05,
      "loss": 1.0825,
      "step": 3116
    },
    {
      "epoch": 29.13203654582965,
      "grad_norm": 0.44532594084739685,
      "learning_rate": 1.662974506743909e-05,
      "loss": 1.0407,
      "step": 3117
    },
    {
      "epoch": 29.141467727674623,
      "grad_norm": 0.4485303461551666,
      "learning_rate": 1.6627478718750413e-05,
      "loss": 1.1175,
      "step": 3118
    },
    {
      "epoch": 29.1508989095196,
      "grad_norm": 0.45360133051872253,
      "learning_rate": 1.6625211762854327e-05,
      "loss": 1.0765,
      "step": 3119
    },
    {
      "epoch": 29.160330091364575,
      "grad_norm": 0.4718356132507324,
      "learning_rate": 1.6622944199958536e-05,
      "loss": 1.0868,
      "step": 3120
    },
    {
      "epoch": 29.16976127320955,
      "grad_norm": 0.46705812215805054,
      "learning_rate": 1.6620676030270785e-05,
      "loss": 1.079,
      "step": 3121
    },
    {
      "epoch": 29.179192455054523,
      "grad_norm": 0.4181315302848816,
      "learning_rate": 1.661840725399889e-05,
      "loss": 1.1325,
      "step": 3122
    },
    {
      "epoch": 29.1886236368995,
      "grad_norm": 0.43009352684020996,
      "learning_rate": 1.6616137871350714e-05,
      "loss": 1.0498,
      "step": 3123
    },
    {
      "epoch": 29.198054818744474,
      "grad_norm": 0.4600887596607208,
      "learning_rate": 1.661386788253418e-05,
      "loss": 1.1168,
      "step": 3124
    },
    {
      "epoch": 29.207486000589448,
      "grad_norm": 0.4450315237045288,
      "learning_rate": 1.6611597287757255e-05,
      "loss": 1.0908,
      "step": 3125
    },
    {
      "epoch": 29.216917182434425,
      "grad_norm": 0.4880869686603546,
      "learning_rate": 1.660932608722798e-05,
      "loss": 1.1092,
      "step": 3126
    },
    {
      "epoch": 29.2263483642794,
      "grad_norm": 0.4310699999332428,
      "learning_rate": 1.6607054281154433e-05,
      "loss": 1.0886,
      "step": 3127
    },
    {
      "epoch": 29.235779546124373,
      "grad_norm": 0.4219316244125366,
      "learning_rate": 1.660478186974476e-05,
      "loss": 1.0802,
      "step": 3128
    },
    {
      "epoch": 29.245210727969347,
      "grad_norm": 0.4522745907306671,
      "learning_rate": 1.660250885320716e-05,
      "loss": 1.1359,
      "step": 3129
    },
    {
      "epoch": 29.254641909814325,
      "grad_norm": 0.41326677799224854,
      "learning_rate": 1.6600235231749885e-05,
      "loss": 1.0955,
      "step": 3130
    },
    {
      "epoch": 29.2640730916593,
      "grad_norm": 0.47404739260673523,
      "learning_rate": 1.659796100558124e-05,
      "loss": 1.0817,
      "step": 3131
    },
    {
      "epoch": 29.273504273504273,
      "grad_norm": 0.4281997084617615,
      "learning_rate": 1.6595686174909596e-05,
      "loss": 1.121,
      "step": 3132
    },
    {
      "epoch": 29.28293545534925,
      "grad_norm": 0.47532355785369873,
      "learning_rate": 1.6593410739943368e-05,
      "loss": 1.0532,
      "step": 3133
    },
    {
      "epoch": 29.292366637194224,
      "grad_norm": 0.4528166949748993,
      "learning_rate": 1.659113470089103e-05,
      "loss": 1.0772,
      "step": 3134
    },
    {
      "epoch": 29.301797819039198,
      "grad_norm": 0.4282262325286865,
      "learning_rate": 1.658885805796111e-05,
      "loss": 1.0685,
      "step": 3135
    },
    {
      "epoch": 29.311229000884172,
      "grad_norm": 0.48420974612236023,
      "learning_rate": 1.65865808113622e-05,
      "loss": 1.1024,
      "step": 3136
    },
    {
      "epoch": 29.32066018272915,
      "grad_norm": 0.45555469393730164,
      "learning_rate": 1.6584302961302935e-05,
      "loss": 1.111,
      "step": 3137
    },
    {
      "epoch": 29.330091364574123,
      "grad_norm": 0.47927325963974,
      "learning_rate": 1.6582024507992015e-05,
      "loss": 1.0992,
      "step": 3138
    },
    {
      "epoch": 29.339522546419097,
      "grad_norm": 0.460347980260849,
      "learning_rate": 1.6579745451638184e-05,
      "loss": 1.0671,
      "step": 3139
    },
    {
      "epoch": 29.348953728264075,
      "grad_norm": 0.46429628133773804,
      "learning_rate": 1.6577465792450252e-05,
      "loss": 1.0691,
      "step": 3140
    },
    {
      "epoch": 29.35838491010905,
      "grad_norm": 0.40898531675338745,
      "learning_rate": 1.6575185530637087e-05,
      "loss": 1.1064,
      "step": 3141
    },
    {
      "epoch": 29.367816091954023,
      "grad_norm": 0.4486446678638458,
      "learning_rate": 1.6572904666407602e-05,
      "loss": 1.0821,
      "step": 3142
    },
    {
      "epoch": 29.377247273798996,
      "grad_norm": 0.4678664207458496,
      "learning_rate": 1.657062319997076e-05,
      "loss": 1.0781,
      "step": 3143
    },
    {
      "epoch": 29.386678455643974,
      "grad_norm": 0.4544718861579895,
      "learning_rate": 1.6568341131535598e-05,
      "loss": 1.0921,
      "step": 3144
    },
    {
      "epoch": 29.396109637488948,
      "grad_norm": 0.47676485776901245,
      "learning_rate": 1.65660584613112e-05,
      "loss": 1.0643,
      "step": 3145
    },
    {
      "epoch": 29.405540819333922,
      "grad_norm": 0.46182650327682495,
      "learning_rate": 1.6563775189506695e-05,
      "loss": 1.0341,
      "step": 3146
    },
    {
      "epoch": 29.4149720011789,
      "grad_norm": 0.5168697834014893,
      "learning_rate": 1.656149131633128e-05,
      "loss": 1.0376,
      "step": 3147
    },
    {
      "epoch": 29.424403183023873,
      "grad_norm": 0.4486193060874939,
      "learning_rate": 1.6559206841994208e-05,
      "loss": 1.0645,
      "step": 3148
    },
    {
      "epoch": 29.433834364868847,
      "grad_norm": 0.4988548159599304,
      "learning_rate": 1.6556921766704776e-05,
      "loss": 1.0971,
      "step": 3149
    },
    {
      "epoch": 29.44326554671382,
      "grad_norm": 0.44306448101997375,
      "learning_rate": 1.6554636090672337e-05,
      "loss": 1.1107,
      "step": 3150
    },
    {
      "epoch": 29.4526967285588,
      "grad_norm": 0.48732322454452515,
      "learning_rate": 1.655234981410631e-05,
      "loss": 1.0832,
      "step": 3151
    },
    {
      "epoch": 29.462127910403773,
      "grad_norm": 0.44356921315193176,
      "learning_rate": 1.6550062937216168e-05,
      "loss": 1.0892,
      "step": 3152
    },
    {
      "epoch": 29.471559092248746,
      "grad_norm": 0.4825076460838318,
      "learning_rate": 1.6547775460211424e-05,
      "loss": 1.0751,
      "step": 3153
    },
    {
      "epoch": 29.480990274093724,
      "grad_norm": 0.4356217086315155,
      "learning_rate": 1.654548738330166e-05,
      "loss": 1.1168,
      "step": 3154
    },
    {
      "epoch": 29.490421455938698,
      "grad_norm": 0.4635337293148041,
      "learning_rate": 1.6543198706696507e-05,
      "loss": 1.1372,
      "step": 3155
    },
    {
      "epoch": 29.499852637783672,
      "grad_norm": 0.4871738851070404,
      "learning_rate": 1.654090943060566e-05,
      "loss": 1.0771,
      "step": 3156
    },
    {
      "epoch": 29.509283819628646,
      "grad_norm": 0.48230695724487305,
      "learning_rate": 1.6538619555238855e-05,
      "loss": 1.0777,
      "step": 3157
    },
    {
      "epoch": 29.518715001473623,
      "grad_norm": 0.4502311646938324,
      "learning_rate": 1.6536329080805885e-05,
      "loss": 1.0696,
      "step": 3158
    },
    {
      "epoch": 29.528146183318597,
      "grad_norm": 0.4398777484893799,
      "learning_rate": 1.6534038007516616e-05,
      "loss": 1.0788,
      "step": 3159
    },
    {
      "epoch": 29.53757736516357,
      "grad_norm": 0.44236475229263306,
      "learning_rate": 1.6531746335580944e-05,
      "loss": 1.0847,
      "step": 3160
    },
    {
      "epoch": 29.54700854700855,
      "grad_norm": 0.42770257592201233,
      "learning_rate": 1.6529454065208837e-05,
      "loss": 1.0984,
      "step": 3161
    },
    {
      "epoch": 29.556439728853523,
      "grad_norm": 0.4240141808986664,
      "learning_rate": 1.652716119661031e-05,
      "loss": 1.1005,
      "step": 3162
    },
    {
      "epoch": 29.565870910698496,
      "grad_norm": 0.45931875705718994,
      "learning_rate": 1.652486772999543e-05,
      "loss": 1.0732,
      "step": 3163
    },
    {
      "epoch": 29.57530209254347,
      "grad_norm": 0.4601695239543915,
      "learning_rate": 1.652257366557434e-05,
      "loss": 1.0486,
      "step": 3164
    },
    {
      "epoch": 29.584733274388448,
      "grad_norm": 0.4477720856666565,
      "learning_rate": 1.65202790035572e-05,
      "loss": 1.0642,
      "step": 3165
    },
    {
      "epoch": 29.594164456233422,
      "grad_norm": 0.4422167241573334,
      "learning_rate": 1.6517983744154257e-05,
      "loss": 1.0549,
      "step": 3166
    },
    {
      "epoch": 29.603595638078396,
      "grad_norm": 0.4718334972858429,
      "learning_rate": 1.6515687887575806e-05,
      "loss": 1.1138,
      "step": 3167
    },
    {
      "epoch": 29.613026819923373,
      "grad_norm": 0.48093727231025696,
      "learning_rate": 1.6513391434032183e-05,
      "loss": 1.0898,
      "step": 3168
    },
    {
      "epoch": 29.622458001768347,
      "grad_norm": 0.4739994406700134,
      "learning_rate": 1.6511094383733798e-05,
      "loss": 1.0957,
      "step": 3169
    },
    {
      "epoch": 29.63188918361332,
      "grad_norm": 0.4517276883125305,
      "learning_rate": 1.6508796736891098e-05,
      "loss": 1.101,
      "step": 3170
    },
    {
      "epoch": 29.641320365458295,
      "grad_norm": 0.48293131589889526,
      "learning_rate": 1.6506498493714597e-05,
      "loss": 1.0648,
      "step": 3171
    },
    {
      "epoch": 29.650751547303273,
      "grad_norm": 0.5107225179672241,
      "learning_rate": 1.6504199654414857e-05,
      "loss": 1.0566,
      "step": 3172
    },
    {
      "epoch": 29.660182729148246,
      "grad_norm": 0.49978628754615784,
      "learning_rate": 1.6501900219202496e-05,
      "loss": 1.0699,
      "step": 3173
    },
    {
      "epoch": 29.66961391099322,
      "grad_norm": 0.4384594261646271,
      "learning_rate": 1.649960018828819e-05,
      "loss": 1.0784,
      "step": 3174
    },
    {
      "epoch": 29.679045092838198,
      "grad_norm": 0.4808735251426697,
      "learning_rate": 1.6497299561882673e-05,
      "loss": 1.0911,
      "step": 3175
    },
    {
      "epoch": 29.688476274683172,
      "grad_norm": 0.4552823007106781,
      "learning_rate": 1.6494998340196714e-05,
      "loss": 1.1338,
      "step": 3176
    },
    {
      "epoch": 29.697907456528146,
      "grad_norm": 0.45339155197143555,
      "learning_rate": 1.6492696523441162e-05,
      "loss": 1.0431,
      "step": 3177
    },
    {
      "epoch": 29.70733863837312,
      "grad_norm": 0.44705817103385925,
      "learning_rate": 1.64903941118269e-05,
      "loss": 1.0609,
      "step": 3178
    },
    {
      "epoch": 29.716769820218097,
      "grad_norm": 0.46391037106513977,
      "learning_rate": 1.648809110556488e-05,
      "loss": 1.0747,
      "step": 3179
    },
    {
      "epoch": 29.72620100206307,
      "grad_norm": 0.48295411467552185,
      "learning_rate": 1.64857875048661e-05,
      "loss": 1.0832,
      "step": 3180
    },
    {
      "epoch": 29.735632183908045,
      "grad_norm": 0.42586830258369446,
      "learning_rate": 1.6483483309941616e-05,
      "loss": 1.0793,
      "step": 3181
    },
    {
      "epoch": 29.745063365753023,
      "grad_norm": 0.4589557647705078,
      "learning_rate": 1.648117852100254e-05,
      "loss": 1.0804,
      "step": 3182
    },
    {
      "epoch": 29.754494547597997,
      "grad_norm": 0.47577857971191406,
      "learning_rate": 1.647887313826003e-05,
      "loss": 1.1049,
      "step": 3183
    },
    {
      "epoch": 29.76392572944297,
      "grad_norm": 0.4826182723045349,
      "learning_rate": 1.6476567161925307e-05,
      "loss": 1.0463,
      "step": 3184
    },
    {
      "epoch": 29.773356911287944,
      "grad_norm": 0.43348410725593567,
      "learning_rate": 1.647426059220965e-05,
      "loss": 1.0903,
      "step": 3185
    },
    {
      "epoch": 29.782788093132922,
      "grad_norm": 0.4413244426250458,
      "learning_rate": 1.6471953429324378e-05,
      "loss": 1.0765,
      "step": 3186
    },
    {
      "epoch": 29.792219274977896,
      "grad_norm": 0.46245232224464417,
      "learning_rate": 1.6469645673480876e-05,
      "loss": 1.0635,
      "step": 3187
    },
    {
      "epoch": 29.80165045682287,
      "grad_norm": 0.4532487988471985,
      "learning_rate": 1.6467337324890578e-05,
      "loss": 1.0383,
      "step": 3188
    },
    {
      "epoch": 29.811081638667847,
      "grad_norm": 0.4442693591117859,
      "learning_rate": 1.6465028383764976e-05,
      "loss": 1.0793,
      "step": 3189
    },
    {
      "epoch": 29.82051282051282,
      "grad_norm": 0.47722381353378296,
      "learning_rate": 1.6462718850315617e-05,
      "loss": 1.0607,
      "step": 3190
    },
    {
      "epoch": 29.829944002357795,
      "grad_norm": 0.46702373027801514,
      "learning_rate": 1.646040872475409e-05,
      "loss": 1.0529,
      "step": 3191
    },
    {
      "epoch": 29.83937518420277,
      "grad_norm": 0.440105140209198,
      "learning_rate": 1.6458098007292058e-05,
      "loss": 1.1104,
      "step": 3192
    },
    {
      "epoch": 29.848806366047747,
      "grad_norm": 0.42538920044898987,
      "learning_rate": 1.6455786698141226e-05,
      "loss": 1.0832,
      "step": 3193
    },
    {
      "epoch": 29.85823754789272,
      "grad_norm": 0.45885103940963745,
      "learning_rate": 1.645347479751335e-05,
      "loss": 1.0983,
      "step": 3194
    },
    {
      "epoch": 29.867668729737694,
      "grad_norm": 0.440823495388031,
      "learning_rate": 1.6451162305620254e-05,
      "loss": 1.1255,
      "step": 3195
    },
    {
      "epoch": 29.877099911582672,
      "grad_norm": 0.45181798934936523,
      "learning_rate": 1.64488492226738e-05,
      "loss": 1.0885,
      "step": 3196
    },
    {
      "epoch": 29.886531093427646,
      "grad_norm": 0.4433861970901489,
      "learning_rate": 1.6446535548885917e-05,
      "loss": 1.1118,
      "step": 3197
    },
    {
      "epoch": 29.89596227527262,
      "grad_norm": 0.4498279392719269,
      "learning_rate": 1.644422128446858e-05,
      "loss": 1.0344,
      "step": 3198
    },
    {
      "epoch": 29.905393457117594,
      "grad_norm": 0.4857002794742584,
      "learning_rate": 1.6441906429633822e-05,
      "loss": 1.1295,
      "step": 3199
    },
    {
      "epoch": 29.91482463896257,
      "grad_norm": 0.4722009301185608,
      "learning_rate": 1.643959098459373e-05,
      "loss": 1.092,
      "step": 3200
    },
    {
      "epoch": 29.924255820807545,
      "grad_norm": 0.4482908248901367,
      "learning_rate": 1.6437274949560446e-05,
      "loss": 1.1076,
      "step": 3201
    },
    {
      "epoch": 29.93368700265252,
      "grad_norm": 0.5002665519714355,
      "learning_rate": 1.643495832474616e-05,
      "loss": 1.1045,
      "step": 3202
    },
    {
      "epoch": 29.943118184497497,
      "grad_norm": 0.4436805546283722,
      "learning_rate": 1.6432641110363122e-05,
      "loss": 1.0535,
      "step": 3203
    },
    {
      "epoch": 29.95254936634247,
      "grad_norm": 0.45425671339035034,
      "learning_rate": 1.6430323306623637e-05,
      "loss": 1.075,
      "step": 3204
    },
    {
      "epoch": 29.961980548187444,
      "grad_norm": 0.45899489521980286,
      "learning_rate": 1.6428004913740058e-05,
      "loss": 1.1063,
      "step": 3205
    },
    {
      "epoch": 29.97141173003242,
      "grad_norm": 0.4167885184288025,
      "learning_rate": 1.6425685931924798e-05,
      "loss": 1.0916,
      "step": 3206
    },
    {
      "epoch": 29.980842911877396,
      "grad_norm": 0.4659806191921234,
      "learning_rate": 1.6423366361390317e-05,
      "loss": 1.057,
      "step": 3207
    },
    {
      "epoch": 29.99027409372237,
      "grad_norm": 0.43557092547416687,
      "learning_rate": 1.642104620234914e-05,
      "loss": 1.058,
      "step": 3208
    },
    {
      "epoch": 29.999705275567344,
      "grad_norm": 0.4264104962348938,
      "learning_rate": 1.6418725455013835e-05,
      "loss": 1.0765,
      "step": 3209
    },
    {
      "epoch": 30.0,
      "grad_norm": 2.7096831798553467,
      "learning_rate": 1.6416404119597026e-05,
      "loss": 0.5692,
      "step": 3210
    },
    {
      "epoch": 30.009431181844974,
      "grad_norm": 0.44263339042663574,
      "learning_rate": 1.6414082196311402e-05,
      "loss": 1.0986,
      "step": 3211
    },
    {
      "epoch": 30.01886236368995,
      "grad_norm": 0.48544126749038696,
      "learning_rate": 1.6411759685369684e-05,
      "loss": 1.0915,
      "step": 3212
    },
    {
      "epoch": 30.028293545534925,
      "grad_norm": 0.484760046005249,
      "learning_rate": 1.6409436586984672e-05,
      "loss": 1.1164,
      "step": 3213
    },
    {
      "epoch": 30.0377247273799,
      "grad_norm": 0.44553622603416443,
      "learning_rate": 1.6407112901369196e-05,
      "loss": 1.0661,
      "step": 3214
    },
    {
      "epoch": 30.047155909224873,
      "grad_norm": 0.4445957541465759,
      "learning_rate": 1.640478862873616e-05,
      "loss": 1.0705,
      "step": 3215
    },
    {
      "epoch": 30.05658709106985,
      "grad_norm": 0.49762147665023804,
      "learning_rate": 1.640246376929851e-05,
      "loss": 1.0986,
      "step": 3216
    },
    {
      "epoch": 30.066018272914825,
      "grad_norm": 0.48344793915748596,
      "learning_rate": 1.640013832326925e-05,
      "loss": 1.0833,
      "step": 3217
    },
    {
      "epoch": 30.0754494547598,
      "grad_norm": 0.47034239768981934,
      "learning_rate": 1.6397812290861438e-05,
      "loss": 1.0922,
      "step": 3218
    },
    {
      "epoch": 30.084880636604776,
      "grad_norm": 0.47588610649108887,
      "learning_rate": 1.6395485672288175e-05,
      "loss": 1.0684,
      "step": 3219
    },
    {
      "epoch": 30.09431181844975,
      "grad_norm": 0.4477972388267517,
      "learning_rate": 1.6393158467762638e-05,
      "loss": 1.0906,
      "step": 3220
    },
    {
      "epoch": 30.103743000294724,
      "grad_norm": 0.49421370029449463,
      "learning_rate": 1.6390830677498037e-05,
      "loss": 1.0654,
      "step": 3221
    },
    {
      "epoch": 30.113174182139698,
      "grad_norm": 0.48107603192329407,
      "learning_rate": 1.6388502301707643e-05,
      "loss": 1.0434,
      "step": 3222
    },
    {
      "epoch": 30.122605363984675,
      "grad_norm": 0.44627711176872253,
      "learning_rate": 1.6386173340604784e-05,
      "loss": 1.0475,
      "step": 3223
    },
    {
      "epoch": 30.13203654582965,
      "grad_norm": 0.4792923927307129,
      "learning_rate": 1.638384379440284e-05,
      "loss": 1.0524,
      "step": 3224
    },
    {
      "epoch": 30.141467727674623,
      "grad_norm": 0.4442045986652374,
      "learning_rate": 1.638151366331524e-05,
      "loss": 1.0691,
      "step": 3225
    },
    {
      "epoch": 30.1508989095196,
      "grad_norm": 0.472593754529953,
      "learning_rate": 1.6379182947555468e-05,
      "loss": 1.0961,
      "step": 3226
    },
    {
      "epoch": 30.160330091364575,
      "grad_norm": 0.43192148208618164,
      "learning_rate": 1.637685164733707e-05,
      "loss": 1.101,
      "step": 3227
    },
    {
      "epoch": 30.16976127320955,
      "grad_norm": 0.48199254274368286,
      "learning_rate": 1.6374519762873633e-05,
      "loss": 1.0974,
      "step": 3228
    },
    {
      "epoch": 30.179192455054523,
      "grad_norm": 0.5103303790092468,
      "learning_rate": 1.6372187294378806e-05,
      "loss": 1.0639,
      "step": 3229
    },
    {
      "epoch": 30.1886236368995,
      "grad_norm": 0.47441765666007996,
      "learning_rate": 1.6369854242066287e-05,
      "loss": 1.0525,
      "step": 3230
    },
    {
      "epoch": 30.198054818744474,
      "grad_norm": 0.4134071171283722,
      "learning_rate": 1.6367520606149835e-05,
      "loss": 1.0907,
      "step": 3231
    },
    {
      "epoch": 30.207486000589448,
      "grad_norm": 0.5145639777183533,
      "learning_rate": 1.636518638684325e-05,
      "loss": 1.0617,
      "step": 3232
    },
    {
      "epoch": 30.216917182434425,
      "grad_norm": 0.4660274386405945,
      "learning_rate": 1.6362851584360395e-05,
      "loss": 1.0727,
      "step": 3233
    },
    {
      "epoch": 30.2263483642794,
      "grad_norm": 0.42622828483581543,
      "learning_rate": 1.636051619891519e-05,
      "loss": 1.083,
      "step": 3234
    },
    {
      "epoch": 30.235779546124373,
      "grad_norm": 0.46947601437568665,
      "learning_rate": 1.635818023072159e-05,
      "loss": 1.087,
      "step": 3235
    },
    {
      "epoch": 30.245210727969347,
      "grad_norm": 0.4493557810783386,
      "learning_rate": 1.6355843679993623e-05,
      "loss": 1.0951,
      "step": 3236
    },
    {
      "epoch": 30.254641909814325,
      "grad_norm": 0.46345680952072144,
      "learning_rate": 1.6353506546945364e-05,
      "loss": 1.1096,
      "step": 3237
    },
    {
      "epoch": 30.2640730916593,
      "grad_norm": 0.4617120921611786,
      "learning_rate": 1.635116883179094e-05,
      "loss": 1.058,
      "step": 3238
    },
    {
      "epoch": 30.273504273504273,
      "grad_norm": 0.44429099559783936,
      "learning_rate": 1.6348830534744528e-05,
      "loss": 1.0359,
      "step": 3239
    },
    {
      "epoch": 30.28293545534925,
      "grad_norm": 0.4586971402168274,
      "learning_rate": 1.6346491656020362e-05,
      "loss": 1.1133,
      "step": 3240
    },
    {
      "epoch": 30.292366637194224,
      "grad_norm": 0.4618646204471588,
      "learning_rate": 1.6344152195832737e-05,
      "loss": 1.0578,
      "step": 3241
    },
    {
      "epoch": 30.301797819039198,
      "grad_norm": 0.49518293142318726,
      "learning_rate": 1.6341812154395986e-05,
      "loss": 1.0379,
      "step": 3242
    },
    {
      "epoch": 30.311229000884172,
      "grad_norm": 0.515759289264679,
      "learning_rate": 1.6339471531924506e-05,
      "loss": 1.1002,
      "step": 3243
    },
    {
      "epoch": 30.32066018272915,
      "grad_norm": 0.443297415971756,
      "learning_rate": 1.633713032863274e-05,
      "loss": 1.0763,
      "step": 3244
    },
    {
      "epoch": 30.330091364574123,
      "grad_norm": 0.44773608446121216,
      "learning_rate": 1.633478854473519e-05,
      "loss": 1.1056,
      "step": 3245
    },
    {
      "epoch": 30.339522546419097,
      "grad_norm": 0.45916688442230225,
      "learning_rate": 1.6332446180446416e-05,
      "loss": 1.0781,
      "step": 3246
    },
    {
      "epoch": 30.348953728264075,
      "grad_norm": 0.4489615857601166,
      "learning_rate": 1.6330103235981014e-05,
      "loss": 1.0768,
      "step": 3247
    },
    {
      "epoch": 30.35838491010905,
      "grad_norm": 0.47991320490837097,
      "learning_rate": 1.6327759711553654e-05,
      "loss": 1.124,
      "step": 3248
    },
    {
      "epoch": 30.367816091954023,
      "grad_norm": 0.4401964843273163,
      "learning_rate": 1.632541560737904e-05,
      "loss": 1.0665,
      "step": 3249
    },
    {
      "epoch": 30.377247273798996,
      "grad_norm": 0.41774722933769226,
      "learning_rate": 1.6323070923671944e-05,
      "loss": 1.0943,
      "step": 3250
    },
    {
      "epoch": 30.386678455643974,
      "grad_norm": 0.49242424964904785,
      "learning_rate": 1.6320725660647182e-05,
      "loss": 1.1193,
      "step": 3251
    },
    {
      "epoch": 30.396109637488948,
      "grad_norm": 0.442611426115036,
      "learning_rate": 1.6318379818519628e-05,
      "loss": 1.0743,
      "step": 3252
    },
    {
      "epoch": 30.405540819333922,
      "grad_norm": 0.4478193521499634,
      "learning_rate": 1.6316033397504208e-05,
      "loss": 1.081,
      "step": 3253
    },
    {
      "epoch": 30.4149720011789,
      "grad_norm": 0.46905848383903503,
      "learning_rate": 1.63136863978159e-05,
      "loss": 1.0689,
      "step": 3254
    },
    {
      "epoch": 30.424403183023873,
      "grad_norm": 0.4204047620296478,
      "learning_rate": 1.6311338819669732e-05,
      "loss": 1.0879,
      "step": 3255
    },
    {
      "epoch": 30.433834364868847,
      "grad_norm": 0.44304215908050537,
      "learning_rate": 1.6308990663280793e-05,
      "loss": 1.0843,
      "step": 3256
    },
    {
      "epoch": 30.44326554671382,
      "grad_norm": 0.5002036690711975,
      "learning_rate": 1.6306641928864214e-05,
      "loss": 1.1132,
      "step": 3257
    },
    {
      "epoch": 30.4526967285588,
      "grad_norm": 0.4676521122455597,
      "learning_rate": 1.6304292616635195e-05,
      "loss": 1.1038,
      "step": 3258
    },
    {
      "epoch": 30.462127910403773,
      "grad_norm": 0.4735334813594818,
      "learning_rate": 1.630194272680897e-05,
      "loss": 1.0658,
      "step": 3259
    },
    {
      "epoch": 30.471559092248746,
      "grad_norm": 0.46059462428092957,
      "learning_rate": 1.629959225960084e-05,
      "loss": 1.0379,
      "step": 3260
    },
    {
      "epoch": 30.480990274093724,
      "grad_norm": 0.47889140248298645,
      "learning_rate": 1.629724121522615e-05,
      "loss": 1.062,
      "step": 3261
    },
    {
      "epoch": 30.490421455938698,
      "grad_norm": 0.46520331501960754,
      "learning_rate": 1.629488959390031e-05,
      "loss": 1.0784,
      "step": 3262
    },
    {
      "epoch": 30.499852637783672,
      "grad_norm": 0.45795685052871704,
      "learning_rate": 1.6292537395838762e-05,
      "loss": 1.1096,
      "step": 3263
    },
    {
      "epoch": 30.509283819628646,
      "grad_norm": 0.4278453588485718,
      "learning_rate": 1.6290184621257025e-05,
      "loss": 1.0705,
      "step": 3264
    },
    {
      "epoch": 30.518715001473623,
      "grad_norm": 0.42623984813690186,
      "learning_rate": 1.628783127037066e-05,
      "loss": 1.1093,
      "step": 3265
    },
    {
      "epoch": 30.528146183318597,
      "grad_norm": 0.47468194365501404,
      "learning_rate": 1.6285477343395267e-05,
      "loss": 1.0918,
      "step": 3266
    },
    {
      "epoch": 30.53757736516357,
      "grad_norm": 0.4943412244319916,
      "learning_rate": 1.6283122840546525e-05,
      "loss": 1.0689,
      "step": 3267
    },
    {
      "epoch": 30.54700854700855,
      "grad_norm": 0.46230554580688477,
      "learning_rate": 1.6280767762040148e-05,
      "loss": 1.0909,
      "step": 3268
    },
    {
      "epoch": 30.556439728853523,
      "grad_norm": 0.4600127339363098,
      "learning_rate": 1.6278412108091907e-05,
      "loss": 1.0964,
      "step": 3269
    },
    {
      "epoch": 30.565870910698496,
      "grad_norm": 0.49373674392700195,
      "learning_rate": 1.627605587891763e-05,
      "loss": 1.0594,
      "step": 3270
    },
    {
      "epoch": 30.57530209254347,
      "grad_norm": 0.4580509662628174,
      "learning_rate": 1.627369907473319e-05,
      "loss": 1.0966,
      "step": 3271
    },
    {
      "epoch": 30.584733274388448,
      "grad_norm": 0.4963715970516205,
      "learning_rate": 1.6271341695754516e-05,
      "loss": 1.09,
      "step": 3272
    },
    {
      "epoch": 30.594164456233422,
      "grad_norm": 0.4485931694507599,
      "learning_rate": 1.6268983742197593e-05,
      "loss": 1.0781,
      "step": 3273
    },
    {
      "epoch": 30.603595638078396,
      "grad_norm": 0.444000244140625,
      "learning_rate": 1.6266625214278454e-05,
      "loss": 1.0875,
      "step": 3274
    },
    {
      "epoch": 30.613026819923373,
      "grad_norm": 0.46560877561569214,
      "learning_rate": 1.626426611221319e-05,
      "loss": 1.0793,
      "step": 3275
    },
    {
      "epoch": 30.622458001768347,
      "grad_norm": 0.43073219060897827,
      "learning_rate": 1.6261906436217937e-05,
      "loss": 1.0658,
      "step": 3276
    },
    {
      "epoch": 30.63188918361332,
      "grad_norm": 0.4595804214477539,
      "learning_rate": 1.625954618650889e-05,
      "loss": 1.077,
      "step": 3277
    },
    {
      "epoch": 30.641320365458295,
      "grad_norm": 0.4464128315448761,
      "learning_rate": 1.6257185363302294e-05,
      "loss": 1.0909,
      "step": 3278
    },
    {
      "epoch": 30.650751547303273,
      "grad_norm": 0.4501131474971771,
      "learning_rate": 1.6254823966814446e-05,
      "loss": 1.0842,
      "step": 3279
    },
    {
      "epoch": 30.660182729148246,
      "grad_norm": 0.4698047339916229,
      "learning_rate": 1.6252461997261695e-05,
      "loss": 1.093,
      "step": 3280
    },
    {
      "epoch": 30.66961391099322,
      "grad_norm": 0.4445096254348755,
      "learning_rate": 1.6250099454860446e-05,
      "loss": 1.0656,
      "step": 3281
    },
    {
      "epoch": 30.679045092838198,
      "grad_norm": 0.4673663079738617,
      "learning_rate": 1.6247736339827157e-05,
      "loss": 1.1124,
      "step": 3282
    },
    {
      "epoch": 30.688476274683172,
      "grad_norm": 0.4321957528591156,
      "learning_rate": 1.624537265237833e-05,
      "loss": 1.0922,
      "step": 3283
    },
    {
      "epoch": 30.697907456528146,
      "grad_norm": 0.4482145607471466,
      "learning_rate": 1.624300839273053e-05,
      "loss": 1.0618,
      "step": 3284
    },
    {
      "epoch": 30.70733863837312,
      "grad_norm": 0.4963640868663788,
      "learning_rate": 1.6240643561100367e-05,
      "loss": 1.1128,
      "step": 3285
    },
    {
      "epoch": 30.716769820218097,
      "grad_norm": 0.4324198067188263,
      "learning_rate": 1.6238278157704507e-05,
      "loss": 1.083,
      "step": 3286
    },
    {
      "epoch": 30.72620100206307,
      "grad_norm": 0.4962151050567627,
      "learning_rate": 1.623591218275967e-05,
      "loss": 1.0669,
      "step": 3287
    },
    {
      "epoch": 30.735632183908045,
      "grad_norm": 0.48641976714134216,
      "learning_rate": 1.623354563648262e-05,
      "loss": 1.0696,
      "step": 3288
    },
    {
      "epoch": 30.745063365753023,
      "grad_norm": 0.4490368664264679,
      "learning_rate": 1.6231178519090184e-05,
      "loss": 1.1074,
      "step": 3289
    },
    {
      "epoch": 30.754494547597997,
      "grad_norm": 0.478863388299942,
      "learning_rate": 1.6228810830799232e-05,
      "loss": 1.0754,
      "step": 3290
    },
    {
      "epoch": 30.76392572944297,
      "grad_norm": 0.45588287711143494,
      "learning_rate": 1.6226442571826697e-05,
      "loss": 1.0714,
      "step": 3291
    },
    {
      "epoch": 30.773356911287944,
      "grad_norm": 0.47351914644241333,
      "learning_rate": 1.6224073742389554e-05,
      "loss": 1.1175,
      "step": 3292
    },
    {
      "epoch": 30.782788093132922,
      "grad_norm": 0.441441148519516,
      "learning_rate": 1.6221704342704833e-05,
      "loss": 1.0959,
      "step": 3293
    },
    {
      "epoch": 30.792219274977896,
      "grad_norm": 0.5120087265968323,
      "learning_rate": 1.6219334372989622e-05,
      "loss": 1.1246,
      "step": 3294
    },
    {
      "epoch": 30.80165045682287,
      "grad_norm": 0.46834802627563477,
      "learning_rate": 1.6216963833461056e-05,
      "loss": 1.0394,
      "step": 3295
    },
    {
      "epoch": 30.811081638667847,
      "grad_norm": 0.4555404782295227,
      "learning_rate": 1.621459272433632e-05,
      "loss": 1.0748,
      "step": 3296
    },
    {
      "epoch": 30.82051282051282,
      "grad_norm": 0.5002263784408569,
      "learning_rate": 1.6212221045832656e-05,
      "loss": 1.0548,
      "step": 3297
    },
    {
      "epoch": 30.829944002357795,
      "grad_norm": 0.4558747708797455,
      "learning_rate": 1.6209848798167355e-05,
      "loss": 1.0816,
      "step": 3298
    },
    {
      "epoch": 30.83937518420277,
      "grad_norm": 0.4295392632484436,
      "learning_rate": 1.6207475981557766e-05,
      "loss": 1.0924,
      "step": 3299
    },
    {
      "epoch": 30.848806366047747,
      "grad_norm": 0.4555697441101074,
      "learning_rate": 1.620510259622128e-05,
      "loss": 1.1054,
      "step": 3300
    },
    {
      "epoch": 30.85823754789272,
      "grad_norm": 0.4273414611816406,
      "learning_rate": 1.620272864237535e-05,
      "loss": 1.1128,
      "step": 3301
    },
    {
      "epoch": 30.867668729737694,
      "grad_norm": 0.4436822235584259,
      "learning_rate": 1.6200354120237478e-05,
      "loss": 1.1083,
      "step": 3302
    },
    {
      "epoch": 30.877099911582672,
      "grad_norm": 0.48393282294273376,
      "learning_rate": 1.619797903002521e-05,
      "loss": 1.0498,
      "step": 3303
    },
    {
      "epoch": 30.886531093427646,
      "grad_norm": 0.4811154007911682,
      "learning_rate": 1.6195603371956155e-05,
      "loss": 1.1151,
      "step": 3304
    },
    {
      "epoch": 30.89596227527262,
      "grad_norm": 0.44913139939308167,
      "learning_rate": 1.6193227146247973e-05,
      "loss": 1.1093,
      "step": 3305
    },
    {
      "epoch": 30.905393457117594,
      "grad_norm": 0.45872339606285095,
      "learning_rate": 1.6190850353118367e-05,
      "loss": 1.077,
      "step": 3306
    },
    {
      "epoch": 30.91482463896257,
      "grad_norm": 0.507793128490448,
      "learning_rate": 1.6188472992785104e-05,
      "loss": 1.051,
      "step": 3307
    },
    {
      "epoch": 30.924255820807545,
      "grad_norm": 0.47125497460365295,
      "learning_rate": 1.618609506546599e-05,
      "loss": 1.0657,
      "step": 3308
    },
    {
      "epoch": 30.93368700265252,
      "grad_norm": 0.4946422278881073,
      "learning_rate": 1.61837165713789e-05,
      "loss": 1.0785,
      "step": 3309
    },
    {
      "epoch": 30.943118184497497,
      "grad_norm": 0.4444851279258728,
      "learning_rate": 1.6181337510741743e-05,
      "loss": 1.0837,
      "step": 3310
    },
    {
      "epoch": 30.95254936634247,
      "grad_norm": 0.45735329389572144,
      "learning_rate": 1.6178957883772488e-05,
      "loss": 1.0544,
      "step": 3311
    },
    {
      "epoch": 30.961980548187444,
      "grad_norm": 0.4680930972099304,
      "learning_rate": 1.6176577690689158e-05,
      "loss": 1.0526,
      "step": 3312
    },
    {
      "epoch": 30.97141173003242,
      "grad_norm": 0.464251846075058,
      "learning_rate": 1.6174196931709826e-05,
      "loss": 1.0841,
      "step": 3313
    },
    {
      "epoch": 30.980842911877396,
      "grad_norm": 0.45963308215141296,
      "learning_rate": 1.6171815607052614e-05,
      "loss": 1.0917,
      "step": 3314
    },
    {
      "epoch": 30.99027409372237,
      "grad_norm": 0.44903674721717834,
      "learning_rate": 1.6169433716935695e-05,
      "loss": 1.0882,
      "step": 3315
    },
    {
      "epoch": 30.999705275567344,
      "grad_norm": 0.44237709045410156,
      "learning_rate": 1.6167051261577305e-05,
      "loss": 1.0714,
      "step": 3316
    },
    {
      "epoch": 31.0,
      "grad_norm": 1.916069746017456,
      "learning_rate": 1.6164668241195722e-05,
      "loss": 1.0131,
      "step": 3317
    },
    {
      "epoch": 31.009431181844974,
      "grad_norm": 0.4581182897090912,
      "learning_rate": 1.6162284656009276e-05,
      "loss": 1.1211,
      "step": 3318
    },
    {
      "epoch": 31.01886236368995,
      "grad_norm": 0.4590427875518799,
      "learning_rate": 1.6159900506236347e-05,
      "loss": 1.1002,
      "step": 3319
    },
    {
      "epoch": 31.028293545534925,
      "grad_norm": 0.48202499747276306,
      "learning_rate": 1.6157515792095373e-05,
      "loss": 1.1048,
      "step": 3320
    },
    {
      "epoch": 31.0377247273799,
      "grad_norm": 0.4862651824951172,
      "learning_rate": 1.6155130513804842e-05,
      "loss": 1.1121,
      "step": 3321
    },
    {
      "epoch": 31.047155909224873,
      "grad_norm": 0.458194762468338,
      "learning_rate": 1.615274467158329e-05,
      "loss": 1.0457,
      "step": 3322
    },
    {
      "epoch": 31.05658709106985,
      "grad_norm": 0.5049185752868652,
      "learning_rate": 1.6150358265649308e-05,
      "loss": 1.073,
      "step": 3323
    },
    {
      "epoch": 31.066018272914825,
      "grad_norm": 0.49795934557914734,
      "learning_rate": 1.614797129622154e-05,
      "loss": 1.058,
      "step": 3324
    },
    {
      "epoch": 31.0754494547598,
      "grad_norm": 0.46653005480766296,
      "learning_rate": 1.6145583763518674e-05,
      "loss": 1.0759,
      "step": 3325
    },
    {
      "epoch": 31.084880636604776,
      "grad_norm": 0.4755033254623413,
      "learning_rate": 1.614319566775946e-05,
      "loss": 1.0501,
      "step": 3326
    },
    {
      "epoch": 31.09431181844975,
      "grad_norm": 0.44505739212036133,
      "learning_rate": 1.614080700916269e-05,
      "loss": 1.0982,
      "step": 3327
    },
    {
      "epoch": 31.103743000294724,
      "grad_norm": 0.47311675548553467,
      "learning_rate": 1.613841778794722e-05,
      "loss": 1.1038,
      "step": 3328
    },
    {
      "epoch": 31.113174182139698,
      "grad_norm": 0.470315545797348,
      "learning_rate": 1.613602800433194e-05,
      "loss": 1.1285,
      "step": 3329
    },
    {
      "epoch": 31.122605363984675,
      "grad_norm": 0.46356889605522156,
      "learning_rate": 1.613363765853581e-05,
      "loss": 1.071,
      "step": 3330
    },
    {
      "epoch": 31.13203654582965,
      "grad_norm": 0.46246635913848877,
      "learning_rate": 1.6131246750777822e-05,
      "loss": 1.0655,
      "step": 3331
    },
    {
      "epoch": 31.141467727674623,
      "grad_norm": 0.4371543526649475,
      "learning_rate": 1.612885528127704e-05,
      "loss": 1.0738,
      "step": 3332
    },
    {
      "epoch": 31.1508989095196,
      "grad_norm": 0.4956558644771576,
      "learning_rate": 1.6126463250252568e-05,
      "loss": 1.068,
      "step": 3333
    },
    {
      "epoch": 31.160330091364575,
      "grad_norm": 0.4631519615650177,
      "learning_rate": 1.612407065792356e-05,
      "loss": 1.0825,
      "step": 3334
    },
    {
      "epoch": 31.16976127320955,
      "grad_norm": 0.4892805516719818,
      "learning_rate": 1.6121677504509224e-05,
      "loss": 1.0619,
      "step": 3335
    },
    {
      "epoch": 31.179192455054523,
      "grad_norm": 0.4402084946632385,
      "learning_rate": 1.6119283790228824e-05,
      "loss": 1.0884,
      "step": 3336
    },
    {
      "epoch": 31.1886236368995,
      "grad_norm": 0.4680144786834717,
      "learning_rate": 1.6116889515301663e-05,
      "loss": 1.0717,
      "step": 3337
    },
    {
      "epoch": 31.198054818744474,
      "grad_norm": 0.4487842917442322,
      "learning_rate": 1.6114494679947116e-05,
      "loss": 1.066,
      "step": 3338
    },
    {
      "epoch": 31.207486000589448,
      "grad_norm": 0.469472199678421,
      "learning_rate": 1.611209928438459e-05,
      "loss": 1.0888,
      "step": 3339
    },
    {
      "epoch": 31.216917182434425,
      "grad_norm": 0.47531142830848694,
      "learning_rate": 1.610970332883355e-05,
      "loss": 1.1043,
      "step": 3340
    },
    {
      "epoch": 31.2263483642794,
      "grad_norm": 0.4461580812931061,
      "learning_rate": 1.6107306813513513e-05,
      "loss": 1.0699,
      "step": 3341
    },
    {
      "epoch": 31.235779546124373,
      "grad_norm": 0.49071118235588074,
      "learning_rate": 1.6104909738644047e-05,
      "loss": 1.0753,
      "step": 3342
    },
    {
      "epoch": 31.245210727969347,
      "grad_norm": 0.5061225891113281,
      "learning_rate": 1.6102512104444773e-05,
      "loss": 1.1042,
      "step": 3343
    },
    {
      "epoch": 31.254641909814325,
      "grad_norm": 0.48573124408721924,
      "learning_rate": 1.610011391113536e-05,
      "loss": 1.0927,
      "step": 3344
    },
    {
      "epoch": 31.2640730916593,
      "grad_norm": 0.4293379783630371,
      "learning_rate": 1.6097715158935532e-05,
      "loss": 1.1063,
      "step": 3345
    },
    {
      "epoch": 31.273504273504273,
      "grad_norm": 0.4961744546890259,
      "learning_rate": 1.609531584806506e-05,
      "loss": 1.0279,
      "step": 3346
    },
    {
      "epoch": 31.28293545534925,
      "grad_norm": 0.4359990060329437,
      "learning_rate": 1.6092915978743763e-05,
      "loss": 1.0654,
      "step": 3347
    },
    {
      "epoch": 31.292366637194224,
      "grad_norm": 0.5171212553977966,
      "learning_rate": 1.6090515551191524e-05,
      "loss": 1.0424,
      "step": 3348
    },
    {
      "epoch": 31.301797819039198,
      "grad_norm": 0.4643031656742096,
      "learning_rate": 1.6088114565628264e-05,
      "loss": 1.0765,
      "step": 3349
    },
    {
      "epoch": 31.311229000884172,
      "grad_norm": 0.4574638903141022,
      "learning_rate": 1.608571302227397e-05,
      "loss": 1.0896,
      "step": 3350
    },
    {
      "epoch": 31.32066018272915,
      "grad_norm": 0.4698573350906372,
      "learning_rate": 1.608331092134866e-05,
      "loss": 1.0432,
      "step": 3351
    },
    {
      "epoch": 31.330091364574123,
      "grad_norm": 0.5252369046211243,
      "learning_rate": 1.6080908263072413e-05,
      "loss": 1.0792,
      "step": 3352
    },
    {
      "epoch": 31.339522546419097,
      "grad_norm": 0.47694090008735657,
      "learning_rate": 1.607850504766537e-05,
      "loss": 1.0985,
      "step": 3353
    },
    {
      "epoch": 31.348953728264075,
      "grad_norm": 0.46523863077163696,
      "learning_rate": 1.6076101275347707e-05,
      "loss": 1.1233,
      "step": 3354
    },
    {
      "epoch": 31.35838491010905,
      "grad_norm": 0.4518386125564575,
      "learning_rate": 1.6073696946339653e-05,
      "loss": 1.0741,
      "step": 3355
    },
    {
      "epoch": 31.367816091954023,
      "grad_norm": 0.48591724038124084,
      "learning_rate": 1.6071292060861497e-05,
      "loss": 1.0569,
      "step": 3356
    },
    {
      "epoch": 31.377247273798996,
      "grad_norm": 0.4499620795249939,
      "learning_rate": 1.606888661913357e-05,
      "loss": 1.0829,
      "step": 3357
    },
    {
      "epoch": 31.386678455643974,
      "grad_norm": 0.42328348755836487,
      "learning_rate": 1.6066480621376263e-05,
      "loss": 1.0952,
      "step": 3358
    },
    {
      "epoch": 31.396109637488948,
      "grad_norm": 0.4799610674381256,
      "learning_rate": 1.606407406781001e-05,
      "loss": 1.0736,
      "step": 3359
    },
    {
      "epoch": 31.405540819333922,
      "grad_norm": 0.486520379781723,
      "learning_rate": 1.6061666958655297e-05,
      "loss": 1.1079,
      "step": 3360
    },
    {
      "epoch": 31.4149720011789,
      "grad_norm": 0.49692702293395996,
      "learning_rate": 1.6059259294132664e-05,
      "loss": 0.9815,
      "step": 3361
    },
    {
      "epoch": 31.424403183023873,
      "grad_norm": 0.4671396315097809,
      "learning_rate": 1.60568510744627e-05,
      "loss": 1.1164,
      "step": 3362
    },
    {
      "epoch": 31.433834364868847,
      "grad_norm": 0.48478469252586365,
      "learning_rate": 1.6054442299866047e-05,
      "loss": 1.1051,
      "step": 3363
    },
    {
      "epoch": 31.44326554671382,
      "grad_norm": 0.47164595127105713,
      "learning_rate": 1.6052032970563394e-05,
      "loss": 1.1021,
      "step": 3364
    },
    {
      "epoch": 31.4526967285588,
      "grad_norm": 0.5012475848197937,
      "learning_rate": 1.6049623086775485e-05,
      "loss": 1.07,
      "step": 3365
    },
    {
      "epoch": 31.462127910403773,
      "grad_norm": 0.48614501953125,
      "learning_rate": 1.604721264872311e-05,
      "loss": 1.1,
      "step": 3366
    },
    {
      "epoch": 31.471559092248746,
      "grad_norm": 0.4379803538322449,
      "learning_rate": 1.6044801656627112e-05,
      "loss": 1.0959,
      "step": 3367
    },
    {
      "epoch": 31.480990274093724,
      "grad_norm": 0.4909341335296631,
      "learning_rate": 1.6042390110708392e-05,
      "loss": 1.0779,
      "step": 3368
    },
    {
      "epoch": 31.490421455938698,
      "grad_norm": 0.4386471211910248,
      "learning_rate": 1.603997801118789e-05,
      "loss": 1.072,
      "step": 3369
    },
    {
      "epoch": 31.499852637783672,
      "grad_norm": 0.45278438925743103,
      "learning_rate": 1.6037565358286603e-05,
      "loss": 1.0592,
      "step": 3370
    },
    {
      "epoch": 31.509283819628646,
      "grad_norm": 0.4951154887676239,
      "learning_rate": 1.6035152152225574e-05,
      "loss": 1.0828,
      "step": 3371
    },
    {
      "epoch": 31.518715001473623,
      "grad_norm": 0.4424459636211395,
      "learning_rate": 1.6032738393225905e-05,
      "loss": 1.0475,
      "step": 3372
    },
    {
      "epoch": 31.528146183318597,
      "grad_norm": 0.4579789340496063,
      "learning_rate": 1.603032408150874e-05,
      "loss": 1.0759,
      "step": 3373
    },
    {
      "epoch": 31.53757736516357,
      "grad_norm": 0.4673171043395996,
      "learning_rate": 1.6027909217295275e-05,
      "loss": 1.0895,
      "step": 3374
    },
    {
      "epoch": 31.54700854700855,
      "grad_norm": 0.455562561750412,
      "learning_rate": 1.602549380080677e-05,
      "loss": 1.0522,
      "step": 3375
    },
    {
      "epoch": 31.556439728853523,
      "grad_norm": 0.4295456111431122,
      "learning_rate": 1.6023077832264514e-05,
      "loss": 1.1268,
      "step": 3376
    },
    {
      "epoch": 31.565870910698496,
      "grad_norm": 0.4814838171005249,
      "learning_rate": 1.602066131188986e-05,
      "loss": 1.0669,
      "step": 3377
    },
    {
      "epoch": 31.57530209254347,
      "grad_norm": 0.47838857769966125,
      "learning_rate": 1.6018244239904212e-05,
      "loss": 1.0903,
      "step": 3378
    },
    {
      "epoch": 31.584733274388448,
      "grad_norm": 0.46164289116859436,
      "learning_rate": 1.601582661652902e-05,
      "loss": 1.0942,
      "step": 3379
    },
    {
      "epoch": 31.594164456233422,
      "grad_norm": 0.4414077401161194,
      "learning_rate": 1.6013408441985783e-05,
      "loss": 1.0705,
      "step": 3380
    },
    {
      "epoch": 31.603595638078396,
      "grad_norm": 0.4335918128490448,
      "learning_rate": 1.6010989716496056e-05,
      "loss": 1.0858,
      "step": 3381
    },
    {
      "epoch": 31.613026819923373,
      "grad_norm": 0.48720160126686096,
      "learning_rate": 1.600857044028144e-05,
      "loss": 1.0496,
      "step": 3382
    },
    {
      "epoch": 31.622458001768347,
      "grad_norm": 0.479326993227005,
      "learning_rate": 1.600615061356359e-05,
      "loss": 1.0617,
      "step": 3383
    },
    {
      "epoch": 31.63188918361332,
      "grad_norm": 0.4988589286804199,
      "learning_rate": 1.6003730236564213e-05,
      "loss": 1.0552,
      "step": 3384
    },
    {
      "epoch": 31.641320365458295,
      "grad_norm": 0.459608256816864,
      "learning_rate": 1.6001309309505056e-05,
      "loss": 1.0729,
      "step": 3385
    },
    {
      "epoch": 31.650751547303273,
      "grad_norm": 0.4609552025794983,
      "learning_rate": 1.5998887832607927e-05,
      "loss": 1.059,
      "step": 3386
    },
    {
      "epoch": 31.660182729148246,
      "grad_norm": 0.44449546933174133,
      "learning_rate": 1.5996465806094682e-05,
      "loss": 1.1194,
      "step": 3387
    },
    {
      "epoch": 31.66961391099322,
      "grad_norm": 0.45401787757873535,
      "learning_rate": 1.5994043230187225e-05,
      "loss": 1.0994,
      "step": 3388
    },
    {
      "epoch": 31.679045092838198,
      "grad_norm": 0.48946505784988403,
      "learning_rate": 1.5991620105107508e-05,
      "loss": 1.0101,
      "step": 3389
    },
    {
      "epoch": 31.688476274683172,
      "grad_norm": 0.47009941935539246,
      "learning_rate": 1.5989196431077542e-05,
      "loss": 1.0896,
      "step": 3390
    },
    {
      "epoch": 31.697907456528146,
      "grad_norm": 0.44809478521347046,
      "learning_rate": 1.5986772208319382e-05,
      "loss": 1.0161,
      "step": 3391
    },
    {
      "epoch": 31.70733863837312,
      "grad_norm": 0.4824470281600952,
      "learning_rate": 1.5984347437055137e-05,
      "loss": 1.0928,
      "step": 3392
    },
    {
      "epoch": 31.716769820218097,
      "grad_norm": 0.4788750112056732,
      "learning_rate": 1.5981922117506957e-05,
      "loss": 1.0785,
      "step": 3393
    },
    {
      "epoch": 31.72620100206307,
      "grad_norm": 0.5017619132995605,
      "learning_rate": 1.5979496249897058e-05,
      "loss": 1.1039,
      "step": 3394
    },
    {
      "epoch": 31.735632183908045,
      "grad_norm": 0.49247094988822937,
      "learning_rate": 1.5977069834447686e-05,
      "loss": 1.0825,
      "step": 3395
    },
    {
      "epoch": 31.745063365753023,
      "grad_norm": 0.4888968765735626,
      "learning_rate": 1.597464287138116e-05,
      "loss": 1.0889,
      "step": 3396
    },
    {
      "epoch": 31.754494547597997,
      "grad_norm": 0.4880641996860504,
      "learning_rate": 1.5972215360919828e-05,
      "loss": 1.0786,
      "step": 3397
    },
    {
      "epoch": 31.76392572944297,
      "grad_norm": 0.4418613612651825,
      "learning_rate": 1.59697873032861e-05,
      "loss": 1.1011,
      "step": 3398
    },
    {
      "epoch": 31.773356911287944,
      "grad_norm": 0.44869914650917053,
      "learning_rate": 1.5967358698702442e-05,
      "loss": 1.084,
      "step": 3399
    },
    {
      "epoch": 31.782788093132922,
      "grad_norm": 0.45368438959121704,
      "learning_rate": 1.5964929547391355e-05,
      "loss": 1.0817,
      "step": 3400
    },
    {
      "epoch": 31.792219274977896,
      "grad_norm": 0.45180559158325195,
      "learning_rate": 1.5962499849575394e-05,
      "loss": 1.1084,
      "step": 3401
    },
    {
      "epoch": 31.80165045682287,
      "grad_norm": 0.46591684222221375,
      "learning_rate": 1.596006960547717e-05,
      "loss": 1.1122,
      "step": 3402
    },
    {
      "epoch": 31.811081638667847,
      "grad_norm": 0.4686490297317505,
      "learning_rate": 1.5957638815319344e-05,
      "loss": 1.0515,
      "step": 3403
    },
    {
      "epoch": 31.82051282051282,
      "grad_norm": 0.45533427596092224,
      "learning_rate": 1.595520747932462e-05,
      "loss": 1.0984,
      "step": 3404
    },
    {
      "epoch": 31.829944002357795,
      "grad_norm": 0.49879661202430725,
      "learning_rate": 1.595277559771576e-05,
      "loss": 1.1409,
      "step": 3405
    },
    {
      "epoch": 31.83937518420277,
      "grad_norm": 0.43560901284217834,
      "learning_rate": 1.5950343170715574e-05,
      "loss": 1.0638,
      "step": 3406
    },
    {
      "epoch": 31.848806366047747,
      "grad_norm": 0.44191843271255493,
      "learning_rate": 1.5947910198546912e-05,
      "loss": 1.1115,
      "step": 3407
    },
    {
      "epoch": 31.85823754789272,
      "grad_norm": 0.46992990374565125,
      "learning_rate": 1.594547668143269e-05,
      "loss": 1.0984,
      "step": 3408
    },
    {
      "epoch": 31.867668729737694,
      "grad_norm": 0.45974549651145935,
      "learning_rate": 1.5943042619595857e-05,
      "loss": 1.1139,
      "step": 3409
    },
    {
      "epoch": 31.877099911582672,
      "grad_norm": 0.46540209650993347,
      "learning_rate": 1.5940608013259433e-05,
      "loss": 1.0672,
      "step": 3410
    },
    {
      "epoch": 31.886531093427646,
      "grad_norm": 0.463136225938797,
      "learning_rate": 1.5938172862646464e-05,
      "loss": 1.117,
      "step": 3411
    },
    {
      "epoch": 31.89596227527262,
      "grad_norm": 0.4207875728607178,
      "learning_rate": 1.5935737167980066e-05,
      "loss": 1.0849,
      "step": 3412
    },
    {
      "epoch": 31.905393457117594,
      "grad_norm": 0.4312000572681427,
      "learning_rate": 1.5933300929483394e-05,
      "loss": 1.0796,
      "step": 3413
    },
    {
      "epoch": 31.91482463896257,
      "grad_norm": 0.4486892521381378,
      "learning_rate": 1.5930864147379655e-05,
      "loss": 1.0534,
      "step": 3414
    },
    {
      "epoch": 31.924255820807545,
      "grad_norm": 0.45287805795669556,
      "learning_rate": 1.5928426821892107e-05,
      "loss": 1.0861,
      "step": 3415
    },
    {
      "epoch": 31.93368700265252,
      "grad_norm": 0.47930464148521423,
      "learning_rate": 1.5925988953244055e-05,
      "loss": 1.0812,
      "step": 3416
    },
    {
      "epoch": 31.943118184497497,
      "grad_norm": 0.44337907433509827,
      "learning_rate": 1.5923550541658858e-05,
      "loss": 1.0274,
      "step": 3417
    },
    {
      "epoch": 31.95254936634247,
      "grad_norm": 0.436722069978714,
      "learning_rate": 1.592111158735992e-05,
      "loss": 1.0886,
      "step": 3418
    },
    {
      "epoch": 31.961980548187444,
      "grad_norm": 0.48167353868484497,
      "learning_rate": 1.5918672090570696e-05,
      "loss": 1.0756,
      "step": 3419
    },
    {
      "epoch": 31.97141173003242,
      "grad_norm": 0.5319398045539856,
      "learning_rate": 1.59162320515147e-05,
      "loss": 1.0473,
      "step": 3420
    },
    {
      "epoch": 31.980842911877396,
      "grad_norm": 0.4688813388347626,
      "learning_rate": 1.5913791470415477e-05,
      "loss": 1.0715,
      "step": 3421
    },
    {
      "epoch": 31.99027409372237,
      "grad_norm": 0.4482418894767761,
      "learning_rate": 1.5911350347496644e-05,
      "loss": 1.0641,
      "step": 3422
    },
    {
      "epoch": 31.999705275567344,
      "grad_norm": 0.47665610909461975,
      "learning_rate": 1.5908908682981844e-05,
      "loss": 1.1201,
      "step": 3423
    },
    {
      "epoch": 32.0,
      "grad_norm": 2.2576684951782227,
      "learning_rate": 1.5906466477094785e-05,
      "loss": 0.8311,
      "step": 3424
    },
    {
      "epoch": 32.009431181844974,
      "grad_norm": 0.49203070998191833,
      "learning_rate": 1.5904023730059227e-05,
      "loss": 1.0819,
      "step": 3425
    },
    {
      "epoch": 32.01886236368995,
      "grad_norm": 0.4695935547351837,
      "learning_rate": 1.590158044209897e-05,
      "loss": 1.0807,
      "step": 3426
    },
    {
      "epoch": 32.02829354553492,
      "grad_norm": 0.44049322605133057,
      "learning_rate": 1.5899136613437862e-05,
      "loss": 1.1004,
      "step": 3427
    },
    {
      "epoch": 32.0377247273799,
      "grad_norm": 0.44917070865631104,
      "learning_rate": 1.5896692244299817e-05,
      "loss": 1.0839,
      "step": 3428
    },
    {
      "epoch": 32.04715590922488,
      "grad_norm": 0.44242778420448303,
      "learning_rate": 1.5894247334908773e-05,
      "loss": 1.0857,
      "step": 3429
    },
    {
      "epoch": 32.05658709106985,
      "grad_norm": 0.4771796464920044,
      "learning_rate": 1.5891801885488746e-05,
      "loss": 1.0691,
      "step": 3430
    },
    {
      "epoch": 32.066018272914825,
      "grad_norm": 0.4666415750980377,
      "learning_rate": 1.5889355896263777e-05,
      "loss": 1.1357,
      "step": 3431
    },
    {
      "epoch": 32.0754494547598,
      "grad_norm": 0.5068798661231995,
      "learning_rate": 1.5886909367457973e-05,
      "loss": 1.11,
      "step": 3432
    },
    {
      "epoch": 32.08488063660477,
      "grad_norm": 0.46160072088241577,
      "learning_rate": 1.5884462299295476e-05,
      "loss": 1.0569,
      "step": 3433
    },
    {
      "epoch": 32.09431181844975,
      "grad_norm": 0.4242953658103943,
      "learning_rate": 1.5882014692000496e-05,
      "loss": 1.0557,
      "step": 3434
    },
    {
      "epoch": 32.10374300029473,
      "grad_norm": 0.47617849707603455,
      "learning_rate": 1.5879566545797277e-05,
      "loss": 1.0627,
      "step": 3435
    },
    {
      "epoch": 32.1131741821397,
      "grad_norm": 0.4615783095359802,
      "learning_rate": 1.587711786091012e-05,
      "loss": 1.0866,
      "step": 3436
    },
    {
      "epoch": 32.122605363984675,
      "grad_norm": 0.4551524221897125,
      "learning_rate": 1.5874668637563368e-05,
      "loss": 1.0865,
      "step": 3437
    },
    {
      "epoch": 32.13203654582965,
      "grad_norm": 0.4894910454750061,
      "learning_rate": 1.587221887598142e-05,
      "loss": 1.0844,
      "step": 3438
    },
    {
      "epoch": 32.14146772767462,
      "grad_norm": 0.49947676062583923,
      "learning_rate": 1.586976857638872e-05,
      "loss": 1.0871,
      "step": 3439
    },
    {
      "epoch": 32.1508989095196,
      "grad_norm": 0.4954303503036499,
      "learning_rate": 1.586731773900977e-05,
      "loss": 1.0747,
      "step": 3440
    },
    {
      "epoch": 32.16033009136457,
      "grad_norm": 0.47732633352279663,
      "learning_rate": 1.5864866364069108e-05,
      "loss": 1.0834,
      "step": 3441
    },
    {
      "epoch": 32.16976127320955,
      "grad_norm": 0.46165943145751953,
      "learning_rate": 1.5862414451791335e-05,
      "loss": 1.108,
      "step": 3442
    },
    {
      "epoch": 32.179192455054526,
      "grad_norm": 0.4909716546535492,
      "learning_rate": 1.5859962002401088e-05,
      "loss": 1.0955,
      "step": 3443
    },
    {
      "epoch": 32.1886236368995,
      "grad_norm": 0.4449497163295746,
      "learning_rate": 1.5857509016123062e-05,
      "loss": 1.0682,
      "step": 3444
    },
    {
      "epoch": 32.198054818744474,
      "grad_norm": 0.4866197109222412,
      "learning_rate": 1.5855055493182e-05,
      "loss": 1.0648,
      "step": 3445
    },
    {
      "epoch": 32.20748600058945,
      "grad_norm": 0.46462133526802063,
      "learning_rate": 1.5852601433802693e-05,
      "loss": 1.0939,
      "step": 3446
    },
    {
      "epoch": 32.21691718243442,
      "grad_norm": 0.43330398201942444,
      "learning_rate": 1.585014683820998e-05,
      "loss": 1.1151,
      "step": 3447
    },
    {
      "epoch": 32.226348364279396,
      "grad_norm": 0.49220067262649536,
      "learning_rate": 1.5847691706628752e-05,
      "loss": 1.0796,
      "step": 3448
    },
    {
      "epoch": 32.23577954612438,
      "grad_norm": 0.471489280462265,
      "learning_rate": 1.584523603928394e-05,
      "loss": 1.0817,
      "step": 3449
    },
    {
      "epoch": 32.24521072796935,
      "grad_norm": 0.4135206639766693,
      "learning_rate": 1.5842779836400542e-05,
      "loss": 1.1075,
      "step": 3450
    },
    {
      "epoch": 32.254641909814325,
      "grad_norm": 0.46355777978897095,
      "learning_rate": 1.5840323098203588e-05,
      "loss": 1.0999,
      "step": 3451
    },
    {
      "epoch": 32.2640730916593,
      "grad_norm": 0.45577046275138855,
      "learning_rate": 1.5837865824918164e-05,
      "loss": 1.0612,
      "step": 3452
    },
    {
      "epoch": 32.27350427350427,
      "grad_norm": 0.4911426603794098,
      "learning_rate": 1.5835408016769412e-05,
      "loss": 1.1144,
      "step": 3453
    },
    {
      "epoch": 32.28293545534925,
      "grad_norm": 0.4770224690437317,
      "learning_rate": 1.5832949673982503e-05,
      "loss": 1.073,
      "step": 3454
    },
    {
      "epoch": 32.29236663719422,
      "grad_norm": 0.44977983832359314,
      "learning_rate": 1.5830490796782677e-05,
      "loss": 1.1041,
      "step": 3455
    },
    {
      "epoch": 32.3017978190392,
      "grad_norm": 0.47350582480430603,
      "learning_rate": 1.5828031385395217e-05,
      "loss": 1.087,
      "step": 3456
    },
    {
      "epoch": 32.311229000884175,
      "grad_norm": 0.47471848130226135,
      "learning_rate": 1.5825571440045448e-05,
      "loss": 1.1167,
      "step": 3457
    },
    {
      "epoch": 32.32066018272915,
      "grad_norm": 0.45665064454078674,
      "learning_rate": 1.5823110960958755e-05,
      "loss": 1.0788,
      "step": 3458
    },
    {
      "epoch": 32.33009136457412,
      "grad_norm": 0.48875147104263306,
      "learning_rate": 1.582064994836056e-05,
      "loss": 1.0164,
      "step": 3459
    },
    {
      "epoch": 32.3395225464191,
      "grad_norm": 0.41910520195961,
      "learning_rate": 1.5818188402476346e-05,
      "loss": 1.061,
      "step": 3460
    },
    {
      "epoch": 32.34895372826407,
      "grad_norm": 0.48426735401153564,
      "learning_rate": 1.5815726323531637e-05,
      "loss": 1.0896,
      "step": 3461
    },
    {
      "epoch": 32.358384910109045,
      "grad_norm": 0.4652785360813141,
      "learning_rate": 1.5813263711752006e-05,
      "loss": 1.0802,
      "step": 3462
    },
    {
      "epoch": 32.367816091954026,
      "grad_norm": 0.478287011384964,
      "learning_rate": 1.581080056736308e-05,
      "loss": 1.0579,
      "step": 3463
    },
    {
      "epoch": 32.377247273799,
      "grad_norm": 0.4835297763347626,
      "learning_rate": 1.5808336890590526e-05,
      "loss": 1.0984,
      "step": 3464
    },
    {
      "epoch": 32.386678455643974,
      "grad_norm": 0.4605141580104828,
      "learning_rate": 1.5805872681660075e-05,
      "loss": 1.0672,
      "step": 3465
    },
    {
      "epoch": 32.39610963748895,
      "grad_norm": 0.44385457038879395,
      "learning_rate": 1.580340794079749e-05,
      "loss": 1.0914,
      "step": 3466
    },
    {
      "epoch": 32.40554081933392,
      "grad_norm": 0.4847593605518341,
      "learning_rate": 1.5800942668228586e-05,
      "loss": 1.1046,
      "step": 3467
    },
    {
      "epoch": 32.414972001178896,
      "grad_norm": 0.4612632393836975,
      "learning_rate": 1.5798476864179238e-05,
      "loss": 1.0831,
      "step": 3468
    },
    {
      "epoch": 32.42440318302387,
      "grad_norm": 0.5101603269577026,
      "learning_rate": 1.579601052887536e-05,
      "loss": 1.0819,
      "step": 3469
    },
    {
      "epoch": 32.43383436486885,
      "grad_norm": 0.45827215909957886,
      "learning_rate": 1.5793543662542915e-05,
      "loss": 1.0916,
      "step": 3470
    },
    {
      "epoch": 32.443265546713825,
      "grad_norm": 0.4573678970336914,
      "learning_rate": 1.5791076265407918e-05,
      "loss": 1.09,
      "step": 3471
    },
    {
      "epoch": 32.4526967285588,
      "grad_norm": 0.4793481230735779,
      "learning_rate": 1.578860833769643e-05,
      "loss": 1.0674,
      "step": 3472
    },
    {
      "epoch": 32.46212791040377,
      "grad_norm": 0.5221004486083984,
      "learning_rate": 1.5786139879634562e-05,
      "loss": 1.058,
      "step": 3473
    },
    {
      "epoch": 32.47155909224875,
      "grad_norm": 0.4813416004180908,
      "learning_rate": 1.5783670891448476e-05,
      "loss": 1.0392,
      "step": 3474
    },
    {
      "epoch": 32.48099027409372,
      "grad_norm": 0.47854799032211304,
      "learning_rate": 1.5781201373364378e-05,
      "loss": 1.0753,
      "step": 3475
    },
    {
      "epoch": 32.490421455938694,
      "grad_norm": 0.46710193157196045,
      "learning_rate": 1.577873132560852e-05,
      "loss": 1.0465,
      "step": 3476
    },
    {
      "epoch": 32.499852637783675,
      "grad_norm": 0.4653666019439697,
      "learning_rate": 1.5776260748407217e-05,
      "loss": 1.0786,
      "step": 3477
    },
    {
      "epoch": 32.50928381962865,
      "grad_norm": 0.4454728364944458,
      "learning_rate": 1.577378964198681e-05,
      "loss": 1.0896,
      "step": 3478
    },
    {
      "epoch": 32.51871500147362,
      "grad_norm": 0.4793749451637268,
      "learning_rate": 1.5771318006573713e-05,
      "loss": 1.0831,
      "step": 3479
    },
    {
      "epoch": 32.5281461833186,
      "grad_norm": 0.46069708466529846,
      "learning_rate": 1.5768845842394368e-05,
      "loss": 1.1021,
      "step": 3480
    },
    {
      "epoch": 32.53757736516357,
      "grad_norm": 0.424671471118927,
      "learning_rate": 1.5766373149675277e-05,
      "loss": 1.0952,
      "step": 3481
    },
    {
      "epoch": 32.547008547008545,
      "grad_norm": 0.4737602174282074,
      "learning_rate": 1.5763899928642987e-05,
      "loss": 1.0534,
      "step": 3482
    },
    {
      "epoch": 32.55643972885352,
      "grad_norm": 0.4463494122028351,
      "learning_rate": 1.5761426179524092e-05,
      "loss": 1.0696,
      "step": 3483
    },
    {
      "epoch": 32.5658709106985,
      "grad_norm": 0.47311311960220337,
      "learning_rate": 1.5758951902545242e-05,
      "loss": 1.0488,
      "step": 3484
    },
    {
      "epoch": 32.575302092543474,
      "grad_norm": 0.48614224791526794,
      "learning_rate": 1.5756477097933124e-05,
      "loss": 1.0444,
      "step": 3485
    },
    {
      "epoch": 32.58473327438845,
      "grad_norm": 0.4438227415084839,
      "learning_rate": 1.575400176591448e-05,
      "loss": 1.0729,
      "step": 3486
    },
    {
      "epoch": 32.59416445623342,
      "grad_norm": 0.4789793789386749,
      "learning_rate": 1.5751525906716097e-05,
      "loss": 1.0736,
      "step": 3487
    },
    {
      "epoch": 32.603595638078396,
      "grad_norm": 0.471868097782135,
      "learning_rate": 1.5749049520564815e-05,
      "loss": 1.0835,
      "step": 3488
    },
    {
      "epoch": 32.61302681992337,
      "grad_norm": 0.475173681974411,
      "learning_rate": 1.5746572607687518e-05,
      "loss": 1.1011,
      "step": 3489
    },
    {
      "epoch": 32.622458001768344,
      "grad_norm": 0.4907746911048889,
      "learning_rate": 1.5744095168311142e-05,
      "loss": 1.0658,
      "step": 3490
    },
    {
      "epoch": 32.631889183613325,
      "grad_norm": 0.4841824173927307,
      "learning_rate": 1.574161720266267e-05,
      "loss": 1.0565,
      "step": 3491
    },
    {
      "epoch": 32.6413203654583,
      "grad_norm": 0.4451293349266052,
      "learning_rate": 1.573913871096913e-05,
      "loss": 1.1045,
      "step": 3492
    },
    {
      "epoch": 32.65075154730327,
      "grad_norm": 0.4744994342327118,
      "learning_rate": 1.5736659693457603e-05,
      "loss": 1.0868,
      "step": 3493
    },
    {
      "epoch": 32.66018272914825,
      "grad_norm": 0.4486106038093567,
      "learning_rate": 1.573418015035521e-05,
      "loss": 1.0749,
      "step": 3494
    },
    {
      "epoch": 32.66961391099322,
      "grad_norm": 0.48668500781059265,
      "learning_rate": 1.5731700081889133e-05,
      "loss": 1.099,
      "step": 3495
    },
    {
      "epoch": 32.679045092838194,
      "grad_norm": 0.4952234625816345,
      "learning_rate": 1.572921948828659e-05,
      "loss": 1.0366,
      "step": 3496
    },
    {
      "epoch": 32.68847627468317,
      "grad_norm": 0.4689553678035736,
      "learning_rate": 1.5726738369774856e-05,
      "loss": 1.0721,
      "step": 3497
    },
    {
      "epoch": 32.69790745652815,
      "grad_norm": 0.48311418294906616,
      "learning_rate": 1.5724256726581248e-05,
      "loss": 1.0707,
      "step": 3498
    },
    {
      "epoch": 32.70733863837312,
      "grad_norm": 0.45161446928977966,
      "learning_rate": 1.5721774558933133e-05,
      "loss": 1.0865,
      "step": 3499
    },
    {
      "epoch": 32.7167698202181,
      "grad_norm": 0.446672648191452,
      "learning_rate": 1.571929186705793e-05,
      "loss": 1.104,
      "step": 3500
    },
    {
      "epoch": 32.72620100206307,
      "grad_norm": 0.471388578414917,
      "learning_rate": 1.5716808651183097e-05,
      "loss": 1.0842,
      "step": 3501
    },
    {
      "epoch": 32.735632183908045,
      "grad_norm": 0.4700999855995178,
      "learning_rate": 1.5714324911536148e-05,
      "loss": 1.0838,
      "step": 3502
    },
    {
      "epoch": 32.74506336575302,
      "grad_norm": 0.46813055872917175,
      "learning_rate": 1.5711840648344643e-05,
      "loss": 1.0888,
      "step": 3503
    },
    {
      "epoch": 32.75449454759799,
      "grad_norm": 0.4726813733577728,
      "learning_rate": 1.570935586183619e-05,
      "loss": 1.0507,
      "step": 3504
    },
    {
      "epoch": 32.763925729442974,
      "grad_norm": 0.46514856815338135,
      "learning_rate": 1.570687055223844e-05,
      "loss": 1.0598,
      "step": 3505
    },
    {
      "epoch": 32.77335691128795,
      "grad_norm": 0.4384719133377075,
      "learning_rate": 1.57043847197791e-05,
      "loss": 1.0706,
      "step": 3506
    },
    {
      "epoch": 32.78278809313292,
      "grad_norm": 0.4939793348312378,
      "learning_rate": 1.5701898364685923e-05,
      "loss": 1.0924,
      "step": 3507
    },
    {
      "epoch": 32.792219274977896,
      "grad_norm": 0.4276270866394043,
      "learning_rate": 1.56994114871867e-05,
      "loss": 1.0876,
      "step": 3508
    },
    {
      "epoch": 32.80165045682287,
      "grad_norm": 0.47059378027915955,
      "learning_rate": 1.569692408750929e-05,
      "loss": 1.0888,
      "step": 3509
    },
    {
      "epoch": 32.811081638667844,
      "grad_norm": 0.4464973211288452,
      "learning_rate": 1.5694436165881576e-05,
      "loss": 1.1006,
      "step": 3510
    },
    {
      "epoch": 32.82051282051282,
      "grad_norm": 0.4405953288078308,
      "learning_rate": 1.5691947722531508e-05,
      "loss": 1.0919,
      "step": 3511
    },
    {
      "epoch": 32.8299440023578,
      "grad_norm": 0.4785563349723816,
      "learning_rate": 1.5689458757687076e-05,
      "loss": 1.0649,
      "step": 3512
    },
    {
      "epoch": 32.83937518420277,
      "grad_norm": 0.47798240184783936,
      "learning_rate": 1.5686969271576313e-05,
      "loss": 1.0503,
      "step": 3513
    },
    {
      "epoch": 32.84880636604775,
      "grad_norm": 0.45868363976478577,
      "learning_rate": 1.568447926442731e-05,
      "loss": 1.1039,
      "step": 3514
    },
    {
      "epoch": 32.85823754789272,
      "grad_norm": 0.43372198939323425,
      "learning_rate": 1.5681988736468196e-05,
      "loss": 1.0498,
      "step": 3515
    },
    {
      "epoch": 32.867668729737694,
      "grad_norm": 0.46888765692710876,
      "learning_rate": 1.567949768792716e-05,
      "loss": 1.0828,
      "step": 3516
    },
    {
      "epoch": 32.87709991158267,
      "grad_norm": 0.45426440238952637,
      "learning_rate": 1.567700611903242e-05,
      "loss": 1.0483,
      "step": 3517
    },
    {
      "epoch": 32.88653109342764,
      "grad_norm": 0.5269008278846741,
      "learning_rate": 1.5674514030012263e-05,
      "loss": 1.0775,
      "step": 3518
    },
    {
      "epoch": 32.89596227527262,
      "grad_norm": 0.46005046367645264,
      "learning_rate": 1.567202142109501e-05,
      "loss": 1.1021,
      "step": 3519
    },
    {
      "epoch": 32.9053934571176,
      "grad_norm": 0.5060405731201172,
      "learning_rate": 1.5669528292509028e-05,
      "loss": 1.0404,
      "step": 3520
    },
    {
      "epoch": 32.91482463896257,
      "grad_norm": 0.4675205647945404,
      "learning_rate": 1.5667034644482742e-05,
      "loss": 1.0471,
      "step": 3521
    },
    {
      "epoch": 32.924255820807545,
      "grad_norm": 0.44147858023643494,
      "learning_rate": 1.5664540477244622e-05,
      "loss": 1.0489,
      "step": 3522
    },
    {
      "epoch": 32.93368700265252,
      "grad_norm": 0.5065305829048157,
      "learning_rate": 1.566204579102317e-05,
      "loss": 1.0592,
      "step": 3523
    },
    {
      "epoch": 32.94311818449749,
      "grad_norm": 0.4607848823070526,
      "learning_rate": 1.5659550586046966e-05,
      "loss": 1.1084,
      "step": 3524
    },
    {
      "epoch": 32.95254936634247,
      "grad_norm": 0.44768181443214417,
      "learning_rate": 1.5657054862544608e-05,
      "loss": 1.0671,
      "step": 3525
    },
    {
      "epoch": 32.96198054818745,
      "grad_norm": 0.4480050206184387,
      "learning_rate": 1.5654558620744754e-05,
      "loss": 1.0888,
      "step": 3526
    },
    {
      "epoch": 32.97141173003242,
      "grad_norm": 0.42596906423568726,
      "learning_rate": 1.5652061860876114e-05,
      "loss": 1.0858,
      "step": 3527
    },
    {
      "epoch": 32.980842911877396,
      "grad_norm": 0.445827454328537,
      "learning_rate": 1.5649564583167434e-05,
      "loss": 1.0816,
      "step": 3528
    },
    {
      "epoch": 32.99027409372237,
      "grad_norm": 0.4539062976837158,
      "learning_rate": 1.564706678784752e-05,
      "loss": 1.139,
      "step": 3529
    },
    {
      "epoch": 32.999705275567344,
      "grad_norm": 0.49940308928489685,
      "learning_rate": 1.5644568475145217e-05,
      "loss": 1.0807,
      "step": 3530
    },
    {
      "epoch": 33.0,
      "grad_norm": 1.9886043071746826,
      "learning_rate": 1.5642069645289417e-05,
      "loss": 0.5164,
      "step": 3531
    },
    {
      "epoch": 33.009431181844974,
      "grad_norm": 0.4902006983757019,
      "learning_rate": 1.5639570298509067e-05,
      "loss": 1.1078,
      "step": 3532
    },
    {
      "epoch": 33.01886236368995,
      "grad_norm": 0.4641540050506592,
      "learning_rate": 1.563707043503315e-05,
      "loss": 1.0649,
      "step": 3533
    },
    {
      "epoch": 33.02829354553492,
      "grad_norm": 0.47610726952552795,
      "learning_rate": 1.5634570055090707e-05,
      "loss": 1.0944,
      "step": 3534
    },
    {
      "epoch": 33.0377247273799,
      "grad_norm": 0.4641781151294708,
      "learning_rate": 1.5632069158910822e-05,
      "loss": 1.0791,
      "step": 3535
    },
    {
      "epoch": 33.04715590922488,
      "grad_norm": 0.47232693433761597,
      "learning_rate": 1.5629567746722623e-05,
      "loss": 1.0781,
      "step": 3536
    },
    {
      "epoch": 33.05658709106985,
      "grad_norm": 0.49643757939338684,
      "learning_rate": 1.5627065818755294e-05,
      "loss": 1.1023,
      "step": 3537
    },
    {
      "epoch": 33.066018272914825,
      "grad_norm": 0.4774211347103119,
      "learning_rate": 1.5624563375238057e-05,
      "loss": 1.0859,
      "step": 3538
    },
    {
      "epoch": 33.0754494547598,
      "grad_norm": 0.46939459443092346,
      "learning_rate": 1.5622060416400182e-05,
      "loss": 1.0317,
      "step": 3539
    },
    {
      "epoch": 33.08488063660477,
      "grad_norm": 0.4709014296531677,
      "learning_rate": 1.5619556942470997e-05,
      "loss": 1.0789,
      "step": 3540
    },
    {
      "epoch": 33.09431181844975,
      "grad_norm": 0.46721309423446655,
      "learning_rate": 1.5617052953679862e-05,
      "loss": 1.0805,
      "step": 3541
    },
    {
      "epoch": 33.10374300029473,
      "grad_norm": 0.4691133201122284,
      "learning_rate": 1.5614548450256198e-05,
      "loss": 1.0717,
      "step": 3542
    },
    {
      "epoch": 33.1131741821397,
      "grad_norm": 0.5212360620498657,
      "learning_rate": 1.561204343242946e-05,
      "loss": 1.0356,
      "step": 3543
    },
    {
      "epoch": 33.122605363984675,
      "grad_norm": 0.4993792176246643,
      "learning_rate": 1.5609537900429164e-05,
      "loss": 1.0816,
      "step": 3544
    },
    {
      "epoch": 33.13203654582965,
      "grad_norm": 0.48278915882110596,
      "learning_rate": 1.560703185448486e-05,
      "loss": 1.1014,
      "step": 3545
    },
    {
      "epoch": 33.14146772767462,
      "grad_norm": 0.49042510986328125,
      "learning_rate": 1.5604525294826153e-05,
      "loss": 1.087,
      "step": 3546
    },
    {
      "epoch": 33.1508989095196,
      "grad_norm": 0.491614431142807,
      "learning_rate": 1.5602018221682698e-05,
      "loss": 1.0859,
      "step": 3547
    },
    {
      "epoch": 33.16033009136457,
      "grad_norm": 0.47943270206451416,
      "learning_rate": 1.5599510635284186e-05,
      "loss": 1.0839,
      "step": 3548
    },
    {
      "epoch": 33.16976127320955,
      "grad_norm": 0.4531792998313904,
      "learning_rate": 1.5597002535860366e-05,
      "loss": 1.085,
      "step": 3549
    },
    {
      "epoch": 33.179192455054526,
      "grad_norm": 0.46126511693000793,
      "learning_rate": 1.559449392364102e-05,
      "loss": 1.0832,
      "step": 3550
    },
    {
      "epoch": 33.1886236368995,
      "grad_norm": 0.4970414340496063,
      "learning_rate": 1.5591984798856e-05,
      "loss": 1.0324,
      "step": 3551
    },
    {
      "epoch": 33.198054818744474,
      "grad_norm": 0.44165462255477905,
      "learning_rate": 1.558947516173518e-05,
      "loss": 1.0887,
      "step": 3552
    },
    {
      "epoch": 33.20748600058945,
      "grad_norm": 0.4527851939201355,
      "learning_rate": 1.5586965012508497e-05,
      "loss": 1.0882,
      "step": 3553
    },
    {
      "epoch": 33.21691718243442,
      "grad_norm": 0.44892674684524536,
      "learning_rate": 1.558445435140593e-05,
      "loss": 1.0676,
      "step": 3554
    },
    {
      "epoch": 33.226348364279396,
      "grad_norm": 0.46032723784446716,
      "learning_rate": 1.5581943178657502e-05,
      "loss": 1.061,
      "step": 3555
    },
    {
      "epoch": 33.23577954612438,
      "grad_norm": 0.4755934774875641,
      "learning_rate": 1.5579431494493288e-05,
      "loss": 1.0234,
      "step": 3556
    },
    {
      "epoch": 33.24521072796935,
      "grad_norm": 0.5083066821098328,
      "learning_rate": 1.5576919299143413e-05,
      "loss": 1.0895,
      "step": 3557
    },
    {
      "epoch": 33.254641909814325,
      "grad_norm": 0.4679918885231018,
      "learning_rate": 1.5574406592838032e-05,
      "loss": 1.0892,
      "step": 3558
    },
    {
      "epoch": 33.2640730916593,
      "grad_norm": 0.45119866728782654,
      "learning_rate": 1.5571893375807367e-05,
      "loss": 1.1171,
      "step": 3559
    },
    {
      "epoch": 33.27350427350427,
      "grad_norm": 0.48832687735557556,
      "learning_rate": 1.5569379648281673e-05,
      "loss": 1.0783,
      "step": 3560
    },
    {
      "epoch": 33.28293545534925,
      "grad_norm": 0.45377686619758606,
      "learning_rate": 1.556686541049126e-05,
      "loss": 1.0498,
      "step": 3561
    },
    {
      "epoch": 33.29236663719422,
      "grad_norm": 0.4963962137699127,
      "learning_rate": 1.556435066266648e-05,
      "loss": 1.0555,
      "step": 3562
    },
    {
      "epoch": 33.3017978190392,
      "grad_norm": 0.4827521741390228,
      "learning_rate": 1.556183540503774e-05,
      "loss": 1.1005,
      "step": 3563
    },
    {
      "epoch": 33.311229000884175,
      "grad_norm": 0.4774227738380432,
      "learning_rate": 1.5559319637835478e-05,
      "loss": 1.0762,
      "step": 3564
    },
    {
      "epoch": 33.32066018272915,
      "grad_norm": 0.4511219561100006,
      "learning_rate": 1.5556803361290192e-05,
      "loss": 1.0957,
      "step": 3565
    },
    {
      "epoch": 33.33009136457412,
      "grad_norm": 0.44422847032546997,
      "learning_rate": 1.5554286575632423e-05,
      "loss": 1.0602,
      "step": 3566
    },
    {
      "epoch": 33.3395225464191,
      "grad_norm": 0.46905818581581116,
      "learning_rate": 1.5551769281092753e-05,
      "loss": 1.0596,
      "step": 3567
    },
    {
      "epoch": 33.34895372826407,
      "grad_norm": 0.47382092475891113,
      "learning_rate": 1.5549251477901823e-05,
      "loss": 1.0749,
      "step": 3568
    },
    {
      "epoch": 33.358384910109045,
      "grad_norm": 0.48597052693367004,
      "learning_rate": 1.5546733166290306e-05,
      "loss": 1.0929,
      "step": 3569
    },
    {
      "epoch": 33.367816091954026,
      "grad_norm": 0.47520923614501953,
      "learning_rate": 1.5544214346488935e-05,
      "loss": 1.0651,
      "step": 3570
    },
    {
      "epoch": 33.377247273799,
      "grad_norm": 0.4747275710105896,
      "learning_rate": 1.554169501872848e-05,
      "loss": 1.0442,
      "step": 3571
    },
    {
      "epoch": 33.386678455643974,
      "grad_norm": 0.4662565588951111,
      "learning_rate": 1.5539175183239763e-05,
      "loss": 1.0596,
      "step": 3572
    },
    {
      "epoch": 33.39610963748895,
      "grad_norm": 0.4761597216129303,
      "learning_rate": 1.553665484025365e-05,
      "loss": 1.0858,
      "step": 3573
    },
    {
      "epoch": 33.40554081933392,
      "grad_norm": 0.48040226101875305,
      "learning_rate": 1.553413399000105e-05,
      "loss": 1.0434,
      "step": 3574
    },
    {
      "epoch": 33.414972001178896,
      "grad_norm": 0.4681159257888794,
      "learning_rate": 1.5531612632712932e-05,
      "loss": 1.0615,
      "step": 3575
    },
    {
      "epoch": 33.42440318302387,
      "grad_norm": 0.49489763379096985,
      "learning_rate": 1.552909076862029e-05,
      "loss": 1.0546,
      "step": 3576
    },
    {
      "epoch": 33.43383436486885,
      "grad_norm": 0.4605233669281006,
      "learning_rate": 1.5526568397954185e-05,
      "loss": 1.1027,
      "step": 3577
    },
    {
      "epoch": 33.443265546713825,
      "grad_norm": 0.4859512746334076,
      "learning_rate": 1.5524045520945712e-05,
      "loss": 1.11,
      "step": 3578
    },
    {
      "epoch": 33.4526967285588,
      "grad_norm": 0.49479976296424866,
      "learning_rate": 1.552152213782602e-05,
      "loss": 1.0888,
      "step": 3579
    },
    {
      "epoch": 33.46212791040377,
      "grad_norm": 0.46929892897605896,
      "learning_rate": 1.5518998248826292e-05,
      "loss": 1.0323,
      "step": 3580
    },
    {
      "epoch": 33.47155909224875,
      "grad_norm": 0.4339628517627716,
      "learning_rate": 1.5516473854177778e-05,
      "loss": 1.0622,
      "step": 3581
    },
    {
      "epoch": 33.48099027409372,
      "grad_norm": 0.44480636715888977,
      "learning_rate": 1.5513948954111754e-05,
      "loss": 1.0937,
      "step": 3582
    },
    {
      "epoch": 33.490421455938694,
      "grad_norm": 0.4992204010486603,
      "learning_rate": 1.551142354885955e-05,
      "loss": 1.0934,
      "step": 3583
    },
    {
      "epoch": 33.499852637783675,
      "grad_norm": 0.5049549341201782,
      "learning_rate": 1.5508897638652544e-05,
      "loss": 1.0719,
      "step": 3584
    },
    {
      "epoch": 33.50928381962865,
      "grad_norm": 0.45671892166137695,
      "learning_rate": 1.5506371223722162e-05,
      "loss": 1.0825,
      "step": 3585
    },
    {
      "epoch": 33.51871500147362,
      "grad_norm": 0.4894103407859802,
      "learning_rate": 1.5503844304299872e-05,
      "loss": 1.0536,
      "step": 3586
    },
    {
      "epoch": 33.5281461833186,
      "grad_norm": 0.4654501974582672,
      "learning_rate": 1.5501316880617188e-05,
      "loss": 1.1177,
      "step": 3587
    },
    {
      "epoch": 33.53757736516357,
      "grad_norm": 0.4516367018222809,
      "learning_rate": 1.549878895290567e-05,
      "loss": 1.0495,
      "step": 3588
    },
    {
      "epoch": 33.547008547008545,
      "grad_norm": 0.5148831009864807,
      "learning_rate": 1.5496260521396934e-05,
      "loss": 1.0631,
      "step": 3589
    },
    {
      "epoch": 33.55643972885352,
      "grad_norm": 0.4669513404369354,
      "learning_rate": 1.5493731586322625e-05,
      "loss": 1.1007,
      "step": 3590
    },
    {
      "epoch": 33.5658709106985,
      "grad_norm": 0.44686707854270935,
      "learning_rate": 1.549120214791445e-05,
      "loss": 1.1253,
      "step": 3591
    },
    {
      "epoch": 33.575302092543474,
      "grad_norm": 0.5331680774688721,
      "learning_rate": 1.5488672206404148e-05,
      "loss": 1.0645,
      "step": 3592
    },
    {
      "epoch": 33.58473327438845,
      "grad_norm": 0.48476460576057434,
      "learning_rate": 1.5486141762023517e-05,
      "loss": 1.0824,
      "step": 3593
    },
    {
      "epoch": 33.59416445623342,
      "grad_norm": 0.45779967308044434,
      "learning_rate": 1.5483610815004398e-05,
      "loss": 1.074,
      "step": 3594
    },
    {
      "epoch": 33.603595638078396,
      "grad_norm": 0.5151344537734985,
      "learning_rate": 1.5481079365578667e-05,
      "loss": 1.0974,
      "step": 3595
    },
    {
      "epoch": 33.61302681992337,
      "grad_norm": 0.5019263029098511,
      "learning_rate": 1.547854741397826e-05,
      "loss": 1.0912,
      "step": 3596
    },
    {
      "epoch": 33.622458001768344,
      "grad_norm": 0.4679156541824341,
      "learning_rate": 1.5476014960435155e-05,
      "loss": 1.0652,
      "step": 3597
    },
    {
      "epoch": 33.631889183613325,
      "grad_norm": 0.4827072322368622,
      "learning_rate": 1.547348200518137e-05,
      "loss": 1.0766,
      "step": 3598
    },
    {
      "epoch": 33.6413203654583,
      "grad_norm": 0.4965687692165375,
      "learning_rate": 1.5470948548448974e-05,
      "loss": 1.0934,
      "step": 3599
    },
    {
      "epoch": 33.65075154730327,
      "grad_norm": 0.42823871970176697,
      "learning_rate": 1.5468414590470087e-05,
      "loss": 1.1109,
      "step": 3600
    },
    {
      "epoch": 33.66018272914825,
      "grad_norm": 0.4396170973777771,
      "learning_rate": 1.5465880131476863e-05,
      "loss": 1.1002,
      "step": 3601
    },
    {
      "epoch": 33.66961391099322,
      "grad_norm": 0.4321323335170746,
      "learning_rate": 1.5463345171701508e-05,
      "loss": 1.1051,
      "step": 3602
    },
    {
      "epoch": 33.679045092838194,
      "grad_norm": 0.46348971128463745,
      "learning_rate": 1.546080971137628e-05,
      "loss": 1.0769,
      "step": 3603
    },
    {
      "epoch": 33.68847627468317,
      "grad_norm": 0.5304240584373474,
      "learning_rate": 1.5458273750733477e-05,
      "loss": 1.1283,
      "step": 3604
    },
    {
      "epoch": 33.69790745652815,
      "grad_norm": 0.4562987983226776,
      "learning_rate": 1.5455737290005436e-05,
      "loss": 1.1102,
      "step": 3605
    },
    {
      "epoch": 33.70733863837312,
      "grad_norm": 0.43636199831962585,
      "learning_rate": 1.5453200329424554e-05,
      "loss": 1.096,
      "step": 3606
    },
    {
      "epoch": 33.7167698202181,
      "grad_norm": 0.48113328218460083,
      "learning_rate": 1.545066286922326e-05,
      "loss": 1.1035,
      "step": 3607
    },
    {
      "epoch": 33.72620100206307,
      "grad_norm": 0.46031221747398376,
      "learning_rate": 1.544812490963404e-05,
      "loss": 1.1161,
      "step": 3608
    },
    {
      "epoch": 33.735632183908045,
      "grad_norm": 0.4751160740852356,
      "learning_rate": 1.544558645088942e-05,
      "loss": 1.1246,
      "step": 3609
    },
    {
      "epoch": 33.74506336575302,
      "grad_norm": 0.4562382698059082,
      "learning_rate": 1.544304749322197e-05,
      "loss": 1.0869,
      "step": 3610
    },
    {
      "epoch": 33.75449454759799,
      "grad_norm": 0.5052545070648193,
      "learning_rate": 1.544050803686431e-05,
      "loss": 1.023,
      "step": 3611
    },
    {
      "epoch": 33.763925729442974,
      "grad_norm": 0.4947199523448944,
      "learning_rate": 1.543796808204911e-05,
      "loss": 1.0935,
      "step": 3612
    },
    {
      "epoch": 33.77335691128795,
      "grad_norm": 0.488671213388443,
      "learning_rate": 1.543542762900907e-05,
      "loss": 1.0658,
      "step": 3613
    },
    {
      "epoch": 33.78278809313292,
      "grad_norm": 0.4792187213897705,
      "learning_rate": 1.543288667797695e-05,
      "loss": 1.0601,
      "step": 3614
    },
    {
      "epoch": 33.792219274977896,
      "grad_norm": 0.4711623191833496,
      "learning_rate": 1.5430345229185554e-05,
      "loss": 1.0547,
      "step": 3615
    },
    {
      "epoch": 33.80165045682287,
      "grad_norm": 0.4614924192428589,
      "learning_rate": 1.5427803282867727e-05,
      "loss": 1.0554,
      "step": 3616
    },
    {
      "epoch": 33.811081638667844,
      "grad_norm": 0.457387775182724,
      "learning_rate": 1.5425260839256355e-05,
      "loss": 1.0632,
      "step": 3617
    },
    {
      "epoch": 33.82051282051282,
      "grad_norm": 0.47576290369033813,
      "learning_rate": 1.5422717898584383e-05,
      "loss": 1.0361,
      "step": 3618
    },
    {
      "epoch": 33.8299440023578,
      "grad_norm": 0.46994659304618835,
      "learning_rate": 1.5420174461084795e-05,
      "loss": 1.0247,
      "step": 3619
    },
    {
      "epoch": 33.83937518420277,
      "grad_norm": 0.4742026925086975,
      "learning_rate": 1.5417630526990613e-05,
      "loss": 1.1031,
      "step": 3620
    },
    {
      "epoch": 33.84880636604775,
      "grad_norm": 0.4574052691459656,
      "learning_rate": 1.5415086096534918e-05,
      "loss": 1.0812,
      "step": 3621
    },
    {
      "epoch": 33.85823754789272,
      "grad_norm": 0.4701026976108551,
      "learning_rate": 1.541254116995083e-05,
      "loss": 1.1076,
      "step": 3622
    },
    {
      "epoch": 33.867668729737694,
      "grad_norm": 0.5147006511688232,
      "learning_rate": 1.5409995747471508e-05,
      "loss": 1.093,
      "step": 3623
    },
    {
      "epoch": 33.87709991158267,
      "grad_norm": 0.5210564136505127,
      "learning_rate": 1.5407449829330167e-05,
      "loss": 1.0902,
      "step": 3624
    },
    {
      "epoch": 33.88653109342764,
      "grad_norm": 0.5331882238388062,
      "learning_rate": 1.5404903415760065e-05,
      "loss": 1.1187,
      "step": 3625
    },
    {
      "epoch": 33.89596227527262,
      "grad_norm": 0.5038096904754639,
      "learning_rate": 1.5402356506994498e-05,
      "loss": 1.0743,
      "step": 3626
    },
    {
      "epoch": 33.9053934571176,
      "grad_norm": 0.45991891622543335,
      "learning_rate": 1.539980910326682e-05,
      "loss": 1.1027,
      "step": 3627
    },
    {
      "epoch": 33.91482463896257,
      "grad_norm": 0.46561944484710693,
      "learning_rate": 1.5397261204810418e-05,
      "loss": 1.0589,
      "step": 3628
    },
    {
      "epoch": 33.924255820807545,
      "grad_norm": 0.4553029537200928,
      "learning_rate": 1.5394712811858728e-05,
      "loss": 1.0947,
      "step": 3629
    },
    {
      "epoch": 33.93368700265252,
      "grad_norm": 0.5114841461181641,
      "learning_rate": 1.539216392464524e-05,
      "loss": 1.066,
      "step": 3630
    },
    {
      "epoch": 33.94311818449749,
      "grad_norm": 0.43011853098869324,
      "learning_rate": 1.5389614543403478e-05,
      "loss": 1.1106,
      "step": 3631
    },
    {
      "epoch": 33.95254936634247,
      "grad_norm": 0.48215043544769287,
      "learning_rate": 1.5387064668367016e-05,
      "loss": 1.0633,
      "step": 3632
    },
    {
      "epoch": 33.96198054818745,
      "grad_norm": 0.45870932936668396,
      "learning_rate": 1.5384514299769473e-05,
      "loss": 1.0835,
      "step": 3633
    },
    {
      "epoch": 33.97141173003242,
      "grad_norm": 0.46211880445480347,
      "learning_rate": 1.5381963437844512e-05,
      "loss": 1.0464,
      "step": 3634
    },
    {
      "epoch": 33.980842911877396,
      "grad_norm": 0.48075565695762634,
      "learning_rate": 1.5379412082825843e-05,
      "loss": 1.0763,
      "step": 3635
    },
    {
      "epoch": 33.99027409372237,
      "grad_norm": 0.5037198662757874,
      "learning_rate": 1.5376860234947216e-05,
      "loss": 1.0445,
      "step": 3636
    },
    {
      "epoch": 33.999705275567344,
      "grad_norm": 0.5258898735046387,
      "learning_rate": 1.537430789444244e-05,
      "loss": 1.0322,
      "step": 3637
    },
    {
      "epoch": 34.0,
      "grad_norm": 3.0023257732391357,
      "learning_rate": 1.5371755061545347e-05,
      "loss": 0.2731,
      "step": 3638
    },
    {
      "epoch": 34.009431181844974,
      "grad_norm": 0.4674982726573944,
      "learning_rate": 1.536920173648984e-05,
      "loss": 1.072,
      "step": 3639
    },
    {
      "epoch": 34.01886236368995,
      "grad_norm": 0.47394347190856934,
      "learning_rate": 1.5366647919509845e-05,
      "loss": 1.0593,
      "step": 3640
    },
    {
      "epoch": 34.02829354553492,
      "grad_norm": 0.4589616358280182,
      "learning_rate": 1.5364093610839343e-05,
      "loss": 1.0812,
      "step": 3641
    },
    {
      "epoch": 34.0377247273799,
      "grad_norm": 0.47344404458999634,
      "learning_rate": 1.5361538810712363e-05,
      "loss": 1.1128,
      "step": 3642
    },
    {
      "epoch": 34.04715590922488,
      "grad_norm": 0.4817546010017395,
      "learning_rate": 1.535898351936297e-05,
      "loss": 1.1092,
      "step": 3643
    },
    {
      "epoch": 34.05658709106985,
      "grad_norm": 0.45844584703445435,
      "learning_rate": 1.535642773702528e-05,
      "loss": 1.1126,
      "step": 3644
    },
    {
      "epoch": 34.066018272914825,
      "grad_norm": 0.48564624786376953,
      "learning_rate": 1.535387146393346e-05,
      "loss": 1.1011,
      "step": 3645
    },
    {
      "epoch": 34.0754494547598,
      "grad_norm": 0.4968012869358063,
      "learning_rate": 1.5351314700321703e-05,
      "loss": 1.0659,
      "step": 3646
    },
    {
      "epoch": 34.08488063660477,
      "grad_norm": 0.46182721853256226,
      "learning_rate": 1.5348757446424266e-05,
      "loss": 1.0793,
      "step": 3647
    },
    {
      "epoch": 34.09431181844975,
      "grad_norm": 0.4995063543319702,
      "learning_rate": 1.534619970247544e-05,
      "loss": 1.0506,
      "step": 3648
    },
    {
      "epoch": 34.10374300029473,
      "grad_norm": 0.4376867115497589,
      "learning_rate": 1.5343641468709573e-05,
      "loss": 1.0742,
      "step": 3649
    },
    {
      "epoch": 34.1131741821397,
      "grad_norm": 0.4829294979572296,
      "learning_rate": 1.534108274536104e-05,
      "loss": 1.1017,
      "step": 3650
    },
    {
      "epoch": 34.122605363984675,
      "grad_norm": 0.4750262200832367,
      "learning_rate": 1.5338523532664274e-05,
      "loss": 1.0544,
      "step": 3651
    },
    {
      "epoch": 34.13203654582965,
      "grad_norm": 0.44697320461273193,
      "learning_rate": 1.533596383085375e-05,
      "loss": 1.0821,
      "step": 3652
    },
    {
      "epoch": 34.14146772767462,
      "grad_norm": 0.4530751407146454,
      "learning_rate": 1.5333403640163987e-05,
      "loss": 1.0913,
      "step": 3653
    },
    {
      "epoch": 34.1508989095196,
      "grad_norm": 0.4627264440059662,
      "learning_rate": 1.5330842960829547e-05,
      "loss": 1.0868,
      "step": 3654
    },
    {
      "epoch": 34.16033009136457,
      "grad_norm": 0.4597436785697937,
      "learning_rate": 1.532828179308504e-05,
      "loss": 1.0815,
      "step": 3655
    },
    {
      "epoch": 34.16976127320955,
      "grad_norm": 0.4865845739841461,
      "learning_rate": 1.532572013716512e-05,
      "loss": 1.028,
      "step": 3656
    },
    {
      "epoch": 34.179192455054526,
      "grad_norm": 0.46219494938850403,
      "learning_rate": 1.5323157993304487e-05,
      "loss": 1.0855,
      "step": 3657
    },
    {
      "epoch": 34.1886236368995,
      "grad_norm": 0.4390369653701782,
      "learning_rate": 1.5320595361737875e-05,
      "loss": 1.0643,
      "step": 3658
    },
    {
      "epoch": 34.198054818744474,
      "grad_norm": 0.49425092339515686,
      "learning_rate": 1.531803224270008e-05,
      "loss": 1.1127,
      "step": 3659
    },
    {
      "epoch": 34.20748600058945,
      "grad_norm": 0.44262662529945374,
      "learning_rate": 1.531546863642593e-05,
      "loss": 1.0939,
      "step": 3660
    },
    {
      "epoch": 34.21691718243442,
      "grad_norm": 0.526279628276825,
      "learning_rate": 1.5312904543150307e-05,
      "loss": 1.0884,
      "step": 3661
    },
    {
      "epoch": 34.226348364279396,
      "grad_norm": 0.4697280824184418,
      "learning_rate": 1.5310339963108125e-05,
      "loss": 1.0664,
      "step": 3662
    },
    {
      "epoch": 34.23577954612438,
      "grad_norm": 0.44736534357070923,
      "learning_rate": 1.5307774896534357e-05,
      "loss": 1.0732,
      "step": 3663
    },
    {
      "epoch": 34.24521072796935,
      "grad_norm": 0.46090760827064514,
      "learning_rate": 1.530520934366401e-05,
      "loss": 1.0581,
      "step": 3664
    },
    {
      "epoch": 34.254641909814325,
      "grad_norm": 0.4348447024822235,
      "learning_rate": 1.5302643304732137e-05,
      "loss": 1.063,
      "step": 3665
    },
    {
      "epoch": 34.2640730916593,
      "grad_norm": 0.4476679265499115,
      "learning_rate": 1.5300076779973837e-05,
      "loss": 1.0465,
      "step": 3666
    },
    {
      "epoch": 34.27350427350427,
      "grad_norm": 0.5037491917610168,
      "learning_rate": 1.529750976962426e-05,
      "loss": 1.07,
      "step": 3667
    },
    {
      "epoch": 34.28293545534925,
      "grad_norm": 0.43031322956085205,
      "learning_rate": 1.5294942273918593e-05,
      "loss": 1.099,
      "step": 3668
    },
    {
      "epoch": 34.29236663719422,
      "grad_norm": 0.4562927186489105,
      "learning_rate": 1.5292374293092067e-05,
      "loss": 1.044,
      "step": 3669
    },
    {
      "epoch": 34.3017978190392,
      "grad_norm": 0.5268856883049011,
      "learning_rate": 1.528980582737996e-05,
      "loss": 1.0471,
      "step": 3670
    },
    {
      "epoch": 34.311229000884175,
      "grad_norm": 0.4394978880882263,
      "learning_rate": 1.5287236877017595e-05,
      "loss": 1.1008,
      "step": 3671
    },
    {
      "epoch": 34.32066018272915,
      "grad_norm": 0.4794338643550873,
      "learning_rate": 1.5284667442240336e-05,
      "loss": 1.0858,
      "step": 3672
    },
    {
      "epoch": 34.33009136457412,
      "grad_norm": 0.429691880941391,
      "learning_rate": 1.5282097523283598e-05,
      "loss": 1.0633,
      "step": 3673
    },
    {
      "epoch": 34.3395225464191,
      "grad_norm": 0.5053947567939758,
      "learning_rate": 1.5279527120382832e-05,
      "loss": 1.0226,
      "step": 3674
    },
    {
      "epoch": 34.34895372826407,
      "grad_norm": 0.42694368958473206,
      "learning_rate": 1.527695623377354e-05,
      "loss": 1.0687,
      "step": 3675
    },
    {
      "epoch": 34.358384910109045,
      "grad_norm": 0.4756320118904114,
      "learning_rate": 1.5274384863691267e-05,
      "loss": 1.069,
      "step": 3676
    },
    {
      "epoch": 34.367816091954026,
      "grad_norm": 0.4870413541793823,
      "learning_rate": 1.5271813010371594e-05,
      "loss": 1.1097,
      "step": 3677
    },
    {
      "epoch": 34.377247273799,
      "grad_norm": 0.46752870082855225,
      "learning_rate": 1.526924067405016e-05,
      "loss": 1.0778,
      "step": 3678
    },
    {
      "epoch": 34.386678455643974,
      "grad_norm": 0.49652472138404846,
      "learning_rate": 1.5266667854962636e-05,
      "loss": 1.0532,
      "step": 3679
    },
    {
      "epoch": 34.39610963748895,
      "grad_norm": 0.4786655008792877,
      "learning_rate": 1.5264094553344753e-05,
      "loss": 1.087,
      "step": 3680
    },
    {
      "epoch": 34.40554081933392,
      "grad_norm": 0.47531116008758545,
      "learning_rate": 1.526152076943227e-05,
      "loss": 1.0817,
      "step": 3681
    },
    {
      "epoch": 34.414972001178896,
      "grad_norm": 0.4788276255130768,
      "learning_rate": 1.525894650346099e-05,
      "loss": 1.0585,
      "step": 3682
    },
    {
      "epoch": 34.42440318302387,
      "grad_norm": 0.4947524964809418,
      "learning_rate": 1.5256371755666777e-05,
      "loss": 1.1086,
      "step": 3683
    },
    {
      "epoch": 34.43383436486885,
      "grad_norm": 0.4850623309612274,
      "learning_rate": 1.5253796526285523e-05,
      "loss": 1.0529,
      "step": 3684
    },
    {
      "epoch": 34.443265546713825,
      "grad_norm": 0.45853152871131897,
      "learning_rate": 1.525122081555317e-05,
      "loss": 1.068,
      "step": 3685
    },
    {
      "epoch": 34.4526967285588,
      "grad_norm": 0.47750765085220337,
      "learning_rate": 1.5248644623705704e-05,
      "loss": 1.0943,
      "step": 3686
    },
    {
      "epoch": 34.46212791040377,
      "grad_norm": 0.48308083415031433,
      "learning_rate": 1.5246067950979156e-05,
      "loss": 1.0595,
      "step": 3687
    },
    {
      "epoch": 34.47155909224875,
      "grad_norm": 0.47673532366752625,
      "learning_rate": 1.5243490797609596e-05,
      "loss": 1.0558,
      "step": 3688
    },
    {
      "epoch": 34.48099027409372,
      "grad_norm": 0.5085093379020691,
      "learning_rate": 1.524091316383315e-05,
      "loss": 1.078,
      "step": 3689
    },
    {
      "epoch": 34.490421455938694,
      "grad_norm": 0.5006676316261292,
      "learning_rate": 1.5238335049885975e-05,
      "loss": 1.0345,
      "step": 3690
    },
    {
      "epoch": 34.499852637783675,
      "grad_norm": 0.4786533713340759,
      "learning_rate": 1.5235756456004275e-05,
      "loss": 1.0486,
      "step": 3691
    },
    {
      "epoch": 34.50928381962865,
      "grad_norm": 0.5366873741149902,
      "learning_rate": 1.52331773824243e-05,
      "loss": 1.1098,
      "step": 3692
    },
    {
      "epoch": 34.51871500147362,
      "grad_norm": 0.47693943977355957,
      "learning_rate": 1.5230597829382347e-05,
      "loss": 1.0852,
      "step": 3693
    },
    {
      "epoch": 34.5281461833186,
      "grad_norm": 0.4434025287628174,
      "learning_rate": 1.5228017797114753e-05,
      "loss": 1.0845,
      "step": 3694
    },
    {
      "epoch": 34.53757736516357,
      "grad_norm": 0.4890669584274292,
      "learning_rate": 1.5225437285857902e-05,
      "loss": 1.0929,
      "step": 3695
    },
    {
      "epoch": 34.547008547008545,
      "grad_norm": 0.43618834018707275,
      "learning_rate": 1.5222856295848211e-05,
      "loss": 1.0855,
      "step": 3696
    },
    {
      "epoch": 34.55643972885352,
      "grad_norm": 0.45415031909942627,
      "learning_rate": 1.5220274827322161e-05,
      "loss": 1.0709,
      "step": 3697
    },
    {
      "epoch": 34.5658709106985,
      "grad_norm": 0.480456680059433,
      "learning_rate": 1.5217692880516258e-05,
      "loss": 1.0644,
      "step": 3698
    },
    {
      "epoch": 34.575302092543474,
      "grad_norm": 0.45302215218544006,
      "learning_rate": 1.5215110455667054e-05,
      "loss": 1.0514,
      "step": 3699
    },
    {
      "epoch": 34.58473327438845,
      "grad_norm": 0.4444808065891266,
      "learning_rate": 1.5212527553011165e-05,
      "loss": 1.0802,
      "step": 3700
    },
    {
      "epoch": 34.59416445623342,
      "grad_norm": 0.47671741247177124,
      "learning_rate": 1.5209944172785225e-05,
      "loss": 1.0426,
      "step": 3701
    },
    {
      "epoch": 34.603595638078396,
      "grad_norm": 0.4793817698955536,
      "learning_rate": 1.5207360315225924e-05,
      "loss": 1.068,
      "step": 3702
    },
    {
      "epoch": 34.61302681992337,
      "grad_norm": 0.5436868667602539,
      "learning_rate": 1.5204775980569992e-05,
      "loss": 1.0603,
      "step": 3703
    },
    {
      "epoch": 34.622458001768344,
      "grad_norm": 0.5195272564888,
      "learning_rate": 1.520219116905421e-05,
      "loss": 1.1089,
      "step": 3704
    },
    {
      "epoch": 34.631889183613325,
      "grad_norm": 0.4964161515235901,
      "learning_rate": 1.5199605880915398e-05,
      "loss": 1.1055,
      "step": 3705
    },
    {
      "epoch": 34.6413203654583,
      "grad_norm": 0.4593527019023895,
      "learning_rate": 1.5197020116390414e-05,
      "loss": 1.053,
      "step": 3706
    },
    {
      "epoch": 34.65075154730327,
      "grad_norm": 0.48915961384773254,
      "learning_rate": 1.5194433875716166e-05,
      "loss": 1.0722,
      "step": 3707
    },
    {
      "epoch": 34.66018272914825,
      "grad_norm": 0.49226993322372437,
      "learning_rate": 1.519184715912961e-05,
      "loss": 1.1092,
      "step": 3708
    },
    {
      "epoch": 34.66961391099322,
      "grad_norm": 0.4703286588191986,
      "learning_rate": 1.5189259966867733e-05,
      "loss": 1.0815,
      "step": 3709
    },
    {
      "epoch": 34.679045092838194,
      "grad_norm": 0.4687921702861786,
      "learning_rate": 1.5186672299167575e-05,
      "loss": 1.0841,
      "step": 3710
    },
    {
      "epoch": 34.68847627468317,
      "grad_norm": 0.520035445690155,
      "learning_rate": 1.518408415626622e-05,
      "loss": 1.0311,
      "step": 3711
    },
    {
      "epoch": 34.69790745652815,
      "grad_norm": 0.48410335183143616,
      "learning_rate": 1.518149553840079e-05,
      "loss": 1.0848,
      "step": 3712
    },
    {
      "epoch": 34.70733863837312,
      "grad_norm": 0.4558734595775604,
      "learning_rate": 1.5178906445808454e-05,
      "loss": 1.0647,
      "step": 3713
    },
    {
      "epoch": 34.7167698202181,
      "grad_norm": 0.42769336700439453,
      "learning_rate": 1.5176316878726428e-05,
      "loss": 1.0549,
      "step": 3714
    },
    {
      "epoch": 34.72620100206307,
      "grad_norm": 0.44945642352104187,
      "learning_rate": 1.5173726837391955e-05,
      "loss": 1.075,
      "step": 3715
    },
    {
      "epoch": 34.735632183908045,
      "grad_norm": 0.47807255387306213,
      "learning_rate": 1.5171136322042348e-05,
      "loss": 1.0998,
      "step": 3716
    },
    {
      "epoch": 34.74506336575302,
      "grad_norm": 0.4577317237854004,
      "learning_rate": 1.5168545332914942e-05,
      "loss": 1.0473,
      "step": 3717
    },
    {
      "epoch": 34.75449454759799,
      "grad_norm": 0.46977320313453674,
      "learning_rate": 1.5165953870247123e-05,
      "loss": 1.0538,
      "step": 3718
    },
    {
      "epoch": 34.763925729442974,
      "grad_norm": 0.4908100366592407,
      "learning_rate": 1.5163361934276322e-05,
      "loss": 1.086,
      "step": 3719
    },
    {
      "epoch": 34.77335691128795,
      "grad_norm": 0.4783147871494293,
      "learning_rate": 1.5160769525240009e-05,
      "loss": 1.0732,
      "step": 3720
    },
    {
      "epoch": 34.78278809313292,
      "grad_norm": 0.47633397579193115,
      "learning_rate": 1.5158176643375702e-05,
      "loss": 1.0747,
      "step": 3721
    },
    {
      "epoch": 34.792219274977896,
      "grad_norm": 0.4609524607658386,
      "learning_rate": 1.5155583288920954e-05,
      "loss": 1.0593,
      "step": 3722
    },
    {
      "epoch": 34.80165045682287,
      "grad_norm": 0.4362720251083374,
      "learning_rate": 1.5152989462113377e-05,
      "loss": 1.0889,
      "step": 3723
    },
    {
      "epoch": 34.811081638667844,
      "grad_norm": 0.47761794924736023,
      "learning_rate": 1.515039516319061e-05,
      "loss": 1.096,
      "step": 3724
    },
    {
      "epoch": 34.82051282051282,
      "grad_norm": 0.5332537889480591,
      "learning_rate": 1.5147800392390343e-05,
      "loss": 1.0862,
      "step": 3725
    },
    {
      "epoch": 34.8299440023578,
      "grad_norm": 0.4997433125972748,
      "learning_rate": 1.5145205149950308e-05,
      "loss": 1.0648,
      "step": 3726
    },
    {
      "epoch": 34.83937518420277,
      "grad_norm": 0.44897720217704773,
      "learning_rate": 1.514260943610828e-05,
      "loss": 1.0624,
      "step": 3727
    },
    {
      "epoch": 34.84880636604775,
      "grad_norm": 0.48486825823783875,
      "learning_rate": 1.514001325110208e-05,
      "loss": 1.099,
      "step": 3728
    },
    {
      "epoch": 34.85823754789272,
      "grad_norm": 0.46290335059165955,
      "learning_rate": 1.5137416595169563e-05,
      "loss": 1.0981,
      "step": 3729
    },
    {
      "epoch": 34.867668729737694,
      "grad_norm": 0.4714241623878479,
      "learning_rate": 1.5134819468548643e-05,
      "loss": 1.1011,
      "step": 3730
    },
    {
      "epoch": 34.87709991158267,
      "grad_norm": 0.47171053290367126,
      "learning_rate": 1.5132221871477262e-05,
      "loss": 1.0713,
      "step": 3731
    },
    {
      "epoch": 34.88653109342764,
      "grad_norm": 0.4647925794124603,
      "learning_rate": 1.5129623804193412e-05,
      "loss": 1.0843,
      "step": 3732
    },
    {
      "epoch": 34.89596227527262,
      "grad_norm": 0.505661129951477,
      "learning_rate": 1.5127025266935125e-05,
      "loss": 1.0794,
      "step": 3733
    },
    {
      "epoch": 34.9053934571176,
      "grad_norm": 0.4631924629211426,
      "learning_rate": 1.5124426259940483e-05,
      "loss": 1.0945,
      "step": 3734
    },
    {
      "epoch": 34.91482463896257,
      "grad_norm": 0.47677645087242126,
      "learning_rate": 1.5121826783447604e-05,
      "loss": 1.0371,
      "step": 3735
    },
    {
      "epoch": 34.924255820807545,
      "grad_norm": 0.4523269236087799,
      "learning_rate": 1.511922683769465e-05,
      "loss": 1.0847,
      "step": 3736
    },
    {
      "epoch": 34.93368700265252,
      "grad_norm": 0.5048777461051941,
      "learning_rate": 1.5116626422919827e-05,
      "loss": 1.0824,
      "step": 3737
    },
    {
      "epoch": 34.94311818449749,
      "grad_norm": 0.49026891589164734,
      "learning_rate": 1.5114025539361388e-05,
      "loss": 1.1164,
      "step": 3738
    },
    {
      "epoch": 34.95254936634247,
      "grad_norm": 0.4583186209201813,
      "learning_rate": 1.5111424187257617e-05,
      "loss": 1.1134,
      "step": 3739
    },
    {
      "epoch": 34.96198054818745,
      "grad_norm": 0.5218675136566162,
      "learning_rate": 1.5108822366846858e-05,
      "loss": 1.0804,
      "step": 3740
    },
    {
      "epoch": 34.97141173003242,
      "grad_norm": 0.4908480942249298,
      "learning_rate": 1.5106220078367484e-05,
      "loss": 1.1241,
      "step": 3741
    },
    {
      "epoch": 34.980842911877396,
      "grad_norm": 0.45402154326438904,
      "learning_rate": 1.5103617322057917e-05,
      "loss": 1.1018,
      "step": 3742
    },
    {
      "epoch": 34.99027409372237,
      "grad_norm": 0.4666402339935303,
      "learning_rate": 1.5101014098156619e-05,
      "loss": 1.0679,
      "step": 3743
    },
    {
      "epoch": 34.999705275567344,
      "grad_norm": 0.5019403100013733,
      "learning_rate": 1.50984104069021e-05,
      "loss": 1.0855,
      "step": 3744
    },
    {
      "epoch": 35.0,
      "grad_norm": 3.011854887008667,
      "learning_rate": 1.5095806248532904e-05,
      "loss": 0.591,
      "step": 3745
    },
    {
      "epoch": 35.009431181844974,
      "grad_norm": 0.44958093762397766,
      "learning_rate": 1.5093201623287631e-05,
      "loss": 1.0877,
      "step": 3746
    },
    {
      "epoch": 35.01886236368995,
      "grad_norm": 0.4728681147098541,
      "learning_rate": 1.5090596531404908e-05,
      "loss": 1.0864,
      "step": 3747
    },
    {
      "epoch": 35.02829354553492,
      "grad_norm": 0.48730868101119995,
      "learning_rate": 1.5087990973123418e-05,
      "loss": 1.0993,
      "step": 3748
    },
    {
      "epoch": 35.0377247273799,
      "grad_norm": 0.4958702027797699,
      "learning_rate": 1.508538494868188e-05,
      "loss": 1.0725,
      "step": 3749
    },
    {
      "epoch": 35.04715590922488,
      "grad_norm": 0.4803849458694458,
      "learning_rate": 1.5082778458319056e-05,
      "loss": 1.0671,
      "step": 3750
    },
    {
      "epoch": 35.05658709106985,
      "grad_norm": 0.49286600947380066,
      "learning_rate": 1.5080171502273752e-05,
      "loss": 1.0931,
      "step": 3751
    },
    {
      "epoch": 35.066018272914825,
      "grad_norm": 0.4621185064315796,
      "learning_rate": 1.5077564080784816e-05,
      "loss": 1.0615,
      "step": 3752
    },
    {
      "epoch": 35.0754494547598,
      "grad_norm": 0.4638083279132843,
      "learning_rate": 1.5074956194091144e-05,
      "loss": 1.0963,
      "step": 3753
    },
    {
      "epoch": 35.08488063660477,
      "grad_norm": 0.4798250198364258,
      "learning_rate": 1.5072347842431661e-05,
      "loss": 1.0661,
      "step": 3754
    },
    {
      "epoch": 35.09431181844975,
      "grad_norm": 0.5093158483505249,
      "learning_rate": 1.5069739026045347e-05,
      "loss": 1.0653,
      "step": 3755
    },
    {
      "epoch": 35.10374300029473,
      "grad_norm": 0.5039659142494202,
      "learning_rate": 1.5067129745171224e-05,
      "loss": 1.1019,
      "step": 3756
    },
    {
      "epoch": 35.1131741821397,
      "grad_norm": 0.4518228769302368,
      "learning_rate": 1.5064520000048353e-05,
      "loss": 1.0854,
      "step": 3757
    },
    {
      "epoch": 35.122605363984675,
      "grad_norm": 0.487813800573349,
      "learning_rate": 1.5061909790915836e-05,
      "loss": 1.0589,
      "step": 3758
    },
    {
      "epoch": 35.13203654582965,
      "grad_norm": 0.4648883640766144,
      "learning_rate": 1.5059299118012819e-05,
      "loss": 1.112,
      "step": 3759
    },
    {
      "epoch": 35.14146772767462,
      "grad_norm": 0.45227134227752686,
      "learning_rate": 1.5056687981578488e-05,
      "loss": 1.0416,
      "step": 3760
    },
    {
      "epoch": 35.1508989095196,
      "grad_norm": 0.4495046138763428,
      "learning_rate": 1.5054076381852083e-05,
      "loss": 1.0682,
      "step": 3761
    },
    {
      "epoch": 35.16033009136457,
      "grad_norm": 0.5056900382041931,
      "learning_rate": 1.5051464319072872e-05,
      "loss": 1.0703,
      "step": 3762
    },
    {
      "epoch": 35.16976127320955,
      "grad_norm": 0.49654170870780945,
      "learning_rate": 1.504885179348017e-05,
      "loss": 1.0752,
      "step": 3763
    },
    {
      "epoch": 35.179192455054526,
      "grad_norm": 0.45828941464424133,
      "learning_rate": 1.5046238805313343e-05,
      "loss": 1.0491,
      "step": 3764
    },
    {
      "epoch": 35.1886236368995,
      "grad_norm": 0.47322770953178406,
      "learning_rate": 1.504362535481178e-05,
      "loss": 1.0987,
      "step": 3765
    },
    {
      "epoch": 35.198054818744474,
      "grad_norm": 0.5170676708221436,
      "learning_rate": 1.5041011442214938e-05,
      "loss": 1.0793,
      "step": 3766
    },
    {
      "epoch": 35.20748600058945,
      "grad_norm": 0.4663366675376892,
      "learning_rate": 1.5038397067762294e-05,
      "loss": 1.0645,
      "step": 3767
    },
    {
      "epoch": 35.21691718243442,
      "grad_norm": 0.48847073316574097,
      "learning_rate": 1.503578223169338e-05,
      "loss": 1.0977,
      "step": 3768
    },
    {
      "epoch": 35.226348364279396,
      "grad_norm": 0.48005786538124084,
      "learning_rate": 1.5033166934247765e-05,
      "loss": 1.0561,
      "step": 3769
    },
    {
      "epoch": 35.23577954612438,
      "grad_norm": 0.47480130195617676,
      "learning_rate": 1.5030551175665057e-05,
      "loss": 1.0681,
      "step": 3770
    },
    {
      "epoch": 35.24521072796935,
      "grad_norm": 0.49960485100746155,
      "learning_rate": 1.5027934956184922e-05,
      "loss": 1.0857,
      "step": 3771
    },
    {
      "epoch": 35.254641909814325,
      "grad_norm": 0.4776102304458618,
      "learning_rate": 1.5025318276047049e-05,
      "loss": 1.0865,
      "step": 3772
    },
    {
      "epoch": 35.2640730916593,
      "grad_norm": 0.489578515291214,
      "learning_rate": 1.502270113549118e-05,
      "loss": 1.0731,
      "step": 3773
    },
    {
      "epoch": 35.27350427350427,
      "grad_norm": 0.47802698612213135,
      "learning_rate": 1.5020083534757093e-05,
      "loss": 1.0909,
      "step": 3774
    },
    {
      "epoch": 35.28293545534925,
      "grad_norm": 0.547398567199707,
      "learning_rate": 1.5017465474084616e-05,
      "loss": 1.0683,
      "step": 3775
    },
    {
      "epoch": 35.29236663719422,
      "grad_norm": 0.49531519412994385,
      "learning_rate": 1.5014846953713611e-05,
      "loss": 1.0594,
      "step": 3776
    },
    {
      "epoch": 35.3017978190392,
      "grad_norm": 0.46921467781066895,
      "learning_rate": 1.5012227973883992e-05,
      "loss": 1.1034,
      "step": 3777
    },
    {
      "epoch": 35.311229000884175,
      "grad_norm": 0.4967974126338959,
      "learning_rate": 1.5009608534835703e-05,
      "loss": 1.0724,
      "step": 3778
    },
    {
      "epoch": 35.32066018272915,
      "grad_norm": 0.5072041749954224,
      "learning_rate": 1.5006988636808739e-05,
      "loss": 1.0678,
      "step": 3779
    },
    {
      "epoch": 35.33009136457412,
      "grad_norm": 0.4714668393135071,
      "learning_rate": 1.5004368280043133e-05,
      "loss": 1.0621,
      "step": 3780
    },
    {
      "epoch": 35.3395225464191,
      "grad_norm": 0.4725233018398285,
      "learning_rate": 1.5001747464778965e-05,
      "loss": 1.0714,
      "step": 3781
    },
    {
      "epoch": 35.34895372826407,
      "grad_norm": 0.4788099527359009,
      "learning_rate": 1.4999126191256346e-05,
      "loss": 1.0632,
      "step": 3782
    },
    {
      "epoch": 35.358384910109045,
      "grad_norm": 0.48001375794410706,
      "learning_rate": 1.4996504459715444e-05,
      "loss": 1.0521,
      "step": 3783
    },
    {
      "epoch": 35.367816091954026,
      "grad_norm": 0.5123843550682068,
      "learning_rate": 1.4993882270396455e-05,
      "loss": 1.0491,
      "step": 3784
    },
    {
      "epoch": 35.377247273799,
      "grad_norm": 0.5232813358306885,
      "learning_rate": 1.4991259623539627e-05,
      "loss": 1.0769,
      "step": 3785
    },
    {
      "epoch": 35.386678455643974,
      "grad_norm": 0.4853661060333252,
      "learning_rate": 1.4988636519385244e-05,
      "loss": 1.0671,
      "step": 3786
    },
    {
      "epoch": 35.39610963748895,
      "grad_norm": 0.43083903193473816,
      "learning_rate": 1.4986012958173637e-05,
      "loss": 1.0934,
      "step": 3787
    },
    {
      "epoch": 35.40554081933392,
      "grad_norm": 0.49168017506599426,
      "learning_rate": 1.4983388940145172e-05,
      "loss": 1.0841,
      "step": 3788
    },
    {
      "epoch": 35.414972001178896,
      "grad_norm": 0.46690839529037476,
      "learning_rate": 1.4980764465540259e-05,
      "loss": 1.049,
      "step": 3789
    },
    {
      "epoch": 35.42440318302387,
      "grad_norm": 0.4673762917518616,
      "learning_rate": 1.4978139534599359e-05,
      "loss": 1.082,
      "step": 3790
    },
    {
      "epoch": 35.43383436486885,
      "grad_norm": 0.48107805848121643,
      "learning_rate": 1.4975514147562959e-05,
      "loss": 1.1139,
      "step": 3791
    },
    {
      "epoch": 35.443265546713825,
      "grad_norm": 0.4858854115009308,
      "learning_rate": 1.4972888304671602e-05,
      "loss": 1.0618,
      "step": 3792
    },
    {
      "epoch": 35.4526967285588,
      "grad_norm": 0.48038944602012634,
      "learning_rate": 1.4970262006165865e-05,
      "loss": 1.0907,
      "step": 3793
    },
    {
      "epoch": 35.46212791040377,
      "grad_norm": 0.5014622807502747,
      "learning_rate": 1.4967635252286367e-05,
      "loss": 1.0488,
      "step": 3794
    },
    {
      "epoch": 35.47155909224875,
      "grad_norm": 0.49273180961608887,
      "learning_rate": 1.4965008043273775e-05,
      "loss": 1.0858,
      "step": 3795
    },
    {
      "epoch": 35.48099027409372,
      "grad_norm": 0.5104458928108215,
      "learning_rate": 1.4962380379368786e-05,
      "loss": 1.076,
      "step": 3796
    },
    {
      "epoch": 35.490421455938694,
      "grad_norm": 0.47175225615501404,
      "learning_rate": 1.4959752260812153e-05,
      "loss": 1.1172,
      "step": 3797
    },
    {
      "epoch": 35.499852637783675,
      "grad_norm": 0.47261616587638855,
      "learning_rate": 1.4957123687844659e-05,
      "loss": 1.0567,
      "step": 3798
    },
    {
      "epoch": 35.50928381962865,
      "grad_norm": 0.47623759508132935,
      "learning_rate": 1.4954494660707133e-05,
      "loss": 1.0643,
      "step": 3799
    },
    {
      "epoch": 35.51871500147362,
      "grad_norm": 0.43779128789901733,
      "learning_rate": 1.4951865179640444e-05,
      "loss": 1.0691,
      "step": 3800
    },
    {
      "epoch": 35.5281461833186,
      "grad_norm": 0.47654807567596436,
      "learning_rate": 1.4949235244885509e-05,
      "loss": 1.078,
      "step": 3801
    },
    {
      "epoch": 35.53757736516357,
      "grad_norm": 0.4649975001811981,
      "learning_rate": 1.494660485668328e-05,
      "loss": 1.0893,
      "step": 3802
    },
    {
      "epoch": 35.547008547008545,
      "grad_norm": 0.49244505167007446,
      "learning_rate": 1.4943974015274752e-05,
      "loss": 1.126,
      "step": 3803
    },
    {
      "epoch": 35.55643972885352,
      "grad_norm": 0.5065696835517883,
      "learning_rate": 1.4941342720900958e-05,
      "loss": 1.0604,
      "step": 3804
    },
    {
      "epoch": 35.5658709106985,
      "grad_norm": 0.47678330540657043,
      "learning_rate": 1.4938710973802984e-05,
      "loss": 1.0861,
      "step": 3805
    },
    {
      "epoch": 35.575302092543474,
      "grad_norm": 0.48439568281173706,
      "learning_rate": 1.4936078774221943e-05,
      "loss": 1.0365,
      "step": 3806
    },
    {
      "epoch": 35.58473327438845,
      "grad_norm": 0.4735984802246094,
      "learning_rate": 1.4933446122398998e-05,
      "loss": 1.0915,
      "step": 3807
    },
    {
      "epoch": 35.59416445623342,
      "grad_norm": 0.50240558385849,
      "learning_rate": 1.4930813018575356e-05,
      "loss": 1.0525,
      "step": 3808
    },
    {
      "epoch": 35.603595638078396,
      "grad_norm": 0.46261367201805115,
      "learning_rate": 1.4928179462992254e-05,
      "loss": 1.0903,
      "step": 3809
    },
    {
      "epoch": 35.61302681992337,
      "grad_norm": 0.5008718371391296,
      "learning_rate": 1.4925545455890982e-05,
      "loss": 1.097,
      "step": 3810
    },
    {
      "epoch": 35.622458001768344,
      "grad_norm": 0.48212483525276184,
      "learning_rate": 1.4922910997512866e-05,
      "loss": 1.079,
      "step": 3811
    },
    {
      "epoch": 35.631889183613325,
      "grad_norm": 0.46660035848617554,
      "learning_rate": 1.4920276088099275e-05,
      "loss": 1.0627,
      "step": 3812
    },
    {
      "epoch": 35.6413203654583,
      "grad_norm": 0.49373552203178406,
      "learning_rate": 1.4917640727891618e-05,
      "loss": 1.069,
      "step": 3813
    },
    {
      "epoch": 35.65075154730327,
      "grad_norm": 0.448224812746048,
      "learning_rate": 1.4915004917131345e-05,
      "loss": 1.0519,
      "step": 3814
    },
    {
      "epoch": 35.66018272914825,
      "grad_norm": 0.47258201241493225,
      "learning_rate": 1.4912368656059947e-05,
      "loss": 1.1236,
      "step": 3815
    },
    {
      "epoch": 35.66961391099322,
      "grad_norm": 0.4943492114543915,
      "learning_rate": 1.4909731944918963e-05,
      "loss": 1.083,
      "step": 3816
    },
    {
      "epoch": 35.679045092838194,
      "grad_norm": 0.49975961446762085,
      "learning_rate": 1.490709478394996e-05,
      "loss": 1.0738,
      "step": 3817
    },
    {
      "epoch": 35.68847627468317,
      "grad_norm": 0.5259835720062256,
      "learning_rate": 1.4904457173394561e-05,
      "loss": 1.1287,
      "step": 3818
    },
    {
      "epoch": 35.69790745652815,
      "grad_norm": 0.49694737792015076,
      "learning_rate": 1.4901819113494416e-05,
      "loss": 1.0647,
      "step": 3819
    },
    {
      "epoch": 35.70733863837312,
      "grad_norm": 0.4919939637184143,
      "learning_rate": 1.4899180604491227e-05,
      "loss": 1.0283,
      "step": 3820
    },
    {
      "epoch": 35.7167698202181,
      "grad_norm": 0.5061200261116028,
      "learning_rate": 1.4896541646626737e-05,
      "loss": 1.1076,
      "step": 3821
    },
    {
      "epoch": 35.72620100206307,
      "grad_norm": 0.46432337164878845,
      "learning_rate": 1.489390224014272e-05,
      "loss": 1.0825,
      "step": 3822
    },
    {
      "epoch": 35.735632183908045,
      "grad_norm": 0.4998580515384674,
      "learning_rate": 1.4891262385280995e-05,
      "loss": 1.0694,
      "step": 3823
    },
    {
      "epoch": 35.74506336575302,
      "grad_norm": 0.4444538950920105,
      "learning_rate": 1.4888622082283435e-05,
      "loss": 1.1048,
      "step": 3824
    },
    {
      "epoch": 35.75449454759799,
      "grad_norm": 0.4637119472026825,
      "learning_rate": 1.4885981331391936e-05,
      "loss": 1.1275,
      "step": 3825
    },
    {
      "epoch": 35.763925729442974,
      "grad_norm": 0.5060583353042603,
      "learning_rate": 1.4883340132848446e-05,
      "loss": 1.0563,
      "step": 3826
    },
    {
      "epoch": 35.77335691128795,
      "grad_norm": 0.47561460733413696,
      "learning_rate": 1.4880698486894949e-05,
      "loss": 1.0709,
      "step": 3827
    },
    {
      "epoch": 35.78278809313292,
      "grad_norm": 0.4919920563697815,
      "learning_rate": 1.4878056393773474e-05,
      "loss": 1.0851,
      "step": 3828
    },
    {
      "epoch": 35.792219274977896,
      "grad_norm": 0.48162952065467834,
      "learning_rate": 1.4875413853726087e-05,
      "loss": 1.0659,
      "step": 3829
    },
    {
      "epoch": 35.80165045682287,
      "grad_norm": 0.4482080042362213,
      "learning_rate": 1.4872770866994894e-05,
      "loss": 1.0838,
      "step": 3830
    },
    {
      "epoch": 35.811081638667844,
      "grad_norm": 0.47926193475723267,
      "learning_rate": 1.4870127433822048e-05,
      "loss": 1.0752,
      "step": 3831
    },
    {
      "epoch": 35.82051282051282,
      "grad_norm": 0.44761887192726135,
      "learning_rate": 1.4867483554449742e-05,
      "loss": 1.0377,
      "step": 3832
    },
    {
      "epoch": 35.8299440023578,
      "grad_norm": 0.46584832668304443,
      "learning_rate": 1.4864839229120202e-05,
      "loss": 1.1036,
      "step": 3833
    },
    {
      "epoch": 35.83937518420277,
      "grad_norm": 0.49862128496170044,
      "learning_rate": 1.4862194458075703e-05,
      "loss": 1.0596,
      "step": 3834
    },
    {
      "epoch": 35.84880636604775,
      "grad_norm": 0.4743478298187256,
      "learning_rate": 1.4859549241558555e-05,
      "loss": 1.0744,
      "step": 3835
    },
    {
      "epoch": 35.85823754789272,
      "grad_norm": 0.4805740714073181,
      "learning_rate": 1.4856903579811117e-05,
      "loss": 1.0498,
      "step": 3836
    },
    {
      "epoch": 35.867668729737694,
      "grad_norm": 0.4954203963279724,
      "learning_rate": 1.4854257473075782e-05,
      "loss": 1.0862,
      "step": 3837
    },
    {
      "epoch": 35.87709991158267,
      "grad_norm": 0.4662252962589264,
      "learning_rate": 1.4851610921594982e-05,
      "loss": 1.0909,
      "step": 3838
    },
    {
      "epoch": 35.88653109342764,
      "grad_norm": 0.49487847089767456,
      "learning_rate": 1.4848963925611194e-05,
      "loss": 1.1034,
      "step": 3839
    },
    {
      "epoch": 35.89596227527262,
      "grad_norm": 0.48177996277809143,
      "learning_rate": 1.4846316485366936e-05,
      "loss": 1.0567,
      "step": 3840
    },
    {
      "epoch": 35.9053934571176,
      "grad_norm": 0.4923446476459503,
      "learning_rate": 1.484366860110477e-05,
      "loss": 1.1238,
      "step": 3841
    },
    {
      "epoch": 35.91482463896257,
      "grad_norm": 0.46503427624702454,
      "learning_rate": 1.484102027306729e-05,
      "loss": 1.0755,
      "step": 3842
    },
    {
      "epoch": 35.924255820807545,
      "grad_norm": 0.459124892950058,
      "learning_rate": 1.4838371501497131e-05,
      "loss": 1.0895,
      "step": 3843
    },
    {
      "epoch": 35.93368700265252,
      "grad_norm": 0.468445360660553,
      "learning_rate": 1.483572228663698e-05,
      "loss": 1.074,
      "step": 3844
    },
    {
      "epoch": 35.94311818449749,
      "grad_norm": 0.4835973083972931,
      "learning_rate": 1.4833072628729555e-05,
      "loss": 1.0847,
      "step": 3845
    },
    {
      "epoch": 35.95254936634247,
      "grad_norm": 0.49894073605537415,
      "learning_rate": 1.4830422528017613e-05,
      "loss": 1.0655,
      "step": 3846
    },
    {
      "epoch": 35.96198054818745,
      "grad_norm": 0.5249610543251038,
      "learning_rate": 1.4827771984743961e-05,
      "loss": 1.1066,
      "step": 3847
    },
    {
      "epoch": 35.97141173003242,
      "grad_norm": 0.4797044098377228,
      "learning_rate": 1.4825120999151436e-05,
      "loss": 1.0903,
      "step": 3848
    },
    {
      "epoch": 35.980842911877396,
      "grad_norm": 0.49412795901298523,
      "learning_rate": 1.4822469571482922e-05,
      "loss": 1.0127,
      "step": 3849
    },
    {
      "epoch": 35.99027409372237,
      "grad_norm": 0.4300459623336792,
      "learning_rate": 1.4819817701981345e-05,
      "loss": 1.0624,
      "step": 3850
    },
    {
      "epoch": 35.999705275567344,
      "grad_norm": 0.46079954504966736,
      "learning_rate": 1.4817165390889664e-05,
      "loss": 1.0558,
      "step": 3851
    },
    {
      "epoch": 36.0,
      "grad_norm": 2.7760393619537354,
      "learning_rate": 1.4814512638450884e-05,
      "loss": 0.9359,
      "step": 3852
    },
    {
      "epoch": 36.009431181844974,
      "grad_norm": 0.4829505980014801,
      "learning_rate": 1.4811859444908053e-05,
      "loss": 1.076,
      "step": 3853
    },
    {
      "epoch": 36.01886236368995,
      "grad_norm": 0.4555017054080963,
      "learning_rate": 1.480920581050425e-05,
      "loss": 1.1302,
      "step": 3854
    },
    {
      "epoch": 36.02829354553492,
      "grad_norm": 0.4756424129009247,
      "learning_rate": 1.4806551735482604e-05,
      "loss": 1.1003,
      "step": 3855
    },
    {
      "epoch": 36.0377247273799,
      "grad_norm": 0.48099204897880554,
      "learning_rate": 1.4803897220086276e-05,
      "loss": 1.061,
      "step": 3856
    },
    {
      "epoch": 36.04715590922488,
      "grad_norm": 0.4729214012622833,
      "learning_rate": 1.4801242264558477e-05,
      "loss": 1.0979,
      "step": 3857
    },
    {
      "epoch": 36.05658709106985,
      "grad_norm": 0.45353344082832336,
      "learning_rate": 1.4798586869142453e-05,
      "loss": 1.0779,
      "step": 3858
    },
    {
      "epoch": 36.066018272914825,
      "grad_norm": 0.5325608849525452,
      "learning_rate": 1.4795931034081487e-05,
      "loss": 1.0773,
      "step": 3859
    },
    {
      "epoch": 36.0754494547598,
      "grad_norm": 0.5518174171447754,
      "learning_rate": 1.4793274759618908e-05,
      "loss": 1.081,
      "step": 3860
    },
    {
      "epoch": 36.08488063660477,
      "grad_norm": 0.48270273208618164,
      "learning_rate": 1.4790618045998081e-05,
      "loss": 1.0885,
      "step": 3861
    },
    {
      "epoch": 36.09431181844975,
      "grad_norm": 0.481020450592041,
      "learning_rate": 1.4787960893462418e-05,
      "loss": 1.0228,
      "step": 3862
    },
    {
      "epoch": 36.10374300029473,
      "grad_norm": 0.4640763998031616,
      "learning_rate": 1.4785303302255361e-05,
      "loss": 1.0811,
      "step": 3863
    },
    {
      "epoch": 36.1131741821397,
      "grad_norm": 0.4797971546649933,
      "learning_rate": 1.4782645272620398e-05,
      "loss": 1.0771,
      "step": 3864
    },
    {
      "epoch": 36.122605363984675,
      "grad_norm": 0.46803197264671326,
      "learning_rate": 1.4779986804801061e-05,
      "loss": 1.0819,
      "step": 3865
    },
    {
      "epoch": 36.13203654582965,
      "grad_norm": 0.4413098394870758,
      "learning_rate": 1.4777327899040915e-05,
      "loss": 1.0752,
      "step": 3866
    },
    {
      "epoch": 36.14146772767462,
      "grad_norm": 0.46443334221839905,
      "learning_rate": 1.4774668555583567e-05,
      "loss": 1.0754,
      "step": 3867
    },
    {
      "epoch": 36.1508989095196,
      "grad_norm": 0.47039690613746643,
      "learning_rate": 1.4772008774672668e-05,
      "loss": 1.0808,
      "step": 3868
    },
    {
      "epoch": 36.16033009136457,
      "grad_norm": 0.47219961881637573,
      "learning_rate": 1.4769348556551903e-05,
      "loss": 1.0801,
      "step": 3869
    },
    {
      "epoch": 36.16976127320955,
      "grad_norm": 0.5090548992156982,
      "learning_rate": 1.4766687901465004e-05,
      "loss": 0.9985,
      "step": 3870
    },
    {
      "epoch": 36.179192455054526,
      "grad_norm": 0.45362207293510437,
      "learning_rate": 1.4764026809655735e-05,
      "loss": 1.0739,
      "step": 3871
    },
    {
      "epoch": 36.1886236368995,
      "grad_norm": 0.46530747413635254,
      "learning_rate": 1.476136528136791e-05,
      "loss": 1.0883,
      "step": 3872
    },
    {
      "epoch": 36.198054818744474,
      "grad_norm": 0.43517470359802246,
      "learning_rate": 1.4758703316845372e-05,
      "loss": 1.0925,
      "step": 3873
    },
    {
      "epoch": 36.20748600058945,
      "grad_norm": 0.4691774249076843,
      "learning_rate": 1.4756040916332013e-05,
      "loss": 1.0664,
      "step": 3874
    },
    {
      "epoch": 36.21691718243442,
      "grad_norm": 0.4967276453971863,
      "learning_rate": 1.475337808007176e-05,
      "loss": 1.0264,
      "step": 3875
    },
    {
      "epoch": 36.226348364279396,
      "grad_norm": 0.4723326563835144,
      "learning_rate": 1.4750714808308579e-05,
      "loss": 1.06,
      "step": 3876
    },
    {
      "epoch": 36.23577954612438,
      "grad_norm": 0.478511244058609,
      "learning_rate": 1.4748051101286482e-05,
      "loss": 1.0903,
      "step": 3877
    },
    {
      "epoch": 36.24521072796935,
      "grad_norm": 0.47914227843284607,
      "learning_rate": 1.4745386959249515e-05,
      "loss": 1.0979,
      "step": 3878
    },
    {
      "epoch": 36.254641909814325,
      "grad_norm": 0.47407326102256775,
      "learning_rate": 1.4742722382441763e-05,
      "loss": 1.0944,
      "step": 3879
    },
    {
      "epoch": 36.2640730916593,
      "grad_norm": 0.49896445870399475,
      "learning_rate": 1.4740057371107359e-05,
      "loss": 1.0761,
      "step": 3880
    },
    {
      "epoch": 36.27350427350427,
      "grad_norm": 0.4958321154117584,
      "learning_rate": 1.4737391925490468e-05,
      "loss": 1.0991,
      "step": 3881
    },
    {
      "epoch": 36.28293545534925,
      "grad_norm": 0.4511531591415405,
      "learning_rate": 1.4734726045835295e-05,
      "loss": 1.053,
      "step": 3882
    },
    {
      "epoch": 36.29236663719422,
      "grad_norm": 0.45324161648750305,
      "learning_rate": 1.4732059732386091e-05,
      "loss": 1.0661,
      "step": 3883
    },
    {
      "epoch": 36.3017978190392,
      "grad_norm": 0.5001733303070068,
      "learning_rate": 1.4729392985387141e-05,
      "loss": 1.0671,
      "step": 3884
    },
    {
      "epoch": 36.311229000884175,
      "grad_norm": 0.47014114260673523,
      "learning_rate": 1.472672580508277e-05,
      "loss": 1.0978,
      "step": 3885
    },
    {
      "epoch": 36.32066018272915,
      "grad_norm": 0.49860233068466187,
      "learning_rate": 1.4724058191717348e-05,
      "loss": 1.0719,
      "step": 3886
    },
    {
      "epoch": 36.33009136457412,
      "grad_norm": 0.49577221274375916,
      "learning_rate": 1.4721390145535277e-05,
      "loss": 1.0562,
      "step": 3887
    },
    {
      "epoch": 36.3395225464191,
      "grad_norm": 0.4836876690387726,
      "learning_rate": 1.4718721666781003e-05,
      "loss": 1.0285,
      "step": 3888
    },
    {
      "epoch": 36.34895372826407,
      "grad_norm": 0.44704878330230713,
      "learning_rate": 1.4716052755699016e-05,
      "loss": 1.097,
      "step": 3889
    },
    {
      "epoch": 36.358384910109045,
      "grad_norm": 0.4899853467941284,
      "learning_rate": 1.471338341253383e-05,
      "loss": 1.0469,
      "step": 3890
    },
    {
      "epoch": 36.367816091954026,
      "grad_norm": 0.4696188271045685,
      "learning_rate": 1.471071363753002e-05,
      "loss": 1.0926,
      "step": 3891
    },
    {
      "epoch": 36.377247273799,
      "grad_norm": 0.47377967834472656,
      "learning_rate": 1.4708043430932186e-05,
      "loss": 1.0753,
      "step": 3892
    },
    {
      "epoch": 36.386678455643974,
      "grad_norm": 0.4962441027164459,
      "learning_rate": 1.4705372792984973e-05,
      "loss": 1.0598,
      "step": 3893
    },
    {
      "epoch": 36.39610963748895,
      "grad_norm": 0.5074552893638611,
      "learning_rate": 1.470270172393306e-05,
      "loss": 1.0774,
      "step": 3894
    },
    {
      "epoch": 36.40554081933392,
      "grad_norm": 0.47210830450057983,
      "learning_rate": 1.4700030224021174e-05,
      "loss": 1.0755,
      "step": 3895
    },
    {
      "epoch": 36.414972001178896,
      "grad_norm": 0.5447521805763245,
      "learning_rate": 1.4697358293494073e-05,
      "loss": 1.0857,
      "step": 3896
    },
    {
      "epoch": 36.42440318302387,
      "grad_norm": 0.4545317590236664,
      "learning_rate": 1.4694685932596557e-05,
      "loss": 1.1179,
      "step": 3897
    },
    {
      "epoch": 36.43383436486885,
      "grad_norm": 0.5052450895309448,
      "learning_rate": 1.4692013141573476e-05,
      "loss": 1.0792,
      "step": 3898
    },
    {
      "epoch": 36.443265546713825,
      "grad_norm": 0.504358172416687,
      "learning_rate": 1.4689339920669702e-05,
      "loss": 1.0719,
      "step": 3899
    },
    {
      "epoch": 36.4526967285588,
      "grad_norm": 0.5022302269935608,
      "learning_rate": 1.4686666270130159e-05,
      "loss": 1.1278,
      "step": 3900
    },
    {
      "epoch": 36.46212791040377,
      "grad_norm": 0.5057170391082764,
      "learning_rate": 1.46839921901998e-05,
      "loss": 1.0722,
      "step": 3901
    },
    {
      "epoch": 36.47155909224875,
      "grad_norm": 0.4852995276451111,
      "learning_rate": 1.468131768112363e-05,
      "loss": 1.0965,
      "step": 3902
    },
    {
      "epoch": 36.48099027409372,
      "grad_norm": 0.4370546340942383,
      "learning_rate": 1.4678642743146682e-05,
      "loss": 1.1106,
      "step": 3903
    },
    {
      "epoch": 36.490421455938694,
      "grad_norm": 0.511368989944458,
      "learning_rate": 1.4675967376514035e-05,
      "loss": 1.0774,
      "step": 3904
    },
    {
      "epoch": 36.499852637783675,
      "grad_norm": 0.46802350878715515,
      "learning_rate": 1.4673291581470808e-05,
      "loss": 1.0801,
      "step": 3905
    },
    {
      "epoch": 36.50928381962865,
      "grad_norm": 0.5391430258750916,
      "learning_rate": 1.467061535826215e-05,
      "loss": 1.091,
      "step": 3906
    },
    {
      "epoch": 36.51871500147362,
      "grad_norm": 0.5335279107093811,
      "learning_rate": 1.466793870713326e-05,
      "loss": 1.0345,
      "step": 3907
    },
    {
      "epoch": 36.5281461833186,
      "grad_norm": 0.49048808217048645,
      "learning_rate": 1.466526162832937e-05,
      "loss": 1.0364,
      "step": 3908
    },
    {
      "epoch": 36.53757736516357,
      "grad_norm": 0.4609324634075165,
      "learning_rate": 1.4662584122095756e-05,
      "loss": 1.0378,
      "step": 3909
    },
    {
      "epoch": 36.547008547008545,
      "grad_norm": 0.470007061958313,
      "learning_rate": 1.4659906188677729e-05,
      "loss": 1.0638,
      "step": 3910
    },
    {
      "epoch": 36.55643972885352,
      "grad_norm": 0.5169931054115295,
      "learning_rate": 1.4657227828320637e-05,
      "loss": 1.0297,
      "step": 3911
    },
    {
      "epoch": 36.5658709106985,
      "grad_norm": 0.5024203658103943,
      "learning_rate": 1.4654549041269874e-05,
      "loss": 1.0683,
      "step": 3912
    },
    {
      "epoch": 36.575302092543474,
      "grad_norm": 0.48005637526512146,
      "learning_rate": 1.465186982777087e-05,
      "loss": 1.063,
      "step": 3913
    },
    {
      "epoch": 36.58473327438845,
      "grad_norm": 0.5135881900787354,
      "learning_rate": 1.4649190188069091e-05,
      "loss": 1.0814,
      "step": 3914
    },
    {
      "epoch": 36.59416445623342,
      "grad_norm": 0.46311575174331665,
      "learning_rate": 1.4646510122410047e-05,
      "loss": 1.073,
      "step": 3915
    },
    {
      "epoch": 36.603595638078396,
      "grad_norm": 0.4706205427646637,
      "learning_rate": 1.4643829631039283e-05,
      "loss": 1.1102,
      "step": 3916
    },
    {
      "epoch": 36.61302681992337,
      "grad_norm": 0.47256091237068176,
      "learning_rate": 1.4641148714202386e-05,
      "loss": 1.042,
      "step": 3917
    },
    {
      "epoch": 36.622458001768344,
      "grad_norm": 0.5056953430175781,
      "learning_rate": 1.4638467372144983e-05,
      "loss": 1.0491,
      "step": 3918
    },
    {
      "epoch": 36.631889183613325,
      "grad_norm": 0.4632115066051483,
      "learning_rate": 1.4635785605112732e-05,
      "loss": 1.0294,
      "step": 3919
    },
    {
      "epoch": 36.6413203654583,
      "grad_norm": 0.4724217653274536,
      "learning_rate": 1.4633103413351337e-05,
      "loss": 1.045,
      "step": 3920
    },
    {
      "epoch": 36.65075154730327,
      "grad_norm": 0.4454318583011627,
      "learning_rate": 1.4630420797106545e-05,
      "loss": 1.0601,
      "step": 3921
    },
    {
      "epoch": 36.66018272914825,
      "grad_norm": 0.46929600834846497,
      "learning_rate": 1.462773775662413e-05,
      "loss": 1.0871,
      "step": 3922
    },
    {
      "epoch": 36.66961391099322,
      "grad_norm": 0.4920738637447357,
      "learning_rate": 1.4625054292149914e-05,
      "loss": 1.1175,
      "step": 3923
    },
    {
      "epoch": 36.679045092838194,
      "grad_norm": 0.48974889516830444,
      "learning_rate": 1.4622370403929755e-05,
      "loss": 1.1119,
      "step": 3924
    },
    {
      "epoch": 36.68847627468317,
      "grad_norm": 0.4744267761707306,
      "learning_rate": 1.4619686092209553e-05,
      "loss": 1.1095,
      "step": 3925
    },
    {
      "epoch": 36.69790745652815,
      "grad_norm": 0.4647144079208374,
      "learning_rate": 1.4617001357235236e-05,
      "loss": 1.0863,
      "step": 3926
    },
    {
      "epoch": 36.70733863837312,
      "grad_norm": 0.5416216254234314,
      "learning_rate": 1.4614316199252785e-05,
      "loss": 1.0398,
      "step": 3927
    },
    {
      "epoch": 36.7167698202181,
      "grad_norm": 0.5188961625099182,
      "learning_rate": 1.461163061850821e-05,
      "loss": 1.0821,
      "step": 3928
    },
    {
      "epoch": 36.72620100206307,
      "grad_norm": 0.48541855812072754,
      "learning_rate": 1.4608944615247569e-05,
      "loss": 1.0881,
      "step": 3929
    },
    {
      "epoch": 36.735632183908045,
      "grad_norm": 0.4805490970611572,
      "learning_rate": 1.4606258189716945e-05,
      "loss": 1.0844,
      "step": 3930
    },
    {
      "epoch": 36.74506336575302,
      "grad_norm": 0.4072497487068176,
      "learning_rate": 1.4603571342162468e-05,
      "loss": 1.0768,
      "step": 3931
    },
    {
      "epoch": 36.75449454759799,
      "grad_norm": 0.4965783655643463,
      "learning_rate": 1.4600884072830313e-05,
      "loss": 1.0698,
      "step": 3932
    },
    {
      "epoch": 36.763925729442974,
      "grad_norm": 0.4867633283138275,
      "learning_rate": 1.4598196381966681e-05,
      "loss": 1.0603,
      "step": 3933
    },
    {
      "epoch": 36.77335691128795,
      "grad_norm": 0.5003675222396851,
      "learning_rate": 1.459550826981782e-05,
      "loss": 1.075,
      "step": 3934
    },
    {
      "epoch": 36.78278809313292,
      "grad_norm": 0.4751201868057251,
      "learning_rate": 1.4592819736630008e-05,
      "loss": 1.0718,
      "step": 3935
    },
    {
      "epoch": 36.792219274977896,
      "grad_norm": 0.4619412422180176,
      "learning_rate": 1.4590130782649576e-05,
      "loss": 1.0847,
      "step": 3936
    },
    {
      "epoch": 36.80165045682287,
      "grad_norm": 0.48893672227859497,
      "learning_rate": 1.458744140812288e-05,
      "loss": 1.095,
      "step": 3937
    },
    {
      "epoch": 36.811081638667844,
      "grad_norm": 0.49013081192970276,
      "learning_rate": 1.4584751613296322e-05,
      "loss": 1.0798,
      "step": 3938
    },
    {
      "epoch": 36.82051282051282,
      "grad_norm": 0.4678023159503937,
      "learning_rate": 1.4582061398416338e-05,
      "loss": 1.045,
      "step": 3939
    },
    {
      "epoch": 36.8299440023578,
      "grad_norm": 0.4770601689815521,
      "learning_rate": 1.4579370763729406e-05,
      "loss": 1.0706,
      "step": 3940
    },
    {
      "epoch": 36.83937518420277,
      "grad_norm": 0.5502834916114807,
      "learning_rate": 1.457667970948204e-05,
      "loss": 1.059,
      "step": 3941
    },
    {
      "epoch": 36.84880636604775,
      "grad_norm": 0.47279077768325806,
      "learning_rate": 1.4573988235920795e-05,
      "loss": 1.0668,
      "step": 3942
    },
    {
      "epoch": 36.85823754789272,
      "grad_norm": 0.4702249765396118,
      "learning_rate": 1.4571296343292263e-05,
      "loss": 1.0699,
      "step": 3943
    },
    {
      "epoch": 36.867668729737694,
      "grad_norm": 0.4922197163105011,
      "learning_rate": 1.4568604031843073e-05,
      "loss": 1.0474,
      "step": 3944
    },
    {
      "epoch": 36.87709991158267,
      "grad_norm": 0.4926634728908539,
      "learning_rate": 1.4565911301819893e-05,
      "loss": 1.085,
      "step": 3945
    },
    {
      "epoch": 36.88653109342764,
      "grad_norm": 0.49043989181518555,
      "learning_rate": 1.456321815346943e-05,
      "loss": 1.067,
      "step": 3946
    },
    {
      "epoch": 36.89596227527262,
      "grad_norm": 0.5047885179519653,
      "learning_rate": 1.4560524587038433e-05,
      "loss": 1.0381,
      "step": 3947
    },
    {
      "epoch": 36.9053934571176,
      "grad_norm": 0.4993226230144501,
      "learning_rate": 1.4557830602773684e-05,
      "loss": 1.0624,
      "step": 3948
    },
    {
      "epoch": 36.91482463896257,
      "grad_norm": 0.486322283744812,
      "learning_rate": 1.4555136200922003e-05,
      "loss": 1.0852,
      "step": 3949
    },
    {
      "epoch": 36.924255820807545,
      "grad_norm": 0.5220658183097839,
      "learning_rate": 1.4552441381730246e-05,
      "loss": 1.0566,
      "step": 3950
    },
    {
      "epoch": 36.93368700265252,
      "grad_norm": 0.4491093158721924,
      "learning_rate": 1.4549746145445323e-05,
      "loss": 1.1061,
      "step": 3951
    },
    {
      "epoch": 36.94311818449749,
      "grad_norm": 0.4758893847465515,
      "learning_rate": 1.4547050492314165e-05,
      "loss": 1.0848,
      "step": 3952
    },
    {
      "epoch": 36.95254936634247,
      "grad_norm": 0.49811050295829773,
      "learning_rate": 1.4544354422583741e-05,
      "loss": 1.0681,
      "step": 3953
    },
    {
      "epoch": 36.96198054818745,
      "grad_norm": 0.4809316396713257,
      "learning_rate": 1.4541657936501072e-05,
      "loss": 1.1317,
      "step": 3954
    },
    {
      "epoch": 36.97141173003242,
      "grad_norm": 0.4988689124584198,
      "learning_rate": 1.4538961034313207e-05,
      "loss": 1.0714,
      "step": 3955
    },
    {
      "epoch": 36.980842911877396,
      "grad_norm": 0.47169503569602966,
      "learning_rate": 1.4536263716267236e-05,
      "loss": 1.0629,
      "step": 3956
    },
    {
      "epoch": 36.99027409372237,
      "grad_norm": 0.4750061631202698,
      "learning_rate": 1.4533565982610283e-05,
      "loss": 1.0808,
      "step": 3957
    },
    {
      "epoch": 36.999705275567344,
      "grad_norm": 0.5053821206092834,
      "learning_rate": 1.4530867833589514e-05,
      "loss": 1.0797,
      "step": 3958
    },
    {
      "epoch": 37.0,
      "grad_norm": 2.3697757720947266,
      "learning_rate": 1.4528169269452139e-05,
      "loss": 0.583,
      "step": 3959
    },
    {
      "epoch": 37.009431181844974,
      "grad_norm": 0.477868914604187,
      "learning_rate": 1.4525470290445392e-05,
      "loss": 1.0708,
      "step": 3960
    },
    {
      "epoch": 37.01886236368995,
      "grad_norm": 0.4469381868839264,
      "learning_rate": 1.4522770896816552e-05,
      "loss": 1.0864,
      "step": 3961
    },
    {
      "epoch": 37.02829354553492,
      "grad_norm": 0.4603142738342285,
      "learning_rate": 1.4520071088812944e-05,
      "loss": 1.1115,
      "step": 3962
    },
    {
      "epoch": 37.0377247273799,
      "grad_norm": 0.514011800289154,
      "learning_rate": 1.4517370866681917e-05,
      "loss": 1.0769,
      "step": 3963
    },
    {
      "epoch": 37.04715590922488,
      "grad_norm": 0.4697907865047455,
      "learning_rate": 1.4514670230670866e-05,
      "loss": 1.1215,
      "step": 3964
    },
    {
      "epoch": 37.05658709106985,
      "grad_norm": 0.4666653573513031,
      "learning_rate": 1.4511969181027225e-05,
      "loss": 1.1187,
      "step": 3965
    },
    {
      "epoch": 37.066018272914825,
      "grad_norm": 0.49644041061401367,
      "learning_rate": 1.4509267717998464e-05,
      "loss": 1.0494,
      "step": 3966
    },
    {
      "epoch": 37.0754494547598,
      "grad_norm": 0.5114052295684814,
      "learning_rate": 1.4506565841832084e-05,
      "loss": 1.0822,
      "step": 3967
    },
    {
      "epoch": 37.08488063660477,
      "grad_norm": 0.5341242551803589,
      "learning_rate": 1.4503863552775632e-05,
      "loss": 1.0573,
      "step": 3968
    },
    {
      "epoch": 37.09431181844975,
      "grad_norm": 0.4538983702659607,
      "learning_rate": 1.4501160851076694e-05,
      "loss": 1.1329,
      "step": 3969
    },
    {
      "epoch": 37.10374300029473,
      "grad_norm": 0.5046061873435974,
      "learning_rate": 1.4498457736982889e-05,
      "loss": 1.0309,
      "step": 3970
    },
    {
      "epoch": 37.1131741821397,
      "grad_norm": 0.4518008828163147,
      "learning_rate": 1.4495754210741875e-05,
      "loss": 1.0598,
      "step": 3971
    },
    {
      "epoch": 37.122605363984675,
      "grad_norm": 0.46312037110328674,
      "learning_rate": 1.449305027260135e-05,
      "loss": 1.0562,
      "step": 3972
    },
    {
      "epoch": 37.13203654582965,
      "grad_norm": 0.4957660138607025,
      "learning_rate": 1.4490345922809044e-05,
      "loss": 1.0738,
      "step": 3973
    },
    {
      "epoch": 37.14146772767462,
      "grad_norm": 0.574622392654419,
      "learning_rate": 1.4487641161612733e-05,
      "loss": 1.0617,
      "step": 3974
    },
    {
      "epoch": 37.1508989095196,
      "grad_norm": 0.5028202533721924,
      "learning_rate": 1.4484935989260225e-05,
      "loss": 1.0743,
      "step": 3975
    },
    {
      "epoch": 37.16033009136457,
      "grad_norm": 0.4837547540664673,
      "learning_rate": 1.4482230405999365e-05,
      "loss": 1.0552,
      "step": 3976
    },
    {
      "epoch": 37.16976127320955,
      "grad_norm": 0.44188907742500305,
      "learning_rate": 1.4479524412078039e-05,
      "loss": 1.0929,
      "step": 3977
    },
    {
      "epoch": 37.179192455054526,
      "grad_norm": 0.4879527688026428,
      "learning_rate": 1.447681800774417e-05,
      "loss": 1.0491,
      "step": 3978
    },
    {
      "epoch": 37.1886236368995,
      "grad_norm": 0.4947788417339325,
      "learning_rate": 1.4474111193245716e-05,
      "loss": 1.0697,
      "step": 3979
    },
    {
      "epoch": 37.198054818744474,
      "grad_norm": 0.5627430081367493,
      "learning_rate": 1.4471403968830678e-05,
      "loss": 1.0879,
      "step": 3980
    },
    {
      "epoch": 37.20748600058945,
      "grad_norm": 0.5619363784790039,
      "learning_rate": 1.4468696334747087e-05,
      "loss": 1.0557,
      "step": 3981
    },
    {
      "epoch": 37.21691718243442,
      "grad_norm": 0.468113511800766,
      "learning_rate": 1.4465988291243018e-05,
      "loss": 1.0875,
      "step": 3982
    },
    {
      "epoch": 37.226348364279396,
      "grad_norm": 0.48007065057754517,
      "learning_rate": 1.446327983856658e-05,
      "loss": 1.0966,
      "step": 3983
    },
    {
      "epoch": 37.23577954612438,
      "grad_norm": 0.5031555891036987,
      "learning_rate": 1.4460570976965919e-05,
      "loss": 1.0678,
      "step": 3984
    },
    {
      "epoch": 37.24521072796935,
      "grad_norm": 0.4977070391178131,
      "learning_rate": 1.4457861706689226e-05,
      "loss": 1.0394,
      "step": 3985
    },
    {
      "epoch": 37.254641909814325,
      "grad_norm": 0.4642651379108429,
      "learning_rate": 1.4455152027984717e-05,
      "loss": 1.0821,
      "step": 3986
    },
    {
      "epoch": 37.2640730916593,
      "grad_norm": 0.46007734537124634,
      "learning_rate": 1.4452441941100652e-05,
      "loss": 1.0837,
      "step": 3987
    },
    {
      "epoch": 37.27350427350427,
      "grad_norm": 0.5028204321861267,
      "learning_rate": 1.4449731446285334e-05,
      "loss": 1.0966,
      "step": 3988
    },
    {
      "epoch": 37.28293545534925,
      "grad_norm": 0.4636981785297394,
      "learning_rate": 1.4447020543787092e-05,
      "loss": 1.0848,
      "step": 3989
    },
    {
      "epoch": 37.29236663719422,
      "grad_norm": 0.4983311593532562,
      "learning_rate": 1.44443092338543e-05,
      "loss": 1.0512,
      "step": 3990
    },
    {
      "epoch": 37.3017978190392,
      "grad_norm": 0.48525285720825195,
      "learning_rate": 1.4441597516735368e-05,
      "loss": 1.0975,
      "step": 3991
    },
    {
      "epoch": 37.311229000884175,
      "grad_norm": 0.506684422492981,
      "learning_rate": 1.443888539267874e-05,
      "loss": 1.0531,
      "step": 3992
    },
    {
      "epoch": 37.32066018272915,
      "grad_norm": 0.49124670028686523,
      "learning_rate": 1.4436172861932906e-05,
      "loss": 1.066,
      "step": 3993
    },
    {
      "epoch": 37.33009136457412,
      "grad_norm": 0.48193320631980896,
      "learning_rate": 1.4433459924746378e-05,
      "loss": 1.1092,
      "step": 3994
    },
    {
      "epoch": 37.3395225464191,
      "grad_norm": 0.4760335087776184,
      "learning_rate": 1.4430746581367722e-05,
      "loss": 1.0916,
      "step": 3995
    },
    {
      "epoch": 37.34895372826407,
      "grad_norm": 0.4838182032108307,
      "learning_rate": 1.4428032832045533e-05,
      "loss": 1.0451,
      "step": 3996
    },
    {
      "epoch": 37.358384910109045,
      "grad_norm": 0.46926644444465637,
      "learning_rate": 1.4425318677028439e-05,
      "loss": 1.0587,
      "step": 3997
    },
    {
      "epoch": 37.367816091954026,
      "grad_norm": 0.5155407786369324,
      "learning_rate": 1.4422604116565114e-05,
      "loss": 1.0692,
      "step": 3998
    },
    {
      "epoch": 37.377247273799,
      "grad_norm": 0.4778248369693756,
      "learning_rate": 1.4419889150904263e-05,
      "loss": 1.0835,
      "step": 3999
    },
    {
      "epoch": 37.386678455643974,
      "grad_norm": 0.5005812644958496,
      "learning_rate": 1.4417173780294631e-05,
      "loss": 1.0792,
      "step": 4000
    },
    {
      "epoch": 37.39610963748895,
      "grad_norm": 0.4603773057460785,
      "learning_rate": 1.4414458004985e-05,
      "loss": 1.0536,
      "step": 4001
    },
    {
      "epoch": 37.40554081933392,
      "grad_norm": 0.48603081703186035,
      "learning_rate": 1.4411741825224188e-05,
      "loss": 1.0648,
      "step": 4002
    },
    {
      "epoch": 37.414972001178896,
      "grad_norm": 0.46857550740242004,
      "learning_rate": 1.4409025241261052e-05,
      "loss": 1.0137,
      "step": 4003
    },
    {
      "epoch": 37.42440318302387,
      "grad_norm": 0.5026954412460327,
      "learning_rate": 1.4406308253344481e-05,
      "loss": 1.038,
      "step": 4004
    },
    {
      "epoch": 37.43383436486885,
      "grad_norm": 0.44794991612434387,
      "learning_rate": 1.4403590861723409e-05,
      "loss": 1.0621,
      "step": 4005
    },
    {
      "epoch": 37.443265546713825,
      "grad_norm": 0.4679419994354248,
      "learning_rate": 1.4400873066646796e-05,
      "loss": 1.0835,
      "step": 4006
    },
    {
      "epoch": 37.4526967285588,
      "grad_norm": 0.48162850737571716,
      "learning_rate": 1.4398154868363654e-05,
      "loss": 1.1083,
      "step": 4007
    },
    {
      "epoch": 37.46212791040377,
      "grad_norm": 0.4870953857898712,
      "learning_rate": 1.4395436267123017e-05,
      "loss": 1.0712,
      "step": 4008
    },
    {
      "epoch": 37.47155909224875,
      "grad_norm": 0.4633422791957855,
      "learning_rate": 1.4392717263173964e-05,
      "loss": 1.119,
      "step": 4009
    },
    {
      "epoch": 37.48099027409372,
      "grad_norm": 0.4621087908744812,
      "learning_rate": 1.438999785676561e-05,
      "loss": 1.0789,
      "step": 4010
    },
    {
      "epoch": 37.490421455938694,
      "grad_norm": 0.4748963713645935,
      "learning_rate": 1.4387278048147106e-05,
      "loss": 1.088,
      "step": 4011
    },
    {
      "epoch": 37.499852637783675,
      "grad_norm": 0.5049076676368713,
      "learning_rate": 1.4384557837567639e-05,
      "loss": 1.0587,
      "step": 4012
    },
    {
      "epoch": 37.50928381962865,
      "grad_norm": 0.4655706584453583,
      "learning_rate": 1.4381837225276433e-05,
      "loss": 1.0446,
      "step": 4013
    },
    {
      "epoch": 37.51871500147362,
      "grad_norm": 0.5005182027816772,
      "learning_rate": 1.4379116211522755e-05,
      "loss": 1.0792,
      "step": 4014
    },
    {
      "epoch": 37.5281461833186,
      "grad_norm": 0.45585015416145325,
      "learning_rate": 1.43763947965559e-05,
      "loss": 1.0853,
      "step": 4015
    },
    {
      "epoch": 37.53757736516357,
      "grad_norm": 0.47666579484939575,
      "learning_rate": 1.4373672980625198e-05,
      "loss": 1.0849,
      "step": 4016
    },
    {
      "epoch": 37.547008547008545,
      "grad_norm": 0.4689919948577881,
      "learning_rate": 1.4370950763980027e-05,
      "loss": 1.042,
      "step": 4017
    },
    {
      "epoch": 37.55643972885352,
      "grad_norm": 0.5066158771514893,
      "learning_rate": 1.4368228146869793e-05,
      "loss": 1.0734,
      "step": 4018
    },
    {
      "epoch": 37.5658709106985,
      "grad_norm": 0.5005863308906555,
      "learning_rate": 1.4365505129543947e-05,
      "loss": 1.1122,
      "step": 4019
    },
    {
      "epoch": 37.575302092543474,
      "grad_norm": 0.4635745882987976,
      "learning_rate": 1.436278171225196e-05,
      "loss": 1.0243,
      "step": 4020
    },
    {
      "epoch": 37.58473327438845,
      "grad_norm": 0.4575798809528351,
      "learning_rate": 1.436005789524336e-05,
      "loss": 1.103,
      "step": 4021
    },
    {
      "epoch": 37.59416445623342,
      "grad_norm": 0.45291298627853394,
      "learning_rate": 1.43573336787677e-05,
      "loss": 1.0931,
      "step": 4022
    },
    {
      "epoch": 37.603595638078396,
      "grad_norm": 0.502837598323822,
      "learning_rate": 1.4354609063074572e-05,
      "loss": 1.06,
      "step": 4023
    },
    {
      "epoch": 37.61302681992337,
      "grad_norm": 0.49786239862442017,
      "learning_rate": 1.4351884048413598e-05,
      "loss": 1.0802,
      "step": 4024
    },
    {
      "epoch": 37.622458001768344,
      "grad_norm": 0.4758162796497345,
      "learning_rate": 1.4349158635034453e-05,
      "loss": 1.064,
      "step": 4025
    },
    {
      "epoch": 37.631889183613325,
      "grad_norm": 0.47382989525794983,
      "learning_rate": 1.4346432823186831e-05,
      "loss": 1.1112,
      "step": 4026
    },
    {
      "epoch": 37.6413203654583,
      "grad_norm": 0.4438668191432953,
      "learning_rate": 1.4343706613120477e-05,
      "loss": 1.0703,
      "step": 4027
    },
    {
      "epoch": 37.65075154730327,
      "grad_norm": 0.5360938310623169,
      "learning_rate": 1.4340980005085155e-05,
      "loss": 1.0372,
      "step": 4028
    },
    {
      "epoch": 37.66018272914825,
      "grad_norm": 0.4671329855918884,
      "learning_rate": 1.4338252999330684e-05,
      "loss": 1.0805,
      "step": 4029
    },
    {
      "epoch": 37.66961391099322,
      "grad_norm": 0.4929155707359314,
      "learning_rate": 1.4335525596106911e-05,
      "loss": 1.0485,
      "step": 4030
    },
    {
      "epoch": 37.679045092838194,
      "grad_norm": 0.48622751235961914,
      "learning_rate": 1.433279779566372e-05,
      "loss": 1.0842,
      "step": 4031
    },
    {
      "epoch": 37.68847627468317,
      "grad_norm": 0.45592039823532104,
      "learning_rate": 1.4330069598251024e-05,
      "loss": 1.0852,
      "step": 4032
    },
    {
      "epoch": 37.69790745652815,
      "grad_norm": 0.4562881290912628,
      "learning_rate": 1.4327341004118789e-05,
      "loss": 1.1112,
      "step": 4033
    },
    {
      "epoch": 37.70733863837312,
      "grad_norm": 0.4769052565097809,
      "learning_rate": 1.4324612013517e-05,
      "loss": 1.1033,
      "step": 4034
    },
    {
      "epoch": 37.7167698202181,
      "grad_norm": 0.4673609733581543,
      "learning_rate": 1.4321882626695693e-05,
      "loss": 1.0937,
      "step": 4035
    },
    {
      "epoch": 37.72620100206307,
      "grad_norm": 0.49705252051353455,
      "learning_rate": 1.4319152843904931e-05,
      "loss": 1.071,
      "step": 4036
    },
    {
      "epoch": 37.735632183908045,
      "grad_norm": 0.4930427372455597,
      "learning_rate": 1.4316422665394813e-05,
      "loss": 1.0707,
      "step": 4037
    },
    {
      "epoch": 37.74506336575302,
      "grad_norm": 0.4694805145263672,
      "learning_rate": 1.4313692091415483e-05,
      "loss": 1.0816,
      "step": 4038
    },
    {
      "epoch": 37.75449454759799,
      "grad_norm": 0.4723323881626129,
      "learning_rate": 1.4310961122217106e-05,
      "loss": 1.0634,
      "step": 4039
    },
    {
      "epoch": 37.763925729442974,
      "grad_norm": 0.5011675953865051,
      "learning_rate": 1.4308229758049902e-05,
      "loss": 1.1103,
      "step": 4040
    },
    {
      "epoch": 37.77335691128795,
      "grad_norm": 0.4849204421043396,
      "learning_rate": 1.4305497999164116e-05,
      "loss": 1.0443,
      "step": 4041
    },
    {
      "epoch": 37.78278809313292,
      "grad_norm": 0.4568411707878113,
      "learning_rate": 1.4302765845810025e-05,
      "loss": 1.0772,
      "step": 4042
    },
    {
      "epoch": 37.792219274977896,
      "grad_norm": 0.4975602328777313,
      "learning_rate": 1.4300033298237952e-05,
      "loss": 1.0613,
      "step": 4043
    },
    {
      "epoch": 37.80165045682287,
      "grad_norm": 0.44970524311065674,
      "learning_rate": 1.4297300356698254e-05,
      "loss": 1.0936,
      "step": 4044
    },
    {
      "epoch": 37.811081638667844,
      "grad_norm": 0.45926591753959656,
      "learning_rate": 1.429456702144132e-05,
      "loss": 1.0502,
      "step": 4045
    },
    {
      "epoch": 37.82051282051282,
      "grad_norm": 0.5075380206108093,
      "learning_rate": 1.4291833292717574e-05,
      "loss": 1.0341,
      "step": 4046
    },
    {
      "epoch": 37.8299440023578,
      "grad_norm": 0.4610663652420044,
      "learning_rate": 1.4289099170777483e-05,
      "loss": 1.0864,
      "step": 4047
    },
    {
      "epoch": 37.83937518420277,
      "grad_norm": 0.4532420039176941,
      "learning_rate": 1.4286364655871547e-05,
      "loss": 1.0523,
      "step": 4048
    },
    {
      "epoch": 37.84880636604775,
      "grad_norm": 0.5363383293151855,
      "learning_rate": 1.4283629748250302e-05,
      "loss": 1.0786,
      "step": 4049
    },
    {
      "epoch": 37.85823754789272,
      "grad_norm": 0.4791543781757355,
      "learning_rate": 1.4280894448164315e-05,
      "loss": 1.0421,
      "step": 4050
    },
    {
      "epoch": 37.867668729737694,
      "grad_norm": 0.49221861362457275,
      "learning_rate": 1.4278158755864198e-05,
      "loss": 1.0689,
      "step": 4051
    },
    {
      "epoch": 37.87709991158267,
      "grad_norm": 0.4607103765010834,
      "learning_rate": 1.4275422671600592e-05,
      "loss": 1.11,
      "step": 4052
    },
    {
      "epoch": 37.88653109342764,
      "grad_norm": 0.5076042413711548,
      "learning_rate": 1.4272686195624175e-05,
      "loss": 1.0424,
      "step": 4053
    },
    {
      "epoch": 37.89596227527262,
      "grad_norm": 0.5086580514907837,
      "learning_rate": 1.4269949328185661e-05,
      "loss": 1.0258,
      "step": 4054
    },
    {
      "epoch": 37.9053934571176,
      "grad_norm": 0.49492713809013367,
      "learning_rate": 1.4267212069535807e-05,
      "loss": 1.0775,
      "step": 4055
    },
    {
      "epoch": 37.91482463896257,
      "grad_norm": 0.4854685962200165,
      "learning_rate": 1.4264474419925395e-05,
      "loss": 1.1186,
      "step": 4056
    },
    {
      "epoch": 37.924255820807545,
      "grad_norm": 0.5487840175628662,
      "learning_rate": 1.4261736379605248e-05,
      "loss": 1.0919,
      "step": 4057
    },
    {
      "epoch": 37.93368700265252,
      "grad_norm": 0.4820915162563324,
      "learning_rate": 1.4258997948826225e-05,
      "loss": 1.0565,
      "step": 4058
    },
    {
      "epoch": 37.94311818449749,
      "grad_norm": 0.476529985666275,
      "learning_rate": 1.425625912783922e-05,
      "loss": 1.0579,
      "step": 4059
    },
    {
      "epoch": 37.95254936634247,
      "grad_norm": 0.567813515663147,
      "learning_rate": 1.4253519916895165e-05,
      "loss": 1.06,
      "step": 4060
    },
    {
      "epoch": 37.96198054818745,
      "grad_norm": 0.45499885082244873,
      "learning_rate": 1.4250780316245019e-05,
      "loss": 1.0279,
      "step": 4061
    },
    {
      "epoch": 37.97141173003242,
      "grad_norm": 0.49043628573417664,
      "learning_rate": 1.424804032613979e-05,
      "loss": 1.0615,
      "step": 4062
    },
    {
      "epoch": 37.980842911877396,
      "grad_norm": 0.4885886013507843,
      "learning_rate": 1.4245299946830511e-05,
      "loss": 1.1098,
      "step": 4063
    },
    {
      "epoch": 37.99027409372237,
      "grad_norm": 0.4721231162548065,
      "learning_rate": 1.4242559178568258e-05,
      "loss": 1.0388,
      "step": 4064
    },
    {
      "epoch": 37.999705275567344,
      "grad_norm": 0.4647626280784607,
      "learning_rate": 1.4239818021604137e-05,
      "loss": 1.1142,
      "step": 4065
    },
    {
      "epoch": 38.0,
      "grad_norm": 2.1578562259674072,
      "learning_rate": 1.4237076476189293e-05,
      "loss": 0.4693,
      "step": 4066
    },
    {
      "epoch": 38.009431181844974,
      "grad_norm": 0.4921598434448242,
      "learning_rate": 1.4234334542574906e-05,
      "loss": 1.0487,
      "step": 4067
    },
    {
      "epoch": 38.01886236368995,
      "grad_norm": 0.47267261147499084,
      "learning_rate": 1.4231592221012184e-05,
      "loss": 1.0399,
      "step": 4068
    },
    {
      "epoch": 38.02829354553492,
      "grad_norm": 0.4457780420780182,
      "learning_rate": 1.422884951175239e-05,
      "loss": 1.0934,
      "step": 4069
    },
    {
      "epoch": 38.0377247273799,
      "grad_norm": 0.47100797295570374,
      "learning_rate": 1.42261064150468e-05,
      "loss": 1.0869,
      "step": 4070
    },
    {
      "epoch": 38.04715590922488,
      "grad_norm": 0.48116493225097656,
      "learning_rate": 1.4223362931146742e-05,
      "loss": 1.0842,
      "step": 4071
    },
    {
      "epoch": 38.05658709106985,
      "grad_norm": 0.4604661464691162,
      "learning_rate": 1.4220619060303571e-05,
      "loss": 1.1064,
      "step": 4072
    },
    {
      "epoch": 38.066018272914825,
      "grad_norm": 0.49551519751548767,
      "learning_rate": 1.4217874802768676e-05,
      "loss": 1.0661,
      "step": 4073
    },
    {
      "epoch": 38.0754494547598,
      "grad_norm": 0.5397734642028809,
      "learning_rate": 1.4215130158793491e-05,
      "loss": 1.0466,
      "step": 4074
    },
    {
      "epoch": 38.08488063660477,
      "grad_norm": 0.5360039472579956,
      "learning_rate": 1.4212385128629475e-05,
      "loss": 1.0433,
      "step": 4075
    },
    {
      "epoch": 38.09431181844975,
      "grad_norm": 0.48785364627838135,
      "learning_rate": 1.4209639712528125e-05,
      "loss": 1.0572,
      "step": 4076
    },
    {
      "epoch": 38.10374300029473,
      "grad_norm": 0.5020986795425415,
      "learning_rate": 1.4206893910740982e-05,
      "loss": 1.1004,
      "step": 4077
    },
    {
      "epoch": 38.1131741821397,
      "grad_norm": 0.49893447756767273,
      "learning_rate": 1.4204147723519613e-05,
      "loss": 1.0584,
      "step": 4078
    },
    {
      "epoch": 38.122605363984675,
      "grad_norm": 0.5201642513275146,
      "learning_rate": 1.420140115111562e-05,
      "loss": 1.076,
      "step": 4079
    },
    {
      "epoch": 38.13203654582965,
      "grad_norm": 0.5262125134468079,
      "learning_rate": 1.4198654193780641e-05,
      "loss": 1.072,
      "step": 4080
    },
    {
      "epoch": 38.14146772767462,
      "grad_norm": 0.5003647804260254,
      "learning_rate": 1.419590685176636e-05,
      "loss": 1.0583,
      "step": 4081
    },
    {
      "epoch": 38.1508989095196,
      "grad_norm": 0.48676273226737976,
      "learning_rate": 1.4193159125324479e-05,
      "loss": 1.0942,
      "step": 4082
    },
    {
      "epoch": 38.16033009136457,
      "grad_norm": 0.4857475161552429,
      "learning_rate": 1.4190411014706746e-05,
      "loss": 1.0394,
      "step": 4083
    },
    {
      "epoch": 38.16976127320955,
      "grad_norm": 0.4860779047012329,
      "learning_rate": 1.4187662520164945e-05,
      "loss": 1.0733,
      "step": 4084
    },
    {
      "epoch": 38.179192455054526,
      "grad_norm": 0.5032294988632202,
      "learning_rate": 1.4184913641950891e-05,
      "loss": 1.0777,
      "step": 4085
    },
    {
      "epoch": 38.1886236368995,
      "grad_norm": 0.4625338315963745,
      "learning_rate": 1.4182164380316432e-05,
      "loss": 1.0932,
      "step": 4086
    },
    {
      "epoch": 38.198054818744474,
      "grad_norm": 0.46647608280181885,
      "learning_rate": 1.4179414735513462e-05,
      "loss": 1.0765,
      "step": 4087
    },
    {
      "epoch": 38.20748600058945,
      "grad_norm": 0.5004920363426208,
      "learning_rate": 1.417666470779389e-05,
      "loss": 1.0538,
      "step": 4088
    },
    {
      "epoch": 38.21691718243442,
      "grad_norm": 0.4897977411746979,
      "learning_rate": 1.4173914297409685e-05,
      "loss": 1.1066,
      "step": 4089
    },
    {
      "epoch": 38.226348364279396,
      "grad_norm": 0.4785718619823456,
      "learning_rate": 1.4171163504612836e-05,
      "loss": 1.0703,
      "step": 4090
    },
    {
      "epoch": 38.23577954612438,
      "grad_norm": 0.4886464774608612,
      "learning_rate": 1.4168412329655363e-05,
      "loss": 1.0859,
      "step": 4091
    },
    {
      "epoch": 38.24521072796935,
      "grad_norm": 0.4683260917663574,
      "learning_rate": 1.4165660772789337e-05,
      "loss": 1.0385,
      "step": 4092
    },
    {
      "epoch": 38.254641909814325,
      "grad_norm": 0.49708694219589233,
      "learning_rate": 1.4162908834266844e-05,
      "loss": 1.0491,
      "step": 4093
    },
    {
      "epoch": 38.2640730916593,
      "grad_norm": 0.48097914457321167,
      "learning_rate": 1.4160156514340027e-05,
      "loss": 1.0305,
      "step": 4094
    },
    {
      "epoch": 38.27350427350427,
      "grad_norm": 0.5106955766677856,
      "learning_rate": 1.4157403813261043e-05,
      "loss": 1.0333,
      "step": 4095
    },
    {
      "epoch": 38.28293545534925,
      "grad_norm": 0.5020928382873535,
      "learning_rate": 1.4154650731282102e-05,
      "loss": 1.0828,
      "step": 4096
    },
    {
      "epoch": 38.29236663719422,
      "grad_norm": 0.47324228286743164,
      "learning_rate": 1.4151897268655435e-05,
      "loss": 1.0664,
      "step": 4097
    },
    {
      "epoch": 38.3017978190392,
      "grad_norm": 0.44736558198928833,
      "learning_rate": 1.4149143425633313e-05,
      "loss": 1.0503,
      "step": 4098
    },
    {
      "epoch": 38.311229000884175,
      "grad_norm": 0.43035033345222473,
      "learning_rate": 1.4146389202468048e-05,
      "loss": 1.0502,
      "step": 4099
    },
    {
      "epoch": 38.32066018272915,
      "grad_norm": 0.467668354511261,
      "learning_rate": 1.4143634599411975e-05,
      "loss": 1.0675,
      "step": 4100
    },
    {
      "epoch": 38.33009136457412,
      "grad_norm": 0.4685802459716797,
      "learning_rate": 1.4140879616717468e-05,
      "loss": 1.0951,
      "step": 4101
    },
    {
      "epoch": 38.3395225464191,
      "grad_norm": 0.48231241106987,
      "learning_rate": 1.4138124254636947e-05,
      "loss": 1.0799,
      "step": 4102
    },
    {
      "epoch": 38.34895372826407,
      "grad_norm": 0.46454623341560364,
      "learning_rate": 1.4135368513422847e-05,
      "loss": 1.0658,
      "step": 4103
    },
    {
      "epoch": 38.358384910109045,
      "grad_norm": 0.509001612663269,
      "learning_rate": 1.4132612393327656e-05,
      "loss": 1.1023,
      "step": 4104
    },
    {
      "epoch": 38.367816091954026,
      "grad_norm": 0.48495614528656006,
      "learning_rate": 1.4129855894603885e-05,
      "loss": 1.0616,
      "step": 4105
    },
    {
      "epoch": 38.377247273799,
      "grad_norm": 0.4953501224517822,
      "learning_rate": 1.4127099017504083e-05,
      "loss": 1.0898,
      "step": 4106
    },
    {
      "epoch": 38.386678455643974,
      "grad_norm": 0.5215416550636292,
      "learning_rate": 1.4124341762280838e-05,
      "loss": 1.0317,
      "step": 4107
    },
    {
      "epoch": 38.39610963748895,
      "grad_norm": 0.5039876103401184,
      "learning_rate": 1.4121584129186765e-05,
      "loss": 1.0832,
      "step": 4108
    },
    {
      "epoch": 38.40554081933392,
      "grad_norm": 0.47791945934295654,
      "learning_rate": 1.4118826118474518e-05,
      "loss": 1.0632,
      "step": 4109
    },
    {
      "epoch": 38.414972001178896,
      "grad_norm": 0.47824567556381226,
      "learning_rate": 1.4116067730396785e-05,
      "loss": 1.0349,
      "step": 4110
    },
    {
      "epoch": 38.42440318302387,
      "grad_norm": 0.5087045431137085,
      "learning_rate": 1.411330896520629e-05,
      "loss": 1.0402,
      "step": 4111
    },
    {
      "epoch": 38.43383436486885,
      "grad_norm": 0.48715415596961975,
      "learning_rate": 1.4110549823155789e-05,
      "loss": 1.0794,
      "step": 4112
    },
    {
      "epoch": 38.443265546713825,
      "grad_norm": 0.48454639315605164,
      "learning_rate": 1.4107790304498075e-05,
      "loss": 1.0835,
      "step": 4113
    },
    {
      "epoch": 38.4526967285588,
      "grad_norm": 0.45621272921562195,
      "learning_rate": 1.4105030409485971e-05,
      "loss": 1.0563,
      "step": 4114
    },
    {
      "epoch": 38.46212791040377,
      "grad_norm": 0.4813968241214752,
      "learning_rate": 1.4102270138372343e-05,
      "loss": 1.0778,
      "step": 4115
    },
    {
      "epoch": 38.47155909224875,
      "grad_norm": 0.5355695486068726,
      "learning_rate": 1.4099509491410082e-05,
      "loss": 1.1032,
      "step": 4116
    },
    {
      "epoch": 38.48099027409372,
      "grad_norm": 0.4913100004196167,
      "learning_rate": 1.4096748468852118e-05,
      "loss": 1.0806,
      "step": 4117
    },
    {
      "epoch": 38.490421455938694,
      "grad_norm": 0.47578340768814087,
      "learning_rate": 1.4093987070951414e-05,
      "loss": 1.1024,
      "step": 4118
    },
    {
      "epoch": 38.499852637783675,
      "grad_norm": 0.5079699754714966,
      "learning_rate": 1.4091225297960971e-05,
      "loss": 1.0858,
      "step": 4119
    },
    {
      "epoch": 38.50928381962865,
      "grad_norm": 0.4756610095500946,
      "learning_rate": 1.4088463150133821e-05,
      "loss": 1.0743,
      "step": 4120
    },
    {
      "epoch": 38.51871500147362,
      "grad_norm": 0.5082915425300598,
      "learning_rate": 1.4085700627723032e-05,
      "loss": 1.0961,
      "step": 4121
    },
    {
      "epoch": 38.5281461833186,
      "grad_norm": 0.4711228311061859,
      "learning_rate": 1.40829377309817e-05,
      "loss": 1.1008,
      "step": 4122
    },
    {
      "epoch": 38.53757736516357,
      "grad_norm": 0.49937546253204346,
      "learning_rate": 1.408017446016297e-05,
      "loss": 1.0907,
      "step": 4123
    },
    {
      "epoch": 38.547008547008545,
      "grad_norm": 0.4837871491909027,
      "learning_rate": 1.4077410815520005e-05,
      "loss": 1.0775,
      "step": 4124
    },
    {
      "epoch": 38.55643972885352,
      "grad_norm": 0.47668761014938354,
      "learning_rate": 1.407464679730601e-05,
      "loss": 1.1081,
      "step": 4125
    },
    {
      "epoch": 38.5658709106985,
      "grad_norm": 0.5191237330436707,
      "learning_rate": 1.4071882405774224e-05,
      "loss": 1.0857,
      "step": 4126
    },
    {
      "epoch": 38.575302092543474,
      "grad_norm": 0.47241780161857605,
      "learning_rate": 1.406911764117792e-05,
      "loss": 1.0966,
      "step": 4127
    },
    {
      "epoch": 38.58473327438845,
      "grad_norm": 0.5127450227737427,
      "learning_rate": 1.4066352503770406e-05,
      "loss": 1.0234,
      "step": 4128
    },
    {
      "epoch": 38.59416445623342,
      "grad_norm": 0.47399258613586426,
      "learning_rate": 1.4063586993805021e-05,
      "loss": 1.1195,
      "step": 4129
    },
    {
      "epoch": 38.603595638078396,
      "grad_norm": 0.4540538191795349,
      "learning_rate": 1.4060821111535142e-05,
      "loss": 1.0627,
      "step": 4130
    },
    {
      "epoch": 38.61302681992337,
      "grad_norm": 0.48601609468460083,
      "learning_rate": 1.4058054857214176e-05,
      "loss": 1.0763,
      "step": 4131
    },
    {
      "epoch": 38.622458001768344,
      "grad_norm": 0.4540484547615051,
      "learning_rate": 1.4055288231095567e-05,
      "loss": 1.0736,
      "step": 4132
    },
    {
      "epoch": 38.631889183613325,
      "grad_norm": 0.48259928822517395,
      "learning_rate": 1.4052521233432794e-05,
      "loss": 1.0455,
      "step": 4133
    },
    {
      "epoch": 38.6413203654583,
      "grad_norm": 0.46738341450691223,
      "learning_rate": 1.4049753864479365e-05,
      "loss": 1.0704,
      "step": 4134
    },
    {
      "epoch": 38.65075154730327,
      "grad_norm": 0.45554402470588684,
      "learning_rate": 1.404698612448883e-05,
      "loss": 1.071,
      "step": 4135
    },
    {
      "epoch": 38.66018272914825,
      "grad_norm": 0.47893670201301575,
      "learning_rate": 1.4044218013714766e-05,
      "loss": 1.043,
      "step": 4136
    },
    {
      "epoch": 38.66961391099322,
      "grad_norm": 0.5277499556541443,
      "learning_rate": 1.4041449532410785e-05,
      "loss": 1.0499,
      "step": 4137
    },
    {
      "epoch": 38.679045092838194,
      "grad_norm": 0.5345359444618225,
      "learning_rate": 1.4038680680830536e-05,
      "loss": 1.0433,
      "step": 4138
    },
    {
      "epoch": 38.68847627468317,
      "grad_norm": 0.45316797494888306,
      "learning_rate": 1.4035911459227702e-05,
      "loss": 1.078,
      "step": 4139
    },
    {
      "epoch": 38.69790745652815,
      "grad_norm": 0.4721057415008545,
      "learning_rate": 1.4033141867855994e-05,
      "loss": 1.0568,
      "step": 4140
    },
    {
      "epoch": 38.70733863837312,
      "grad_norm": 0.48949307203292847,
      "learning_rate": 1.4030371906969164e-05,
      "loss": 1.1039,
      "step": 4141
    },
    {
      "epoch": 38.7167698202181,
      "grad_norm": 0.4789782762527466,
      "learning_rate": 1.4027601576820994e-05,
      "loss": 1.0449,
      "step": 4142
    },
    {
      "epoch": 38.72620100206307,
      "grad_norm": 0.5531871914863586,
      "learning_rate": 1.4024830877665303e-05,
      "loss": 1.0667,
      "step": 4143
    },
    {
      "epoch": 38.735632183908045,
      "grad_norm": 0.46835610270500183,
      "learning_rate": 1.4022059809755937e-05,
      "loss": 1.0906,
      "step": 4144
    },
    {
      "epoch": 38.74506336575302,
      "grad_norm": 0.49890562891960144,
      "learning_rate": 1.4019288373346784e-05,
      "loss": 1.088,
      "step": 4145
    },
    {
      "epoch": 38.75449454759799,
      "grad_norm": 0.5031705498695374,
      "learning_rate": 1.401651656869176e-05,
      "loss": 1.0602,
      "step": 4146
    },
    {
      "epoch": 38.763925729442974,
      "grad_norm": 0.4710695445537567,
      "learning_rate": 1.4013744396044816e-05,
      "loss": 1.1093,
      "step": 4147
    },
    {
      "epoch": 38.77335691128795,
      "grad_norm": 0.4911133944988251,
      "learning_rate": 1.4010971855659943e-05,
      "loss": 1.0732,
      "step": 4148
    },
    {
      "epoch": 38.78278809313292,
      "grad_norm": 0.5277432799339294,
      "learning_rate": 1.4008198947791156e-05,
      "loss": 1.0984,
      "step": 4149
    },
    {
      "epoch": 38.792219274977896,
      "grad_norm": 0.511116623878479,
      "learning_rate": 1.4005425672692506e-05,
      "loss": 1.0511,
      "step": 4150
    },
    {
      "epoch": 38.80165045682287,
      "grad_norm": 0.5055237412452698,
      "learning_rate": 1.4002652030618082e-05,
      "loss": 1.0632,
      "step": 4151
    },
    {
      "epoch": 38.811081638667844,
      "grad_norm": 0.4345136880874634,
      "learning_rate": 1.3999878021822006e-05,
      "loss": 1.0991,
      "step": 4152
    },
    {
      "epoch": 38.82051282051282,
      "grad_norm": 0.49841615557670593,
      "learning_rate": 1.3997103646558428e-05,
      "loss": 1.0674,
      "step": 4153
    },
    {
      "epoch": 38.8299440023578,
      "grad_norm": 0.5079799890518188,
      "learning_rate": 1.399432890508154e-05,
      "loss": 1.0612,
      "step": 4154
    },
    {
      "epoch": 38.83937518420277,
      "grad_norm": 0.5626371502876282,
      "learning_rate": 1.3991553797645556e-05,
      "loss": 1.0918,
      "step": 4155
    },
    {
      "epoch": 38.84880636604775,
      "grad_norm": 0.45625874400138855,
      "learning_rate": 1.3988778324504736e-05,
      "loss": 1.0881,
      "step": 4156
    },
    {
      "epoch": 38.85823754789272,
      "grad_norm": 0.5104401707649231,
      "learning_rate": 1.3986002485913368e-05,
      "loss": 1.0931,
      "step": 4157
    },
    {
      "epoch": 38.867668729737694,
      "grad_norm": 0.49136385321617126,
      "learning_rate": 1.3983226282125773e-05,
      "loss": 1.1055,
      "step": 4158
    },
    {
      "epoch": 38.87709991158267,
      "grad_norm": 0.5307900309562683,
      "learning_rate": 1.39804497133963e-05,
      "loss": 1.0636,
      "step": 4159
    },
    {
      "epoch": 38.88653109342764,
      "grad_norm": 0.48623430728912354,
      "learning_rate": 1.3977672779979346e-05,
      "loss": 1.0597,
      "step": 4160
    },
    {
      "epoch": 38.89596227527262,
      "grad_norm": 0.5021236538887024,
      "learning_rate": 1.3974895482129328e-05,
      "loss": 1.0392,
      "step": 4161
    },
    {
      "epoch": 38.9053934571176,
      "grad_norm": 0.5004566311836243,
      "learning_rate": 1.3972117820100699e-05,
      "loss": 1.0691,
      "step": 4162
    },
    {
      "epoch": 38.91482463896257,
      "grad_norm": 0.4674093425273895,
      "learning_rate": 1.3969339794147954e-05,
      "loss": 1.1129,
      "step": 4163
    },
    {
      "epoch": 38.924255820807545,
      "grad_norm": 0.4695524573326111,
      "learning_rate": 1.396656140452561e-05,
      "loss": 1.0943,
      "step": 4164
    },
    {
      "epoch": 38.93368700265252,
      "grad_norm": 0.48276078701019287,
      "learning_rate": 1.396378265148822e-05,
      "loss": 1.0651,
      "step": 4165
    },
    {
      "epoch": 38.94311818449749,
      "grad_norm": 0.48933082818984985,
      "learning_rate": 1.3961003535290378e-05,
      "loss": 1.0724,
      "step": 4166
    },
    {
      "epoch": 38.95254936634247,
      "grad_norm": 0.47450321912765503,
      "learning_rate": 1.3958224056186703e-05,
      "loss": 1.0459,
      "step": 4167
    },
    {
      "epoch": 38.96198054818745,
      "grad_norm": 0.4967683255672455,
      "learning_rate": 1.3955444214431853e-05,
      "loss": 1.1075,
      "step": 4168
    },
    {
      "epoch": 38.97141173003242,
      "grad_norm": 0.5043179392814636,
      "learning_rate": 1.3952664010280513e-05,
      "loss": 1.0679,
      "step": 4169
    },
    {
      "epoch": 38.980842911877396,
      "grad_norm": 0.5074142813682556,
      "learning_rate": 1.3949883443987402e-05,
      "loss": 1.0925,
      "step": 4170
    },
    {
      "epoch": 38.99027409372237,
      "grad_norm": 0.49540024995803833,
      "learning_rate": 1.3947102515807279e-05,
      "loss": 1.0601,
      "step": 4171
    },
    {
      "epoch": 38.999705275567344,
      "grad_norm": 0.4952608644962311,
      "learning_rate": 1.394432122599493e-05,
      "loss": 1.0899,
      "step": 4172
    },
    {
      "epoch": 39.0,
      "grad_norm": 2.1005072593688965,
      "learning_rate": 1.3941539574805178e-05,
      "loss": 0.7197,
      "step": 4173
    },
    {
      "epoch": 39.009431181844974,
      "grad_norm": 0.4755491614341736,
      "learning_rate": 1.3938757562492873e-05,
      "loss": 1.0719,
      "step": 4174
    },
    {
      "epoch": 39.01886236368995,
      "grad_norm": 0.4823411703109741,
      "learning_rate": 1.3935975189312904e-05,
      "loss": 1.0595,
      "step": 4175
    },
    {
      "epoch": 39.02829354553492,
      "grad_norm": 0.46426212787628174,
      "learning_rate": 1.3933192455520193e-05,
      "loss": 1.0868,
      "step": 4176
    },
    {
      "epoch": 39.0377247273799,
      "grad_norm": 0.477123886346817,
      "learning_rate": 1.393040936136969e-05,
      "loss": 1.0636,
      "step": 4177
    },
    {
      "epoch": 39.04715590922488,
      "grad_norm": 0.478142648935318,
      "learning_rate": 1.3927625907116383e-05,
      "loss": 1.0917,
      "step": 4178
    },
    {
      "epoch": 39.05658709106985,
      "grad_norm": 0.5430795550346375,
      "learning_rate": 1.392484209301529e-05,
      "loss": 1.0467,
      "step": 4179
    },
    {
      "epoch": 39.066018272914825,
      "grad_norm": 0.502923846244812,
      "learning_rate": 1.3922057919321466e-05,
      "loss": 1.1251,
      "step": 4180
    },
    {
      "epoch": 39.0754494547598,
      "grad_norm": 0.5186079144477844,
      "learning_rate": 1.391927338628999e-05,
      "loss": 1.0741,
      "step": 4181
    },
    {
      "epoch": 39.08488063660477,
      "grad_norm": 0.5201694369316101,
      "learning_rate": 1.3916488494175986e-05,
      "loss": 1.0737,
      "step": 4182
    },
    {
      "epoch": 39.09431181844975,
      "grad_norm": 0.5371423363685608,
      "learning_rate": 1.3913703243234605e-05,
      "loss": 1.09,
      "step": 4183
    },
    {
      "epoch": 39.10374300029473,
      "grad_norm": 0.4790119230747223,
      "learning_rate": 1.3910917633721024e-05,
      "loss": 1.0552,
      "step": 4184
    },
    {
      "epoch": 39.1131741821397,
      "grad_norm": 0.508850634098053,
      "learning_rate": 1.3908131665890465e-05,
      "loss": 1.0669,
      "step": 4185
    },
    {
      "epoch": 39.122605363984675,
      "grad_norm": 0.4645909070968628,
      "learning_rate": 1.390534533999818e-05,
      "loss": 1.0278,
      "step": 4186
    },
    {
      "epoch": 39.13203654582965,
      "grad_norm": 0.4931982755661011,
      "learning_rate": 1.3902558656299445e-05,
      "loss": 1.0006,
      "step": 4187
    },
    {
      "epoch": 39.14146772767462,
      "grad_norm": 0.45206284523010254,
      "learning_rate": 1.3899771615049577e-05,
      "loss": 1.0698,
      "step": 4188
    },
    {
      "epoch": 39.1508989095196,
      "grad_norm": 0.4950929284095764,
      "learning_rate": 1.3896984216503927e-05,
      "loss": 1.0727,
      "step": 4189
    },
    {
      "epoch": 39.16033009136457,
      "grad_norm": 0.5133909583091736,
      "learning_rate": 1.3894196460917875e-05,
      "loss": 1.0632,
      "step": 4190
    },
    {
      "epoch": 39.16976127320955,
      "grad_norm": 0.5438973307609558,
      "learning_rate": 1.3891408348546828e-05,
      "loss": 1.1118,
      "step": 4191
    },
    {
      "epoch": 39.179192455054526,
      "grad_norm": 0.48168715834617615,
      "learning_rate": 1.388861987964624e-05,
      "loss": 1.078,
      "step": 4192
    },
    {
      "epoch": 39.1886236368995,
      "grad_norm": 0.4515022933483124,
      "learning_rate": 1.3885831054471584e-05,
      "loss": 1.0983,
      "step": 4193
    },
    {
      "epoch": 39.198054818744474,
      "grad_norm": 0.5178540349006653,
      "learning_rate": 1.3883041873278373e-05,
      "loss": 1.0464,
      "step": 4194
    },
    {
      "epoch": 39.20748600058945,
      "grad_norm": 0.5091530084609985,
      "learning_rate": 1.3880252336322154e-05,
      "loss": 1.067,
      "step": 4195
    },
    {
      "epoch": 39.21691718243442,
      "grad_norm": 0.48521187901496887,
      "learning_rate": 1.3877462443858493e-05,
      "loss": 1.0895,
      "step": 4196
    },
    {
      "epoch": 39.226348364279396,
      "grad_norm": 0.5046826601028442,
      "learning_rate": 1.3874672196143013e-05,
      "loss": 1.0828,
      "step": 4197
    },
    {
      "epoch": 39.23577954612438,
      "grad_norm": 0.5232385396957397,
      "learning_rate": 1.3871881593431347e-05,
      "loss": 1.0797,
      "step": 4198
    },
    {
      "epoch": 39.24521072796935,
      "grad_norm": 0.5328229665756226,
      "learning_rate": 1.3869090635979173e-05,
      "loss": 1.0647,
      "step": 4199
    },
    {
      "epoch": 39.254641909814325,
      "grad_norm": 0.46476438641548157,
      "learning_rate": 1.3866299324042193e-05,
      "loss": 1.091,
      "step": 4200
    },
    {
      "epoch": 39.2640730916593,
      "grad_norm": 0.4797065854072571,
      "learning_rate": 1.3863507657876151e-05,
      "loss": 1.0368,
      "step": 4201
    },
    {
      "epoch": 39.27350427350427,
      "grad_norm": 0.50262850522995,
      "learning_rate": 1.3860715637736817e-05,
      "loss": 1.0628,
      "step": 4202
    },
    {
      "epoch": 39.28293545534925,
      "grad_norm": 0.4566088914871216,
      "learning_rate": 1.3857923263879994e-05,
      "loss": 1.0571,
      "step": 4203
    },
    {
      "epoch": 39.29236663719422,
      "grad_norm": 0.48227381706237793,
      "learning_rate": 1.3855130536561517e-05,
      "loss": 1.0941,
      "step": 4204
    },
    {
      "epoch": 39.3017978190392,
      "grad_norm": 0.46908724308013916,
      "learning_rate": 1.3852337456037259e-05,
      "loss": 1.06,
      "step": 4205
    },
    {
      "epoch": 39.311229000884175,
      "grad_norm": 0.49666255712509155,
      "learning_rate": 1.3849544022563117e-05,
      "loss": 1.0232,
      "step": 4206
    },
    {
      "epoch": 39.32066018272915,
      "grad_norm": 0.5345005393028259,
      "learning_rate": 1.3846750236395026e-05,
      "loss": 1.0768,
      "step": 4207
    },
    {
      "epoch": 39.33009136457412,
      "grad_norm": 0.4717165529727936,
      "learning_rate": 1.3843956097788954e-05,
      "loss": 1.1086,
      "step": 4208
    },
    {
      "epoch": 39.3395225464191,
      "grad_norm": 0.4978986978530884,
      "learning_rate": 1.3841161607000895e-05,
      "loss": 1.017,
      "step": 4209
    },
    {
      "epoch": 39.34895372826407,
      "grad_norm": 0.49729734659194946,
      "learning_rate": 1.3838366764286881e-05,
      "loss": 1.0435,
      "step": 4210
    },
    {
      "epoch": 39.358384910109045,
      "grad_norm": 0.5156709551811218,
      "learning_rate": 1.3835571569902975e-05,
      "loss": 1.0524,
      "step": 4211
    },
    {
      "epoch": 39.367816091954026,
      "grad_norm": 0.46623653173446655,
      "learning_rate": 1.3832776024105272e-05,
      "loss": 1.101,
      "step": 4212
    },
    {
      "epoch": 39.377247273799,
      "grad_norm": 0.4748134911060333,
      "learning_rate": 1.3829980127149898e-05,
      "loss": 1.0916,
      "step": 4213
    },
    {
      "epoch": 39.386678455643974,
      "grad_norm": 0.4918186366558075,
      "learning_rate": 1.3827183879293014e-05,
      "loss": 1.0635,
      "step": 4214
    },
    {
      "epoch": 39.39610963748895,
      "grad_norm": 0.5229716897010803,
      "learning_rate": 1.3824387280790807e-05,
      "loss": 1.0936,
      "step": 4215
    },
    {
      "epoch": 39.40554081933392,
      "grad_norm": 0.5025723576545715,
      "learning_rate": 1.3821590331899505e-05,
      "loss": 1.0607,
      "step": 4216
    },
    {
      "epoch": 39.414972001178896,
      "grad_norm": 0.4695991575717926,
      "learning_rate": 1.3818793032875366e-05,
      "loss": 1.0778,
      "step": 4217
    },
    {
      "epoch": 39.42440318302387,
      "grad_norm": 0.5079054832458496,
      "learning_rate": 1.3815995383974668e-05,
      "loss": 1.0788,
      "step": 4218
    },
    {
      "epoch": 39.43383436486885,
      "grad_norm": 0.5090555548667908,
      "learning_rate": 1.3813197385453739e-05,
      "loss": 1.0639,
      "step": 4219
    },
    {
      "epoch": 39.443265546713825,
      "grad_norm": 0.5142926573753357,
      "learning_rate": 1.3810399037568926e-05,
      "loss": 1.0553,
      "step": 4220
    },
    {
      "epoch": 39.4526967285588,
      "grad_norm": 0.5265275239944458,
      "learning_rate": 1.3807600340576617e-05,
      "loss": 1.0611,
      "step": 4221
    },
    {
      "epoch": 39.46212791040377,
      "grad_norm": 0.47455257177352905,
      "learning_rate": 1.3804801294733223e-05,
      "loss": 1.0854,
      "step": 4222
    },
    {
      "epoch": 39.47155909224875,
      "grad_norm": 0.5436558723449707,
      "learning_rate": 1.3802001900295196e-05,
      "loss": 1.0752,
      "step": 4223
    },
    {
      "epoch": 39.48099027409372,
      "grad_norm": 0.5080870985984802,
      "learning_rate": 1.3799202157519011e-05,
      "loss": 1.0871,
      "step": 4224
    },
    {
      "epoch": 39.490421455938694,
      "grad_norm": 0.5069491863250732,
      "learning_rate": 1.3796402066661186e-05,
      "loss": 1.0991,
      "step": 4225
    },
    {
      "epoch": 39.499852637783675,
      "grad_norm": 0.4601267874240875,
      "learning_rate": 1.3793601627978257e-05,
      "loss": 1.0645,
      "step": 4226
    },
    {
      "epoch": 39.50928381962865,
      "grad_norm": 0.5082706212997437,
      "learning_rate": 1.3790800841726804e-05,
      "loss": 1.0733,
      "step": 4227
    },
    {
      "epoch": 39.51871500147362,
      "grad_norm": 0.47981613874435425,
      "learning_rate": 1.3787999708163433e-05,
      "loss": 1.0974,
      "step": 4228
    },
    {
      "epoch": 39.5281461833186,
      "grad_norm": 0.563783586025238,
      "learning_rate": 1.3785198227544782e-05,
      "loss": 1.092,
      "step": 4229
    },
    {
      "epoch": 39.53757736516357,
      "grad_norm": 0.4635530710220337,
      "learning_rate": 1.3782396400127525e-05,
      "loss": 1.0905,
      "step": 4230
    },
    {
      "epoch": 39.547008547008545,
      "grad_norm": 0.46395936608314514,
      "learning_rate": 1.3779594226168356e-05,
      "loss": 1.1091,
      "step": 4231
    },
    {
      "epoch": 39.55643972885352,
      "grad_norm": 0.5224483609199524,
      "learning_rate": 1.3776791705924021e-05,
      "loss": 1.0891,
      "step": 4232
    },
    {
      "epoch": 39.5658709106985,
      "grad_norm": 0.5005354285240173,
      "learning_rate": 1.377398883965128e-05,
      "loss": 1.0459,
      "step": 4233
    },
    {
      "epoch": 39.575302092543474,
      "grad_norm": 0.4898999035358429,
      "learning_rate": 1.3771185627606926e-05,
      "loss": 1.0596,
      "step": 4234
    },
    {
      "epoch": 39.58473327438845,
      "grad_norm": 0.4606305658817291,
      "learning_rate": 1.37683820700478e-05,
      "loss": 1.0996,
      "step": 4235
    },
    {
      "epoch": 39.59416445623342,
      "grad_norm": 0.5015935301780701,
      "learning_rate": 1.3765578167230754e-05,
      "loss": 1.0221,
      "step": 4236
    },
    {
      "epoch": 39.603595638078396,
      "grad_norm": 0.48445653915405273,
      "learning_rate": 1.376277391941268e-05,
      "loss": 1.0885,
      "step": 4237
    },
    {
      "epoch": 39.61302681992337,
      "grad_norm": 0.464819997549057,
      "learning_rate": 1.3759969326850506e-05,
      "loss": 1.0749,
      "step": 4238
    },
    {
      "epoch": 39.622458001768344,
      "grad_norm": 0.4662446975708008,
      "learning_rate": 1.375716438980119e-05,
      "loss": 1.0736,
      "step": 4239
    },
    {
      "epoch": 39.631889183613325,
      "grad_norm": 0.5283907651901245,
      "learning_rate": 1.3754359108521715e-05,
      "loss": 1.0575,
      "step": 4240
    },
    {
      "epoch": 39.6413203654583,
      "grad_norm": 0.5104774236679077,
      "learning_rate": 1.37515534832691e-05,
      "loss": 1.0654,
      "step": 4241
    },
    {
      "epoch": 39.65075154730327,
      "grad_norm": 0.49448084831237793,
      "learning_rate": 1.3748747514300397e-05,
      "loss": 1.0574,
      "step": 4242
    },
    {
      "epoch": 39.66018272914825,
      "grad_norm": 0.47996458411216736,
      "learning_rate": 1.374594120187269e-05,
      "loss": 1.0641,
      "step": 4243
    },
    {
      "epoch": 39.66961391099322,
      "grad_norm": 0.4806230068206787,
      "learning_rate": 1.374313454624309e-05,
      "loss": 1.1006,
      "step": 4244
    },
    {
      "epoch": 39.679045092838194,
      "grad_norm": 0.5409398078918457,
      "learning_rate": 1.3740327547668739e-05,
      "loss": 1.0691,
      "step": 4245
    },
    {
      "epoch": 39.68847627468317,
      "grad_norm": 0.4570801854133606,
      "learning_rate": 1.3737520206406818e-05,
      "loss": 1.0684,
      "step": 4246
    },
    {
      "epoch": 39.69790745652815,
      "grad_norm": 0.5160848498344421,
      "learning_rate": 1.3734712522714535e-05,
      "loss": 1.0779,
      "step": 4247
    },
    {
      "epoch": 39.70733863837312,
      "grad_norm": 0.5076695084571838,
      "learning_rate": 1.3731904496849124e-05,
      "loss": 1.0407,
      "step": 4248
    },
    {
      "epoch": 39.7167698202181,
      "grad_norm": 0.4904089868068695,
      "learning_rate": 1.3729096129067861e-05,
      "loss": 1.0395,
      "step": 4249
    },
    {
      "epoch": 39.72620100206307,
      "grad_norm": 0.4676268994808197,
      "learning_rate": 1.3726287419628044e-05,
      "loss": 1.1074,
      "step": 4250
    },
    {
      "epoch": 39.735632183908045,
      "grad_norm": 0.47765782475471497,
      "learning_rate": 1.3723478368787008e-05,
      "loss": 1.0813,
      "step": 4251
    },
    {
      "epoch": 39.74506336575302,
      "grad_norm": 0.47419068217277527,
      "learning_rate": 1.3720668976802115e-05,
      "loss": 1.0735,
      "step": 4252
    },
    {
      "epoch": 39.75449454759799,
      "grad_norm": 0.4850064516067505,
      "learning_rate": 1.3717859243930765e-05,
      "loss": 1.1009,
      "step": 4253
    },
    {
      "epoch": 39.763925729442974,
      "grad_norm": 0.4861427843570709,
      "learning_rate": 1.3715049170430379e-05,
      "loss": 1.0753,
      "step": 4254
    },
    {
      "epoch": 39.77335691128795,
      "grad_norm": 0.48069459199905396,
      "learning_rate": 1.3712238756558418e-05,
      "loss": 1.0578,
      "step": 4255
    },
    {
      "epoch": 39.78278809313292,
      "grad_norm": 0.43649977445602417,
      "learning_rate": 1.370942800257237e-05,
      "loss": 1.0899,
      "step": 4256
    },
    {
      "epoch": 39.792219274977896,
      "grad_norm": 0.49424177408218384,
      "learning_rate": 1.3706616908729759e-05,
      "loss": 1.0524,
      "step": 4257
    },
    {
      "epoch": 39.80165045682287,
      "grad_norm": 0.521533191204071,
      "learning_rate": 1.3703805475288135e-05,
      "loss": 1.0823,
      "step": 4258
    },
    {
      "epoch": 39.811081638667844,
      "grad_norm": 0.5146252512931824,
      "learning_rate": 1.3700993702505076e-05,
      "loss": 1.0763,
      "step": 4259
    },
    {
      "epoch": 39.82051282051282,
      "grad_norm": 0.4708664119243622,
      "learning_rate": 1.3698181590638202e-05,
      "loss": 1.0731,
      "step": 4260
    },
    {
      "epoch": 39.8299440023578,
      "grad_norm": 0.484264612197876,
      "learning_rate": 1.3695369139945153e-05,
      "loss": 1.0655,
      "step": 4261
    },
    {
      "epoch": 39.83937518420277,
      "grad_norm": 0.4993264973163605,
      "learning_rate": 1.3692556350683608e-05,
      "loss": 1.0426,
      "step": 4262
    },
    {
      "epoch": 39.84880636604775,
      "grad_norm": 0.5039510130882263,
      "learning_rate": 1.3689743223111273e-05,
      "loss": 1.1016,
      "step": 4263
    },
    {
      "epoch": 39.85823754789272,
      "grad_norm": 0.4916107654571533,
      "learning_rate": 1.3686929757485883e-05,
      "loss": 1.0466,
      "step": 4264
    },
    {
      "epoch": 39.867668729737694,
      "grad_norm": 0.49925336241722107,
      "learning_rate": 1.368411595406521e-05,
      "loss": 1.0591,
      "step": 4265
    },
    {
      "epoch": 39.87709991158267,
      "grad_norm": 0.4738803505897522,
      "learning_rate": 1.3681301813107055e-05,
      "loss": 1.0827,
      "step": 4266
    },
    {
      "epoch": 39.88653109342764,
      "grad_norm": 0.5038799047470093,
      "learning_rate": 1.3678487334869247e-05,
      "loss": 1.0627,
      "step": 4267
    },
    {
      "epoch": 39.89596227527262,
      "grad_norm": 0.48568108677864075,
      "learning_rate": 1.3675672519609644e-05,
      "loss": 1.0739,
      "step": 4268
    },
    {
      "epoch": 39.9053934571176,
      "grad_norm": 0.5188812613487244,
      "learning_rate": 1.3672857367586143e-05,
      "loss": 1.057,
      "step": 4269
    },
    {
      "epoch": 39.91482463896257,
      "grad_norm": 0.5100230574607849,
      "learning_rate": 1.3670041879056668e-05,
      "loss": 1.0789,
      "step": 4270
    },
    {
      "epoch": 39.924255820807545,
      "grad_norm": 0.4659998416900635,
      "learning_rate": 1.3667226054279169e-05,
      "loss": 1.0617,
      "step": 4271
    },
    {
      "epoch": 39.93368700265252,
      "grad_norm": 0.5097382068634033,
      "learning_rate": 1.3664409893511633e-05,
      "loss": 1.0364,
      "step": 4272
    },
    {
      "epoch": 39.94311818449749,
      "grad_norm": 0.46051326394081116,
      "learning_rate": 1.3661593397012076e-05,
      "loss": 1.0698,
      "step": 4273
    },
    {
      "epoch": 39.95254936634247,
      "grad_norm": 0.4886883795261383,
      "learning_rate": 1.3658776565038543e-05,
      "loss": 1.0846,
      "step": 4274
    },
    {
      "epoch": 39.96198054818745,
      "grad_norm": 0.5174442529678345,
      "learning_rate": 1.3655959397849113e-05,
      "loss": 1.0439,
      "step": 4275
    },
    {
      "epoch": 39.97141173003242,
      "grad_norm": 0.485041081905365,
      "learning_rate": 1.3653141895701895e-05,
      "loss": 1.0763,
      "step": 4276
    },
    {
      "epoch": 39.980842911877396,
      "grad_norm": 0.5308175086975098,
      "learning_rate": 1.3650324058855025e-05,
      "loss": 1.0773,
      "step": 4277
    },
    {
      "epoch": 39.99027409372237,
      "grad_norm": 0.55327969789505,
      "learning_rate": 1.364750588756667e-05,
      "loss": 1.0608,
      "step": 4278
    },
    {
      "epoch": 39.999705275567344,
      "grad_norm": 0.5043714642524719,
      "learning_rate": 1.3644687382095039e-05,
      "loss": 1.0729,
      "step": 4279
    },
    {
      "epoch": 40.0,
      "grad_norm": 3.6580467224121094,
      "learning_rate": 1.3641868542698353e-05,
      "loss": 0.4921,
      "step": 4280
    },
    {
      "epoch": 40.009431181844974,
      "grad_norm": 0.5139085054397583,
      "learning_rate": 1.3639049369634878e-05,
      "loss": 0.9952,
      "step": 4281
    },
    {
      "epoch": 40.01886236368995,
      "grad_norm": 0.5020179748535156,
      "learning_rate": 1.3636229863162902e-05,
      "loss": 1.052,
      "step": 4282
    },
    {
      "epoch": 40.02829354553492,
      "grad_norm": 0.5104756355285645,
      "learning_rate": 1.3633410023540751e-05,
      "loss": 1.0472,
      "step": 4283
    },
    {
      "epoch": 40.0377247273799,
      "grad_norm": 0.5134881138801575,
      "learning_rate": 1.3630589851026778e-05,
      "loss": 1.1276,
      "step": 4284
    },
    {
      "epoch": 40.04715590922488,
      "grad_norm": 0.49833714962005615,
      "learning_rate": 1.3627769345879365e-05,
      "loss": 1.1109,
      "step": 4285
    },
    {
      "epoch": 40.05658709106985,
      "grad_norm": 0.5104328393936157,
      "learning_rate": 1.3624948508356924e-05,
      "loss": 1.0734,
      "step": 4286
    },
    {
      "epoch": 40.066018272914825,
      "grad_norm": 0.5066585540771484,
      "learning_rate": 1.3622127338717901e-05,
      "loss": 1.0706,
      "step": 4287
    },
    {
      "epoch": 40.0754494547598,
      "grad_norm": 0.45742449164390564,
      "learning_rate": 1.361930583722077e-05,
      "loss": 1.0735,
      "step": 4288
    },
    {
      "epoch": 40.08488063660477,
      "grad_norm": 0.4481666088104248,
      "learning_rate": 1.3616484004124036e-05,
      "loss": 1.112,
      "step": 4289
    },
    {
      "epoch": 40.09431181844975,
      "grad_norm": 0.48604658246040344,
      "learning_rate": 1.3613661839686236e-05,
      "loss": 1.0703,
      "step": 4290
    },
    {
      "epoch": 40.10374300029473,
      "grad_norm": 0.5206895470619202,
      "learning_rate": 1.3610839344165936e-05,
      "loss": 1.0681,
      "step": 4291
    },
    {
      "epoch": 40.1131741821397,
      "grad_norm": 0.5096409320831299,
      "learning_rate": 1.360801651782173e-05,
      "loss": 1.0511,
      "step": 4292
    },
    {
      "epoch": 40.122605363984675,
      "grad_norm": 0.4855838716030121,
      "learning_rate": 1.360519336091224e-05,
      "loss": 1.0638,
      "step": 4293
    },
    {
      "epoch": 40.13203654582965,
      "grad_norm": 0.4896095097064972,
      "learning_rate": 1.3602369873696132e-05,
      "loss": 1.0397,
      "step": 4294
    },
    {
      "epoch": 40.14146772767462,
      "grad_norm": 0.5114918947219849,
      "learning_rate": 1.359954605643209e-05,
      "loss": 1.0649,
      "step": 4295
    },
    {
      "epoch": 40.1508989095196,
      "grad_norm": 0.4995691478252411,
      "learning_rate": 1.3596721909378827e-05,
      "loss": 1.1028,
      "step": 4296
    },
    {
      "epoch": 40.16033009136457,
      "grad_norm": 0.5191645622253418,
      "learning_rate": 1.3593897432795098e-05,
      "loss": 1.1061,
      "step": 4297
    },
    {
      "epoch": 40.16976127320955,
      "grad_norm": 0.48310616612434387,
      "learning_rate": 1.359107262693967e-05,
      "loss": 1.0667,
      "step": 4298
    },
    {
      "epoch": 40.179192455054526,
      "grad_norm": 0.5166624784469604,
      "learning_rate": 1.358824749207136e-05,
      "loss": 1.0553,
      "step": 4299
    },
    {
      "epoch": 40.1886236368995,
      "grad_norm": 0.4981468617916107,
      "learning_rate": 1.3585422028449e-05,
      "loss": 1.0985,
      "step": 4300
    },
    {
      "epoch": 40.198054818744474,
      "grad_norm": 0.5294782519340515,
      "learning_rate": 1.3582596236331464e-05,
      "loss": 1.0188,
      "step": 4301
    },
    {
      "epoch": 40.20748600058945,
      "grad_norm": 0.4757855236530304,
      "learning_rate": 1.3579770115977646e-05,
      "loss": 1.0361,
      "step": 4302
    },
    {
      "epoch": 40.21691718243442,
      "grad_norm": 0.4656246602535248,
      "learning_rate": 1.3576943667646476e-05,
      "loss": 1.0476,
      "step": 4303
    },
    {
      "epoch": 40.226348364279396,
      "grad_norm": 0.4913504421710968,
      "learning_rate": 1.357411689159691e-05,
      "loss": 1.0711,
      "step": 4304
    },
    {
      "epoch": 40.23577954612438,
      "grad_norm": 0.5240936279296875,
      "learning_rate": 1.357128978808794e-05,
      "loss": 1.0948,
      "step": 4305
    },
    {
      "epoch": 40.24521072796935,
      "grad_norm": 0.4790472686290741,
      "learning_rate": 1.3568462357378581e-05,
      "loss": 1.1296,
      "step": 4306
    },
    {
      "epoch": 40.254641909814325,
      "grad_norm": 0.4927250146865845,
      "learning_rate": 1.3565634599727879e-05,
      "loss": 1.097,
      "step": 4307
    },
    {
      "epoch": 40.2640730916593,
      "grad_norm": 0.4858308732509613,
      "learning_rate": 1.3562806515394918e-05,
      "loss": 1.0602,
      "step": 4308
    },
    {
      "epoch": 40.27350427350427,
      "grad_norm": 0.48828908801078796,
      "learning_rate": 1.3559978104638802e-05,
      "loss": 1.1044,
      "step": 4309
    },
    {
      "epoch": 40.28293545534925,
      "grad_norm": 0.509000301361084,
      "learning_rate": 1.3557149367718675e-05,
      "loss": 1.0119,
      "step": 4310
    },
    {
      "epoch": 40.29236663719422,
      "grad_norm": 0.5056911706924438,
      "learning_rate": 1.3554320304893695e-05,
      "loss": 1.0866,
      "step": 4311
    },
    {
      "epoch": 40.3017978190392,
      "grad_norm": 0.4928590655326843,
      "learning_rate": 1.3551490916423067e-05,
      "loss": 1.0578,
      "step": 4312
    },
    {
      "epoch": 40.311229000884175,
      "grad_norm": 0.48756536841392517,
      "learning_rate": 1.3548661202566018e-05,
      "loss": 1.0693,
      "step": 4313
    },
    {
      "epoch": 40.32066018272915,
      "grad_norm": 0.483521044254303,
      "learning_rate": 1.3545831163581805e-05,
      "loss": 1.0676,
      "step": 4314
    },
    {
      "epoch": 40.33009136457412,
      "grad_norm": 0.4916403889656067,
      "learning_rate": 1.3543000799729714e-05,
      "loss": 1.086,
      "step": 4315
    },
    {
      "epoch": 40.3395225464191,
      "grad_norm": 0.5203500986099243,
      "learning_rate": 1.354017011126906e-05,
      "loss": 1.046,
      "step": 4316
    },
    {
      "epoch": 40.34895372826407,
      "grad_norm": 0.5591743588447571,
      "learning_rate": 1.3537339098459196e-05,
      "loss": 1.0218,
      "step": 4317
    },
    {
      "epoch": 40.358384910109045,
      "grad_norm": 0.5058739185333252,
      "learning_rate": 1.3534507761559494e-05,
      "loss": 1.081,
      "step": 4318
    },
    {
      "epoch": 40.367816091954026,
      "grad_norm": 0.46517428755760193,
      "learning_rate": 1.3531676100829361e-05,
      "loss": 1.0926,
      "step": 4319
    },
    {
      "epoch": 40.377247273799,
      "grad_norm": 0.49457311630249023,
      "learning_rate": 1.3528844116528232e-05,
      "loss": 1.1029,
      "step": 4320
    },
    {
      "epoch": 40.386678455643974,
      "grad_norm": 0.4683007299900055,
      "learning_rate": 1.3526011808915574e-05,
      "loss": 1.0719,
      "step": 4321
    },
    {
      "epoch": 40.39610963748895,
      "grad_norm": 0.5338796377182007,
      "learning_rate": 1.3523179178250883e-05,
      "loss": 1.0596,
      "step": 4322
    },
    {
      "epoch": 40.40554081933392,
      "grad_norm": 0.45976775884628296,
      "learning_rate": 1.3520346224793678e-05,
      "loss": 1.0782,
      "step": 4323
    },
    {
      "epoch": 40.414972001178896,
      "grad_norm": 0.49394357204437256,
      "learning_rate": 1.3517512948803519e-05,
      "loss": 1.0652,
      "step": 4324
    },
    {
      "epoch": 40.42440318302387,
      "grad_norm": 0.5040791034698486,
      "learning_rate": 1.351467935053999e-05,
      "loss": 1.0444,
      "step": 4325
    },
    {
      "epoch": 40.43383436486885,
      "grad_norm": 0.4945686459541321,
      "learning_rate": 1.35118454302627e-05,
      "loss": 1.0508,
      "step": 4326
    },
    {
      "epoch": 40.443265546713825,
      "grad_norm": 0.4521924555301666,
      "learning_rate": 1.3509011188231296e-05,
      "loss": 1.0882,
      "step": 4327
    },
    {
      "epoch": 40.4526967285588,
      "grad_norm": 0.4902913272380829,
      "learning_rate": 1.3506176624705444e-05,
      "loss": 1.0666,
      "step": 4328
    },
    {
      "epoch": 40.46212791040377,
      "grad_norm": 0.45315322279930115,
      "learning_rate": 1.3503341739944857e-05,
      "loss": 1.0608,
      "step": 4329
    },
    {
      "epoch": 40.47155909224875,
      "grad_norm": 0.5374978184700012,
      "learning_rate": 1.3500506534209259e-05,
      "loss": 1.026,
      "step": 4330
    },
    {
      "epoch": 40.48099027409372,
      "grad_norm": 0.4927510619163513,
      "learning_rate": 1.3497671007758405e-05,
      "loss": 1.0347,
      "step": 4331
    },
    {
      "epoch": 40.490421455938694,
      "grad_norm": 0.5634987950325012,
      "learning_rate": 1.3494835160852098e-05,
      "loss": 1.0282,
      "step": 4332
    },
    {
      "epoch": 40.499852637783675,
      "grad_norm": 0.516739547252655,
      "learning_rate": 1.3491998993750148e-05,
      "loss": 1.0747,
      "step": 4333
    },
    {
      "epoch": 40.50928381962865,
      "grad_norm": 0.45718783140182495,
      "learning_rate": 1.3489162506712406e-05,
      "loss": 1.0757,
      "step": 4334
    },
    {
      "epoch": 40.51871500147362,
      "grad_norm": 0.5022515058517456,
      "learning_rate": 1.348632569999875e-05,
      "loss": 1.0894,
      "step": 4335
    },
    {
      "epoch": 40.5281461833186,
      "grad_norm": 0.5200893878936768,
      "learning_rate": 1.3483488573869089e-05,
      "loss": 1.0584,
      "step": 4336
    },
    {
      "epoch": 40.53757736516357,
      "grad_norm": 0.4679352343082428,
      "learning_rate": 1.3480651128583358e-05,
      "loss": 1.0921,
      "step": 4337
    },
    {
      "epoch": 40.547008547008545,
      "grad_norm": 0.5026217699050903,
      "learning_rate": 1.347781336440152e-05,
      "loss": 1.0833,
      "step": 4338
    },
    {
      "epoch": 40.55643972885352,
      "grad_norm": 0.5285956859588623,
      "learning_rate": 1.3474975281583576e-05,
      "loss": 1.0934,
      "step": 4339
    },
    {
      "epoch": 40.5658709106985,
      "grad_norm": 0.48018679022789,
      "learning_rate": 1.3472136880389546e-05,
      "loss": 1.0848,
      "step": 4340
    },
    {
      "epoch": 40.575302092543474,
      "grad_norm": 0.46937692165374756,
      "learning_rate": 1.3469298161079484e-05,
      "loss": 1.0781,
      "step": 4341
    },
    {
      "epoch": 40.58473327438845,
      "grad_norm": 0.5001813769340515,
      "learning_rate": 1.3466459123913473e-05,
      "loss": 1.0944,
      "step": 4342
    },
    {
      "epoch": 40.59416445623342,
      "grad_norm": 0.46633997559547424,
      "learning_rate": 1.3463619769151625e-05,
      "loss": 1.0831,
      "step": 4343
    },
    {
      "epoch": 40.603595638078396,
      "grad_norm": 0.47603562474250793,
      "learning_rate": 1.3460780097054082e-05,
      "loss": 1.0651,
      "step": 4344
    },
    {
      "epoch": 40.61302681992337,
      "grad_norm": 0.5001668334007263,
      "learning_rate": 1.345794010788101e-05,
      "loss": 1.0521,
      "step": 4345
    },
    {
      "epoch": 40.622458001768344,
      "grad_norm": 0.48174211382865906,
      "learning_rate": 1.345509980189261e-05,
      "loss": 1.0736,
      "step": 4346
    },
    {
      "epoch": 40.631889183613325,
      "grad_norm": 0.49315863847732544,
      "learning_rate": 1.3452259179349112e-05,
      "loss": 1.0823,
      "step": 4347
    },
    {
      "epoch": 40.6413203654583,
      "grad_norm": 0.4922042787075043,
      "learning_rate": 1.344941824051077e-05,
      "loss": 1.0999,
      "step": 4348
    },
    {
      "epoch": 40.65075154730327,
      "grad_norm": 0.4738619327545166,
      "learning_rate": 1.344657698563787e-05,
      "loss": 1.0874,
      "step": 4349
    },
    {
      "epoch": 40.66018272914825,
      "grad_norm": 0.47692930698394775,
      "learning_rate": 1.3443735414990728e-05,
      "loss": 1.0276,
      "step": 4350
    },
    {
      "epoch": 40.66961391099322,
      "grad_norm": 0.5006148219108582,
      "learning_rate": 1.3440893528829689e-05,
      "loss": 1.0702,
      "step": 4351
    },
    {
      "epoch": 40.679045092838194,
      "grad_norm": 0.4828684628009796,
      "learning_rate": 1.3438051327415123e-05,
      "loss": 1.0717,
      "step": 4352
    },
    {
      "epoch": 40.68847627468317,
      "grad_norm": 0.47380509972572327,
      "learning_rate": 1.343520881100743e-05,
      "loss": 1.09,
      "step": 4353
    },
    {
      "epoch": 40.69790745652815,
      "grad_norm": 0.4919281303882599,
      "learning_rate": 1.3432365979867046e-05,
      "loss": 1.0955,
      "step": 4354
    },
    {
      "epoch": 40.70733863837312,
      "grad_norm": 0.4721389412879944,
      "learning_rate": 1.3429522834254426e-05,
      "loss": 1.0747,
      "step": 4355
    },
    {
      "epoch": 40.7167698202181,
      "grad_norm": 0.4847375154495239,
      "learning_rate": 1.3426679374430058e-05,
      "loss": 1.0329,
      "step": 4356
    },
    {
      "epoch": 40.72620100206307,
      "grad_norm": 0.4556761682033539,
      "learning_rate": 1.3423835600654462e-05,
      "loss": 1.0612,
      "step": 4357
    },
    {
      "epoch": 40.735632183908045,
      "grad_norm": 0.49318456649780273,
      "learning_rate": 1.3420991513188182e-05,
      "loss": 1.1145,
      "step": 4358
    },
    {
      "epoch": 40.74506336575302,
      "grad_norm": 0.502659022808075,
      "learning_rate": 1.3418147112291791e-05,
      "loss": 1.0468,
      "step": 4359
    },
    {
      "epoch": 40.75449454759799,
      "grad_norm": 0.5048707127571106,
      "learning_rate": 1.3415302398225895e-05,
      "loss": 1.0844,
      "step": 4360
    },
    {
      "epoch": 40.763925729442974,
      "grad_norm": 0.5061051249504089,
      "learning_rate": 1.3412457371251124e-05,
      "loss": 1.0635,
      "step": 4361
    },
    {
      "epoch": 40.77335691128795,
      "grad_norm": 0.48292306065559387,
      "learning_rate": 1.3409612031628135e-05,
      "loss": 1.0816,
      "step": 4362
    },
    {
      "epoch": 40.78278809313292,
      "grad_norm": 0.5312627553939819,
      "learning_rate": 1.3406766379617621e-05,
      "loss": 1.0747,
      "step": 4363
    },
    {
      "epoch": 40.792219274977896,
      "grad_norm": 0.4914393723011017,
      "learning_rate": 1.34039204154803e-05,
      "loss": 1.0948,
      "step": 4364
    },
    {
      "epoch": 40.80165045682287,
      "grad_norm": 0.5208936333656311,
      "learning_rate": 1.3401074139476919e-05,
      "loss": 1.0879,
      "step": 4365
    },
    {
      "epoch": 40.811081638667844,
      "grad_norm": 0.4803810119628906,
      "learning_rate": 1.339822755186825e-05,
      "loss": 1.0861,
      "step": 4366
    },
    {
      "epoch": 40.82051282051282,
      "grad_norm": 0.4841412305831909,
      "learning_rate": 1.3395380652915099e-05,
      "loss": 1.0489,
      "step": 4367
    },
    {
      "epoch": 40.8299440023578,
      "grad_norm": 0.4655526876449585,
      "learning_rate": 1.3392533442878294e-05,
      "loss": 1.07,
      "step": 4368
    },
    {
      "epoch": 40.83937518420277,
      "grad_norm": 0.5282781720161438,
      "learning_rate": 1.3389685922018703e-05,
      "loss": 1.0742,
      "step": 4369
    },
    {
      "epoch": 40.84880636604775,
      "grad_norm": 0.47977280616760254,
      "learning_rate": 1.3386838090597207e-05,
      "loss": 1.1166,
      "step": 4370
    },
    {
      "epoch": 40.85823754789272,
      "grad_norm": 0.5450972318649292,
      "learning_rate": 1.3383989948874723e-05,
      "loss": 1.0829,
      "step": 4371
    },
    {
      "epoch": 40.867668729737694,
      "grad_norm": 0.5118150115013123,
      "learning_rate": 1.3381141497112204e-05,
      "loss": 1.0494,
      "step": 4372
    },
    {
      "epoch": 40.87709991158267,
      "grad_norm": 0.5063422918319702,
      "learning_rate": 1.3378292735570621e-05,
      "loss": 1.0656,
      "step": 4373
    },
    {
      "epoch": 40.88653109342764,
      "grad_norm": 0.48180773854255676,
      "learning_rate": 1.3375443664510974e-05,
      "loss": 1.0331,
      "step": 4374
    },
    {
      "epoch": 40.89596227527262,
      "grad_norm": 0.4730941951274872,
      "learning_rate": 1.3372594284194295e-05,
      "loss": 1.0884,
      "step": 4375
    },
    {
      "epoch": 40.9053934571176,
      "grad_norm": 0.57681804895401,
      "learning_rate": 1.3369744594881648e-05,
      "loss": 1.0852,
      "step": 4376
    },
    {
      "epoch": 40.91482463896257,
      "grad_norm": 0.5095327496528625,
      "learning_rate": 1.3366894596834115e-05,
      "loss": 1.0656,
      "step": 4377
    },
    {
      "epoch": 40.924255820807545,
      "grad_norm": 0.5220453143119812,
      "learning_rate": 1.3364044290312814e-05,
      "loss": 1.0332,
      "step": 4378
    },
    {
      "epoch": 40.93368700265252,
      "grad_norm": 0.5253127217292786,
      "learning_rate": 1.3361193675578887e-05,
      "loss": 1.0359,
      "step": 4379
    },
    {
      "epoch": 40.94311818449749,
      "grad_norm": 0.5400444269180298,
      "learning_rate": 1.335834275289351e-05,
      "loss": 1.0563,
      "step": 4380
    },
    {
      "epoch": 40.95254936634247,
      "grad_norm": 0.5181432366371155,
      "learning_rate": 1.335549152251788e-05,
      "loss": 1.0846,
      "step": 4381
    },
    {
      "epoch": 40.96198054818745,
      "grad_norm": 0.4853573441505432,
      "learning_rate": 1.335263998471323e-05,
      "loss": 1.0535,
      "step": 4382
    },
    {
      "epoch": 40.97141173003242,
      "grad_norm": 0.5111157298088074,
      "learning_rate": 1.3349788139740812e-05,
      "loss": 1.0584,
      "step": 4383
    },
    {
      "epoch": 40.980842911877396,
      "grad_norm": 0.5048902034759521,
      "learning_rate": 1.3346935987861916e-05,
      "loss": 1.0583,
      "step": 4384
    },
    {
      "epoch": 40.99027409372237,
      "grad_norm": 0.48869431018829346,
      "learning_rate": 1.3344083529337852e-05,
      "loss": 1.0385,
      "step": 4385
    },
    {
      "epoch": 40.999705275567344,
      "grad_norm": 0.47885873913764954,
      "learning_rate": 1.334123076442996e-05,
      "loss": 1.0632,
      "step": 4386
    },
    {
      "epoch": 41.0,
      "grad_norm": 2.3988120555877686,
      "learning_rate": 1.3338377693399614e-05,
      "loss": 0.5345,
      "step": 4387
    },
    {
      "epoch": 41.009431181844974,
      "grad_norm": 0.5184801816940308,
      "learning_rate": 1.3335524316508208e-05,
      "loss": 1.0264,
      "step": 4388
    },
    {
      "epoch": 41.01886236368995,
      "grad_norm": 0.4916328191757202,
      "learning_rate": 1.3332670634017171e-05,
      "loss": 1.0868,
      "step": 4389
    },
    {
      "epoch": 41.02829354553492,
      "grad_norm": 0.5264792442321777,
      "learning_rate": 1.332981664618795e-05,
      "loss": 1.0459,
      "step": 4390
    },
    {
      "epoch": 41.0377247273799,
      "grad_norm": 0.5072145462036133,
      "learning_rate": 1.3326962353282032e-05,
      "loss": 1.0862,
      "step": 4391
    },
    {
      "epoch": 41.04715590922488,
      "grad_norm": 0.4752058684825897,
      "learning_rate": 1.3324107755560922e-05,
      "loss": 1.0123,
      "step": 4392
    },
    {
      "epoch": 41.05658709106985,
      "grad_norm": 0.4917624294757843,
      "learning_rate": 1.3321252853286162e-05,
      "loss": 1.0372,
      "step": 4393
    },
    {
      "epoch": 41.066018272914825,
      "grad_norm": 0.4744899868965149,
      "learning_rate": 1.3318397646719316e-05,
      "loss": 1.0489,
      "step": 4394
    },
    {
      "epoch": 41.0754494547598,
      "grad_norm": 0.5247185826301575,
      "learning_rate": 1.3315542136121972e-05,
      "loss": 1.0375,
      "step": 4395
    },
    {
      "epoch": 41.08488063660477,
      "grad_norm": 0.5222724080085754,
      "learning_rate": 1.331268632175576e-05,
      "loss": 1.0557,
      "step": 4396
    },
    {
      "epoch": 41.09431181844975,
      "grad_norm": 0.5501579642295837,
      "learning_rate": 1.3309830203882324e-05,
      "loss": 1.0807,
      "step": 4397
    },
    {
      "epoch": 41.10374300029473,
      "grad_norm": 0.4926486611366272,
      "learning_rate": 1.3306973782763337e-05,
      "loss": 1.0474,
      "step": 4398
    },
    {
      "epoch": 41.1131741821397,
      "grad_norm": 0.5145727396011353,
      "learning_rate": 1.3304117058660508e-05,
      "loss": 1.0483,
      "step": 4399
    },
    {
      "epoch": 41.122605363984675,
      "grad_norm": 0.472085177898407,
      "learning_rate": 1.330126003183557e-05,
      "loss": 1.064,
      "step": 4400
    },
    {
      "epoch": 41.13203654582965,
      "grad_norm": 0.5105521082878113,
      "learning_rate": 1.3298402702550277e-05,
      "loss": 1.077,
      "step": 4401
    },
    {
      "epoch": 41.14146772767462,
      "grad_norm": 0.47459644079208374,
      "learning_rate": 1.3295545071066425e-05,
      "loss": 1.0655,
      "step": 4402
    },
    {
      "epoch": 41.1508989095196,
      "grad_norm": 0.4633348286151886,
      "learning_rate": 1.3292687137645824e-05,
      "loss": 1.0388,
      "step": 4403
    },
    {
      "epoch": 41.16033009136457,
      "grad_norm": 0.4892977178096771,
      "learning_rate": 1.3289828902550315e-05,
      "loss": 1.0562,
      "step": 4404
    },
    {
      "epoch": 41.16976127320955,
      "grad_norm": 0.5088832378387451,
      "learning_rate": 1.3286970366041771e-05,
      "loss": 1.0956,
      "step": 4405
    },
    {
      "epoch": 41.179192455054526,
      "grad_norm": 0.46818089485168457,
      "learning_rate": 1.328411152838209e-05,
      "loss": 1.0953,
      "step": 4406
    },
    {
      "epoch": 41.1886236368995,
      "grad_norm": 0.4863697290420532,
      "learning_rate": 1.3281252389833202e-05,
      "loss": 1.0608,
      "step": 4407
    },
    {
      "epoch": 41.198054818744474,
      "grad_norm": 0.4969319701194763,
      "learning_rate": 1.3278392950657053e-05,
      "loss": 1.0275,
      "step": 4408
    },
    {
      "epoch": 41.20748600058945,
      "grad_norm": 0.49319401383399963,
      "learning_rate": 1.3275533211115627e-05,
      "loss": 1.0847,
      "step": 4409
    },
    {
      "epoch": 41.21691718243442,
      "grad_norm": 0.5075884461402893,
      "learning_rate": 1.3272673171470933e-05,
      "loss": 1.1148,
      "step": 4410
    },
    {
      "epoch": 41.226348364279396,
      "grad_norm": 0.533791184425354,
      "learning_rate": 1.3269812831985008e-05,
      "loss": 1.0696,
      "step": 4411
    },
    {
      "epoch": 41.23577954612438,
      "grad_norm": 0.49841606616973877,
      "learning_rate": 1.3266952192919911e-05,
      "loss": 1.0677,
      "step": 4412
    },
    {
      "epoch": 41.24521072796935,
      "grad_norm": 0.48828819394111633,
      "learning_rate": 1.3264091254537737e-05,
      "loss": 1.0524,
      "step": 4413
    },
    {
      "epoch": 41.254641909814325,
      "grad_norm": 0.4908972680568695,
      "learning_rate": 1.3261230017100604e-05,
      "loss": 1.0746,
      "step": 4414
    },
    {
      "epoch": 41.2640730916593,
      "grad_norm": 0.4827578365802765,
      "learning_rate": 1.3258368480870656e-05,
      "loss": 1.0918,
      "step": 4415
    },
    {
      "epoch": 41.27350427350427,
      "grad_norm": 0.4662325084209442,
      "learning_rate": 1.3255506646110066e-05,
      "loss": 1.051,
      "step": 4416
    },
    {
      "epoch": 41.28293545534925,
      "grad_norm": 0.5030012726783752,
      "learning_rate": 1.3252644513081034e-05,
      "loss": 1.0362,
      "step": 4417
    },
    {
      "epoch": 41.29236663719422,
      "grad_norm": 0.48888230323791504,
      "learning_rate": 1.324978208204579e-05,
      "loss": 1.0791,
      "step": 4418
    },
    {
      "epoch": 41.3017978190392,
      "grad_norm": 0.49053844809532166,
      "learning_rate": 1.3246919353266587e-05,
      "loss": 1.0959,
      "step": 4419
    },
    {
      "epoch": 41.311229000884175,
      "grad_norm": 0.4514959752559662,
      "learning_rate": 1.3244056327005708e-05,
      "loss": 1.0736,
      "step": 4420
    },
    {
      "epoch": 41.32066018272915,
      "grad_norm": 0.47324687242507935,
      "learning_rate": 1.3241193003525462e-05,
      "loss": 1.0488,
      "step": 4421
    },
    {
      "epoch": 41.33009136457412,
      "grad_norm": 0.4891767203807831,
      "learning_rate": 1.3238329383088187e-05,
      "loss": 1.046,
      "step": 4422
    },
    {
      "epoch": 41.3395225464191,
      "grad_norm": 0.46349862217903137,
      "learning_rate": 1.3235465465956243e-05,
      "loss": 1.0635,
      "step": 4423
    },
    {
      "epoch": 41.34895372826407,
      "grad_norm": 0.5454474687576294,
      "learning_rate": 1.3232601252392026e-05,
      "loss": 1.1013,
      "step": 4424
    },
    {
      "epoch": 41.358384910109045,
      "grad_norm": 0.45897477865219116,
      "learning_rate": 1.3229736742657954e-05,
      "loss": 1.0503,
      "step": 4425
    },
    {
      "epoch": 41.367816091954026,
      "grad_norm": 0.4719695448875427,
      "learning_rate": 1.3226871937016469e-05,
      "loss": 1.0938,
      "step": 4426
    },
    {
      "epoch": 41.377247273799,
      "grad_norm": 0.4870302975177765,
      "learning_rate": 1.3224006835730045e-05,
      "loss": 1.0579,
      "step": 4427
    },
    {
      "epoch": 41.386678455643974,
      "grad_norm": 0.5082299113273621,
      "learning_rate": 1.3221141439061183e-05,
      "loss": 1.0595,
      "step": 4428
    },
    {
      "epoch": 41.39610963748895,
      "grad_norm": 0.5364267826080322,
      "learning_rate": 1.3218275747272409e-05,
      "loss": 1.034,
      "step": 4429
    },
    {
      "epoch": 41.40554081933392,
      "grad_norm": 0.5416699647903442,
      "learning_rate": 1.3215409760626275e-05,
      "loss": 1.0908,
      "step": 4430
    },
    {
      "epoch": 41.414972001178896,
      "grad_norm": 0.4642266035079956,
      "learning_rate": 1.3212543479385364e-05,
      "loss": 1.0735,
      "step": 4431
    },
    {
      "epoch": 41.42440318302387,
      "grad_norm": 0.4935670793056488,
      "learning_rate": 1.3209676903812282e-05,
      "loss": 1.0645,
      "step": 4432
    },
    {
      "epoch": 41.43383436486885,
      "grad_norm": 0.5147370100021362,
      "learning_rate": 1.3206810034169668e-05,
      "loss": 1.0511,
      "step": 4433
    },
    {
      "epoch": 41.443265546713825,
      "grad_norm": 0.48947709798812866,
      "learning_rate": 1.3203942870720179e-05,
      "loss": 1.0895,
      "step": 4434
    },
    {
      "epoch": 41.4526967285588,
      "grad_norm": 0.541752278804779,
      "learning_rate": 1.3201075413726504e-05,
      "loss": 1.0408,
      "step": 4435
    },
    {
      "epoch": 41.46212791040377,
      "grad_norm": 0.4786173701286316,
      "learning_rate": 1.3198207663451359e-05,
      "loss": 1.0736,
      "step": 4436
    },
    {
      "epoch": 41.47155909224875,
      "grad_norm": 0.4777697026729584,
      "learning_rate": 1.3195339620157489e-05,
      "loss": 1.0764,
      "step": 4437
    },
    {
      "epoch": 41.48099027409372,
      "grad_norm": 0.5185825228691101,
      "learning_rate": 1.3192471284107658e-05,
      "loss": 1.1041,
      "step": 4438
    },
    {
      "epoch": 41.490421455938694,
      "grad_norm": 0.4942139685153961,
      "learning_rate": 1.3189602655564663e-05,
      "loss": 1.0556,
      "step": 4439
    },
    {
      "epoch": 41.499852637783675,
      "grad_norm": 0.5182409286499023,
      "learning_rate": 1.3186733734791333e-05,
      "loss": 1.0477,
      "step": 4440
    },
    {
      "epoch": 41.50928381962865,
      "grad_norm": 0.522526204586029,
      "learning_rate": 1.3183864522050511e-05,
      "loss": 1.0716,
      "step": 4441
    },
    {
      "epoch": 41.51871500147362,
      "grad_norm": 0.5185372233390808,
      "learning_rate": 1.3180995017605076e-05,
      "loss": 1.0918,
      "step": 4442
    },
    {
      "epoch": 41.5281461833186,
      "grad_norm": 0.5123880505561829,
      "learning_rate": 1.317812522171793e-05,
      "loss": 1.0778,
      "step": 4443
    },
    {
      "epoch": 41.53757736516357,
      "grad_norm": 0.5035391449928284,
      "learning_rate": 1.3175255134652005e-05,
      "loss": 1.0607,
      "step": 4444
    },
    {
      "epoch": 41.547008547008545,
      "grad_norm": 0.4878804385662079,
      "learning_rate": 1.3172384756670255e-05,
      "loss": 1.0456,
      "step": 4445
    },
    {
      "epoch": 41.55643972885352,
      "grad_norm": 0.5324501991271973,
      "learning_rate": 1.3169514088035663e-05,
      "loss": 1.0865,
      "step": 4446
    },
    {
      "epoch": 41.5658709106985,
      "grad_norm": 0.4676642417907715,
      "learning_rate": 1.316664312901124e-05,
      "loss": 1.0565,
      "step": 4447
    },
    {
      "epoch": 41.575302092543474,
      "grad_norm": 0.49149057269096375,
      "learning_rate": 1.3163771879860024e-05,
      "loss": 1.0791,
      "step": 4448
    },
    {
      "epoch": 41.58473327438845,
      "grad_norm": 0.5244767665863037,
      "learning_rate": 1.3160900340845073e-05,
      "loss": 1.0703,
      "step": 4449
    },
    {
      "epoch": 41.59416445623342,
      "grad_norm": 0.5050148963928223,
      "learning_rate": 1.315802851222948e-05,
      "loss": 1.0831,
      "step": 4450
    },
    {
      "epoch": 41.603595638078396,
      "grad_norm": 0.4686722755432129,
      "learning_rate": 1.3155156394276361e-05,
      "loss": 1.0862,
      "step": 4451
    },
    {
      "epoch": 41.61302681992337,
      "grad_norm": 0.4864124059677124,
      "learning_rate": 1.3152283987248856e-05,
      "loss": 1.0723,
      "step": 4452
    },
    {
      "epoch": 41.622458001768344,
      "grad_norm": 0.4945542812347412,
      "learning_rate": 1.3149411291410139e-05,
      "loss": 1.0846,
      "step": 4453
    },
    {
      "epoch": 41.631889183613325,
      "grad_norm": 0.4922734797000885,
      "learning_rate": 1.3146538307023398e-05,
      "loss": 1.1061,
      "step": 4454
    },
    {
      "epoch": 41.6413203654583,
      "grad_norm": 0.5336689352989197,
      "learning_rate": 1.314366503435186e-05,
      "loss": 1.0796,
      "step": 4455
    },
    {
      "epoch": 41.65075154730327,
      "grad_norm": 0.4937728941440582,
      "learning_rate": 1.3140791473658771e-05,
      "loss": 1.0361,
      "step": 4456
    },
    {
      "epoch": 41.66018272914825,
      "grad_norm": 0.5219006538391113,
      "learning_rate": 1.313791762520741e-05,
      "loss": 1.0543,
      "step": 4457
    },
    {
      "epoch": 41.66961391099322,
      "grad_norm": 0.5523231625556946,
      "learning_rate": 1.3135043489261077e-05,
      "loss": 1.0872,
      "step": 4458
    },
    {
      "epoch": 41.679045092838194,
      "grad_norm": 0.4934343099594116,
      "learning_rate": 1.3132169066083091e-05,
      "loss": 1.0421,
      "step": 4459
    },
    {
      "epoch": 41.68847627468317,
      "grad_norm": 0.4824764132499695,
      "learning_rate": 1.3129294355936817e-05,
      "loss": 1.0787,
      "step": 4460
    },
    {
      "epoch": 41.69790745652815,
      "grad_norm": 0.5243996381759644,
      "learning_rate": 1.3126419359085627e-05,
      "loss": 1.0673,
      "step": 4461
    },
    {
      "epoch": 41.70733863837312,
      "grad_norm": 0.5311523079872131,
      "learning_rate": 1.3123544075792931e-05,
      "loss": 1.0967,
      "step": 4462
    },
    {
      "epoch": 41.7167698202181,
      "grad_norm": 0.4757355749607086,
      "learning_rate": 1.3120668506322163e-05,
      "loss": 1.0943,
      "step": 4463
    },
    {
      "epoch": 41.72620100206307,
      "grad_norm": 0.48073112964630127,
      "learning_rate": 1.3117792650936778e-05,
      "loss": 1.1072,
      "step": 4464
    },
    {
      "epoch": 41.735632183908045,
      "grad_norm": 0.46453070640563965,
      "learning_rate": 1.3114916509900262e-05,
      "loss": 1.0547,
      "step": 4465
    },
    {
      "epoch": 41.74506336575302,
      "grad_norm": 0.488355427980423,
      "learning_rate": 1.3112040083476129e-05,
      "loss": 0.9919,
      "step": 4466
    },
    {
      "epoch": 41.75449454759799,
      "grad_norm": 0.5117141604423523,
      "learning_rate": 1.3109163371927914e-05,
      "loss": 1.1026,
      "step": 4467
    },
    {
      "epoch": 41.763925729442974,
      "grad_norm": 0.4969407320022583,
      "learning_rate": 1.3106286375519176e-05,
      "loss": 1.0786,
      "step": 4468
    },
    {
      "epoch": 41.77335691128795,
      "grad_norm": 0.4817048907279968,
      "learning_rate": 1.310340909451351e-05,
      "loss": 1.0712,
      "step": 4469
    },
    {
      "epoch": 41.78278809313292,
      "grad_norm": 0.4804638624191284,
      "learning_rate": 1.310053152917453e-05,
      "loss": 1.0704,
      "step": 4470
    },
    {
      "epoch": 41.792219274977896,
      "grad_norm": 0.5115506649017334,
      "learning_rate": 1.3097653679765883e-05,
      "loss": 1.0653,
      "step": 4471
    },
    {
      "epoch": 41.80165045682287,
      "grad_norm": 0.5388602018356323,
      "learning_rate": 1.3094775546551223e-05,
      "loss": 1.1158,
      "step": 4472
    },
    {
      "epoch": 41.811081638667844,
      "grad_norm": 0.486826092004776,
      "learning_rate": 1.3091897129794258e-05,
      "loss": 1.0444,
      "step": 4473
    },
    {
      "epoch": 41.82051282051282,
      "grad_norm": 0.5004971027374268,
      "learning_rate": 1.3089018429758702e-05,
      "loss": 1.0737,
      "step": 4474
    },
    {
      "epoch": 41.8299440023578,
      "grad_norm": 0.5165568590164185,
      "learning_rate": 1.3086139446708297e-05,
      "loss": 1.0589,
      "step": 4475
    },
    {
      "epoch": 41.83937518420277,
      "grad_norm": 0.5017555952072144,
      "learning_rate": 1.3083260180906816e-05,
      "loss": 1.0495,
      "step": 4476
    },
    {
      "epoch": 41.84880636604775,
      "grad_norm": 0.4781043231487274,
      "learning_rate": 1.3080380632618064e-05,
      "loss": 1.0861,
      "step": 4477
    },
    {
      "epoch": 41.85823754789272,
      "grad_norm": 0.5500777959823608,
      "learning_rate": 1.3077500802105854e-05,
      "loss": 1.1148,
      "step": 4478
    },
    {
      "epoch": 41.867668729737694,
      "grad_norm": 0.4911227226257324,
      "learning_rate": 1.307462068963404e-05,
      "loss": 1.0666,
      "step": 4479
    },
    {
      "epoch": 41.87709991158267,
      "grad_norm": 0.5021314024925232,
      "learning_rate": 1.3071740295466495e-05,
      "loss": 1.0957,
      "step": 4480
    },
    {
      "epoch": 41.88653109342764,
      "grad_norm": 0.47708749771118164,
      "learning_rate": 1.3068859619867123e-05,
      "loss": 1.0694,
      "step": 4481
    },
    {
      "epoch": 41.89596227527262,
      "grad_norm": 0.4828623831272125,
      "learning_rate": 1.3065978663099848e-05,
      "loss": 1.0543,
      "step": 4482
    },
    {
      "epoch": 41.9053934571176,
      "grad_norm": 0.514746904373169,
      "learning_rate": 1.3063097425428622e-05,
      "loss": 1.0982,
      "step": 4483
    },
    {
      "epoch": 41.91482463896257,
      "grad_norm": 0.5160788893699646,
      "learning_rate": 1.3060215907117426e-05,
      "loss": 1.0851,
      "step": 4484
    },
    {
      "epoch": 41.924255820807545,
      "grad_norm": 0.49299880862236023,
      "learning_rate": 1.3057334108430259e-05,
      "loss": 1.0568,
      "step": 4485
    },
    {
      "epoch": 41.93368700265252,
      "grad_norm": 0.4707726538181305,
      "learning_rate": 1.3054452029631158e-05,
      "loss": 1.0845,
      "step": 4486
    },
    {
      "epoch": 41.94311818449749,
      "grad_norm": 0.4906339943408966,
      "learning_rate": 1.3051569670984168e-05,
      "loss": 1.07,
      "step": 4487
    },
    {
      "epoch": 41.95254936634247,
      "grad_norm": 0.4701423645019531,
      "learning_rate": 1.3048687032753375e-05,
      "loss": 1.0842,
      "step": 4488
    },
    {
      "epoch": 41.96198054818745,
      "grad_norm": 0.5172576308250427,
      "learning_rate": 1.3045804115202887e-05,
      "loss": 1.1109,
      "step": 4489
    },
    {
      "epoch": 41.97141173003242,
      "grad_norm": 0.4882325530052185,
      "learning_rate": 1.3042920918596835e-05,
      "loss": 1.031,
      "step": 4490
    },
    {
      "epoch": 41.980842911877396,
      "grad_norm": 0.5182719826698303,
      "learning_rate": 1.3040037443199376e-05,
      "loss": 1.0769,
      "step": 4491
    },
    {
      "epoch": 41.99027409372237,
      "grad_norm": 0.4976736903190613,
      "learning_rate": 1.3037153689274691e-05,
      "loss": 1.0909,
      "step": 4492
    },
    {
      "epoch": 41.999705275567344,
      "grad_norm": 0.5145350694656372,
      "learning_rate": 1.3034269657086993e-05,
      "loss": 1.0909,
      "step": 4493
    },
    {
      "epoch": 42.0,
      "grad_norm": 2.1945087909698486,
      "learning_rate": 1.3031385346900512e-05,
      "loss": 0.6499,
      "step": 4494
    },
    {
      "epoch": 42.009431181844974,
      "grad_norm": 0.4509444832801819,
      "learning_rate": 1.3028500758979507e-05,
      "loss": 1.0976,
      "step": 4495
    },
    {
      "epoch": 42.01886236368995,
      "grad_norm": 0.47054150700569153,
      "learning_rate": 1.3025615893588269e-05,
      "loss": 1.0617,
      "step": 4496
    },
    {
      "epoch": 42.02829354553492,
      "grad_norm": 0.49819454550743103,
      "learning_rate": 1.3022730750991104e-05,
      "loss": 1.0942,
      "step": 4497
    },
    {
      "epoch": 42.0377247273799,
      "grad_norm": 0.4772277772426605,
      "learning_rate": 1.3019845331452346e-05,
      "loss": 1.0945,
      "step": 4498
    },
    {
      "epoch": 42.04715590922488,
      "grad_norm": 0.5157723426818848,
      "learning_rate": 1.3016959635236359e-05,
      "loss": 1.0556,
      "step": 4499
    },
    {
      "epoch": 42.05658709106985,
      "grad_norm": 0.5165498852729797,
      "learning_rate": 1.3014073662607531e-05,
      "loss": 1.0741,
      "step": 4500
    },
    {
      "epoch": 42.066018272914825,
      "grad_norm": 0.48235246539115906,
      "learning_rate": 1.301118741383027e-05,
      "loss": 1.0664,
      "step": 4501
    },
    {
      "epoch": 42.0754494547598,
      "grad_norm": 0.5050532221794128,
      "learning_rate": 1.3008300889169015e-05,
      "loss": 1.0641,
      "step": 4502
    },
    {
      "epoch": 42.08488063660477,
      "grad_norm": 0.5531384348869324,
      "learning_rate": 1.3005414088888228e-05,
      "loss": 1.0632,
      "step": 4503
    },
    {
      "epoch": 42.09431181844975,
      "grad_norm": 0.48618635535240173,
      "learning_rate": 1.30025270132524e-05,
      "loss": 1.0487,
      "step": 4504
    },
    {
      "epoch": 42.10374300029473,
      "grad_norm": 0.4565557837486267,
      "learning_rate": 1.2999639662526037e-05,
      "loss": 1.0728,
      "step": 4505
    },
    {
      "epoch": 42.1131741821397,
      "grad_norm": 0.5558594465255737,
      "learning_rate": 1.2996752036973685e-05,
      "loss": 1.0825,
      "step": 4506
    },
    {
      "epoch": 42.122605363984675,
      "grad_norm": 0.4838314950466156,
      "learning_rate": 1.2993864136859902e-05,
      "loss": 1.0763,
      "step": 4507
    },
    {
      "epoch": 42.13203654582965,
      "grad_norm": 0.49420711398124695,
      "learning_rate": 1.2990975962449277e-05,
      "loss": 1.045,
      "step": 4508
    },
    {
      "epoch": 42.14146772767462,
      "grad_norm": 0.4825560748577118,
      "learning_rate": 1.2988087514006426e-05,
      "loss": 1.0976,
      "step": 4509
    },
    {
      "epoch": 42.1508989095196,
      "grad_norm": 0.5434964299201965,
      "learning_rate": 1.2985198791795982e-05,
      "loss": 1.0677,
      "step": 4510
    },
    {
      "epoch": 42.16033009136457,
      "grad_norm": 0.4895619750022888,
      "learning_rate": 1.298230979608262e-05,
      "loss": 1.1186,
      "step": 4511
    },
    {
      "epoch": 42.16976127320955,
      "grad_norm": 0.5038337111473083,
      "learning_rate": 1.2979420527131018e-05,
      "loss": 1.0675,
      "step": 4512
    },
    {
      "epoch": 42.179192455054526,
      "grad_norm": 0.4978240728378296,
      "learning_rate": 1.2976530985205894e-05,
      "loss": 1.0737,
      "step": 4513
    },
    {
      "epoch": 42.1886236368995,
      "grad_norm": 0.5429701209068298,
      "learning_rate": 1.2973641170571986e-05,
      "loss": 1.0426,
      "step": 4514
    },
    {
      "epoch": 42.198054818744474,
      "grad_norm": 0.5252403020858765,
      "learning_rate": 1.297075108349406e-05,
      "loss": 1.0385,
      "step": 4515
    },
    {
      "epoch": 42.20748600058945,
      "grad_norm": 0.4846504330635071,
      "learning_rate": 1.2967860724236903e-05,
      "loss": 1.0762,
      "step": 4516
    },
    {
      "epoch": 42.21691718243442,
      "grad_norm": 0.5319147706031799,
      "learning_rate": 1.2964970093065326e-05,
      "loss": 1.0666,
      "step": 4517
    },
    {
      "epoch": 42.226348364279396,
      "grad_norm": 0.5137059688568115,
      "learning_rate": 1.2962079190244173e-05,
      "loss": 1.0764,
      "step": 4518
    },
    {
      "epoch": 42.23577954612438,
      "grad_norm": 0.5138444304466248,
      "learning_rate": 1.2959188016038305e-05,
      "loss": 1.0702,
      "step": 4519
    },
    {
      "epoch": 42.24521072796935,
      "grad_norm": 0.4986647069454193,
      "learning_rate": 1.2956296570712608e-05,
      "loss": 1.0857,
      "step": 4520
    },
    {
      "epoch": 42.254641909814325,
      "grad_norm": 0.4963633716106415,
      "learning_rate": 1.2953404854532e-05,
      "loss": 1.0459,
      "step": 4521
    },
    {
      "epoch": 42.2640730916593,
      "grad_norm": 0.4695287346839905,
      "learning_rate": 1.295051286776142e-05,
      "loss": 1.0652,
      "step": 4522
    },
    {
      "epoch": 42.27350427350427,
      "grad_norm": 0.5401489734649658,
      "learning_rate": 1.2947620610665822e-05,
      "loss": 1.0704,
      "step": 4523
    },
    {
      "epoch": 42.28293545534925,
      "grad_norm": 0.4939168393611908,
      "learning_rate": 1.2944728083510203e-05,
      "loss": 1.088,
      "step": 4524
    },
    {
      "epoch": 42.29236663719422,
      "grad_norm": 0.49865660071372986,
      "learning_rate": 1.2941835286559572e-05,
      "loss": 1.062,
      "step": 4525
    },
    {
      "epoch": 42.3017978190392,
      "grad_norm": 0.5048994421958923,
      "learning_rate": 1.2938942220078966e-05,
      "loss": 1.0924,
      "step": 4526
    },
    {
      "epoch": 42.311229000884175,
      "grad_norm": 0.48949360847473145,
      "learning_rate": 1.2936048884333445e-05,
      "loss": 1.0536,
      "step": 4527
    },
    {
      "epoch": 42.32066018272915,
      "grad_norm": 0.5192714929580688,
      "learning_rate": 1.2933155279588099e-05,
      "loss": 1.0565,
      "step": 4528
    },
    {
      "epoch": 42.33009136457412,
      "grad_norm": 0.49101606011390686,
      "learning_rate": 1.2930261406108036e-05,
      "loss": 1.1149,
      "step": 4529
    },
    {
      "epoch": 42.3395225464191,
      "grad_norm": 0.5264947414398193,
      "learning_rate": 1.2927367264158394e-05,
      "loss": 1.0137,
      "step": 4530
    },
    {
      "epoch": 42.34895372826407,
      "grad_norm": 0.49231430888175964,
      "learning_rate": 1.2924472854004333e-05,
      "loss": 1.0362,
      "step": 4531
    },
    {
      "epoch": 42.358384910109045,
      "grad_norm": 0.5017834901809692,
      "learning_rate": 1.2921578175911037e-05,
      "loss": 1.0887,
      "step": 4532
    },
    {
      "epoch": 42.367816091954026,
      "grad_norm": 0.47120583057403564,
      "learning_rate": 1.2918683230143714e-05,
      "loss": 1.071,
      "step": 4533
    },
    {
      "epoch": 42.377247273799,
      "grad_norm": 0.5295580625534058,
      "learning_rate": 1.2915788016967601e-05,
      "loss": 1.0901,
      "step": 4534
    },
    {
      "epoch": 42.386678455643974,
      "grad_norm": 0.5252014994621277,
      "learning_rate": 1.2912892536647957e-05,
      "loss": 1.0295,
      "step": 4535
    },
    {
      "epoch": 42.39610963748895,
      "grad_norm": 0.5163169503211975,
      "learning_rate": 1.290999678945006e-05,
      "loss": 1.0494,
      "step": 4536
    },
    {
      "epoch": 42.40554081933392,
      "grad_norm": 0.4966897666454315,
      "learning_rate": 1.290710077563922e-05,
      "loss": 1.0616,
      "step": 4537
    },
    {
      "epoch": 42.414972001178896,
      "grad_norm": 0.4930514395236969,
      "learning_rate": 1.2904204495480772e-05,
      "loss": 1.0537,
      "step": 4538
    },
    {
      "epoch": 42.42440318302387,
      "grad_norm": 0.5284250378608704,
      "learning_rate": 1.2901307949240066e-05,
      "loss": 1.0669,
      "step": 4539
    },
    {
      "epoch": 42.43383436486885,
      "grad_norm": 0.5012519359588623,
      "learning_rate": 1.2898411137182492e-05,
      "loss": 1.1042,
      "step": 4540
    },
    {
      "epoch": 42.443265546713825,
      "grad_norm": 0.4990909695625305,
      "learning_rate": 1.2895514059573445e-05,
      "loss": 1.029,
      "step": 4541
    },
    {
      "epoch": 42.4526967285588,
      "grad_norm": 0.48474496603012085,
      "learning_rate": 1.289261671667836e-05,
      "loss": 1.0984,
      "step": 4542
    },
    {
      "epoch": 42.46212791040377,
      "grad_norm": 0.46543508768081665,
      "learning_rate": 1.2889719108762688e-05,
      "loss": 1.0798,
      "step": 4543
    },
    {
      "epoch": 42.47155909224875,
      "grad_norm": 0.5242084860801697,
      "learning_rate": 1.288682123609191e-05,
      "loss": 1.1174,
      "step": 4544
    },
    {
      "epoch": 42.48099027409372,
      "grad_norm": 0.4800076186656952,
      "learning_rate": 1.2883923098931525e-05,
      "loss": 1.074,
      "step": 4545
    },
    {
      "epoch": 42.490421455938694,
      "grad_norm": 0.5122963190078735,
      "learning_rate": 1.288102469754706e-05,
      "loss": 1.1022,
      "step": 4546
    },
    {
      "epoch": 42.499852637783675,
      "grad_norm": 0.5194799900054932,
      "learning_rate": 1.2878126032204063e-05,
      "loss": 1.0505,
      "step": 4547
    },
    {
      "epoch": 42.50928381962865,
      "grad_norm": 0.5502778887748718,
      "learning_rate": 1.2875227103168117e-05,
      "loss": 1.1241,
      "step": 4548
    },
    {
      "epoch": 42.51871500147362,
      "grad_norm": 0.4706237316131592,
      "learning_rate": 1.2872327910704815e-05,
      "loss": 1.0607,
      "step": 4549
    },
    {
      "epoch": 42.5281461833186,
      "grad_norm": 0.5257568955421448,
      "learning_rate": 1.2869428455079778e-05,
      "loss": 1.0954,
      "step": 4550
    },
    {
      "epoch": 42.53757736516357,
      "grad_norm": 0.493757039308548,
      "learning_rate": 1.2866528736558656e-05,
      "loss": 1.0907,
      "step": 4551
    },
    {
      "epoch": 42.547008547008545,
      "grad_norm": 0.4867704510688782,
      "learning_rate": 1.286362875540712e-05,
      "loss": 1.081,
      "step": 4552
    },
    {
      "epoch": 42.55643972885352,
      "grad_norm": 0.49492013454437256,
      "learning_rate": 1.2860728511890867e-05,
      "loss": 1.0094,
      "step": 4553
    },
    {
      "epoch": 42.5658709106985,
      "grad_norm": 0.4879486560821533,
      "learning_rate": 1.2857828006275614e-05,
      "loss": 1.0576,
      "step": 4554
    },
    {
      "epoch": 42.575302092543474,
      "grad_norm": 0.5059676170349121,
      "learning_rate": 1.2854927238827106e-05,
      "loss": 1.06,
      "step": 4555
    },
    {
      "epoch": 42.58473327438845,
      "grad_norm": 0.507332980632782,
      "learning_rate": 1.2852026209811107e-05,
      "loss": 1.0547,
      "step": 4556
    },
    {
      "epoch": 42.59416445623342,
      "grad_norm": 0.48957690596580505,
      "learning_rate": 1.2849124919493413e-05,
      "loss": 1.1163,
      "step": 4557
    },
    {
      "epoch": 42.603595638078396,
      "grad_norm": 0.5187500715255737,
      "learning_rate": 1.2846223368139837e-05,
      "loss": 1.047,
      "step": 4558
    },
    {
      "epoch": 42.61302681992337,
      "grad_norm": 0.5218111276626587,
      "learning_rate": 1.284332155601622e-05,
      "loss": 1.0626,
      "step": 4559
    },
    {
      "epoch": 42.622458001768344,
      "grad_norm": 0.4673811197280884,
      "learning_rate": 1.284041948338842e-05,
      "loss": 1.0992,
      "step": 4560
    },
    {
      "epoch": 42.631889183613325,
      "grad_norm": 0.5224528312683105,
      "learning_rate": 1.2837517150522332e-05,
      "loss": 1.0934,
      "step": 4561
    },
    {
      "epoch": 42.6413203654583,
      "grad_norm": 0.48284658789634705,
      "learning_rate": 1.2834614557683856e-05,
      "loss": 1.1022,
      "step": 4562
    },
    {
      "epoch": 42.65075154730327,
      "grad_norm": 0.5012354254722595,
      "learning_rate": 1.2831711705138936e-05,
      "loss": 1.064,
      "step": 4563
    },
    {
      "epoch": 42.66018272914825,
      "grad_norm": 0.4857088625431061,
      "learning_rate": 1.2828808593153527e-05,
      "loss": 1.0665,
      "step": 4564
    },
    {
      "epoch": 42.66961391099322,
      "grad_norm": 0.5038887858390808,
      "learning_rate": 1.282590522199361e-05,
      "loss": 1.0543,
      "step": 4565
    },
    {
      "epoch": 42.679045092838194,
      "grad_norm": 0.5371248722076416,
      "learning_rate": 1.2823001591925192e-05,
      "loss": 1.0897,
      "step": 4566
    },
    {
      "epoch": 42.68847627468317,
      "grad_norm": 0.5195830464363098,
      "learning_rate": 1.2820097703214305e-05,
      "loss": 1.0524,
      "step": 4567
    },
    {
      "epoch": 42.69790745652815,
      "grad_norm": 0.5003401637077332,
      "learning_rate": 1.2817193556126999e-05,
      "loss": 1.0097,
      "step": 4568
    },
    {
      "epoch": 42.70733863837312,
      "grad_norm": 0.4968195855617523,
      "learning_rate": 1.281428915092935e-05,
      "loss": 1.085,
      "step": 4569
    },
    {
      "epoch": 42.7167698202181,
      "grad_norm": 0.4926622807979584,
      "learning_rate": 1.2811384487887464e-05,
      "loss": 1.1029,
      "step": 4570
    },
    {
      "epoch": 42.72620100206307,
      "grad_norm": 0.5025905966758728,
      "learning_rate": 1.280847956726746e-05,
      "loss": 1.1016,
      "step": 4571
    },
    {
      "epoch": 42.735632183908045,
      "grad_norm": 0.5103155970573425,
      "learning_rate": 1.280557438933549e-05,
      "loss": 0.9938,
      "step": 4572
    },
    {
      "epoch": 42.74506336575302,
      "grad_norm": 0.4964284598827362,
      "learning_rate": 1.280266895435772e-05,
      "loss": 1.088,
      "step": 4573
    },
    {
      "epoch": 42.75449454759799,
      "grad_norm": 0.4770718216896057,
      "learning_rate": 1.2799763262600349e-05,
      "loss": 1.0473,
      "step": 4574
    },
    {
      "epoch": 42.763925729442974,
      "grad_norm": 0.4681449830532074,
      "learning_rate": 1.2796857314329597e-05,
      "loss": 1.0747,
      "step": 4575
    },
    {
      "epoch": 42.77335691128795,
      "grad_norm": 0.4981802999973297,
      "learning_rate": 1.2793951109811702e-05,
      "loss": 1.0927,
      "step": 4576
    },
    {
      "epoch": 42.78278809313292,
      "grad_norm": 0.5191787481307983,
      "learning_rate": 1.279104464931293e-05,
      "loss": 1.0405,
      "step": 4577
    },
    {
      "epoch": 42.792219274977896,
      "grad_norm": 0.4901871681213379,
      "learning_rate": 1.2788137933099574e-05,
      "loss": 1.0854,
      "step": 4578
    },
    {
      "epoch": 42.80165045682287,
      "grad_norm": 0.5055627822875977,
      "learning_rate": 1.2785230961437941e-05,
      "loss": 1.0695,
      "step": 4579
    },
    {
      "epoch": 42.811081638667844,
      "grad_norm": 0.5101877450942993,
      "learning_rate": 1.2782323734594373e-05,
      "loss": 1.0701,
      "step": 4580
    },
    {
      "epoch": 42.82051282051282,
      "grad_norm": 0.5494883060455322,
      "learning_rate": 1.2779416252835222e-05,
      "loss": 1.0604,
      "step": 4581
    },
    {
      "epoch": 42.8299440023578,
      "grad_norm": 0.49963122606277466,
      "learning_rate": 1.2776508516426876e-05,
      "loss": 1.0928,
      "step": 4582
    },
    {
      "epoch": 42.83937518420277,
      "grad_norm": 0.5238412022590637,
      "learning_rate": 1.2773600525635739e-05,
      "loss": 1.0357,
      "step": 4583
    },
    {
      "epoch": 42.84880636604775,
      "grad_norm": 0.5048878192901611,
      "learning_rate": 1.2770692280728236e-05,
      "loss": 1.0481,
      "step": 4584
    },
    {
      "epoch": 42.85823754789272,
      "grad_norm": 0.49899330735206604,
      "learning_rate": 1.2767783781970829e-05,
      "loss": 1.0277,
      "step": 4585
    },
    {
      "epoch": 42.867668729737694,
      "grad_norm": 0.5076756477355957,
      "learning_rate": 1.2764875029629984e-05,
      "loss": 1.0622,
      "step": 4586
    },
    {
      "epoch": 42.87709991158267,
      "grad_norm": 0.5158212184906006,
      "learning_rate": 1.2761966023972206e-05,
      "loss": 1.0879,
      "step": 4587
    },
    {
      "epoch": 42.88653109342764,
      "grad_norm": 0.5226877331733704,
      "learning_rate": 1.2759056765264014e-05,
      "loss": 1.0356,
      "step": 4588
    },
    {
      "epoch": 42.89596227527262,
      "grad_norm": 0.4986221492290497,
      "learning_rate": 1.2756147253771955e-05,
      "loss": 1.0325,
      "step": 4589
    },
    {
      "epoch": 42.9053934571176,
      "grad_norm": 0.5318790674209595,
      "learning_rate": 1.27532374897626e-05,
      "loss": 1.0663,
      "step": 4590
    },
    {
      "epoch": 42.91482463896257,
      "grad_norm": 0.4668961763381958,
      "learning_rate": 1.2750327473502535e-05,
      "loss": 1.1091,
      "step": 4591
    },
    {
      "epoch": 42.924255820807545,
      "grad_norm": 0.48405173420906067,
      "learning_rate": 1.2747417205258376e-05,
      "loss": 1.0403,
      "step": 4592
    },
    {
      "epoch": 42.93368700265252,
      "grad_norm": 0.5330125093460083,
      "learning_rate": 1.2744506685296764e-05,
      "loss": 1.0841,
      "step": 4593
    },
    {
      "epoch": 42.94311818449749,
      "grad_norm": 0.5095690488815308,
      "learning_rate": 1.2741595913884362e-05,
      "loss": 1.0702,
      "step": 4594
    },
    {
      "epoch": 42.95254936634247,
      "grad_norm": 0.48940309882164,
      "learning_rate": 1.2738684891287844e-05,
      "loss": 1.0402,
      "step": 4595
    },
    {
      "epoch": 42.96198054818745,
      "grad_norm": 0.49770793318748474,
      "learning_rate": 1.2735773617773926e-05,
      "loss": 1.0649,
      "step": 4596
    },
    {
      "epoch": 42.97141173003242,
      "grad_norm": 0.5381997227668762,
      "learning_rate": 1.2732862093609334e-05,
      "loss": 1.0618,
      "step": 4597
    },
    {
      "epoch": 42.980842911877396,
      "grad_norm": 0.49819836020469666,
      "learning_rate": 1.2729950319060821e-05,
      "loss": 1.0555,
      "step": 4598
    },
    {
      "epoch": 42.99027409372237,
      "grad_norm": 0.5028862357139587,
      "learning_rate": 1.2727038294395165e-05,
      "loss": 1.0678,
      "step": 4599
    },
    {
      "epoch": 42.999705275567344,
      "grad_norm": 0.5072230100631714,
      "learning_rate": 1.2724126019879162e-05,
      "loss": 1.0347,
      "step": 4600
    },
    {
      "epoch": 43.0,
      "grad_norm": 2.7797422409057617,
      "learning_rate": 1.2721213495779636e-05,
      "loss": 0.4165,
      "step": 4601
    },
    {
      "epoch": 43.009431181844974,
      "grad_norm": 0.504192590713501,
      "learning_rate": 1.2718300722363431e-05,
      "loss": 1.0633,
      "step": 4602
    },
    {
      "epoch": 43.01886236368995,
      "grad_norm": 0.4870297908782959,
      "learning_rate": 1.271538769989741e-05,
      "loss": 1.0944,
      "step": 4603
    },
    {
      "epoch": 43.02829354553492,
      "grad_norm": 0.49814462661743164,
      "learning_rate": 1.2712474428648471e-05,
      "loss": 1.0538,
      "step": 4604
    },
    {
      "epoch": 43.0377247273799,
      "grad_norm": 0.49784979224205017,
      "learning_rate": 1.270956090888352e-05,
      "loss": 1.0621,
      "step": 4605
    },
    {
      "epoch": 43.04715590922488,
      "grad_norm": 0.5251594185829163,
      "learning_rate": 1.2706647140869499e-05,
      "loss": 1.0551,
      "step": 4606
    },
    {
      "epoch": 43.05658709106985,
      "grad_norm": 0.48422348499298096,
      "learning_rate": 1.2703733124873358e-05,
      "loss": 1.0271,
      "step": 4607
    },
    {
      "epoch": 43.066018272914825,
      "grad_norm": 0.5222798585891724,
      "learning_rate": 1.2700818861162087e-05,
      "loss": 1.082,
      "step": 4608
    },
    {
      "epoch": 43.0754494547598,
      "grad_norm": 0.471346914768219,
      "learning_rate": 1.2697904350002688e-05,
      "loss": 1.0547,
      "step": 4609
    },
    {
      "epoch": 43.08488063660477,
      "grad_norm": 0.4941938817501068,
      "learning_rate": 1.2694989591662181e-05,
      "loss": 1.0374,
      "step": 4610
    },
    {
      "epoch": 43.09431181844975,
      "grad_norm": 0.5024819374084473,
      "learning_rate": 1.2692074586407622e-05,
      "loss": 1.0783,
      "step": 4611
    },
    {
      "epoch": 43.10374300029473,
      "grad_norm": 0.5179615616798401,
      "learning_rate": 1.268915933450608e-05,
      "loss": 1.0786,
      "step": 4612
    },
    {
      "epoch": 43.1131741821397,
      "grad_norm": 0.479392945766449,
      "learning_rate": 1.268624383622465e-05,
      "loss": 1.076,
      "step": 4613
    },
    {
      "epoch": 43.122605363984675,
      "grad_norm": 0.5050191879272461,
      "learning_rate": 1.2683328091830446e-05,
      "loss": 1.068,
      "step": 4614
    },
    {
      "epoch": 43.13203654582965,
      "grad_norm": 0.5288492441177368,
      "learning_rate": 1.2680412101590615e-05,
      "loss": 1.0502,
      "step": 4615
    },
    {
      "epoch": 43.14146772767462,
      "grad_norm": 0.48786643147468567,
      "learning_rate": 1.2677495865772316e-05,
      "loss": 1.0722,
      "step": 4616
    },
    {
      "epoch": 43.1508989095196,
      "grad_norm": 0.5220980048179626,
      "learning_rate": 1.267457938464273e-05,
      "loss": 1.0836,
      "step": 4617
    },
    {
      "epoch": 43.16033009136457,
      "grad_norm": 0.48931893706321716,
      "learning_rate": 1.2671662658469063e-05,
      "loss": 1.039,
      "step": 4618
    },
    {
      "epoch": 43.16976127320955,
      "grad_norm": 0.5224186778068542,
      "learning_rate": 1.2668745687518552e-05,
      "loss": 1.0822,
      "step": 4619
    },
    {
      "epoch": 43.179192455054526,
      "grad_norm": 0.46583321690559387,
      "learning_rate": 1.2665828472058441e-05,
      "loss": 1.0663,
      "step": 4620
    },
    {
      "epoch": 43.1886236368995,
      "grad_norm": 0.4647563099861145,
      "learning_rate": 1.2662911012356012e-05,
      "loss": 1.0322,
      "step": 4621
    },
    {
      "epoch": 43.198054818744474,
      "grad_norm": 0.4896462559700012,
      "learning_rate": 1.2659993308678554e-05,
      "loss": 1.067,
      "step": 4622
    },
    {
      "epoch": 43.20748600058945,
      "grad_norm": 0.5245769023895264,
      "learning_rate": 1.2657075361293392e-05,
      "loss": 1.0665,
      "step": 4623
    },
    {
      "epoch": 43.21691718243442,
      "grad_norm": 0.49406468868255615,
      "learning_rate": 1.2654157170467865e-05,
      "loss": 1.0491,
      "step": 4624
    },
    {
      "epoch": 43.226348364279396,
      "grad_norm": 0.5058091282844543,
      "learning_rate": 1.2651238736469334e-05,
      "loss": 1.0766,
      "step": 4625
    },
    {
      "epoch": 43.23577954612438,
      "grad_norm": 0.5325600504875183,
      "learning_rate": 1.2648320059565192e-05,
      "loss": 1.051,
      "step": 4626
    },
    {
      "epoch": 43.24521072796935,
      "grad_norm": 0.49196791648864746,
      "learning_rate": 1.2645401140022839e-05,
      "loss": 1.0583,
      "step": 4627
    },
    {
      "epoch": 43.254641909814325,
      "grad_norm": 0.49164479970932007,
      "learning_rate": 1.2642481978109713e-05,
      "loss": 1.0569,
      "step": 4628
    },
    {
      "epoch": 43.2640730916593,
      "grad_norm": 0.4855068027973175,
      "learning_rate": 1.2639562574093258e-05,
      "loss": 1.0811,
      "step": 4629
    },
    {
      "epoch": 43.27350427350427,
      "grad_norm": 0.5277425050735474,
      "learning_rate": 1.2636642928240959e-05,
      "loss": 1.0664,
      "step": 4630
    },
    {
      "epoch": 43.28293545534925,
      "grad_norm": 0.5180621147155762,
      "learning_rate": 1.2633723040820305e-05,
      "loss": 1.0745,
      "step": 4631
    },
    {
      "epoch": 43.29236663719422,
      "grad_norm": 0.5597467422485352,
      "learning_rate": 1.2630802912098818e-05,
      "loss": 1.0395,
      "step": 4632
    },
    {
      "epoch": 43.3017978190392,
      "grad_norm": 0.49904167652130127,
      "learning_rate": 1.2627882542344038e-05,
      "loss": 1.0593,
      "step": 4633
    },
    {
      "epoch": 43.311229000884175,
      "grad_norm": 0.4843919575214386,
      "learning_rate": 1.2624961931823534e-05,
      "loss": 1.0823,
      "step": 4634
    },
    {
      "epoch": 43.32066018272915,
      "grad_norm": 0.5289977788925171,
      "learning_rate": 1.2622041080804882e-05,
      "loss": 1.0107,
      "step": 4635
    },
    {
      "epoch": 43.33009136457412,
      "grad_norm": 0.481292724609375,
      "learning_rate": 1.26191199895557e-05,
      "loss": 1.0821,
      "step": 4636
    },
    {
      "epoch": 43.3395225464191,
      "grad_norm": 0.5085613131523132,
      "learning_rate": 1.2616198658343603e-05,
      "loss": 1.0696,
      "step": 4637
    },
    {
      "epoch": 43.34895372826407,
      "grad_norm": 0.5054290294647217,
      "learning_rate": 1.2613277087436257e-05,
      "loss": 1.077,
      "step": 4638
    },
    {
      "epoch": 43.358384910109045,
      "grad_norm": 0.4992879033088684,
      "learning_rate": 1.2610355277101329e-05,
      "loss": 1.0696,
      "step": 4639
    },
    {
      "epoch": 43.367816091954026,
      "grad_norm": 0.480575293302536,
      "learning_rate": 1.2607433227606514e-05,
      "loss": 1.0601,
      "step": 4640
    },
    {
      "epoch": 43.377247273799,
      "grad_norm": 0.4748682677745819,
      "learning_rate": 1.2604510939219529e-05,
      "loss": 1.0649,
      "step": 4641
    },
    {
      "epoch": 43.386678455643974,
      "grad_norm": 0.4814101755619049,
      "learning_rate": 1.2601588412208115e-05,
      "loss": 1.0729,
      "step": 4642
    },
    {
      "epoch": 43.39610963748895,
      "grad_norm": 0.5075930953025818,
      "learning_rate": 1.2598665646840033e-05,
      "loss": 1.0793,
      "step": 4643
    },
    {
      "epoch": 43.40554081933392,
      "grad_norm": 0.49318811297416687,
      "learning_rate": 1.2595742643383062e-05,
      "loss": 1.0661,
      "step": 4644
    },
    {
      "epoch": 43.414972001178896,
      "grad_norm": 0.49704471230506897,
      "learning_rate": 1.2592819402105012e-05,
      "loss": 1.099,
      "step": 4645
    },
    {
      "epoch": 43.42440318302387,
      "grad_norm": 0.5145227909088135,
      "learning_rate": 1.2589895923273704e-05,
      "loss": 1.0823,
      "step": 4646
    },
    {
      "epoch": 43.43383436486885,
      "grad_norm": 0.46635258197784424,
      "learning_rate": 1.2586972207156993e-05,
      "loss": 1.045,
      "step": 4647
    },
    {
      "epoch": 43.443265546713825,
      "grad_norm": 0.48590216040611267,
      "learning_rate": 1.2584048254022741e-05,
      "loss": 1.0434,
      "step": 4648
    },
    {
      "epoch": 43.4526967285588,
      "grad_norm": 0.500801146030426,
      "learning_rate": 1.2581124064138845e-05,
      "loss": 1.076,
      "step": 4649
    },
    {
      "epoch": 43.46212791040377,
      "grad_norm": 0.5514289140701294,
      "learning_rate": 1.2578199637773217e-05,
      "loss": 1.0742,
      "step": 4650
    },
    {
      "epoch": 43.47155909224875,
      "grad_norm": 0.5197349786758423,
      "learning_rate": 1.257527497519379e-05,
      "loss": 1.0674,
      "step": 4651
    },
    {
      "epoch": 43.48099027409372,
      "grad_norm": 0.4638310372829437,
      "learning_rate": 1.2572350076668523e-05,
      "loss": 1.0807,
      "step": 4652
    },
    {
      "epoch": 43.490421455938694,
      "grad_norm": 0.528930127620697,
      "learning_rate": 1.2569424942465394e-05,
      "loss": 1.0471,
      "step": 4653
    },
    {
      "epoch": 43.499852637783675,
      "grad_norm": 0.5035791993141174,
      "learning_rate": 1.2566499572852402e-05,
      "loss": 1.066,
      "step": 4654
    },
    {
      "epoch": 43.50928381962865,
      "grad_norm": 0.5056904554367065,
      "learning_rate": 1.2563573968097566e-05,
      "loss": 1.0806,
      "step": 4655
    },
    {
      "epoch": 43.51871500147362,
      "grad_norm": 0.47892996668815613,
      "learning_rate": 1.2560648128468938e-05,
      "loss": 1.0873,
      "step": 4656
    },
    {
      "epoch": 43.5281461833186,
      "grad_norm": 0.5202915668487549,
      "learning_rate": 1.2557722054234573e-05,
      "loss": 1.0374,
      "step": 4657
    },
    {
      "epoch": 43.53757736516357,
      "grad_norm": 0.5268232822418213,
      "learning_rate": 1.2554795745662562e-05,
      "loss": 1.0123,
      "step": 4658
    },
    {
      "epoch": 43.547008547008545,
      "grad_norm": 0.5087347030639648,
      "learning_rate": 1.2551869203021005e-05,
      "loss": 1.0227,
      "step": 4659
    },
    {
      "epoch": 43.55643972885352,
      "grad_norm": 0.503093957901001,
      "learning_rate": 1.2548942426578043e-05,
      "loss": 1.1023,
      "step": 4660
    },
    {
      "epoch": 43.5658709106985,
      "grad_norm": 0.504145085811615,
      "learning_rate": 1.2546015416601818e-05,
      "loss": 1.0529,
      "step": 4661
    },
    {
      "epoch": 43.575302092543474,
      "grad_norm": 0.4837217330932617,
      "learning_rate": 1.2543088173360503e-05,
      "loss": 1.0816,
      "step": 4662
    },
    {
      "epoch": 43.58473327438845,
      "grad_norm": 0.5283605456352234,
      "learning_rate": 1.2540160697122288e-05,
      "loss": 1.046,
      "step": 4663
    },
    {
      "epoch": 43.59416445623342,
      "grad_norm": 0.48206961154937744,
      "learning_rate": 1.2537232988155394e-05,
      "loss": 1.0435,
      "step": 4664
    },
    {
      "epoch": 43.603595638078396,
      "grad_norm": 0.46585264801979065,
      "learning_rate": 1.2534305046728055e-05,
      "loss": 1.0612,
      "step": 4665
    },
    {
      "epoch": 43.61302681992337,
      "grad_norm": 0.4752447307109833,
      "learning_rate": 1.2531376873108522e-05,
      "loss": 1.057,
      "step": 4666
    },
    {
      "epoch": 43.622458001768344,
      "grad_norm": 0.4854462444782257,
      "learning_rate": 1.2528448467565082e-05,
      "loss": 1.0534,
      "step": 4667
    },
    {
      "epoch": 43.631889183613325,
      "grad_norm": 0.5400210618972778,
      "learning_rate": 1.2525519830366032e-05,
      "loss": 1.1157,
      "step": 4668
    },
    {
      "epoch": 43.6413203654583,
      "grad_norm": 0.5104957222938538,
      "learning_rate": 1.2522590961779688e-05,
      "loss": 1.0589,
      "step": 4669
    },
    {
      "epoch": 43.65075154730327,
      "grad_norm": 0.484738826751709,
      "learning_rate": 1.2519661862074395e-05,
      "loss": 1.0404,
      "step": 4670
    },
    {
      "epoch": 43.66018272914825,
      "grad_norm": 0.5102861523628235,
      "learning_rate": 1.2516732531518518e-05,
      "loss": 1.0608,
      "step": 4671
    },
    {
      "epoch": 43.66961391099322,
      "grad_norm": 0.5026674270629883,
      "learning_rate": 1.2513802970380437e-05,
      "loss": 1.0856,
      "step": 4672
    },
    {
      "epoch": 43.679045092838194,
      "grad_norm": 0.5434066653251648,
      "learning_rate": 1.2510873178928565e-05,
      "loss": 1.049,
      "step": 4673
    },
    {
      "epoch": 43.68847627468317,
      "grad_norm": 0.5434489250183105,
      "learning_rate": 1.2507943157431318e-05,
      "loss": 1.0807,
      "step": 4674
    },
    {
      "epoch": 43.69790745652815,
      "grad_norm": 0.5171283483505249,
      "learning_rate": 1.2505012906157152e-05,
      "loss": 1.0556,
      "step": 4675
    },
    {
      "epoch": 43.70733863837312,
      "grad_norm": 0.5304656624794006,
      "learning_rate": 1.2502082425374533e-05,
      "loss": 1.0332,
      "step": 4676
    },
    {
      "epoch": 43.7167698202181,
      "grad_norm": 0.5366683006286621,
      "learning_rate": 1.2499151715351951e-05,
      "loss": 1.0236,
      "step": 4677
    },
    {
      "epoch": 43.72620100206307,
      "grad_norm": 0.5313934087753296,
      "learning_rate": 1.2496220776357914e-05,
      "loss": 1.0907,
      "step": 4678
    },
    {
      "epoch": 43.735632183908045,
      "grad_norm": 0.5162071585655212,
      "learning_rate": 1.2493289608660957e-05,
      "loss": 1.0799,
      "step": 4679
    },
    {
      "epoch": 43.74506336575302,
      "grad_norm": 0.504711389541626,
      "learning_rate": 1.2490358212529631e-05,
      "loss": 1.0772,
      "step": 4680
    },
    {
      "epoch": 43.75449454759799,
      "grad_norm": 0.5238114595413208,
      "learning_rate": 1.2487426588232513e-05,
      "loss": 1.0691,
      "step": 4681
    },
    {
      "epoch": 43.763925729442974,
      "grad_norm": 0.449654221534729,
      "learning_rate": 1.2484494736038191e-05,
      "loss": 1.0626,
      "step": 4682
    },
    {
      "epoch": 43.77335691128795,
      "grad_norm": 0.503523588180542,
      "learning_rate": 1.2481562656215285e-05,
      "loss": 1.0855,
      "step": 4683
    },
    {
      "epoch": 43.78278809313292,
      "grad_norm": 0.48709195852279663,
      "learning_rate": 1.2478630349032428e-05,
      "loss": 1.0671,
      "step": 4684
    },
    {
      "epoch": 43.792219274977896,
      "grad_norm": 0.49970942735671997,
      "learning_rate": 1.2475697814758281e-05,
      "loss": 1.1082,
      "step": 4685
    },
    {
      "epoch": 43.80165045682287,
      "grad_norm": 0.4734046757221222,
      "learning_rate": 1.247276505366152e-05,
      "loss": 1.0848,
      "step": 4686
    },
    {
      "epoch": 43.811081638667844,
      "grad_norm": 0.5124837756156921,
      "learning_rate": 1.2469832066010843e-05,
      "loss": 1.0633,
      "step": 4687
    },
    {
      "epoch": 43.82051282051282,
      "grad_norm": 0.4767795503139496,
      "learning_rate": 1.2466898852074969e-05,
      "loss": 1.0787,
      "step": 4688
    },
    {
      "epoch": 43.8299440023578,
      "grad_norm": 0.5055187940597534,
      "learning_rate": 1.2463965412122638e-05,
      "loss": 1.0863,
      "step": 4689
    },
    {
      "epoch": 43.83937518420277,
      "grad_norm": 0.4918765723705292,
      "learning_rate": 1.2461031746422614e-05,
      "loss": 1.0729,
      "step": 4690
    },
    {
      "epoch": 43.84880636604775,
      "grad_norm": 0.4905902147293091,
      "learning_rate": 1.2458097855243677e-05,
      "loss": 1.0441,
      "step": 4691
    },
    {
      "epoch": 43.85823754789272,
      "grad_norm": 0.4984501600265503,
      "learning_rate": 1.2455163738854627e-05,
      "loss": 1.0492,
      "step": 4692
    },
    {
      "epoch": 43.867668729737694,
      "grad_norm": 0.5006220936775208,
      "learning_rate": 1.2452229397524287e-05,
      "loss": 1.0694,
      "step": 4693
    },
    {
      "epoch": 43.87709991158267,
      "grad_norm": 0.4861879348754883,
      "learning_rate": 1.24492948315215e-05,
      "loss": 1.0958,
      "step": 4694
    },
    {
      "epoch": 43.88653109342764,
      "grad_norm": 0.5214107036590576,
      "learning_rate": 1.2446360041115137e-05,
      "loss": 1.0593,
      "step": 4695
    },
    {
      "epoch": 43.89596227527262,
      "grad_norm": 0.46818485856056213,
      "learning_rate": 1.2443425026574072e-05,
      "loss": 1.1011,
      "step": 4696
    },
    {
      "epoch": 43.9053934571176,
      "grad_norm": 0.5043911933898926,
      "learning_rate": 1.2440489788167216e-05,
      "loss": 1.0868,
      "step": 4697
    },
    {
      "epoch": 43.91482463896257,
      "grad_norm": 0.5579220652580261,
      "learning_rate": 1.2437554326163494e-05,
      "loss": 1.1026,
      "step": 4698
    },
    {
      "epoch": 43.924255820807545,
      "grad_norm": 0.4911378026008606,
      "learning_rate": 1.2434618640831852e-05,
      "loss": 1.074,
      "step": 4699
    },
    {
      "epoch": 43.93368700265252,
      "grad_norm": 0.5012003779411316,
      "learning_rate": 1.2431682732441255e-05,
      "loss": 1.0633,
      "step": 4700
    },
    {
      "epoch": 43.94311818449749,
      "grad_norm": 0.48255395889282227,
      "learning_rate": 1.2428746601260692e-05,
      "loss": 1.0988,
      "step": 4701
    },
    {
      "epoch": 43.95254936634247,
      "grad_norm": 0.5322487354278564,
      "learning_rate": 1.2425810247559171e-05,
      "loss": 1.1038,
      "step": 4702
    },
    {
      "epoch": 43.96198054818745,
      "grad_norm": 0.4936636686325073,
      "learning_rate": 1.2422873671605717e-05,
      "loss": 1.0843,
      "step": 4703
    },
    {
      "epoch": 43.97141173003242,
      "grad_norm": 0.503411054611206,
      "learning_rate": 1.2419936873669375e-05,
      "loss": 1.0682,
      "step": 4704
    },
    {
      "epoch": 43.980842911877396,
      "grad_norm": 0.5422892570495605,
      "learning_rate": 1.2416999854019225e-05,
      "loss": 1.054,
      "step": 4705
    },
    {
      "epoch": 43.99027409372237,
      "grad_norm": 0.49140238761901855,
      "learning_rate": 1.2414062612924345e-05,
      "loss": 1.0734,
      "step": 4706
    },
    {
      "epoch": 43.999705275567344,
      "grad_norm": 0.49900978803634644,
      "learning_rate": 1.2411125150653846e-05,
      "loss": 1.0668,
      "step": 4707
    },
    {
      "epoch": 44.0,
      "grad_norm": 4.440542697906494,
      "learning_rate": 1.2408187467476862e-05,
      "loss": 0.4829,
      "step": 4708
    },
    {
      "epoch": 44.009431181844974,
      "grad_norm": 0.49749258160591125,
      "learning_rate": 1.2405249563662539e-05,
      "loss": 1.0357,
      "step": 4709
    },
    {
      "epoch": 44.01886236368995,
      "grad_norm": 0.47851768136024475,
      "learning_rate": 1.2402311439480046e-05,
      "loss": 1.0498,
      "step": 4710
    },
    {
      "epoch": 44.02829354553492,
      "grad_norm": 0.5428802371025085,
      "learning_rate": 1.2399373095198573e-05,
      "loss": 1.0357,
      "step": 4711
    },
    {
      "epoch": 44.0377247273799,
      "grad_norm": 0.5134526491165161,
      "learning_rate": 1.2396434531087334e-05,
      "loss": 1.0467,
      "step": 4712
    },
    {
      "epoch": 44.04715590922488,
      "grad_norm": 0.4736410677433014,
      "learning_rate": 1.2393495747415556e-05,
      "loss": 1.0561,
      "step": 4713
    },
    {
      "epoch": 44.05658709106985,
      "grad_norm": 0.5086113214492798,
      "learning_rate": 1.2390556744452491e-05,
      "loss": 1.0517,
      "step": 4714
    },
    {
      "epoch": 44.066018272914825,
      "grad_norm": 0.505027711391449,
      "learning_rate": 1.2387617522467403e-05,
      "loss": 1.0662,
      "step": 4715
    },
    {
      "epoch": 44.0754494547598,
      "grad_norm": 0.4602670669555664,
      "learning_rate": 1.2384678081729592e-05,
      "loss": 1.0748,
      "step": 4716
    },
    {
      "epoch": 44.08488063660477,
      "grad_norm": 0.5034034848213196,
      "learning_rate": 1.2381738422508365e-05,
      "loss": 1.0503,
      "step": 4717
    },
    {
      "epoch": 44.09431181844975,
      "grad_norm": 0.5032168030738831,
      "learning_rate": 1.2378798545073054e-05,
      "loss": 1.0764,
      "step": 4718
    },
    {
      "epoch": 44.10374300029473,
      "grad_norm": 0.4856911301612854,
      "learning_rate": 1.2375858449693003e-05,
      "loss": 1.0492,
      "step": 4719
    },
    {
      "epoch": 44.1131741821397,
      "grad_norm": 0.48255354166030884,
      "learning_rate": 1.237291813663759e-05,
      "loss": 1.0314,
      "step": 4720
    },
    {
      "epoch": 44.122605363984675,
      "grad_norm": 0.5009976625442505,
      "learning_rate": 1.2369977606176204e-05,
      "loss": 1.048,
      "step": 4721
    },
    {
      "epoch": 44.13203654582965,
      "grad_norm": 0.4884820878505707,
      "learning_rate": 1.2367036858578252e-05,
      "loss": 1.0263,
      "step": 4722
    },
    {
      "epoch": 44.14146772767462,
      "grad_norm": 0.47975432872772217,
      "learning_rate": 1.2364095894113168e-05,
      "loss": 1.056,
      "step": 4723
    },
    {
      "epoch": 44.1508989095196,
      "grad_norm": 0.5424911379814148,
      "learning_rate": 1.2361154713050403e-05,
      "loss": 1.0787,
      "step": 4724
    },
    {
      "epoch": 44.16033009136457,
      "grad_norm": 0.5188061594963074,
      "learning_rate": 1.2358213315659422e-05,
      "loss": 1.1034,
      "step": 4725
    },
    {
      "epoch": 44.16976127320955,
      "grad_norm": 0.4790523648262024,
      "learning_rate": 1.2355271702209716e-05,
      "loss": 1.0956,
      "step": 4726
    },
    {
      "epoch": 44.179192455054526,
      "grad_norm": 0.5148712396621704,
      "learning_rate": 1.23523298729708e-05,
      "loss": 1.0865,
      "step": 4727
    },
    {
      "epoch": 44.1886236368995,
      "grad_norm": 0.5342432856559753,
      "learning_rate": 1.23493878282122e-05,
      "loss": 1.0323,
      "step": 4728
    },
    {
      "epoch": 44.198054818744474,
      "grad_norm": 0.502383291721344,
      "learning_rate": 1.2346445568203464e-05,
      "loss": 1.0536,
      "step": 4729
    },
    {
      "epoch": 44.20748600058945,
      "grad_norm": 0.47237494587898254,
      "learning_rate": 1.234350309321416e-05,
      "loss": 1.0807,
      "step": 4730
    },
    {
      "epoch": 44.21691718243442,
      "grad_norm": 0.4947200417518616,
      "learning_rate": 1.234056040351388e-05,
      "loss": 1.0581,
      "step": 4731
    },
    {
      "epoch": 44.226348364279396,
      "grad_norm": 0.46084514260292053,
      "learning_rate": 1.2337617499372231e-05,
      "loss": 1.0648,
      "step": 4732
    },
    {
      "epoch": 44.23577954612438,
      "grad_norm": 0.49732598662376404,
      "learning_rate": 1.2334674381058842e-05,
      "loss": 1.0068,
      "step": 4733
    },
    {
      "epoch": 44.24521072796935,
      "grad_norm": 0.5069977641105652,
      "learning_rate": 1.2331731048843354e-05,
      "loss": 1.0502,
      "step": 4734
    },
    {
      "epoch": 44.254641909814325,
      "grad_norm": 0.5058279037475586,
      "learning_rate": 1.2328787502995443e-05,
      "loss": 1.0454,
      "step": 4735
    },
    {
      "epoch": 44.2640730916593,
      "grad_norm": 0.4879036247730255,
      "learning_rate": 1.2325843743784794e-05,
      "loss": 1.0941,
      "step": 4736
    },
    {
      "epoch": 44.27350427350427,
      "grad_norm": 0.468305379152298,
      "learning_rate": 1.2322899771481106e-05,
      "loss": 1.1145,
      "step": 4737
    },
    {
      "epoch": 44.28293545534925,
      "grad_norm": 0.5017635226249695,
      "learning_rate": 1.2319955586354113e-05,
      "loss": 1.0668,
      "step": 4738
    },
    {
      "epoch": 44.29236663719422,
      "grad_norm": 0.5144129991531372,
      "learning_rate": 1.2317011188673558e-05,
      "loss": 1.0623,
      "step": 4739
    },
    {
      "epoch": 44.3017978190392,
      "grad_norm": 0.5123069882392883,
      "learning_rate": 1.2314066578709203e-05,
      "loss": 1.0896,
      "step": 4740
    },
    {
      "epoch": 44.311229000884175,
      "grad_norm": 0.48877519369125366,
      "learning_rate": 1.2311121756730833e-05,
      "loss": 1.0804,
      "step": 4741
    },
    {
      "epoch": 44.32066018272915,
      "grad_norm": 0.5705971121788025,
      "learning_rate": 1.2308176723008257e-05,
      "loss": 1.0078,
      "step": 4742
    },
    {
      "epoch": 44.33009136457412,
      "grad_norm": 0.5198885798454285,
      "learning_rate": 1.230523147781129e-05,
      "loss": 1.0748,
      "step": 4743
    },
    {
      "epoch": 44.3395225464191,
      "grad_norm": 0.4984634518623352,
      "learning_rate": 1.2302286021409782e-05,
      "loss": 1.0915,
      "step": 4744
    },
    {
      "epoch": 44.34895372826407,
      "grad_norm": 0.5004709959030151,
      "learning_rate": 1.2299340354073586e-05,
      "loss": 1.0378,
      "step": 4745
    },
    {
      "epoch": 44.358384910109045,
      "grad_norm": 0.5488479137420654,
      "learning_rate": 1.2296394476072591e-05,
      "loss": 1.0401,
      "step": 4746
    },
    {
      "epoch": 44.367816091954026,
      "grad_norm": 0.5281381011009216,
      "learning_rate": 1.2293448387676695e-05,
      "loss": 1.0598,
      "step": 4747
    },
    {
      "epoch": 44.377247273799,
      "grad_norm": 0.5315407514572144,
      "learning_rate": 1.2290502089155814e-05,
      "loss": 1.0798,
      "step": 4748
    },
    {
      "epoch": 44.386678455643974,
      "grad_norm": 0.5099145770072937,
      "learning_rate": 1.2287555580779891e-05,
      "loss": 1.0632,
      "step": 4749
    },
    {
      "epoch": 44.39610963748895,
      "grad_norm": 0.4972279369831085,
      "learning_rate": 1.2284608862818883e-05,
      "loss": 1.1009,
      "step": 4750
    },
    {
      "epoch": 44.40554081933392,
      "grad_norm": 0.5162376761436462,
      "learning_rate": 1.2281661935542769e-05,
      "loss": 1.0402,
      "step": 4751
    },
    {
      "epoch": 44.414972001178896,
      "grad_norm": 0.48882949352264404,
      "learning_rate": 1.2278714799221542e-05,
      "loss": 1.0901,
      "step": 4752
    },
    {
      "epoch": 44.42440318302387,
      "grad_norm": 0.46682286262512207,
      "learning_rate": 1.227576745412522e-05,
      "loss": 1.0684,
      "step": 4753
    },
    {
      "epoch": 44.43383436486885,
      "grad_norm": 0.49421343207359314,
      "learning_rate": 1.2272819900523838e-05,
      "loss": 1.0649,
      "step": 4754
    },
    {
      "epoch": 44.443265546713825,
      "grad_norm": 0.5220606327056885,
      "learning_rate": 1.2269872138687448e-05,
      "loss": 1.1075,
      "step": 4755
    },
    {
      "epoch": 44.4526967285588,
      "grad_norm": 0.49394261837005615,
      "learning_rate": 1.2266924168886123e-05,
      "loss": 1.0329,
      "step": 4756
    },
    {
      "epoch": 44.46212791040377,
      "grad_norm": 0.5098066926002502,
      "learning_rate": 1.226397599138996e-05,
      "loss": 1.0404,
      "step": 4757
    },
    {
      "epoch": 44.47155909224875,
      "grad_norm": 0.4766665995121002,
      "learning_rate": 1.2261027606469065e-05,
      "loss": 1.0627,
      "step": 4758
    },
    {
      "epoch": 44.48099027409372,
      "grad_norm": 0.5293686985969543,
      "learning_rate": 1.2258079014393566e-05,
      "loss": 1.0885,
      "step": 4759
    },
    {
      "epoch": 44.490421455938694,
      "grad_norm": 0.488854318857193,
      "learning_rate": 1.2255130215433617e-05,
      "loss": 1.0646,
      "step": 4760
    },
    {
      "epoch": 44.499852637783675,
      "grad_norm": 0.5290549397468567,
      "learning_rate": 1.2252181209859386e-05,
      "loss": 1.092,
      "step": 4761
    },
    {
      "epoch": 44.50928381962865,
      "grad_norm": 0.49207156896591187,
      "learning_rate": 1.2249231997941058e-05,
      "loss": 1.0426,
      "step": 4762
    },
    {
      "epoch": 44.51871500147362,
      "grad_norm": 0.5288107395172119,
      "learning_rate": 1.2246282579948837e-05,
      "loss": 1.0916,
      "step": 4763
    },
    {
      "epoch": 44.5281461833186,
      "grad_norm": 0.517284095287323,
      "learning_rate": 1.2243332956152952e-05,
      "loss": 1.1006,
      "step": 4764
    },
    {
      "epoch": 44.53757736516357,
      "grad_norm": 0.5133883357048035,
      "learning_rate": 1.2240383126823644e-05,
      "loss": 1.0676,
      "step": 4765
    },
    {
      "epoch": 44.547008547008545,
      "grad_norm": 0.5272760987281799,
      "learning_rate": 1.2237433092231172e-05,
      "loss": 1.0825,
      "step": 4766
    },
    {
      "epoch": 44.55643972885352,
      "grad_norm": 0.4752357006072998,
      "learning_rate": 1.2234482852645822e-05,
      "loss": 1.0697,
      "step": 4767
    },
    {
      "epoch": 44.5658709106985,
      "grad_norm": 0.5001272559165955,
      "learning_rate": 1.2231532408337896e-05,
      "loss": 1.0996,
      "step": 4768
    },
    {
      "epoch": 44.575302092543474,
      "grad_norm": 0.4884883761405945,
      "learning_rate": 1.2228581759577707e-05,
      "loss": 1.0943,
      "step": 4769
    },
    {
      "epoch": 44.58473327438845,
      "grad_norm": 0.5259696245193481,
      "learning_rate": 1.2225630906635597e-05,
      "loss": 1.0753,
      "step": 4770
    },
    {
      "epoch": 44.59416445623342,
      "grad_norm": 0.4645330607891083,
      "learning_rate": 1.2222679849781915e-05,
      "loss": 1.0599,
      "step": 4771
    },
    {
      "epoch": 44.603595638078396,
      "grad_norm": 0.4917657971382141,
      "learning_rate": 1.2219728589287048e-05,
      "loss": 1.0667,
      "step": 4772
    },
    {
      "epoch": 44.61302681992337,
      "grad_norm": 0.5054764747619629,
      "learning_rate": 1.221677712542138e-05,
      "loss": 1.1133,
      "step": 4773
    },
    {
      "epoch": 44.622458001768344,
      "grad_norm": 0.5813475847244263,
      "learning_rate": 1.2213825458455324e-05,
      "loss": 1.0564,
      "step": 4774
    },
    {
      "epoch": 44.631889183613325,
      "grad_norm": 0.5069020390510559,
      "learning_rate": 1.221087358865931e-05,
      "loss": 1.0675,
      "step": 4775
    },
    {
      "epoch": 44.6413203654583,
      "grad_norm": 0.5023509860038757,
      "learning_rate": 1.2207921516303796e-05,
      "loss": 1.0827,
      "step": 4776
    },
    {
      "epoch": 44.65075154730327,
      "grad_norm": 0.4972259998321533,
      "learning_rate": 1.2204969241659241e-05,
      "loss": 1.0808,
      "step": 4777
    },
    {
      "epoch": 44.66018272914825,
      "grad_norm": 0.49430638551712036,
      "learning_rate": 1.2202016764996132e-05,
      "loss": 1.0829,
      "step": 4778
    },
    {
      "epoch": 44.66961391099322,
      "grad_norm": 0.5321018695831299,
      "learning_rate": 1.2199064086584978e-05,
      "loss": 1.0967,
      "step": 4779
    },
    {
      "epoch": 44.679045092838194,
      "grad_norm": 0.5414420962333679,
      "learning_rate": 1.21961112066963e-05,
      "loss": 1.0632,
      "step": 4780
    },
    {
      "epoch": 44.68847627468317,
      "grad_norm": 0.49755001068115234,
      "learning_rate": 1.219315812560064e-05,
      "loss": 1.0943,
      "step": 4781
    },
    {
      "epoch": 44.69790745652815,
      "grad_norm": 0.5456342101097107,
      "learning_rate": 1.2190204843568559e-05,
      "loss": 1.0918,
      "step": 4782
    },
    {
      "epoch": 44.70733863837312,
      "grad_norm": 0.5237525105476379,
      "learning_rate": 1.2187251360870634e-05,
      "loss": 1.0787,
      "step": 4783
    },
    {
      "epoch": 44.7167698202181,
      "grad_norm": 0.5121671557426453,
      "learning_rate": 1.2184297677777463e-05,
      "loss": 1.0603,
      "step": 4784
    },
    {
      "epoch": 44.72620100206307,
      "grad_norm": 0.5248687863349915,
      "learning_rate": 1.2181343794559665e-05,
      "loss": 1.0456,
      "step": 4785
    },
    {
      "epoch": 44.735632183908045,
      "grad_norm": 0.5054583549499512,
      "learning_rate": 1.2178389711487869e-05,
      "loss": 1.074,
      "step": 4786
    },
    {
      "epoch": 44.74506336575302,
      "grad_norm": 0.4887649118900299,
      "learning_rate": 1.217543542883273e-05,
      "loss": 1.0673,
      "step": 4787
    },
    {
      "epoch": 44.75449454759799,
      "grad_norm": 0.5473950505256653,
      "learning_rate": 1.2172480946864916e-05,
      "loss": 1.1006,
      "step": 4788
    },
    {
      "epoch": 44.763925729442974,
      "grad_norm": 0.5216667056083679,
      "learning_rate": 1.2169526265855116e-05,
      "loss": 1.1231,
      "step": 4789
    },
    {
      "epoch": 44.77335691128795,
      "grad_norm": 0.4875560998916626,
      "learning_rate": 1.2166571386074038e-05,
      "loss": 1.0779,
      "step": 4790
    },
    {
      "epoch": 44.78278809313292,
      "grad_norm": 0.4883314371109009,
      "learning_rate": 1.216361630779241e-05,
      "loss": 1.05,
      "step": 4791
    },
    {
      "epoch": 44.792219274977896,
      "grad_norm": 0.4760235846042633,
      "learning_rate": 1.216066103128097e-05,
      "loss": 0.9931,
      "step": 4792
    },
    {
      "epoch": 44.80165045682287,
      "grad_norm": 0.5062671899795532,
      "learning_rate": 1.215770555681048e-05,
      "loss": 1.0412,
      "step": 4793
    },
    {
      "epoch": 44.811081638667844,
      "grad_norm": 0.5200695395469666,
      "learning_rate": 1.2154749884651722e-05,
      "loss": 1.0938,
      "step": 4794
    },
    {
      "epoch": 44.82051282051282,
      "grad_norm": 0.5444120764732361,
      "learning_rate": 1.2151794015075495e-05,
      "loss": 1.0702,
      "step": 4795
    },
    {
      "epoch": 44.8299440023578,
      "grad_norm": 0.4963391423225403,
      "learning_rate": 1.2148837948352612e-05,
      "loss": 1.0326,
      "step": 4796
    },
    {
      "epoch": 44.83937518420277,
      "grad_norm": 0.5254226326942444,
      "learning_rate": 1.2145881684753904e-05,
      "loss": 1.0484,
      "step": 4797
    },
    {
      "epoch": 44.84880636604775,
      "grad_norm": 0.5066537857055664,
      "learning_rate": 1.214292522455023e-05,
      "loss": 1.0459,
      "step": 4798
    },
    {
      "epoch": 44.85823754789272,
      "grad_norm": 0.48620808124542236,
      "learning_rate": 1.2139968568012457e-05,
      "loss": 1.0926,
      "step": 4799
    },
    {
      "epoch": 44.867668729737694,
      "grad_norm": 0.4632420837879181,
      "learning_rate": 1.213701171541147e-05,
      "loss": 1.0601,
      "step": 4800
    },
    {
      "epoch": 44.87709991158267,
      "grad_norm": 0.54347825050354,
      "learning_rate": 1.2134054667018179e-05,
      "loss": 1.0844,
      "step": 4801
    },
    {
      "epoch": 44.88653109342764,
      "grad_norm": 0.5015242695808411,
      "learning_rate": 1.2131097423103504e-05,
      "loss": 1.0664,
      "step": 4802
    },
    {
      "epoch": 44.89596227527262,
      "grad_norm": 0.4994272291660309,
      "learning_rate": 1.212813998393839e-05,
      "loss": 1.0727,
      "step": 4803
    },
    {
      "epoch": 44.9053934571176,
      "grad_norm": 0.5233644247055054,
      "learning_rate": 1.2125182349793797e-05,
      "loss": 1.0582,
      "step": 4804
    },
    {
      "epoch": 44.91482463896257,
      "grad_norm": 0.5065621733665466,
      "learning_rate": 1.2122224520940695e-05,
      "loss": 1.1014,
      "step": 4805
    },
    {
      "epoch": 44.924255820807545,
      "grad_norm": 0.5187067985534668,
      "learning_rate": 1.2119266497650091e-05,
      "loss": 1.0604,
      "step": 4806
    },
    {
      "epoch": 44.93368700265252,
      "grad_norm": 0.473105788230896,
      "learning_rate": 1.211630828019299e-05,
      "loss": 1.0516,
      "step": 4807
    },
    {
      "epoch": 44.94311818449749,
      "grad_norm": 0.5562224984169006,
      "learning_rate": 1.2113349868840422e-05,
      "loss": 1.0973,
      "step": 4808
    },
    {
      "epoch": 44.95254936634247,
      "grad_norm": 0.5393129587173462,
      "learning_rate": 1.2110391263863442e-05,
      "loss": 1.0458,
      "step": 4809
    },
    {
      "epoch": 44.96198054818745,
      "grad_norm": 0.5225141048431396,
      "learning_rate": 1.2107432465533114e-05,
      "loss": 1.0666,
      "step": 4810
    },
    {
      "epoch": 44.97141173003242,
      "grad_norm": 0.5313158631324768,
      "learning_rate": 1.210447347412052e-05,
      "loss": 1.0783,
      "step": 4811
    },
    {
      "epoch": 44.980842911877396,
      "grad_norm": 0.5259534120559692,
      "learning_rate": 1.2101514289896758e-05,
      "loss": 1.0345,
      "step": 4812
    },
    {
      "epoch": 44.99027409372237,
      "grad_norm": 0.5003887414932251,
      "learning_rate": 1.2098554913132958e-05,
      "loss": 1.0652,
      "step": 4813
    },
    {
      "epoch": 44.999705275567344,
      "grad_norm": 0.49052032828330994,
      "learning_rate": 1.2095595344100248e-05,
      "loss": 1.0639,
      "step": 4814
    },
    {
      "epoch": 45.0,
      "grad_norm": 3.153012275695801,
      "learning_rate": 1.2092635583069792e-05,
      "loss": 0.405,
      "step": 4815
    },
    {
      "epoch": 45.009431181844974,
      "grad_norm": 0.5227983593940735,
      "learning_rate": 1.2089675630312755e-05,
      "loss": 1.1041,
      "step": 4816
    },
    {
      "epoch": 45.01886236368995,
      "grad_norm": 0.546749472618103,
      "learning_rate": 1.2086715486100325e-05,
      "loss": 1.0767,
      "step": 4817
    },
    {
      "epoch": 45.02829354553492,
      "grad_norm": 0.5259672403335571,
      "learning_rate": 1.2083755150703714e-05,
      "loss": 1.0804,
      "step": 4818
    },
    {
      "epoch": 45.0377247273799,
      "grad_norm": 0.5174475312232971,
      "learning_rate": 1.2080794624394145e-05,
      "loss": 1.0556,
      "step": 4819
    },
    {
      "epoch": 45.04715590922488,
      "grad_norm": 0.5275866985321045,
      "learning_rate": 1.2077833907442864e-05,
      "loss": 1.0641,
      "step": 4820
    },
    {
      "epoch": 45.05658709106985,
      "grad_norm": 0.5064834356307983,
      "learning_rate": 1.2074873000121127e-05,
      "loss": 1.0571,
      "step": 4821
    },
    {
      "epoch": 45.066018272914825,
      "grad_norm": 0.5625993609428406,
      "learning_rate": 1.2071911902700216e-05,
      "loss": 1.0466,
      "step": 4822
    },
    {
      "epoch": 45.0754494547598,
      "grad_norm": 0.5535951256752014,
      "learning_rate": 1.2068950615451418e-05,
      "loss": 1.0723,
      "step": 4823
    },
    {
      "epoch": 45.08488063660477,
      "grad_norm": 0.4917502701282501,
      "learning_rate": 1.206598913864605e-05,
      "loss": 1.0496,
      "step": 4824
    },
    {
      "epoch": 45.09431181844975,
      "grad_norm": 0.4748583436012268,
      "learning_rate": 1.2063027472555444e-05,
      "loss": 1.0801,
      "step": 4825
    },
    {
      "epoch": 45.10374300029473,
      "grad_norm": 0.5486068725585938,
      "learning_rate": 1.2060065617450943e-05,
      "loss": 1.0746,
      "step": 4826
    },
    {
      "epoch": 45.1131741821397,
      "grad_norm": 0.49094030261039734,
      "learning_rate": 1.205710357360391e-05,
      "loss": 1.0512,
      "step": 4827
    },
    {
      "epoch": 45.122605363984675,
      "grad_norm": 0.48959022760391235,
      "learning_rate": 1.205414134128573e-05,
      "loss": 1.0759,
      "step": 4828
    },
    {
      "epoch": 45.13203654582965,
      "grad_norm": 0.4813189208507538,
      "learning_rate": 1.2051178920767801e-05,
      "loss": 1.0541,
      "step": 4829
    },
    {
      "epoch": 45.14146772767462,
      "grad_norm": 0.4919016659259796,
      "learning_rate": 1.2048216312321539e-05,
      "loss": 1.0507,
      "step": 4830
    },
    {
      "epoch": 45.1508989095196,
      "grad_norm": 0.48880890011787415,
      "learning_rate": 1.2045253516218374e-05,
      "loss": 1.0878,
      "step": 4831
    },
    {
      "epoch": 45.16033009136457,
      "grad_norm": 0.5178472995758057,
      "learning_rate": 1.2042290532729759e-05,
      "loss": 1.0801,
      "step": 4832
    },
    {
      "epoch": 45.16976127320955,
      "grad_norm": 0.5119066834449768,
      "learning_rate": 1.2039327362127165e-05,
      "loss": 1.0618,
      "step": 4833
    },
    {
      "epoch": 45.179192455054526,
      "grad_norm": 0.5415460467338562,
      "learning_rate": 1.2036364004682068e-05,
      "loss": 1.054,
      "step": 4834
    },
    {
      "epoch": 45.1886236368995,
      "grad_norm": 0.5244242548942566,
      "learning_rate": 1.2033400460665979e-05,
      "loss": 1.0928,
      "step": 4835
    },
    {
      "epoch": 45.198054818744474,
      "grad_norm": 0.4900732934474945,
      "learning_rate": 1.2030436730350413e-05,
      "loss": 1.0425,
      "step": 4836
    },
    {
      "epoch": 45.20748600058945,
      "grad_norm": 0.519028902053833,
      "learning_rate": 1.2027472814006904e-05,
      "loss": 1.0614,
      "step": 4837
    },
    {
      "epoch": 45.21691718243442,
      "grad_norm": 0.4775128960609436,
      "learning_rate": 1.2024508711907006e-05,
      "loss": 1.0861,
      "step": 4838
    },
    {
      "epoch": 45.226348364279396,
      "grad_norm": 0.49243688583374023,
      "learning_rate": 1.2021544424322294e-05,
      "loss": 1.0629,
      "step": 4839
    },
    {
      "epoch": 45.23577954612438,
      "grad_norm": 0.48228031396865845,
      "learning_rate": 1.2018579951524348e-05,
      "loss": 1.0678,
      "step": 4840
    },
    {
      "epoch": 45.24521072796935,
      "grad_norm": 0.49481913447380066,
      "learning_rate": 1.2015615293784777e-05,
      "loss": 1.093,
      "step": 4841
    },
    {
      "epoch": 45.254641909814325,
      "grad_norm": 0.523501992225647,
      "learning_rate": 1.2012650451375198e-05,
      "loss": 1.0342,
      "step": 4842
    },
    {
      "epoch": 45.2640730916593,
      "grad_norm": 0.5388510823249817,
      "learning_rate": 1.2009685424567252e-05,
      "loss": 1.046,
      "step": 4843
    },
    {
      "epoch": 45.27350427350427,
      "grad_norm": 0.48255985975265503,
      "learning_rate": 1.2006720213632591e-05,
      "loss": 1.1138,
      "step": 4844
    },
    {
      "epoch": 45.28293545534925,
      "grad_norm": 0.5139954090118408,
      "learning_rate": 1.200375481884289e-05,
      "loss": 1.0743,
      "step": 4845
    },
    {
      "epoch": 45.29236663719422,
      "grad_norm": 0.513889729976654,
      "learning_rate": 1.2000789240469837e-05,
      "loss": 1.0927,
      "step": 4846
    },
    {
      "epoch": 45.3017978190392,
      "grad_norm": 0.5211244821548462,
      "learning_rate": 1.1997823478785133e-05,
      "loss": 1.0149,
      "step": 4847
    },
    {
      "epoch": 45.311229000884175,
      "grad_norm": 0.5100012421607971,
      "learning_rate": 1.1994857534060507e-05,
      "loss": 1.1225,
      "step": 4848
    },
    {
      "epoch": 45.32066018272915,
      "grad_norm": 0.6004391312599182,
      "learning_rate": 1.1991891406567693e-05,
      "loss": 1.0484,
      "step": 4849
    },
    {
      "epoch": 45.33009136457412,
      "grad_norm": 0.5164588689804077,
      "learning_rate": 1.1988925096578446e-05,
      "loss": 1.0586,
      "step": 4850
    },
    {
      "epoch": 45.3395225464191,
      "grad_norm": 0.5221835374832153,
      "learning_rate": 1.1985958604364542e-05,
      "loss": 1.0673,
      "step": 4851
    },
    {
      "epoch": 45.34895372826407,
      "grad_norm": 0.4525623023509979,
      "learning_rate": 1.1982991930197767e-05,
      "loss": 1.0702,
      "step": 4852
    },
    {
      "epoch": 45.358384910109045,
      "grad_norm": 0.4956890642642975,
      "learning_rate": 1.1980025074349929e-05,
      "loss": 1.0774,
      "step": 4853
    },
    {
      "epoch": 45.367816091954026,
      "grad_norm": 0.5228610038757324,
      "learning_rate": 1.1977058037092848e-05,
      "loss": 1.0698,
      "step": 4854
    },
    {
      "epoch": 45.377247273799,
      "grad_norm": 0.5399162769317627,
      "learning_rate": 1.1974090818698366e-05,
      "loss": 1.0366,
      "step": 4855
    },
    {
      "epoch": 45.386678455643974,
      "grad_norm": 0.5113661885261536,
      "learning_rate": 1.1971123419438334e-05,
      "loss": 1.0647,
      "step": 4856
    },
    {
      "epoch": 45.39610963748895,
      "grad_norm": 0.512728214263916,
      "learning_rate": 1.1968155839584628e-05,
      "loss": 1.0602,
      "step": 4857
    },
    {
      "epoch": 45.40554081933392,
      "grad_norm": 0.5190832614898682,
      "learning_rate": 1.1965188079409136e-05,
      "loss": 1.0662,
      "step": 4858
    },
    {
      "epoch": 45.414972001178896,
      "grad_norm": 0.49131298065185547,
      "learning_rate": 1.1962220139183765e-05,
      "loss": 1.0453,
      "step": 4859
    },
    {
      "epoch": 45.42440318302387,
      "grad_norm": 0.4769326448440552,
      "learning_rate": 1.1959252019180432e-05,
      "loss": 1.0401,
      "step": 4860
    },
    {
      "epoch": 45.43383436486885,
      "grad_norm": 0.4844127893447876,
      "learning_rate": 1.1956283719671076e-05,
      "loss": 1.0401,
      "step": 4861
    },
    {
      "epoch": 45.443265546713825,
      "grad_norm": 0.5290850400924683,
      "learning_rate": 1.1953315240927655e-05,
      "loss": 1.0875,
      "step": 4862
    },
    {
      "epoch": 45.4526967285588,
      "grad_norm": 0.6163800954818726,
      "learning_rate": 1.195034658322214e-05,
      "loss": 1.0721,
      "step": 4863
    },
    {
      "epoch": 45.46212791040377,
      "grad_norm": 0.5135213136672974,
      "learning_rate": 1.1947377746826512e-05,
      "loss": 1.0821,
      "step": 4864
    },
    {
      "epoch": 45.47155909224875,
      "grad_norm": 0.4895988702774048,
      "learning_rate": 1.1944408732012782e-05,
      "loss": 1.029,
      "step": 4865
    },
    {
      "epoch": 45.48099027409372,
      "grad_norm": 0.5118553042411804,
      "learning_rate": 1.1941439539052971e-05,
      "loss": 1.0868,
      "step": 4866
    },
    {
      "epoch": 45.490421455938694,
      "grad_norm": 0.5010197758674622,
      "learning_rate": 1.1938470168219109e-05,
      "loss": 1.075,
      "step": 4867
    },
    {
      "epoch": 45.499852637783675,
      "grad_norm": 0.49079152941703796,
      "learning_rate": 1.1935500619783252e-05,
      "loss": 1.1072,
      "step": 4868
    },
    {
      "epoch": 45.50928381962865,
      "grad_norm": 0.5252240896224976,
      "learning_rate": 1.1932530894017469e-05,
      "loss": 1.0393,
      "step": 4869
    },
    {
      "epoch": 45.51871500147362,
      "grad_norm": 0.4789491295814514,
      "learning_rate": 1.1929560991193844e-05,
      "loss": 1.1115,
      "step": 4870
    },
    {
      "epoch": 45.5281461833186,
      "grad_norm": 0.5255048274993896,
      "learning_rate": 1.1926590911584484e-05,
      "loss": 1.0585,
      "step": 4871
    },
    {
      "epoch": 45.53757736516357,
      "grad_norm": 0.4883630871772766,
      "learning_rate": 1.1923620655461498e-05,
      "loss": 1.0738,
      "step": 4872
    },
    {
      "epoch": 45.547008547008545,
      "grad_norm": 0.5224630832672119,
      "learning_rate": 1.1920650223097026e-05,
      "loss": 1.0767,
      "step": 4873
    },
    {
      "epoch": 45.55643972885352,
      "grad_norm": 0.4899919331073761,
      "learning_rate": 1.1917679614763218e-05,
      "loss": 1.053,
      "step": 4874
    },
    {
      "epoch": 45.5658709106985,
      "grad_norm": 0.506068229675293,
      "learning_rate": 1.1914708830732235e-05,
      "loss": 1.0576,
      "step": 4875
    },
    {
      "epoch": 45.575302092543474,
      "grad_norm": 0.47458481788635254,
      "learning_rate": 1.1911737871276266e-05,
      "loss": 1.0587,
      "step": 4876
    },
    {
      "epoch": 45.58473327438845,
      "grad_norm": 0.5272055268287659,
      "learning_rate": 1.1908766736667506e-05,
      "loss": 1.0442,
      "step": 4877
    },
    {
      "epoch": 45.59416445623342,
      "grad_norm": 0.5384933352470398,
      "learning_rate": 1.1905795427178167e-05,
      "loss": 1.0771,
      "step": 4878
    },
    {
      "epoch": 45.603595638078396,
      "grad_norm": 0.4861391484737396,
      "learning_rate": 1.1902823943080483e-05,
      "loss": 1.0818,
      "step": 4879
    },
    {
      "epoch": 45.61302681992337,
      "grad_norm": 0.5211024284362793,
      "learning_rate": 1.18998522846467e-05,
      "loss": 1.0789,
      "step": 4880
    },
    {
      "epoch": 45.622458001768344,
      "grad_norm": 0.4844914972782135,
      "learning_rate": 1.1896880452149077e-05,
      "loss": 1.0264,
      "step": 4881
    },
    {
      "epoch": 45.631889183613325,
      "grad_norm": 0.5416277050971985,
      "learning_rate": 1.1893908445859898e-05,
      "loss": 1.0486,
      "step": 4882
    },
    {
      "epoch": 45.6413203654583,
      "grad_norm": 0.5052101612091064,
      "learning_rate": 1.1890936266051453e-05,
      "loss": 1.0849,
      "step": 4883
    },
    {
      "epoch": 45.65075154730327,
      "grad_norm": 0.48798877000808716,
      "learning_rate": 1.1887963912996054e-05,
      "loss": 1.0538,
      "step": 4884
    },
    {
      "epoch": 45.66018272914825,
      "grad_norm": 0.4861200749874115,
      "learning_rate": 1.1884991386966027e-05,
      "loss": 1.091,
      "step": 4885
    },
    {
      "epoch": 45.66961391099322,
      "grad_norm": 0.4974667429924011,
      "learning_rate": 1.1882018688233714e-05,
      "loss": 1.0704,
      "step": 4886
    },
    {
      "epoch": 45.679045092838194,
      "grad_norm": 0.5131087899208069,
      "learning_rate": 1.1879045817071468e-05,
      "loss": 1.0234,
      "step": 4887
    },
    {
      "epoch": 45.68847627468317,
      "grad_norm": 0.5077456831932068,
      "learning_rate": 1.1876072773751668e-05,
      "loss": 1.0393,
      "step": 4888
    },
    {
      "epoch": 45.69790745652815,
      "grad_norm": 0.5171899795532227,
      "learning_rate": 1.1873099558546704e-05,
      "loss": 1.0348,
      "step": 4889
    },
    {
      "epoch": 45.70733863837312,
      "grad_norm": 0.4791061282157898,
      "learning_rate": 1.1870126171728975e-05,
      "loss": 1.0618,
      "step": 4890
    },
    {
      "epoch": 45.7167698202181,
      "grad_norm": 0.49374252557754517,
      "learning_rate": 1.1867152613570907e-05,
      "loss": 1.0532,
      "step": 4891
    },
    {
      "epoch": 45.72620100206307,
      "grad_norm": 0.5533888936042786,
      "learning_rate": 1.1864178884344935e-05,
      "loss": 1.0124,
      "step": 4892
    },
    {
      "epoch": 45.735632183908045,
      "grad_norm": 0.47342902421951294,
      "learning_rate": 1.1861204984323512e-05,
      "loss": 1.0661,
      "step": 4893
    },
    {
      "epoch": 45.74506336575302,
      "grad_norm": 0.49706965684890747,
      "learning_rate": 1.1858230913779101e-05,
      "loss": 1.0527,
      "step": 4894
    },
    {
      "epoch": 45.75449454759799,
      "grad_norm": 0.48415035009384155,
      "learning_rate": 1.185525667298419e-05,
      "loss": 1.0967,
      "step": 4895
    },
    {
      "epoch": 45.763925729442974,
      "grad_norm": 0.5203937292098999,
      "learning_rate": 1.1852282262211279e-05,
      "loss": 1.066,
      "step": 4896
    },
    {
      "epoch": 45.77335691128795,
      "grad_norm": 0.5472111105918884,
      "learning_rate": 1.1849307681732878e-05,
      "loss": 1.0192,
      "step": 4897
    },
    {
      "epoch": 45.78278809313292,
      "grad_norm": 0.5248478055000305,
      "learning_rate": 1.1846332931821517e-05,
      "loss": 1.0312,
      "step": 4898
    },
    {
      "epoch": 45.792219274977896,
      "grad_norm": 0.4634602963924408,
      "learning_rate": 1.1843358012749751e-05,
      "loss": 1.0651,
      "step": 4899
    },
    {
      "epoch": 45.80165045682287,
      "grad_norm": 0.5110455751419067,
      "learning_rate": 1.184038292479013e-05,
      "loss": 1.0618,
      "step": 4900
    },
    {
      "epoch": 45.811081638667844,
      "grad_norm": 0.5309406518936157,
      "learning_rate": 1.1837407668215234e-05,
      "loss": 1.0499,
      "step": 4901
    },
    {
      "epoch": 45.82051282051282,
      "grad_norm": 0.5122696161270142,
      "learning_rate": 1.1834432243297658e-05,
      "loss": 1.0891,
      "step": 4902
    },
    {
      "epoch": 45.8299440023578,
      "grad_norm": 0.4819691479206085,
      "learning_rate": 1.1831456650310004e-05,
      "loss": 1.0906,
      "step": 4903
    },
    {
      "epoch": 45.83937518420277,
      "grad_norm": 0.5622611045837402,
      "learning_rate": 1.1828480889524903e-05,
      "loss": 1.0176,
      "step": 4904
    },
    {
      "epoch": 45.84880636604775,
      "grad_norm": 0.48934122920036316,
      "learning_rate": 1.1825504961214982e-05,
      "loss": 1.0756,
      "step": 4905
    },
    {
      "epoch": 45.85823754789272,
      "grad_norm": 0.49775204062461853,
      "learning_rate": 1.1822528865652904e-05,
      "loss": 1.045,
      "step": 4906
    },
    {
      "epoch": 45.867668729737694,
      "grad_norm": 0.4979499578475952,
      "learning_rate": 1.1819552603111338e-05,
      "loss": 1.076,
      "step": 4907
    },
    {
      "epoch": 45.87709991158267,
      "grad_norm": 0.5162128210067749,
      "learning_rate": 1.1816576173862961e-05,
      "loss": 1.1171,
      "step": 4908
    },
    {
      "epoch": 45.88653109342764,
      "grad_norm": 0.5222750306129456,
      "learning_rate": 1.1813599578180475e-05,
      "loss": 1.0529,
      "step": 4909
    },
    {
      "epoch": 45.89596227527262,
      "grad_norm": 0.4782489538192749,
      "learning_rate": 1.1810622816336599e-05,
      "loss": 1.0591,
      "step": 4910
    },
    {
      "epoch": 45.9053934571176,
      "grad_norm": 0.4986887574195862,
      "learning_rate": 1.1807645888604057e-05,
      "loss": 1.0693,
      "step": 4911
    },
    {
      "epoch": 45.91482463896257,
      "grad_norm": 0.5494959950447083,
      "learning_rate": 1.18046687952556e-05,
      "loss": 1.0531,
      "step": 4912
    },
    {
      "epoch": 45.924255820807545,
      "grad_norm": 0.5228692293167114,
      "learning_rate": 1.1801691536563984e-05,
      "loss": 1.0581,
      "step": 4913
    },
    {
      "epoch": 45.93368700265252,
      "grad_norm": 0.49421992897987366,
      "learning_rate": 1.1798714112801986e-05,
      "loss": 1.0829,
      "step": 4914
    },
    {
      "epoch": 45.94311818449749,
      "grad_norm": 0.4444861114025116,
      "learning_rate": 1.1795736524242396e-05,
      "loss": 1.0837,
      "step": 4915
    },
    {
      "epoch": 45.95254936634247,
      "grad_norm": 0.49395695328712463,
      "learning_rate": 1.1792758771158021e-05,
      "loss": 1.0479,
      "step": 4916
    },
    {
      "epoch": 45.96198054818745,
      "grad_norm": 0.47983503341674805,
      "learning_rate": 1.1789780853821681e-05,
      "loss": 1.0673,
      "step": 4917
    },
    {
      "epoch": 45.97141173003242,
      "grad_norm": 0.548128604888916,
      "learning_rate": 1.1786802772506212e-05,
      "loss": 1.0778,
      "step": 4918
    },
    {
      "epoch": 45.980842911877396,
      "grad_norm": 0.5038270950317383,
      "learning_rate": 1.1783824527484465e-05,
      "loss": 1.0507,
      "step": 4919
    },
    {
      "epoch": 45.99027409372237,
      "grad_norm": 0.5152488946914673,
      "learning_rate": 1.1780846119029303e-05,
      "loss": 1.0665,
      "step": 4920
    },
    {
      "epoch": 45.999705275567344,
      "grad_norm": 0.4995690584182739,
      "learning_rate": 1.177786754741361e-05,
      "loss": 1.0858,
      "step": 4921
    },
    {
      "epoch": 46.0,
      "grad_norm": 3.508472442626953,
      "learning_rate": 1.1774888812910283e-05,
      "loss": 0.4726,
      "step": 4922
    },
    {
      "epoch": 46.009431181844974,
      "grad_norm": 0.489278107881546,
      "learning_rate": 1.177190991579223e-05,
      "loss": 1.0241,
      "step": 4923
    },
    {
      "epoch": 46.01886236368995,
      "grad_norm": 0.5104386210441589,
      "learning_rate": 1.1768930856332375e-05,
      "loss": 1.0358,
      "step": 4924
    },
    {
      "epoch": 46.02829354553492,
      "grad_norm": 0.5013202428817749,
      "learning_rate": 1.176595163480366e-05,
      "loss": 1.0444,
      "step": 4925
    },
    {
      "epoch": 46.0377247273799,
      "grad_norm": 0.48563626408576965,
      "learning_rate": 1.1762972251479043e-05,
      "loss": 1.0559,
      "step": 4926
    },
    {
      "epoch": 46.04715590922488,
      "grad_norm": 0.5076807141304016,
      "learning_rate": 1.1759992706631494e-05,
      "loss": 1.0728,
      "step": 4927
    },
    {
      "epoch": 46.05658709106985,
      "grad_norm": 0.47224485874176025,
      "learning_rate": 1.1757013000533992e-05,
      "loss": 1.0801,
      "step": 4928
    },
    {
      "epoch": 46.066018272914825,
      "grad_norm": 0.4812193214893341,
      "learning_rate": 1.1754033133459541e-05,
      "loss": 1.0782,
      "step": 4929
    },
    {
      "epoch": 46.0754494547598,
      "grad_norm": 0.514706015586853,
      "learning_rate": 1.1751053105681156e-05,
      "loss": 1.0484,
      "step": 4930
    },
    {
      "epoch": 46.08488063660477,
      "grad_norm": 0.5353006720542908,
      "learning_rate": 1.1748072917471865e-05,
      "loss": 1.0583,
      "step": 4931
    },
    {
      "epoch": 46.09431181844975,
      "grad_norm": 0.4951072335243225,
      "learning_rate": 1.1745092569104712e-05,
      "loss": 1.0594,
      "step": 4932
    },
    {
      "epoch": 46.10374300029473,
      "grad_norm": 0.5227992534637451,
      "learning_rate": 1.1742112060852758e-05,
      "loss": 1.0203,
      "step": 4933
    },
    {
      "epoch": 46.1131741821397,
      "grad_norm": 0.5398555994033813,
      "learning_rate": 1.1739131392989071e-05,
      "loss": 1.0354,
      "step": 4934
    },
    {
      "epoch": 46.122605363984675,
      "grad_norm": 0.5440331101417542,
      "learning_rate": 1.1736150565786739e-05,
      "loss": 1.0397,
      "step": 4935
    },
    {
      "epoch": 46.13203654582965,
      "grad_norm": 0.4561214745044708,
      "learning_rate": 1.173316957951887e-05,
      "loss": 1.0885,
      "step": 4936
    },
    {
      "epoch": 46.14146772767462,
      "grad_norm": 0.5364029407501221,
      "learning_rate": 1.173018843445858e-05,
      "loss": 1.0404,
      "step": 4937
    },
    {
      "epoch": 46.1508989095196,
      "grad_norm": 0.49172863364219666,
      "learning_rate": 1.1727207130878997e-05,
      "loss": 1.0468,
      "step": 4938
    },
    {
      "epoch": 46.16033009136457,
      "grad_norm": 0.5247974991798401,
      "learning_rate": 1.1724225669053267e-05,
      "loss": 1.0593,
      "step": 4939
    },
    {
      "epoch": 46.16976127320955,
      "grad_norm": 0.5249826312065125,
      "learning_rate": 1.1721244049254552e-05,
      "loss": 1.021,
      "step": 4940
    },
    {
      "epoch": 46.179192455054526,
      "grad_norm": 0.5071228742599487,
      "learning_rate": 1.1718262271756029e-05,
      "loss": 1.0712,
      "step": 4941
    },
    {
      "epoch": 46.1886236368995,
      "grad_norm": 0.5554696917533875,
      "learning_rate": 1.1715280336830881e-05,
      "loss": 1.085,
      "step": 4942
    },
    {
      "epoch": 46.198054818744474,
      "grad_norm": 0.5191724300384521,
      "learning_rate": 1.171229824475232e-05,
      "loss": 1.0547,
      "step": 4943
    },
    {
      "epoch": 46.20748600058945,
      "grad_norm": 0.4609737992286682,
      "learning_rate": 1.170931599579356e-05,
      "loss": 1.0897,
      "step": 4944
    },
    {
      "epoch": 46.21691718243442,
      "grad_norm": 0.4924611747264862,
      "learning_rate": 1.1706333590227833e-05,
      "loss": 1.069,
      "step": 4945
    },
    {
      "epoch": 46.226348364279396,
      "grad_norm": 0.5146198868751526,
      "learning_rate": 1.1703351028328392e-05,
      "loss": 1.0391,
      "step": 4946
    },
    {
      "epoch": 46.23577954612438,
      "grad_norm": 0.5248799920082092,
      "learning_rate": 1.170036831036849e-05,
      "loss": 1.0707,
      "step": 4947
    },
    {
      "epoch": 46.24521072796935,
      "grad_norm": 0.5034068822860718,
      "learning_rate": 1.1697385436621406e-05,
      "loss": 1.0637,
      "step": 4948
    },
    {
      "epoch": 46.254641909814325,
      "grad_norm": 0.48204246163368225,
      "learning_rate": 1.1694402407360433e-05,
      "loss": 1.0668,
      "step": 4949
    },
    {
      "epoch": 46.2640730916593,
      "grad_norm": 0.499006450176239,
      "learning_rate": 1.1691419222858868e-05,
      "loss": 1.1102,
      "step": 4950
    },
    {
      "epoch": 46.27350427350427,
      "grad_norm": 0.5211129188537598,
      "learning_rate": 1.1688435883390036e-05,
      "loss": 1.0485,
      "step": 4951
    },
    {
      "epoch": 46.28293545534925,
      "grad_norm": 0.5358166098594666,
      "learning_rate": 1.168545238922727e-05,
      "loss": 1.032,
      "step": 4952
    },
    {
      "epoch": 46.29236663719422,
      "grad_norm": 0.4936729967594147,
      "learning_rate": 1.1682468740643911e-05,
      "loss": 1.0686,
      "step": 4953
    },
    {
      "epoch": 46.3017978190392,
      "grad_norm": 0.4963116943836212,
      "learning_rate": 1.1679484937913323e-05,
      "loss": 1.0285,
      "step": 4954
    },
    {
      "epoch": 46.311229000884175,
      "grad_norm": 0.5200867652893066,
      "learning_rate": 1.1676500981308883e-05,
      "loss": 1.0474,
      "step": 4955
    },
    {
      "epoch": 46.32066018272915,
      "grad_norm": 0.48515909910202026,
      "learning_rate": 1.1673516871103977e-05,
      "loss": 1.0817,
      "step": 4956
    },
    {
      "epoch": 46.33009136457412,
      "grad_norm": 0.5245356559753418,
      "learning_rate": 1.1670532607572009e-05,
      "loss": 1.066,
      "step": 4957
    },
    {
      "epoch": 46.3395225464191,
      "grad_norm": 0.5128077864646912,
      "learning_rate": 1.1667548190986395e-05,
      "loss": 1.0724,
      "step": 4958
    },
    {
      "epoch": 46.34895372826407,
      "grad_norm": 0.5021846890449524,
      "learning_rate": 1.166456362162057e-05,
      "loss": 1.0586,
      "step": 4959
    },
    {
      "epoch": 46.358384910109045,
      "grad_norm": 0.4983033239841461,
      "learning_rate": 1.1661578899747977e-05,
      "loss": 1.05,
      "step": 4960
    },
    {
      "epoch": 46.367816091954026,
      "grad_norm": 0.4970397353172302,
      "learning_rate": 1.1658594025642073e-05,
      "loss": 1.0501,
      "step": 4961
    },
    {
      "epoch": 46.377247273799,
      "grad_norm": 0.5334359407424927,
      "learning_rate": 1.1655608999576335e-05,
      "loss": 1.0625,
      "step": 4962
    },
    {
      "epoch": 46.386678455643974,
      "grad_norm": 0.5142315626144409,
      "learning_rate": 1.1652623821824251e-05,
      "loss": 1.0446,
      "step": 4963
    },
    {
      "epoch": 46.39610963748895,
      "grad_norm": 0.51640784740448,
      "learning_rate": 1.1649638492659319e-05,
      "loss": 1.0537,
      "step": 4964
    },
    {
      "epoch": 46.40554081933392,
      "grad_norm": 0.5073076486587524,
      "learning_rate": 1.1646653012355049e-05,
      "loss": 1.1166,
      "step": 4965
    },
    {
      "epoch": 46.414972001178896,
      "grad_norm": 0.5441819429397583,
      "learning_rate": 1.164366738118498e-05,
      "loss": 1.0673,
      "step": 4966
    },
    {
      "epoch": 46.42440318302387,
      "grad_norm": 0.480042964220047,
      "learning_rate": 1.164068159942265e-05,
      "loss": 1.0477,
      "step": 4967
    },
    {
      "epoch": 46.43383436486885,
      "grad_norm": 0.5209150314331055,
      "learning_rate": 1.1637695667341613e-05,
      "loss": 1.1087,
      "step": 4968
    },
    {
      "epoch": 46.443265546713825,
      "grad_norm": 0.535070538520813,
      "learning_rate": 1.1634709585215443e-05,
      "loss": 1.1077,
      "step": 4969
    },
    {
      "epoch": 46.4526967285588,
      "grad_norm": 0.49778231978416443,
      "learning_rate": 1.163172335331772e-05,
      "loss": 1.0527,
      "step": 4970
    },
    {
      "epoch": 46.46212791040377,
      "grad_norm": 0.5156708359718323,
      "learning_rate": 1.1628736971922046e-05,
      "loss": 1.0721,
      "step": 4971
    },
    {
      "epoch": 46.47155909224875,
      "grad_norm": 0.5278416872024536,
      "learning_rate": 1.1625750441302029e-05,
      "loss": 1.051,
      "step": 4972
    },
    {
      "epoch": 46.48099027409372,
      "grad_norm": 0.5154152512550354,
      "learning_rate": 1.162276376173129e-05,
      "loss": 1.0705,
      "step": 4973
    },
    {
      "epoch": 46.490421455938694,
      "grad_norm": 0.5072928667068481,
      "learning_rate": 1.1619776933483477e-05,
      "loss": 1.0853,
      "step": 4974
    },
    {
      "epoch": 46.499852637783675,
      "grad_norm": 0.5301060080528259,
      "learning_rate": 1.1616789956832238e-05,
      "loss": 1.0612,
      "step": 4975
    },
    {
      "epoch": 46.50928381962865,
      "grad_norm": 0.4759882390499115,
      "learning_rate": 1.1613802832051236e-05,
      "loss": 1.0462,
      "step": 4976
    },
    {
      "epoch": 46.51871500147362,
      "grad_norm": 0.5076797604560852,
      "learning_rate": 1.1610815559414155e-05,
      "loss": 1.0877,
      "step": 4977
    },
    {
      "epoch": 46.5281461833186,
      "grad_norm": 0.5101344585418701,
      "learning_rate": 1.1607828139194683e-05,
      "loss": 1.081,
      "step": 4978
    },
    {
      "epoch": 46.53757736516357,
      "grad_norm": 0.4797233045101166,
      "learning_rate": 1.1604840571666531e-05,
      "loss": 1.0886,
      "step": 4979
    },
    {
      "epoch": 46.547008547008545,
      "grad_norm": 0.5645163059234619,
      "learning_rate": 1.1601852857103417e-05,
      "loss": 1.0562,
      "step": 4980
    },
    {
      "epoch": 46.55643972885352,
      "grad_norm": 0.5058425664901733,
      "learning_rate": 1.1598864995779073e-05,
      "loss": 1.0499,
      "step": 4981
    },
    {
      "epoch": 46.5658709106985,
      "grad_norm": 0.5067737102508545,
      "learning_rate": 1.1595876987967252e-05,
      "loss": 1.0768,
      "step": 4982
    },
    {
      "epoch": 46.575302092543474,
      "grad_norm": 0.5661231875419617,
      "learning_rate": 1.1592888833941707e-05,
      "loss": 1.039,
      "step": 4983
    },
    {
      "epoch": 46.58473327438845,
      "grad_norm": 0.5322173237800598,
      "learning_rate": 1.1589900533976213e-05,
      "loss": 1.0471,
      "step": 4984
    },
    {
      "epoch": 46.59416445623342,
      "grad_norm": 0.49668991565704346,
      "learning_rate": 1.158691208834456e-05,
      "loss": 1.1013,
      "step": 4985
    },
    {
      "epoch": 46.603595638078396,
      "grad_norm": 0.4863620102405548,
      "learning_rate": 1.1583923497320545e-05,
      "loss": 0.9952,
      "step": 4986
    },
    {
      "epoch": 46.61302681992337,
      "grad_norm": 0.524147093296051,
      "learning_rate": 1.1580934761177985e-05,
      "loss": 1.0602,
      "step": 4987
    },
    {
      "epoch": 46.622458001768344,
      "grad_norm": 0.5264772176742554,
      "learning_rate": 1.1577945880190707e-05,
      "loss": 1.0252,
      "step": 4988
    },
    {
      "epoch": 46.631889183613325,
      "grad_norm": 0.4843198359012604,
      "learning_rate": 1.1574956854632549e-05,
      "loss": 1.0491,
      "step": 4989
    },
    {
      "epoch": 46.6413203654583,
      "grad_norm": 0.5187544226646423,
      "learning_rate": 1.1571967684777362e-05,
      "loss": 1.0664,
      "step": 4990
    },
    {
      "epoch": 46.65075154730327,
      "grad_norm": 0.5114802718162537,
      "learning_rate": 1.156897837089902e-05,
      "loss": 1.0841,
      "step": 4991
    },
    {
      "epoch": 46.66018272914825,
      "grad_norm": 0.51655513048172,
      "learning_rate": 1.1565988913271398e-05,
      "loss": 1.0522,
      "step": 4992
    },
    {
      "epoch": 46.66961391099322,
      "grad_norm": 0.49845167994499207,
      "learning_rate": 1.156299931216839e-05,
      "loss": 1.0296,
      "step": 4993
    },
    {
      "epoch": 46.679045092838194,
      "grad_norm": 0.48475006222724915,
      "learning_rate": 1.1560009567863904e-05,
      "loss": 1.062,
      "step": 4994
    },
    {
      "epoch": 46.68847627468317,
      "grad_norm": 0.5308312177658081,
      "learning_rate": 1.1557019680631854e-05,
      "loss": 1.0661,
      "step": 4995
    },
    {
      "epoch": 46.69790745652815,
      "grad_norm": 0.5280548334121704,
      "learning_rate": 1.155402965074618e-05,
      "loss": 1.108,
      "step": 4996
    },
    {
      "epoch": 46.70733863837312,
      "grad_norm": 0.5491032600402832,
      "learning_rate": 1.1551039478480823e-05,
      "loss": 1.0514,
      "step": 4997
    },
    {
      "epoch": 46.7167698202181,
      "grad_norm": 0.5068010687828064,
      "learning_rate": 1.1548049164109743e-05,
      "loss": 1.0932,
      "step": 4998
    },
    {
      "epoch": 46.72620100206307,
      "grad_norm": 0.5068361759185791,
      "learning_rate": 1.1545058707906909e-05,
      "loss": 1.0998,
      "step": 4999
    },
    {
      "epoch": 46.735632183908045,
      "grad_norm": 0.5288742184638977,
      "learning_rate": 1.154206811014631e-05,
      "loss": 1.0662,
      "step": 5000
    },
    {
      "epoch": 46.74506336575302,
      "grad_norm": 0.48790061473846436,
      "learning_rate": 1.1539077371101939e-05,
      "loss": 1.0615,
      "step": 5001
    },
    {
      "epoch": 46.75449454759799,
      "grad_norm": 0.48953279852867126,
      "learning_rate": 1.1536086491047808e-05,
      "loss": 1.0685,
      "step": 5002
    },
    {
      "epoch": 46.763925729442974,
      "grad_norm": 0.5450941920280457,
      "learning_rate": 1.1533095470257945e-05,
      "loss": 1.062,
      "step": 5003
    },
    {
      "epoch": 46.77335691128795,
      "grad_norm": 0.5074625611305237,
      "learning_rate": 1.153010430900638e-05,
      "loss": 1.0762,
      "step": 5004
    },
    {
      "epoch": 46.78278809313292,
      "grad_norm": 0.4909260869026184,
      "learning_rate": 1.1527113007567168e-05,
      "loss": 1.0557,
      "step": 5005
    },
    {
      "epoch": 46.792219274977896,
      "grad_norm": 0.5280821323394775,
      "learning_rate": 1.1524121566214364e-05,
      "loss": 1.1028,
      "step": 5006
    },
    {
      "epoch": 46.80165045682287,
      "grad_norm": 0.5052995681762695,
      "learning_rate": 1.1521129985222048e-05,
      "loss": 1.0765,
      "step": 5007
    },
    {
      "epoch": 46.811081638667844,
      "grad_norm": 0.4998474717140198,
      "learning_rate": 1.1518138264864302e-05,
      "loss": 1.0923,
      "step": 5008
    },
    {
      "epoch": 46.82051282051282,
      "grad_norm": 0.5326126217842102,
      "learning_rate": 1.1515146405415236e-05,
      "loss": 1.0584,
      "step": 5009
    },
    {
      "epoch": 46.8299440023578,
      "grad_norm": 0.5161521434783936,
      "learning_rate": 1.1512154407148958e-05,
      "loss": 1.0585,
      "step": 5010
    },
    {
      "epoch": 46.83937518420277,
      "grad_norm": 0.5116077065467834,
      "learning_rate": 1.1509162270339594e-05,
      "loss": 1.0451,
      "step": 5011
    },
    {
      "epoch": 46.84880636604775,
      "grad_norm": 0.5452271699905396,
      "learning_rate": 1.150616999526128e-05,
      "loss": 1.0531,
      "step": 5012
    },
    {
      "epoch": 46.85823754789272,
      "grad_norm": 0.48407822847366333,
      "learning_rate": 1.1503177582188171e-05,
      "loss": 1.0794,
      "step": 5013
    },
    {
      "epoch": 46.867668729737694,
      "grad_norm": 0.5018329620361328,
      "learning_rate": 1.1500185031394429e-05,
      "loss": 1.0735,
      "step": 5014
    },
    {
      "epoch": 46.87709991158267,
      "grad_norm": 0.4993705749511719,
      "learning_rate": 1.1497192343154232e-05,
      "loss": 1.0763,
      "step": 5015
    },
    {
      "epoch": 46.88653109342764,
      "grad_norm": 0.558904230594635,
      "learning_rate": 1.1494199517741769e-05,
      "loss": 1.0423,
      "step": 5016
    },
    {
      "epoch": 46.89596227527262,
      "grad_norm": 0.5863021016120911,
      "learning_rate": 1.1491206555431236e-05,
      "loss": 1.0612,
      "step": 5017
    },
    {
      "epoch": 46.9053934571176,
      "grad_norm": 0.5030761361122131,
      "learning_rate": 1.1488213456496857e-05,
      "loss": 1.1085,
      "step": 5018
    },
    {
      "epoch": 46.91482463896257,
      "grad_norm": 0.5175719857215881,
      "learning_rate": 1.1485220221212852e-05,
      "loss": 1.036,
      "step": 5019
    },
    {
      "epoch": 46.924255820807545,
      "grad_norm": 0.5918428301811218,
      "learning_rate": 1.1482226849853465e-05,
      "loss": 1.1076,
      "step": 5020
    },
    {
      "epoch": 46.93368700265252,
      "grad_norm": 0.5114413499832153,
      "learning_rate": 1.147923334269294e-05,
      "loss": 1.1327,
      "step": 5021
    },
    {
      "epoch": 46.94311818449749,
      "grad_norm": 0.4582779109477997,
      "learning_rate": 1.1476239700005546e-05,
      "loss": 1.0231,
      "step": 5022
    },
    {
      "epoch": 46.95254936634247,
      "grad_norm": 0.539023220539093,
      "learning_rate": 1.1473245922065564e-05,
      "loss": 1.0791,
      "step": 5023
    },
    {
      "epoch": 46.96198054818745,
      "grad_norm": 0.5558559894561768,
      "learning_rate": 1.1470252009147276e-05,
      "loss": 1.0324,
      "step": 5024
    },
    {
      "epoch": 46.97141173003242,
      "grad_norm": 0.5150998830795288,
      "learning_rate": 1.1467257961524985e-05,
      "loss": 1.0792,
      "step": 5025
    },
    {
      "epoch": 46.980842911877396,
      "grad_norm": 0.5156293511390686,
      "learning_rate": 1.1464263779473007e-05,
      "loss": 1.0647,
      "step": 5026
    },
    {
      "epoch": 46.99027409372237,
      "grad_norm": 0.4902206361293793,
      "learning_rate": 1.1461269463265663e-05,
      "loss": 1.0909,
      "step": 5027
    },
    {
      "epoch": 46.999705275567344,
      "grad_norm": 0.5316380858421326,
      "learning_rate": 1.1458275013177298e-05,
      "loss": 1.0856,
      "step": 5028
    },
    {
      "epoch": 47.0,
      "grad_norm": 2.647142171859741,
      "learning_rate": 1.1455280429482256e-05,
      "loss": 0.7662,
      "step": 5029
    },
    {
      "epoch": 47.009431181844974,
      "grad_norm": 0.520127534866333,
      "learning_rate": 1.1452285712454905e-05,
      "loss": 1.0498,
      "step": 5030
    },
    {
      "epoch": 47.01886236368995,
      "grad_norm": 0.5178743004798889,
      "learning_rate": 1.1449290862369617e-05,
      "loss": 1.0325,
      "step": 5031
    },
    {
      "epoch": 47.02829354553492,
      "grad_norm": 0.5284842252731323,
      "learning_rate": 1.144629587950078e-05,
      "loss": 1.0738,
      "step": 5032
    },
    {
      "epoch": 47.0377247273799,
      "grad_norm": 0.49214208126068115,
      "learning_rate": 1.1443300764122794e-05,
      "loss": 1.095,
      "step": 5033
    },
    {
      "epoch": 47.04715590922488,
      "grad_norm": 0.48297616839408875,
      "learning_rate": 1.1440305516510068e-05,
      "loss": 1.0763,
      "step": 5034
    },
    {
      "epoch": 47.05658709106985,
      "grad_norm": 0.5079252123832703,
      "learning_rate": 1.143731013693703e-05,
      "loss": 1.0298,
      "step": 5035
    },
    {
      "epoch": 47.066018272914825,
      "grad_norm": 0.48583662509918213,
      "learning_rate": 1.1434314625678112e-05,
      "loss": 1.0451,
      "step": 5036
    },
    {
      "epoch": 47.0754494547598,
      "grad_norm": 0.48305854201316833,
      "learning_rate": 1.1431318983007762e-05,
      "loss": 1.0846,
      "step": 5037
    },
    {
      "epoch": 47.08488063660477,
      "grad_norm": 0.47394847869873047,
      "learning_rate": 1.1428323209200443e-05,
      "loss": 1.0437,
      "step": 5038
    },
    {
      "epoch": 47.09431181844975,
      "grad_norm": 0.5233921408653259,
      "learning_rate": 1.1425327304530621e-05,
      "loss": 1.0274,
      "step": 5039
    },
    {
      "epoch": 47.10374300029473,
      "grad_norm": 0.4750811457633972,
      "learning_rate": 1.1422331269272784e-05,
      "loss": 1.0834,
      "step": 5040
    },
    {
      "epoch": 47.1131741821397,
      "grad_norm": 0.5127264857292175,
      "learning_rate": 1.141933510370143e-05,
      "loss": 1.048,
      "step": 5041
    },
    {
      "epoch": 47.122605363984675,
      "grad_norm": 0.48732274770736694,
      "learning_rate": 1.1416338808091058e-05,
      "loss": 1.0768,
      "step": 5042
    },
    {
      "epoch": 47.13203654582965,
      "grad_norm": 0.5050778388977051,
      "learning_rate": 1.1413342382716199e-05,
      "loss": 1.0529,
      "step": 5043
    },
    {
      "epoch": 47.14146772767462,
      "grad_norm": 0.49732083082199097,
      "learning_rate": 1.1410345827851374e-05,
      "loss": 1.0438,
      "step": 5044
    },
    {
      "epoch": 47.1508989095196,
      "grad_norm": 0.4967287480831146,
      "learning_rate": 1.1407349143771136e-05,
      "loss": 1.0677,
      "step": 5045
    },
    {
      "epoch": 47.16033009136457,
      "grad_norm": 0.5178801417350769,
      "learning_rate": 1.1404352330750033e-05,
      "loss": 1.0494,
      "step": 5046
    },
    {
      "epoch": 47.16976127320955,
      "grad_norm": 0.4975084066390991,
      "learning_rate": 1.1401355389062634e-05,
      "loss": 1.0722,
      "step": 5047
    },
    {
      "epoch": 47.179192455054526,
      "grad_norm": 0.49837133288383484,
      "learning_rate": 1.1398358318983519e-05,
      "loss": 1.0953,
      "step": 5048
    },
    {
      "epoch": 47.1886236368995,
      "grad_norm": 0.5360572338104248,
      "learning_rate": 1.1395361120787279e-05,
      "loss": 1.0589,
      "step": 5049
    },
    {
      "epoch": 47.198054818744474,
      "grad_norm": 0.5168077349662781,
      "learning_rate": 1.1392363794748514e-05,
      "loss": 1.057,
      "step": 5050
    },
    {
      "epoch": 47.20748600058945,
      "grad_norm": 0.5022526979446411,
      "learning_rate": 1.1389366341141837e-05,
      "loss": 1.0591,
      "step": 5051
    },
    {
      "epoch": 47.21691718243442,
      "grad_norm": 0.48780134320259094,
      "learning_rate": 1.1386368760241876e-05,
      "loss": 1.0921,
      "step": 5052
    },
    {
      "epoch": 47.226348364279396,
      "grad_norm": 0.4686269462108612,
      "learning_rate": 1.138337105232327e-05,
      "loss": 1.0688,
      "step": 5053
    },
    {
      "epoch": 47.23577954612438,
      "grad_norm": 0.5135404467582703,
      "learning_rate": 1.1380373217660667e-05,
      "loss": 1.0362,
      "step": 5054
    },
    {
      "epoch": 47.24521072796935,
      "grad_norm": 0.4981291592121124,
      "learning_rate": 1.1377375256528724e-05,
      "loss": 1.0897,
      "step": 5055
    },
    {
      "epoch": 47.254641909814325,
      "grad_norm": 0.5027174353599548,
      "learning_rate": 1.1374377169202117e-05,
      "loss": 1.075,
      "step": 5056
    },
    {
      "epoch": 47.2640730916593,
      "grad_norm": 0.49416008591651917,
      "learning_rate": 1.1371378955955532e-05,
      "loss": 1.065,
      "step": 5057
    },
    {
      "epoch": 47.27350427350427,
      "grad_norm": 0.5175797939300537,
      "learning_rate": 1.1368380617063656e-05,
      "loss": 1.0468,
      "step": 5058
    },
    {
      "epoch": 47.28293545534925,
      "grad_norm": 0.5219405293464661,
      "learning_rate": 1.1365382152801203e-05,
      "loss": 1.075,
      "step": 5059
    },
    {
      "epoch": 47.29236663719422,
      "grad_norm": 0.48941659927368164,
      "learning_rate": 1.1362383563442891e-05,
      "loss": 1.068,
      "step": 5060
    },
    {
      "epoch": 47.3017978190392,
      "grad_norm": 0.5327327847480774,
      "learning_rate": 1.1359384849263445e-05,
      "loss": 1.0905,
      "step": 5061
    },
    {
      "epoch": 47.311229000884175,
      "grad_norm": 0.4780679941177368,
      "learning_rate": 1.135638601053761e-05,
      "loss": 1.0424,
      "step": 5062
    },
    {
      "epoch": 47.32066018272915,
      "grad_norm": 0.503482460975647,
      "learning_rate": 1.1353387047540139e-05,
      "loss": 1.0702,
      "step": 5063
    },
    {
      "epoch": 47.33009136457412,
      "grad_norm": 0.5023531317710876,
      "learning_rate": 1.1350387960545794e-05,
      "loss": 1.0684,
      "step": 5064
    },
    {
      "epoch": 47.3395225464191,
      "grad_norm": 0.493254154920578,
      "learning_rate": 1.1347388749829353e-05,
      "loss": 1.0897,
      "step": 5065
    },
    {
      "epoch": 47.34895372826407,
      "grad_norm": 0.535944938659668,
      "learning_rate": 1.1344389415665597e-05,
      "loss": 1.0369,
      "step": 5066
    },
    {
      "epoch": 47.358384910109045,
      "grad_norm": 0.5007795095443726,
      "learning_rate": 1.1341389958329332e-05,
      "loss": 1.1068,
      "step": 5067
    },
    {
      "epoch": 47.367816091954026,
      "grad_norm": 0.5249669551849365,
      "learning_rate": 1.1338390378095361e-05,
      "loss": 1.0575,
      "step": 5068
    },
    {
      "epoch": 47.377247273799,
      "grad_norm": 0.4863998591899872,
      "learning_rate": 1.1335390675238507e-05,
      "loss": 1.0581,
      "step": 5069
    },
    {
      "epoch": 47.386678455643974,
      "grad_norm": 0.4963964819908142,
      "learning_rate": 1.1332390850033598e-05,
      "loss": 1.0656,
      "step": 5070
    },
    {
      "epoch": 47.39610963748895,
      "grad_norm": 0.49793168902397156,
      "learning_rate": 1.1329390902755484e-05,
      "loss": 1.0718,
      "step": 5071
    },
    {
      "epoch": 47.40554081933392,
      "grad_norm": 0.5468529462814331,
      "learning_rate": 1.1326390833679014e-05,
      "loss": 1.0834,
      "step": 5072
    },
    {
      "epoch": 47.414972001178896,
      "grad_norm": 0.48852479457855225,
      "learning_rate": 1.1323390643079055e-05,
      "loss": 1.0454,
      "step": 5073
    },
    {
      "epoch": 47.42440318302387,
      "grad_norm": 0.5449930429458618,
      "learning_rate": 1.1320390331230486e-05,
      "loss": 1.0552,
      "step": 5074
    },
    {
      "epoch": 47.43383436486885,
      "grad_norm": 0.4740810692310333,
      "learning_rate": 1.1317389898408188e-05,
      "loss": 1.0868,
      "step": 5075
    },
    {
      "epoch": 47.443265546713825,
      "grad_norm": 0.5263463258743286,
      "learning_rate": 1.1314389344887067e-05,
      "loss": 1.0442,
      "step": 5076
    },
    {
      "epoch": 47.4526967285588,
      "grad_norm": 0.5200948715209961,
      "learning_rate": 1.1311388670942028e-05,
      "loss": 1.0699,
      "step": 5077
    },
    {
      "epoch": 47.46212791040377,
      "grad_norm": 0.5234559178352356,
      "learning_rate": 1.1308387876847993e-05,
      "loss": 1.0809,
      "step": 5078
    },
    {
      "epoch": 47.47155909224875,
      "grad_norm": 0.4838687777519226,
      "learning_rate": 1.1305386962879894e-05,
      "loss": 1.0587,
      "step": 5079
    },
    {
      "epoch": 47.48099027409372,
      "grad_norm": 0.4935756325721741,
      "learning_rate": 1.1302385929312675e-05,
      "loss": 1.0875,
      "step": 5080
    },
    {
      "epoch": 47.490421455938694,
      "grad_norm": 0.49918708205223083,
      "learning_rate": 1.1299384776421284e-05,
      "loss": 1.0351,
      "step": 5081
    },
    {
      "epoch": 47.499852637783675,
      "grad_norm": 0.4967958629131317,
      "learning_rate": 1.1296383504480695e-05,
      "loss": 1.0692,
      "step": 5082
    },
    {
      "epoch": 47.50928381962865,
      "grad_norm": 0.5021281242370605,
      "learning_rate": 1.1293382113765876e-05,
      "loss": 1.076,
      "step": 5083
    },
    {
      "epoch": 47.51871500147362,
      "grad_norm": 0.5090051293373108,
      "learning_rate": 1.1290380604551819e-05,
      "loss": 1.0725,
      "step": 5084
    },
    {
      "epoch": 47.5281461833186,
      "grad_norm": 0.5025062561035156,
      "learning_rate": 1.1287378977113513e-05,
      "loss": 1.0644,
      "step": 5085
    },
    {
      "epoch": 47.53757736516357,
      "grad_norm": 0.532209575176239,
      "learning_rate": 1.1284377231725973e-05,
      "loss": 1.0559,
      "step": 5086
    },
    {
      "epoch": 47.547008547008545,
      "grad_norm": 0.5663564205169678,
      "learning_rate": 1.1281375368664221e-05,
      "loss": 1.073,
      "step": 5087
    },
    {
      "epoch": 47.55643972885352,
      "grad_norm": 0.5106321573257446,
      "learning_rate": 1.1278373388203273e-05,
      "loss": 1.0562,
      "step": 5088
    },
    {
      "epoch": 47.5658709106985,
      "grad_norm": 0.49618619680404663,
      "learning_rate": 1.1275371290618186e-05,
      "loss": 1.056,
      "step": 5089
    },
    {
      "epoch": 47.575302092543474,
      "grad_norm": 0.5185182094573975,
      "learning_rate": 1.1272369076184003e-05,
      "loss": 1.0772,
      "step": 5090
    },
    {
      "epoch": 47.58473327438845,
      "grad_norm": 0.5751948356628418,
      "learning_rate": 1.1269366745175785e-05,
      "loss": 1.0767,
      "step": 5091
    },
    {
      "epoch": 47.59416445623342,
      "grad_norm": 0.5368341207504272,
      "learning_rate": 1.1266364297868603e-05,
      "loss": 1.0672,
      "step": 5092
    },
    {
      "epoch": 47.603595638078396,
      "grad_norm": 0.5164337754249573,
      "learning_rate": 1.1263361734537547e-05,
      "loss": 1.0546,
      "step": 5093
    },
    {
      "epoch": 47.61302681992337,
      "grad_norm": 0.5532370209693909,
      "learning_rate": 1.1260359055457706e-05,
      "loss": 1.0675,
      "step": 5094
    },
    {
      "epoch": 47.622458001768344,
      "grad_norm": 0.5530776977539062,
      "learning_rate": 1.1257356260904186e-05,
      "loss": 1.0357,
      "step": 5095
    },
    {
      "epoch": 47.631889183613325,
      "grad_norm": 0.49542832374572754,
      "learning_rate": 1.1254353351152101e-05,
      "loss": 1.0585,
      "step": 5096
    },
    {
      "epoch": 47.6413203654583,
      "grad_norm": 0.5541474223136902,
      "learning_rate": 1.1251350326476578e-05,
      "loss": 1.0709,
      "step": 5097
    },
    {
      "epoch": 47.65075154730327,
      "grad_norm": 0.508323609828949,
      "learning_rate": 1.1248347187152752e-05,
      "loss": 1.0781,
      "step": 5098
    },
    {
      "epoch": 47.66018272914825,
      "grad_norm": 0.5168141722679138,
      "learning_rate": 1.1245343933455768e-05,
      "loss": 1.0946,
      "step": 5099
    },
    {
      "epoch": 47.66961391099322,
      "grad_norm": 0.5498034358024597,
      "learning_rate": 1.1242340565660788e-05,
      "loss": 1.0619,
      "step": 5100
    },
    {
      "epoch": 47.679045092838194,
      "grad_norm": 0.526161253452301,
      "learning_rate": 1.1239337084042975e-05,
      "loss": 1.0718,
      "step": 5101
    },
    {
      "epoch": 47.68847627468317,
      "grad_norm": 0.5077440738677979,
      "learning_rate": 1.1236333488877512e-05,
      "loss": 1.0589,
      "step": 5102
    },
    {
      "epoch": 47.69790745652815,
      "grad_norm": 0.4818386435508728,
      "learning_rate": 1.1233329780439578e-05,
      "loss": 1.0652,
      "step": 5103
    },
    {
      "epoch": 47.70733863837312,
      "grad_norm": 0.5068922638893127,
      "learning_rate": 1.1230325959004382e-05,
      "loss": 1.0554,
      "step": 5104
    },
    {
      "epoch": 47.7167698202181,
      "grad_norm": 0.5372790694236755,
      "learning_rate": 1.1227322024847129e-05,
      "loss": 1.0618,
      "step": 5105
    },
    {
      "epoch": 47.72620100206307,
      "grad_norm": 0.5231959223747253,
      "learning_rate": 1.1224317978243035e-05,
      "loss": 1.0628,
      "step": 5106
    },
    {
      "epoch": 47.735632183908045,
      "grad_norm": 0.5124754905700684,
      "learning_rate": 1.1221313819467337e-05,
      "loss": 1.0728,
      "step": 5107
    },
    {
      "epoch": 47.74506336575302,
      "grad_norm": 0.5255483388900757,
      "learning_rate": 1.1218309548795268e-05,
      "loss": 1.051,
      "step": 5108
    },
    {
      "epoch": 47.75449454759799,
      "grad_norm": 0.4825477600097656,
      "learning_rate": 1.1215305166502084e-05,
      "loss": 1.0673,
      "step": 5109
    },
    {
      "epoch": 47.763925729442974,
      "grad_norm": 0.4938085079193115,
      "learning_rate": 1.1212300672863044e-05,
      "loss": 1.0665,
      "step": 5110
    },
    {
      "epoch": 47.77335691128795,
      "grad_norm": 0.5032636523246765,
      "learning_rate": 1.1209296068153413e-05,
      "loss": 1.0897,
      "step": 5111
    },
    {
      "epoch": 47.78278809313292,
      "grad_norm": 0.5305772423744202,
      "learning_rate": 1.120629135264848e-05,
      "loss": 1.0589,
      "step": 5112
    },
    {
      "epoch": 47.792219274977896,
      "grad_norm": 0.554482638835907,
      "learning_rate": 1.1203286526623533e-05,
      "loss": 1.0569,
      "step": 5113
    },
    {
      "epoch": 47.80165045682287,
      "grad_norm": 0.5159949064254761,
      "learning_rate": 1.120028159035387e-05,
      "loss": 1.035,
      "step": 5114
    },
    {
      "epoch": 47.811081638667844,
      "grad_norm": 0.507459819316864,
      "learning_rate": 1.1197276544114809e-05,
      "loss": 1.0684,
      "step": 5115
    },
    {
      "epoch": 47.82051282051282,
      "grad_norm": 0.5195584893226624,
      "learning_rate": 1.1194271388181665e-05,
      "loss": 1.0257,
      "step": 5116
    },
    {
      "epoch": 47.8299440023578,
      "grad_norm": 0.5003102421760559,
      "learning_rate": 1.1191266122829773e-05,
      "loss": 1.0521,
      "step": 5117
    },
    {
      "epoch": 47.83937518420277,
      "grad_norm": 0.4999904930591583,
      "learning_rate": 1.118826074833447e-05,
      "loss": 1.0578,
      "step": 5118
    },
    {
      "epoch": 47.84880636604775,
      "grad_norm": 0.47411221265792847,
      "learning_rate": 1.1185255264971113e-05,
      "loss": 1.0937,
      "step": 5119
    },
    {
      "epoch": 47.85823754789272,
      "grad_norm": 0.5083984732627869,
      "learning_rate": 1.1182249673015061e-05,
      "loss": 1.0731,
      "step": 5120
    },
    {
      "epoch": 47.867668729737694,
      "grad_norm": 0.5392367839813232,
      "learning_rate": 1.1179243972741687e-05,
      "loss": 1.0406,
      "step": 5121
    },
    {
      "epoch": 47.87709991158267,
      "grad_norm": 0.5358696579933167,
      "learning_rate": 1.1176238164426367e-05,
      "loss": 1.1095,
      "step": 5122
    },
    {
      "epoch": 47.88653109342764,
      "grad_norm": 0.5435312390327454,
      "learning_rate": 1.11732322483445e-05,
      "loss": 1.1132,
      "step": 5123
    },
    {
      "epoch": 47.89596227527262,
      "grad_norm": 0.5473992824554443,
      "learning_rate": 1.1170226224771481e-05,
      "loss": 1.0459,
      "step": 5124
    },
    {
      "epoch": 47.9053934571176,
      "grad_norm": 0.4969501495361328,
      "learning_rate": 1.1167220093982722e-05,
      "loss": 1.0731,
      "step": 5125
    },
    {
      "epoch": 47.91482463896257,
      "grad_norm": 0.5115700364112854,
      "learning_rate": 1.1164213856253642e-05,
      "loss": 1.0624,
      "step": 5126
    },
    {
      "epoch": 47.924255820807545,
      "grad_norm": 0.49873971939086914,
      "learning_rate": 1.1161207511859677e-05,
      "loss": 1.0578,
      "step": 5127
    },
    {
      "epoch": 47.93368700265252,
      "grad_norm": 0.48711279034614563,
      "learning_rate": 1.1158201061076267e-05,
      "loss": 1.0623,
      "step": 5128
    },
    {
      "epoch": 47.94311818449749,
      "grad_norm": 0.5128322243690491,
      "learning_rate": 1.1155194504178852e-05,
      "loss": 1.1068,
      "step": 5129
    },
    {
      "epoch": 47.95254936634247,
      "grad_norm": 0.4925679862499237,
      "learning_rate": 1.1152187841442905e-05,
      "loss": 1.0569,
      "step": 5130
    },
    {
      "epoch": 47.96198054818745,
      "grad_norm": 0.5440824031829834,
      "learning_rate": 1.1149181073143887e-05,
      "loss": 1.0239,
      "step": 5131
    },
    {
      "epoch": 47.97141173003242,
      "grad_norm": 0.4986647665500641,
      "learning_rate": 1.114617419955728e-05,
      "loss": 1.0514,
      "step": 5132
    },
    {
      "epoch": 47.980842911877396,
      "grad_norm": 0.5103418231010437,
      "learning_rate": 1.114316722095857e-05,
      "loss": 1.0264,
      "step": 5133
    },
    {
      "epoch": 47.99027409372237,
      "grad_norm": 0.5330959558486938,
      "learning_rate": 1.1140160137623261e-05,
      "loss": 1.0452,
      "step": 5134
    },
    {
      "epoch": 47.999705275567344,
      "grad_norm": 0.4844103455543518,
      "learning_rate": 1.1137152949826858e-05,
      "loss": 1.0516,
      "step": 5135
    },
    {
      "epoch": 48.0,
      "grad_norm": 3.254155397415161,
      "learning_rate": 1.1134145657844874e-05,
      "loss": 0.8598,
      "step": 5136
    },
    {
      "epoch": 48.009431181844974,
      "grad_norm": 0.5129268765449524,
      "learning_rate": 1.1131138261952845e-05,
      "loss": 1.0714,
      "step": 5137
    },
    {
      "epoch": 48.01886236368995,
      "grad_norm": 0.5178202390670776,
      "learning_rate": 1.11281307624263e-05,
      "loss": 1.0719,
      "step": 5138
    },
    {
      "epoch": 48.02829354553492,
      "grad_norm": 0.5173031687736511,
      "learning_rate": 1.1125123159540788e-05,
      "loss": 1.0014,
      "step": 5139
    },
    {
      "epoch": 48.0377247273799,
      "grad_norm": 0.49676576256752014,
      "learning_rate": 1.1122115453571866e-05,
      "loss": 1.0878,
      "step": 5140
    },
    {
      "epoch": 48.04715590922488,
      "grad_norm": 0.5116748213768005,
      "learning_rate": 1.1119107644795096e-05,
      "loss": 1.0843,
      "step": 5141
    },
    {
      "epoch": 48.05658709106985,
      "grad_norm": 0.4913562536239624,
      "learning_rate": 1.1116099733486057e-05,
      "loss": 1.0756,
      "step": 5142
    },
    {
      "epoch": 48.066018272914825,
      "grad_norm": 0.4988953769207001,
      "learning_rate": 1.1113091719920328e-05,
      "loss": 1.0381,
      "step": 5143
    },
    {
      "epoch": 48.0754494547598,
      "grad_norm": 0.5359849333763123,
      "learning_rate": 1.1110083604373503e-05,
      "loss": 1.0542,
      "step": 5144
    },
    {
      "epoch": 48.08488063660477,
      "grad_norm": 0.48089680075645447,
      "learning_rate": 1.1107075387121186e-05,
      "loss": 1.0452,
      "step": 5145
    },
    {
      "epoch": 48.09431181844975,
      "grad_norm": 0.4795691668987274,
      "learning_rate": 1.110406706843899e-05,
      "loss": 1.1156,
      "step": 5146
    },
    {
      "epoch": 48.10374300029473,
      "grad_norm": 0.5394229888916016,
      "learning_rate": 1.1101058648602536e-05,
      "loss": 1.0498,
      "step": 5147
    },
    {
      "epoch": 48.1131741821397,
      "grad_norm": 0.48682132363319397,
      "learning_rate": 1.109805012788745e-05,
      "loss": 1.0442,
      "step": 5148
    },
    {
      "epoch": 48.122605363984675,
      "grad_norm": 0.4933399260044098,
      "learning_rate": 1.1095041506569376e-05,
      "loss": 1.0447,
      "step": 5149
    },
    {
      "epoch": 48.13203654582965,
      "grad_norm": 0.5058544874191284,
      "learning_rate": 1.1092032784923962e-05,
      "loss": 1.0491,
      "step": 5150
    },
    {
      "epoch": 48.14146772767462,
      "grad_norm": 0.5081380605697632,
      "learning_rate": 1.1089023963226867e-05,
      "loss": 1.0941,
      "step": 5151
    },
    {
      "epoch": 48.1508989095196,
      "grad_norm": 0.5120680928230286,
      "learning_rate": 1.1086015041753752e-05,
      "loss": 1.0689,
      "step": 5152
    },
    {
      "epoch": 48.16033009136457,
      "grad_norm": 0.5160539746284485,
      "learning_rate": 1.1083006020780305e-05,
      "loss": 1.0742,
      "step": 5153
    },
    {
      "epoch": 48.16976127320955,
      "grad_norm": 0.5593828558921814,
      "learning_rate": 1.1079996900582202e-05,
      "loss": 1.0536,
      "step": 5154
    },
    {
      "epoch": 48.179192455054526,
      "grad_norm": 0.4805314838886261,
      "learning_rate": 1.1076987681435138e-05,
      "loss": 1.052,
      "step": 5155
    },
    {
      "epoch": 48.1886236368995,
      "grad_norm": 0.5316733717918396,
      "learning_rate": 1.1073978363614821e-05,
      "loss": 1.061,
      "step": 5156
    },
    {
      "epoch": 48.198054818744474,
      "grad_norm": 0.5040477514266968,
      "learning_rate": 1.1070968947396965e-05,
      "loss": 1.0776,
      "step": 5157
    },
    {
      "epoch": 48.20748600058945,
      "grad_norm": 0.5045741200447083,
      "learning_rate": 1.1067959433057284e-05,
      "loss": 1.0291,
      "step": 5158
    },
    {
      "epoch": 48.21691718243442,
      "grad_norm": 0.49598562717437744,
      "learning_rate": 1.1064949820871515e-05,
      "loss": 1.0596,
      "step": 5159
    },
    {
      "epoch": 48.226348364279396,
      "grad_norm": 0.4830690324306488,
      "learning_rate": 1.1061940111115397e-05,
      "loss": 1.0379,
      "step": 5160
    },
    {
      "epoch": 48.23577954612438,
      "grad_norm": 0.5142917633056641,
      "learning_rate": 1.1058930304064676e-05,
      "loss": 1.0453,
      "step": 5161
    },
    {
      "epoch": 48.24521072796935,
      "grad_norm": 0.5500958561897278,
      "learning_rate": 1.1055920399995114e-05,
      "loss": 1.0714,
      "step": 5162
    },
    {
      "epoch": 48.254641909814325,
      "grad_norm": 0.5564147233963013,
      "learning_rate": 1.105291039918247e-05,
      "loss": 1.043,
      "step": 5163
    },
    {
      "epoch": 48.2640730916593,
      "grad_norm": 0.5160756707191467,
      "learning_rate": 1.1049900301902528e-05,
      "loss": 1.0635,
      "step": 5164
    },
    {
      "epoch": 48.27350427350427,
      "grad_norm": 0.5333313941955566,
      "learning_rate": 1.1046890108431068e-05,
      "loss": 1.0615,
      "step": 5165
    },
    {
      "epoch": 48.28293545534925,
      "grad_norm": 0.5170122981071472,
      "learning_rate": 1.104387981904388e-05,
      "loss": 1.0798,
      "step": 5166
    },
    {
      "epoch": 48.29236663719422,
      "grad_norm": 0.4599545896053314,
      "learning_rate": 1.1040869434016768e-05,
      "loss": 1.0633,
      "step": 5167
    },
    {
      "epoch": 48.3017978190392,
      "grad_norm": 0.45006513595581055,
      "learning_rate": 1.1037858953625547e-05,
      "loss": 1.0663,
      "step": 5168
    },
    {
      "epoch": 48.311229000884175,
      "grad_norm": 0.559527575969696,
      "learning_rate": 1.103484837814603e-05,
      "loss": 1.0468,
      "step": 5169
    },
    {
      "epoch": 48.32066018272915,
      "grad_norm": 0.4723837971687317,
      "learning_rate": 1.1031837707854044e-05,
      "loss": 1.0779,
      "step": 5170
    },
    {
      "epoch": 48.33009136457412,
      "grad_norm": 0.5110293030738831,
      "learning_rate": 1.1028826943025432e-05,
      "loss": 1.0395,
      "step": 5171
    },
    {
      "epoch": 48.3395225464191,
      "grad_norm": 0.5303466320037842,
      "learning_rate": 1.1025816083936036e-05,
      "loss": 1.0289,
      "step": 5172
    },
    {
      "epoch": 48.34895372826407,
      "grad_norm": 0.4907532036304474,
      "learning_rate": 1.1022805130861709e-05,
      "loss": 1.0893,
      "step": 5173
    },
    {
      "epoch": 48.358384910109045,
      "grad_norm": 0.4957808256149292,
      "learning_rate": 1.1019794084078314e-05,
      "loss": 1.0629,
      "step": 5174
    },
    {
      "epoch": 48.367816091954026,
      "grad_norm": 0.5382192730903625,
      "learning_rate": 1.1016782943861725e-05,
      "loss": 1.0846,
      "step": 5175
    },
    {
      "epoch": 48.377247273799,
      "grad_norm": 0.5302462577819824,
      "learning_rate": 1.101377171048782e-05,
      "loss": 1.0382,
      "step": 5176
    },
    {
      "epoch": 48.386678455643974,
      "grad_norm": 0.5438966155052185,
      "learning_rate": 1.1010760384232487e-05,
      "loss": 1.0828,
      "step": 5177
    },
    {
      "epoch": 48.39610963748895,
      "grad_norm": 0.48411858081817627,
      "learning_rate": 1.1007748965371616e-05,
      "loss": 1.0196,
      "step": 5178
    },
    {
      "epoch": 48.40554081933392,
      "grad_norm": 0.48766177892684937,
      "learning_rate": 1.1004737454181124e-05,
      "loss": 1.0776,
      "step": 5179
    },
    {
      "epoch": 48.414972001178896,
      "grad_norm": 0.4990120530128479,
      "learning_rate": 1.1001725850936917e-05,
      "loss": 1.0554,
      "step": 5180
    },
    {
      "epoch": 48.42440318302387,
      "grad_norm": 0.4914858937263489,
      "learning_rate": 1.0998714155914922e-05,
      "loss": 1.071,
      "step": 5181
    },
    {
      "epoch": 48.43383436486885,
      "grad_norm": 0.5240755677223206,
      "learning_rate": 1.099570236939106e-05,
      "loss": 1.0338,
      "step": 5182
    },
    {
      "epoch": 48.443265546713825,
      "grad_norm": 0.5347602367401123,
      "learning_rate": 1.0992690491641285e-05,
      "loss": 1.0817,
      "step": 5183
    },
    {
      "epoch": 48.4526967285588,
      "grad_norm": 0.4924027919769287,
      "learning_rate": 1.0989678522941532e-05,
      "loss": 1.0734,
      "step": 5184
    },
    {
      "epoch": 48.46212791040377,
      "grad_norm": 0.5512605309486389,
      "learning_rate": 1.0986666463567759e-05,
      "loss": 1.0813,
      "step": 5185
    },
    {
      "epoch": 48.47155909224875,
      "grad_norm": 0.5313063263893127,
      "learning_rate": 1.0983654313795932e-05,
      "loss": 1.0435,
      "step": 5186
    },
    {
      "epoch": 48.48099027409372,
      "grad_norm": 0.5165737271308899,
      "learning_rate": 1.0980642073902023e-05,
      "loss": 1.0433,
      "step": 5187
    },
    {
      "epoch": 48.490421455938694,
      "grad_norm": 0.5326406955718994,
      "learning_rate": 1.0977629744162013e-05,
      "loss": 1.0605,
      "step": 5188
    },
    {
      "epoch": 48.499852637783675,
      "grad_norm": 0.5113357305526733,
      "learning_rate": 1.0974617324851887e-05,
      "loss": 1.0419,
      "step": 5189
    },
    {
      "epoch": 48.50928381962865,
      "grad_norm": 0.49539923667907715,
      "learning_rate": 1.0971604816247646e-05,
      "loss": 1.084,
      "step": 5190
    },
    {
      "epoch": 48.51871500147362,
      "grad_norm": 0.48166748881340027,
      "learning_rate": 1.0968592218625294e-05,
      "loss": 1.0565,
      "step": 5191
    },
    {
      "epoch": 48.5281461833186,
      "grad_norm": 0.5308089256286621,
      "learning_rate": 1.0965579532260842e-05,
      "loss": 1.0639,
      "step": 5192
    },
    {
      "epoch": 48.53757736516357,
      "grad_norm": 0.49710211157798767,
      "learning_rate": 1.0962566757430313e-05,
      "loss": 1.0381,
      "step": 5193
    },
    {
      "epoch": 48.547008547008545,
      "grad_norm": 0.5560992956161499,
      "learning_rate": 1.0959553894409735e-05,
      "loss": 1.0141,
      "step": 5194
    },
    {
      "epoch": 48.55643972885352,
      "grad_norm": 0.5222088098526001,
      "learning_rate": 1.0956540943475152e-05,
      "loss": 1.0927,
      "step": 5195
    },
    {
      "epoch": 48.5658709106985,
      "grad_norm": 0.47869250178337097,
      "learning_rate": 1.09535279049026e-05,
      "loss": 1.0724,
      "step": 5196
    },
    {
      "epoch": 48.575302092543474,
      "grad_norm": 0.535136342048645,
      "learning_rate": 1.0950514778968137e-05,
      "loss": 1.078,
      "step": 5197
    },
    {
      "epoch": 48.58473327438845,
      "grad_norm": 0.5187461972236633,
      "learning_rate": 1.0947501565947826e-05,
      "loss": 1.0466,
      "step": 5198
    },
    {
      "epoch": 48.59416445623342,
      "grad_norm": 0.5428279638290405,
      "learning_rate": 1.0944488266117737e-05,
      "loss": 1.1069,
      "step": 5199
    },
    {
      "epoch": 48.603595638078396,
      "grad_norm": 0.48719218373298645,
      "learning_rate": 1.094147487975394e-05,
      "loss": 1.0814,
      "step": 5200
    },
    {
      "epoch": 48.61302681992337,
      "grad_norm": 0.49294227361679077,
      "learning_rate": 1.093846140713253e-05,
      "loss": 1.0969,
      "step": 5201
    },
    {
      "epoch": 48.622458001768344,
      "grad_norm": 0.5214895606040955,
      "learning_rate": 1.09354478485296e-05,
      "loss": 1.0654,
      "step": 5202
    },
    {
      "epoch": 48.631889183613325,
      "grad_norm": 0.491115003824234,
      "learning_rate": 1.0932434204221241e-05,
      "loss": 1.0764,
      "step": 5203
    },
    {
      "epoch": 48.6413203654583,
      "grad_norm": 0.5094084143638611,
      "learning_rate": 1.0929420474483571e-05,
      "loss": 1.0278,
      "step": 5204
    },
    {
      "epoch": 48.65075154730327,
      "grad_norm": 0.48142755031585693,
      "learning_rate": 1.0926406659592705e-05,
      "loss": 1.0791,
      "step": 5205
    },
    {
      "epoch": 48.66018272914825,
      "grad_norm": 0.5047820210456848,
      "learning_rate": 1.0923392759824769e-05,
      "loss": 1.0742,
      "step": 5206
    },
    {
      "epoch": 48.66961391099322,
      "grad_norm": 0.5196675062179565,
      "learning_rate": 1.0920378775455895e-05,
      "loss": 1.081,
      "step": 5207
    },
    {
      "epoch": 48.679045092838194,
      "grad_norm": 0.5158475041389465,
      "learning_rate": 1.0917364706762219e-05,
      "loss": 1.125,
      "step": 5208
    },
    {
      "epoch": 48.68847627468317,
      "grad_norm": 0.5276939868927002,
      "learning_rate": 1.0914350554019895e-05,
      "loss": 1.0231,
      "step": 5209
    },
    {
      "epoch": 48.69790745652815,
      "grad_norm": 0.5086817741394043,
      "learning_rate": 1.0911336317505074e-05,
      "loss": 1.0576,
      "step": 5210
    },
    {
      "epoch": 48.70733863837312,
      "grad_norm": 0.4925823211669922,
      "learning_rate": 1.0908321997493923e-05,
      "loss": 1.0589,
      "step": 5211
    },
    {
      "epoch": 48.7167698202181,
      "grad_norm": 0.5569533109664917,
      "learning_rate": 1.0905307594262611e-05,
      "loss": 1.0373,
      "step": 5212
    },
    {
      "epoch": 48.72620100206307,
      "grad_norm": 0.5038877725601196,
      "learning_rate": 1.0902293108087316e-05,
      "loss": 1.054,
      "step": 5213
    },
    {
      "epoch": 48.735632183908045,
      "grad_norm": 0.48973190784454346,
      "learning_rate": 1.0899278539244226e-05,
      "loss": 1.0657,
      "step": 5214
    },
    {
      "epoch": 48.74506336575302,
      "grad_norm": 0.5407907962799072,
      "learning_rate": 1.0896263888009533e-05,
      "loss": 1.0849,
      "step": 5215
    },
    {
      "epoch": 48.75449454759799,
      "grad_norm": 0.5068647265434265,
      "learning_rate": 1.089324915465944e-05,
      "loss": 1.0491,
      "step": 5216
    },
    {
      "epoch": 48.763925729442974,
      "grad_norm": 0.5136620402336121,
      "learning_rate": 1.0890234339470155e-05,
      "loss": 1.0943,
      "step": 5217
    },
    {
      "epoch": 48.77335691128795,
      "grad_norm": 0.5434364676475525,
      "learning_rate": 1.0887219442717896e-05,
      "loss": 1.0997,
      "step": 5218
    },
    {
      "epoch": 48.78278809313292,
      "grad_norm": 0.5346980690956116,
      "learning_rate": 1.0884204464678883e-05,
      "loss": 1.1013,
      "step": 5219
    },
    {
      "epoch": 48.792219274977896,
      "grad_norm": 0.501930832862854,
      "learning_rate": 1.088118940562935e-05,
      "loss": 1.0269,
      "step": 5220
    },
    {
      "epoch": 48.80165045682287,
      "grad_norm": 0.5236774682998657,
      "learning_rate": 1.0878174265845538e-05,
      "loss": 1.0529,
      "step": 5221
    },
    {
      "epoch": 48.811081638667844,
      "grad_norm": 0.4976962208747864,
      "learning_rate": 1.087515904560369e-05,
      "loss": 1.0908,
      "step": 5222
    },
    {
      "epoch": 48.82051282051282,
      "grad_norm": 0.5384220480918884,
      "learning_rate": 1.0872143745180056e-05,
      "loss": 1.0185,
      "step": 5223
    },
    {
      "epoch": 48.8299440023578,
      "grad_norm": 0.5226768851280212,
      "learning_rate": 1.0869128364850903e-05,
      "loss": 1.0357,
      "step": 5224
    },
    {
      "epoch": 48.83937518420277,
      "grad_norm": 0.5480595827102661,
      "learning_rate": 1.0866112904892498e-05,
      "loss": 1.1121,
      "step": 5225
    },
    {
      "epoch": 48.84880636604775,
      "grad_norm": 0.502617597579956,
      "learning_rate": 1.0863097365581112e-05,
      "loss": 1.0492,
      "step": 5226
    },
    {
      "epoch": 48.85823754789272,
      "grad_norm": 0.4848478138446808,
      "learning_rate": 1.0860081747193036e-05,
      "loss": 1.0876,
      "step": 5227
    },
    {
      "epoch": 48.867668729737694,
      "grad_norm": 0.5009629726409912,
      "learning_rate": 1.0857066050004553e-05,
      "loss": 1.0932,
      "step": 5228
    },
    {
      "epoch": 48.87709991158267,
      "grad_norm": 0.5181910395622253,
      "learning_rate": 1.0854050274291964e-05,
      "loss": 1.0608,
      "step": 5229
    },
    {
      "epoch": 48.88653109342764,
      "grad_norm": 0.5037984251976013,
      "learning_rate": 1.0851034420331568e-05,
      "loss": 1.0314,
      "step": 5230
    },
    {
      "epoch": 48.89596227527262,
      "grad_norm": 0.5484284162521362,
      "learning_rate": 1.0848018488399681e-05,
      "loss": 1.0611,
      "step": 5231
    },
    {
      "epoch": 48.9053934571176,
      "grad_norm": 0.5087588429450989,
      "learning_rate": 1.0845002478772621e-05,
      "loss": 1.0851,
      "step": 5232
    },
    {
      "epoch": 48.91482463896257,
      "grad_norm": 0.5398514270782471,
      "learning_rate": 1.0841986391726713e-05,
      "loss": 1.0701,
      "step": 5233
    },
    {
      "epoch": 48.924255820807545,
      "grad_norm": 0.5107548832893372,
      "learning_rate": 1.0838970227538293e-05,
      "loss": 1.0743,
      "step": 5234
    },
    {
      "epoch": 48.93368700265252,
      "grad_norm": 0.4926207363605499,
      "learning_rate": 1.0835953986483699e-05,
      "loss": 1.0953,
      "step": 5235
    },
    {
      "epoch": 48.94311818449749,
      "grad_norm": 0.4928373396396637,
      "learning_rate": 1.0832937668839274e-05,
      "loss": 1.0758,
      "step": 5236
    },
    {
      "epoch": 48.95254936634247,
      "grad_norm": 0.515451192855835,
      "learning_rate": 1.0829921274881377e-05,
      "loss": 1.0838,
      "step": 5237
    },
    {
      "epoch": 48.96198054818745,
      "grad_norm": 0.5297410488128662,
      "learning_rate": 1.0826904804886367e-05,
      "loss": 1.0715,
      "step": 5238
    },
    {
      "epoch": 48.97141173003242,
      "grad_norm": 0.503853440284729,
      "learning_rate": 1.0823888259130613e-05,
      "loss": 1.0671,
      "step": 5239
    },
    {
      "epoch": 48.980842911877396,
      "grad_norm": 0.5333567261695862,
      "learning_rate": 1.0820871637890491e-05,
      "loss": 1.0347,
      "step": 5240
    },
    {
      "epoch": 48.99027409372237,
      "grad_norm": 0.5376279950141907,
      "learning_rate": 1.0817854941442378e-05,
      "loss": 1.0559,
      "step": 5241
    },
    {
      "epoch": 48.999705275567344,
      "grad_norm": 0.5704081058502197,
      "learning_rate": 1.081483817006267e-05,
      "loss": 1.0227,
      "step": 5242
    },
    {
      "epoch": 49.0,
      "grad_norm": 2.6028637886047363,
      "learning_rate": 1.0811821324027756e-05,
      "loss": 0.5859,
      "step": 5243
    },
    {
      "epoch": 49.009431181844974,
      "grad_norm": 0.47808462381362915,
      "learning_rate": 1.0808804403614044e-05,
      "loss": 1.0385,
      "step": 5244
    },
    {
      "epoch": 49.01886236368995,
      "grad_norm": 0.5226879715919495,
      "learning_rate": 1.0805787409097937e-05,
      "loss": 1.0624,
      "step": 5245
    },
    {
      "epoch": 49.02829354553492,
      "grad_norm": 0.528540313243866,
      "learning_rate": 1.0802770340755855e-05,
      "loss": 1.088,
      "step": 5246
    },
    {
      "epoch": 49.0377247273799,
      "grad_norm": 0.5351727604866028,
      "learning_rate": 1.079975319886422e-05,
      "loss": 1.0767,
      "step": 5247
    },
    {
      "epoch": 49.04715590922488,
      "grad_norm": 0.5234679579734802,
      "learning_rate": 1.0796735983699464e-05,
      "loss": 1.0484,
      "step": 5248
    },
    {
      "epoch": 49.05658709106985,
      "grad_norm": 0.5218492746353149,
      "learning_rate": 1.0793718695538019e-05,
      "loss": 1.0996,
      "step": 5249
    },
    {
      "epoch": 49.066018272914825,
      "grad_norm": 0.508327305316925,
      "learning_rate": 1.079070133465633e-05,
      "loss": 1.0747,
      "step": 5250
    },
    {
      "epoch": 49.0754494547598,
      "grad_norm": 0.532106339931488,
      "learning_rate": 1.0787683901330848e-05,
      "loss": 1.0287,
      "step": 5251
    },
    {
      "epoch": 49.08488063660477,
      "grad_norm": 0.5258245468139648,
      "learning_rate": 1.0784666395838026e-05,
      "loss": 1.0215,
      "step": 5252
    },
    {
      "epoch": 49.09431181844975,
      "grad_norm": 0.5197099447250366,
      "learning_rate": 1.0781648818454327e-05,
      "loss": 1.0574,
      "step": 5253
    },
    {
      "epoch": 49.10374300029473,
      "grad_norm": 0.5314270257949829,
      "learning_rate": 1.0778631169456224e-05,
      "loss": 1.0766,
      "step": 5254
    },
    {
      "epoch": 49.1131741821397,
      "grad_norm": 0.4785619080066681,
      "learning_rate": 1.0775613449120192e-05,
      "loss": 1.0523,
      "step": 5255
    },
    {
      "epoch": 49.122605363984675,
      "grad_norm": 0.5219099521636963,
      "learning_rate": 1.077259565772271e-05,
      "loss": 1.0209,
      "step": 5256
    },
    {
      "epoch": 49.13203654582965,
      "grad_norm": 0.523665726184845,
      "learning_rate": 1.0769577795540271e-05,
      "loss": 1.0304,
      "step": 5257
    },
    {
      "epoch": 49.14146772767462,
      "grad_norm": 0.5056554079055786,
      "learning_rate": 1.076655986284937e-05,
      "loss": 1.0367,
      "step": 5258
    },
    {
      "epoch": 49.1508989095196,
      "grad_norm": 0.5036462545394897,
      "learning_rate": 1.076354185992651e-05,
      "loss": 1.0565,
      "step": 5259
    },
    {
      "epoch": 49.16033009136457,
      "grad_norm": 0.49864980578422546,
      "learning_rate": 1.076052378704819e-05,
      "loss": 1.0666,
      "step": 5260
    },
    {
      "epoch": 49.16976127320955,
      "grad_norm": 0.5295200347900391,
      "learning_rate": 1.0757505644490939e-05,
      "loss": 1.0715,
      "step": 5261
    },
    {
      "epoch": 49.179192455054526,
      "grad_norm": 0.5252785086631775,
      "learning_rate": 1.075448743253127e-05,
      "loss": 1.0676,
      "step": 5262
    },
    {
      "epoch": 49.1886236368995,
      "grad_norm": 0.5303230285644531,
      "learning_rate": 1.0751469151445715e-05,
      "loss": 1.0759,
      "step": 5263
    },
    {
      "epoch": 49.198054818744474,
      "grad_norm": 0.5304791927337646,
      "learning_rate": 1.0748450801510802e-05,
      "loss": 1.0934,
      "step": 5264
    },
    {
      "epoch": 49.20748600058945,
      "grad_norm": 0.5161765813827515,
      "learning_rate": 1.0745432383003074e-05,
      "loss": 1.0632,
      "step": 5265
    },
    {
      "epoch": 49.21691718243442,
      "grad_norm": 0.5393592119216919,
      "learning_rate": 1.074241389619908e-05,
      "loss": 1.0856,
      "step": 5266
    },
    {
      "epoch": 49.226348364279396,
      "grad_norm": 0.5320269465446472,
      "learning_rate": 1.0739395341375368e-05,
      "loss": 1.0786,
      "step": 5267
    },
    {
      "epoch": 49.23577954612438,
      "grad_norm": 0.49589934945106506,
      "learning_rate": 1.0736376718808499e-05,
      "loss": 1.0624,
      "step": 5268
    },
    {
      "epoch": 49.24521072796935,
      "grad_norm": 0.49543315172195435,
      "learning_rate": 1.073335802877504e-05,
      "loss": 1.0884,
      "step": 5269
    },
    {
      "epoch": 49.254641909814325,
      "grad_norm": 0.5066086649894714,
      "learning_rate": 1.0730339271551563e-05,
      "loss": 1.0919,
      "step": 5270
    },
    {
      "epoch": 49.2640730916593,
      "grad_norm": 0.5200486183166504,
      "learning_rate": 1.0727320447414642e-05,
      "loss": 1.0268,
      "step": 5271
    },
    {
      "epoch": 49.27350427350427,
      "grad_norm": 0.5050088167190552,
      "learning_rate": 1.0724301556640862e-05,
      "loss": 1.045,
      "step": 5272
    },
    {
      "epoch": 49.28293545534925,
      "grad_norm": 0.5011102557182312,
      "learning_rate": 1.0721282599506816e-05,
      "loss": 1.0784,
      "step": 5273
    },
    {
      "epoch": 49.29236663719422,
      "grad_norm": 0.5331813097000122,
      "learning_rate": 1.0718263576289094e-05,
      "loss": 1.0548,
      "step": 5274
    },
    {
      "epoch": 49.3017978190392,
      "grad_norm": 0.5728378295898438,
      "learning_rate": 1.0715244487264301e-05,
      "loss": 1.0747,
      "step": 5275
    },
    {
      "epoch": 49.311229000884175,
      "grad_norm": 0.5362672209739685,
      "learning_rate": 1.0712225332709045e-05,
      "loss": 1.0528,
      "step": 5276
    },
    {
      "epoch": 49.32066018272915,
      "grad_norm": 0.4900151491165161,
      "learning_rate": 1.0709206112899941e-05,
      "loss": 1.0594,
      "step": 5277
    },
    {
      "epoch": 49.33009136457412,
      "grad_norm": 0.5040959119796753,
      "learning_rate": 1.070618682811361e-05,
      "loss": 1.0506,
      "step": 5278
    },
    {
      "epoch": 49.3395225464191,
      "grad_norm": 0.5077119469642639,
      "learning_rate": 1.070316747862667e-05,
      "loss": 1.0585,
      "step": 5279
    },
    {
      "epoch": 49.34895372826407,
      "grad_norm": 0.5344879627227783,
      "learning_rate": 1.0700148064715762e-05,
      "loss": 1.0545,
      "step": 5280
    },
    {
      "epoch": 49.358384910109045,
      "grad_norm": 0.5013949871063232,
      "learning_rate": 1.0697128586657521e-05,
      "loss": 1.0548,
      "step": 5281
    },
    {
      "epoch": 49.367816091954026,
      "grad_norm": 0.5089251399040222,
      "learning_rate": 1.0694109044728587e-05,
      "loss": 1.0803,
      "step": 5282
    },
    {
      "epoch": 49.377247273799,
      "grad_norm": 0.5143842697143555,
      "learning_rate": 1.0691089439205613e-05,
      "loss": 1.0378,
      "step": 5283
    },
    {
      "epoch": 49.386678455643974,
      "grad_norm": 0.5433492064476013,
      "learning_rate": 1.0688069770365255e-05,
      "loss": 1.0817,
      "step": 5284
    },
    {
      "epoch": 49.39610963748895,
      "grad_norm": 0.5428045988082886,
      "learning_rate": 1.068505003848417e-05,
      "loss": 1.0973,
      "step": 5285
    },
    {
      "epoch": 49.40554081933392,
      "grad_norm": 0.5701993107795715,
      "learning_rate": 1.0682030243839028e-05,
      "loss": 1.0656,
      "step": 5286
    },
    {
      "epoch": 49.414972001178896,
      "grad_norm": 0.4855886697769165,
      "learning_rate": 1.0679010386706502e-05,
      "loss": 1.0526,
      "step": 5287
    },
    {
      "epoch": 49.42440318302387,
      "grad_norm": 0.5060150623321533,
      "learning_rate": 1.0675990467363268e-05,
      "loss": 1.0649,
      "step": 5288
    },
    {
      "epoch": 49.43383436486885,
      "grad_norm": 0.5163794159889221,
      "learning_rate": 1.0672970486086014e-05,
      "loss": 1.0651,
      "step": 5289
    },
    {
      "epoch": 49.443265546713825,
      "grad_norm": 0.5401992797851562,
      "learning_rate": 1.0669950443151422e-05,
      "loss": 1.0734,
      "step": 5290
    },
    {
      "epoch": 49.4526967285588,
      "grad_norm": 0.5438742637634277,
      "learning_rate": 1.0666930338836194e-05,
      "loss": 1.0597,
      "step": 5291
    },
    {
      "epoch": 49.46212791040377,
      "grad_norm": 0.5405029654502869,
      "learning_rate": 1.0663910173417032e-05,
      "loss": 1.0422,
      "step": 5292
    },
    {
      "epoch": 49.47155909224875,
      "grad_norm": 0.5734820365905762,
      "learning_rate": 1.0660889947170639e-05,
      "loss": 1.0153,
      "step": 5293
    },
    {
      "epoch": 49.48099027409372,
      "grad_norm": 0.5254543423652649,
      "learning_rate": 1.0657869660373723e-05,
      "loss": 1.0721,
      "step": 5294
    },
    {
      "epoch": 49.490421455938694,
      "grad_norm": 0.5086501836776733,
      "learning_rate": 1.065484931330301e-05,
      "loss": 1.0879,
      "step": 5295
    },
    {
      "epoch": 49.499852637783675,
      "grad_norm": 0.5082686543464661,
      "learning_rate": 1.065182890623522e-05,
      "loss": 1.0959,
      "step": 5296
    },
    {
      "epoch": 49.50928381962865,
      "grad_norm": 0.5558949112892151,
      "learning_rate": 1.064880843944708e-05,
      "loss": 1.0376,
      "step": 5297
    },
    {
      "epoch": 49.51871500147362,
      "grad_norm": 0.5298042297363281,
      "learning_rate": 1.0645787913215326e-05,
      "loss": 1.0861,
      "step": 5298
    },
    {
      "epoch": 49.5281461833186,
      "grad_norm": 0.5134958028793335,
      "learning_rate": 1.06427673278167e-05,
      "loss": 1.0442,
      "step": 5299
    },
    {
      "epoch": 49.53757736516357,
      "grad_norm": 0.519975483417511,
      "learning_rate": 1.0639746683527941e-05,
      "loss": 1.0804,
      "step": 5300
    },
    {
      "epoch": 49.547008547008545,
      "grad_norm": 0.5566266775131226,
      "learning_rate": 1.0636725980625802e-05,
      "loss": 1.0552,
      "step": 5301
    },
    {
      "epoch": 49.55643972885352,
      "grad_norm": 0.5603598356246948,
      "learning_rate": 1.063370521938704e-05,
      "loss": 1.0574,
      "step": 5302
    },
    {
      "epoch": 49.5658709106985,
      "grad_norm": 0.49111294746398926,
      "learning_rate": 1.0630684400088417e-05,
      "loss": 1.0652,
      "step": 5303
    },
    {
      "epoch": 49.575302092543474,
      "grad_norm": 0.4767717719078064,
      "learning_rate": 1.0627663523006699e-05,
      "loss": 1.0687,
      "step": 5304
    },
    {
      "epoch": 49.58473327438845,
      "grad_norm": 0.4901890754699707,
      "learning_rate": 1.0624642588418652e-05,
      "loss": 1.0768,
      "step": 5305
    },
    {
      "epoch": 49.59416445623342,
      "grad_norm": 0.48314398527145386,
      "learning_rate": 1.0621621596601064e-05,
      "loss": 1.0702,
      "step": 5306
    },
    {
      "epoch": 49.603595638078396,
      "grad_norm": 0.5582726001739502,
      "learning_rate": 1.0618600547830709e-05,
      "loss": 1.0931,
      "step": 5307
    },
    {
      "epoch": 49.61302681992337,
      "grad_norm": 0.492933064699173,
      "learning_rate": 1.0615579442384376e-05,
      "loss": 1.0401,
      "step": 5308
    },
    {
      "epoch": 49.622458001768344,
      "grad_norm": 0.4921426773071289,
      "learning_rate": 1.0612558280538856e-05,
      "loss": 1.0439,
      "step": 5309
    },
    {
      "epoch": 49.631889183613325,
      "grad_norm": 0.5139868259429932,
      "learning_rate": 1.060953706257095e-05,
      "loss": 1.0484,
      "step": 5310
    },
    {
      "epoch": 49.6413203654583,
      "grad_norm": 0.5724824070930481,
      "learning_rate": 1.0606515788757464e-05,
      "loss": 1.0697,
      "step": 5311
    },
    {
      "epoch": 49.65075154730327,
      "grad_norm": 0.4836226999759674,
      "learning_rate": 1.0603494459375198e-05,
      "loss": 1.0787,
      "step": 5312
    },
    {
      "epoch": 49.66018272914825,
      "grad_norm": 0.5242606997489929,
      "learning_rate": 1.0600473074700971e-05,
      "loss": 1.0687,
      "step": 5313
    },
    {
      "epoch": 49.66961391099322,
      "grad_norm": 0.4759129583835602,
      "learning_rate": 1.0597451635011602e-05,
      "loss": 1.0898,
      "step": 5314
    },
    {
      "epoch": 49.679045092838194,
      "grad_norm": 0.4960581660270691,
      "learning_rate": 1.0594430140583908e-05,
      "loss": 1.0446,
      "step": 5315
    },
    {
      "epoch": 49.68847627468317,
      "grad_norm": 0.5360612869262695,
      "learning_rate": 1.0591408591694723e-05,
      "loss": 1.0511,
      "step": 5316
    },
    {
      "epoch": 49.69790745652815,
      "grad_norm": 0.5322323441505432,
      "learning_rate": 1.058838698862088e-05,
      "loss": 1.0647,
      "step": 5317
    },
    {
      "epoch": 49.70733863837312,
      "grad_norm": 0.4816572666168213,
      "learning_rate": 1.0585365331639214e-05,
      "loss": 1.0937,
      "step": 5318
    },
    {
      "epoch": 49.7167698202181,
      "grad_norm": 0.5180289149284363,
      "learning_rate": 1.0582343621026572e-05,
      "loss": 1.022,
      "step": 5319
    },
    {
      "epoch": 49.72620100206307,
      "grad_norm": 0.5089784860610962,
      "learning_rate": 1.05793218570598e-05,
      "loss": 1.0519,
      "step": 5320
    },
    {
      "epoch": 49.735632183908045,
      "grad_norm": 0.4793456494808197,
      "learning_rate": 1.0576300040015748e-05,
      "loss": 1.0225,
      "step": 5321
    },
    {
      "epoch": 49.74506336575302,
      "grad_norm": 0.49008607864379883,
      "learning_rate": 1.0573278170171281e-05,
      "loss": 1.0549,
      "step": 5322
    },
    {
      "epoch": 49.75449454759799,
      "grad_norm": 0.502405047416687,
      "learning_rate": 1.0570256247803254e-05,
      "loss": 1.0605,
      "step": 5323
    },
    {
      "epoch": 49.763925729442974,
      "grad_norm": 0.5151404738426208,
      "learning_rate": 1.0567234273188543e-05,
      "loss": 1.0445,
      "step": 5324
    },
    {
      "epoch": 49.77335691128795,
      "grad_norm": 0.5040981769561768,
      "learning_rate": 1.0564212246604013e-05,
      "loss": 1.0397,
      "step": 5325
    },
    {
      "epoch": 49.78278809313292,
      "grad_norm": 0.5140259265899658,
      "learning_rate": 1.0561190168326545e-05,
      "loss": 1.0524,
      "step": 5326
    },
    {
      "epoch": 49.792219274977896,
      "grad_norm": 0.541385293006897,
      "learning_rate": 1.0558168038633016e-05,
      "loss": 1.0411,
      "step": 5327
    },
    {
      "epoch": 49.80165045682287,
      "grad_norm": 0.5075770616531372,
      "learning_rate": 1.055514585780032e-05,
      "loss": 1.0576,
      "step": 5328
    },
    {
      "epoch": 49.811081638667844,
      "grad_norm": 0.4746605455875397,
      "learning_rate": 1.0552123626105341e-05,
      "loss": 1.0729,
      "step": 5329
    },
    {
      "epoch": 49.82051282051282,
      "grad_norm": 0.5210254192352295,
      "learning_rate": 1.0549101343824983e-05,
      "loss": 1.1093,
      "step": 5330
    },
    {
      "epoch": 49.8299440023578,
      "grad_norm": 0.5382940769195557,
      "learning_rate": 1.0546079011236137e-05,
      "loss": 1.0617,
      "step": 5331
    },
    {
      "epoch": 49.83937518420277,
      "grad_norm": 0.5179776549339294,
      "learning_rate": 1.0543056628615716e-05,
      "loss": 1.0456,
      "step": 5332
    },
    {
      "epoch": 49.84880636604775,
      "grad_norm": 0.5171189308166504,
      "learning_rate": 1.0540034196240622e-05,
      "loss": 1.0416,
      "step": 5333
    },
    {
      "epoch": 49.85823754789272,
      "grad_norm": 0.49703449010849,
      "learning_rate": 1.0537011714387777e-05,
      "loss": 1.0616,
      "step": 5334
    },
    {
      "epoch": 49.867668729737694,
      "grad_norm": 0.49813738465309143,
      "learning_rate": 1.0533989183334097e-05,
      "loss": 1.0999,
      "step": 5335
    },
    {
      "epoch": 49.87709991158267,
      "grad_norm": 0.5012653470039368,
      "learning_rate": 1.0530966603356503e-05,
      "loss": 1.0746,
      "step": 5336
    },
    {
      "epoch": 49.88653109342764,
      "grad_norm": 0.48583441972732544,
      "learning_rate": 1.0527943974731926e-05,
      "loss": 1.0732,
      "step": 5337
    },
    {
      "epoch": 49.89596227527262,
      "grad_norm": 0.5428352355957031,
      "learning_rate": 1.0524921297737294e-05,
      "loss": 1.0383,
      "step": 5338
    },
    {
      "epoch": 49.9053934571176,
      "grad_norm": 0.5508549809455872,
      "learning_rate": 1.052189857264955e-05,
      "loss": 1.0344,
      "step": 5339
    },
    {
      "epoch": 49.91482463896257,
      "grad_norm": 0.5427358746528625,
      "learning_rate": 1.0518875799745632e-05,
      "loss": 1.0359,
      "step": 5340
    },
    {
      "epoch": 49.924255820807545,
      "grad_norm": 0.5169715881347656,
      "learning_rate": 1.0515852979302482e-05,
      "loss": 1.0537,
      "step": 5341
    },
    {
      "epoch": 49.93368700265252,
      "grad_norm": 0.5244999527931213,
      "learning_rate": 1.0512830111597054e-05,
      "loss": 1.0816,
      "step": 5342
    },
    {
      "epoch": 49.94311818449749,
      "grad_norm": 0.4781780540943146,
      "learning_rate": 1.0509807196906302e-05,
      "loss": 1.0639,
      "step": 5343
    },
    {
      "epoch": 49.95254936634247,
      "grad_norm": 0.4862041771411896,
      "learning_rate": 1.0506784235507184e-05,
      "loss": 1.0618,
      "step": 5344
    },
    {
      "epoch": 49.96198054818745,
      "grad_norm": 0.568413496017456,
      "learning_rate": 1.0503761227676662e-05,
      "loss": 1.1022,
      "step": 5345
    },
    {
      "epoch": 49.97141173003242,
      "grad_norm": 0.5070628523826599,
      "learning_rate": 1.0500738173691699e-05,
      "loss": 1.0895,
      "step": 5346
    },
    {
      "epoch": 49.980842911877396,
      "grad_norm": 0.5123960971832275,
      "learning_rate": 1.0497715073829273e-05,
      "loss": 1.0806,
      "step": 5347
    },
    {
      "epoch": 49.99027409372237,
      "grad_norm": 0.5291430354118347,
      "learning_rate": 1.0494691928366359e-05,
      "loss": 1.0382,
      "step": 5348
    },
    {
      "epoch": 49.999705275567344,
      "grad_norm": 0.5195531845092773,
      "learning_rate": 1.0491668737579932e-05,
      "loss": 1.0939,
      "step": 5349
    },
    {
      "epoch": 50.0,
      "grad_norm": 3.0364785194396973,
      "learning_rate": 1.0488645501746978e-05,
      "loss": 0.6224,
      "step": 5350
    },
    {
      "epoch": 50.009431181844974,
      "grad_norm": 0.5171604752540588,
      "learning_rate": 1.0485622221144485e-05,
      "loss": 1.0692,
      "step": 5351
    },
    {
      "epoch": 50.01886236368995,
      "grad_norm": 0.5559306144714355,
      "learning_rate": 1.0482598896049444e-05,
      "loss": 1.0525,
      "step": 5352
    },
    {
      "epoch": 50.02829354553492,
      "grad_norm": 0.5593299269676208,
      "learning_rate": 1.0479575526738852e-05,
      "loss": 1.1022,
      "step": 5353
    },
    {
      "epoch": 50.0377247273799,
      "grad_norm": 0.4840291142463684,
      "learning_rate": 1.047655211348971e-05,
      "loss": 1.0816,
      "step": 5354
    },
    {
      "epoch": 50.04715590922488,
      "grad_norm": 0.5150090456008911,
      "learning_rate": 1.0473528656579021e-05,
      "loss": 1.065,
      "step": 5355
    },
    {
      "epoch": 50.05658709106985,
      "grad_norm": 0.49745818972587585,
      "learning_rate": 1.0470505156283794e-05,
      "loss": 1.0438,
      "step": 5356
    },
    {
      "epoch": 50.066018272914825,
      "grad_norm": 0.5005938410758972,
      "learning_rate": 1.0467481612881039e-05,
      "loss": 1.1075,
      "step": 5357
    },
    {
      "epoch": 50.0754494547598,
      "grad_norm": 0.5191488862037659,
      "learning_rate": 1.0464458026647775e-05,
      "loss": 1.0928,
      "step": 5358
    },
    {
      "epoch": 50.08488063660477,
      "grad_norm": 0.5153788328170776,
      "learning_rate": 1.0461434397861019e-05,
      "loss": 1.0789,
      "step": 5359
    },
    {
      "epoch": 50.09431181844975,
      "grad_norm": 0.5431423783302307,
      "learning_rate": 1.0458410726797796e-05,
      "loss": 1.0585,
      "step": 5360
    },
    {
      "epoch": 50.10374300029473,
      "grad_norm": 0.5093986392021179,
      "learning_rate": 1.0455387013735135e-05,
      "loss": 1.0663,
      "step": 5361
    },
    {
      "epoch": 50.1131741821397,
      "grad_norm": 0.4930590093135834,
      "learning_rate": 1.0452363258950066e-05,
      "loss": 1.0744,
      "step": 5362
    },
    {
      "epoch": 50.122605363984675,
      "grad_norm": 0.5280934572219849,
      "learning_rate": 1.0449339462719625e-05,
      "loss": 1.046,
      "step": 5363
    },
    {
      "epoch": 50.13203654582965,
      "grad_norm": 0.4922430217266083,
      "learning_rate": 1.0446315625320851e-05,
      "loss": 1.0528,
      "step": 5364
    },
    {
      "epoch": 50.14146772767462,
      "grad_norm": 0.4771445691585541,
      "learning_rate": 1.0443291747030788e-05,
      "loss": 1.0817,
      "step": 5365
    },
    {
      "epoch": 50.1508989095196,
      "grad_norm": 0.5532970428466797,
      "learning_rate": 1.0440267828126478e-05,
      "loss": 1.0354,
      "step": 5366
    },
    {
      "epoch": 50.16033009136457,
      "grad_norm": 0.5393028259277344,
      "learning_rate": 1.0437243868884981e-05,
      "loss": 1.0382,
      "step": 5367
    },
    {
      "epoch": 50.16976127320955,
      "grad_norm": 0.5136730074882507,
      "learning_rate": 1.0434219869583342e-05,
      "loss": 1.0592,
      "step": 5368
    },
    {
      "epoch": 50.179192455054526,
      "grad_norm": 0.49337899684906006,
      "learning_rate": 1.0431195830498621e-05,
      "loss": 1.0688,
      "step": 5369
    },
    {
      "epoch": 50.1886236368995,
      "grad_norm": 0.5194981098175049,
      "learning_rate": 1.0428171751907885e-05,
      "loss": 1.0692,
      "step": 5370
    },
    {
      "epoch": 50.198054818744474,
      "grad_norm": 0.4998760521411896,
      "learning_rate": 1.0425147634088195e-05,
      "loss": 1.0763,
      "step": 5371
    },
    {
      "epoch": 50.20748600058945,
      "grad_norm": 0.515407145023346,
      "learning_rate": 1.0422123477316614e-05,
      "loss": 1.0493,
      "step": 5372
    },
    {
      "epoch": 50.21691718243442,
      "grad_norm": 0.5250090956687927,
      "learning_rate": 1.0419099281870224e-05,
      "loss": 1.0866,
      "step": 5373
    },
    {
      "epoch": 50.226348364279396,
      "grad_norm": 0.5389906764030457,
      "learning_rate": 1.0416075048026097e-05,
      "loss": 1.0776,
      "step": 5374
    },
    {
      "epoch": 50.23577954612438,
      "grad_norm": 0.5145555734634399,
      "learning_rate": 1.041305077606131e-05,
      "loss": 1.0247,
      "step": 5375
    },
    {
      "epoch": 50.24521072796935,
      "grad_norm": 0.4951982796192169,
      "learning_rate": 1.041002646625295e-05,
      "loss": 1.0049,
      "step": 5376
    },
    {
      "epoch": 50.254641909814325,
      "grad_norm": 0.4948718249797821,
      "learning_rate": 1.0407002118878099e-05,
      "loss": 1.0891,
      "step": 5377
    },
    {
      "epoch": 50.2640730916593,
      "grad_norm": 0.5619181394577026,
      "learning_rate": 1.0403977734213848e-05,
      "loss": 1.0419,
      "step": 5378
    },
    {
      "epoch": 50.27350427350427,
      "grad_norm": 0.5400648713111877,
      "learning_rate": 1.040095331253729e-05,
      "loss": 1.0256,
      "step": 5379
    },
    {
      "epoch": 50.28293545534925,
      "grad_norm": 0.5244431495666504,
      "learning_rate": 1.0397928854125526e-05,
      "loss": 1.0381,
      "step": 5380
    },
    {
      "epoch": 50.29236663719422,
      "grad_norm": 0.5366798639297485,
      "learning_rate": 1.0394904359255649e-05,
      "loss": 1.0483,
      "step": 5381
    },
    {
      "epoch": 50.3017978190392,
      "grad_norm": 0.5272756814956665,
      "learning_rate": 1.0391879828204767e-05,
      "loss": 1.1069,
      "step": 5382
    },
    {
      "epoch": 50.311229000884175,
      "grad_norm": 0.5353110432624817,
      "learning_rate": 1.0388855261249984e-05,
      "loss": 1.0492,
      "step": 5383
    },
    {
      "epoch": 50.32066018272915,
      "grad_norm": 0.5354304313659668,
      "learning_rate": 1.038583065866841e-05,
      "loss": 1.056,
      "step": 5384
    },
    {
      "epoch": 50.33009136457412,
      "grad_norm": 0.4851675033569336,
      "learning_rate": 1.0382806020737162e-05,
      "loss": 1.089,
      "step": 5385
    },
    {
      "epoch": 50.3395225464191,
      "grad_norm": 0.5326469540596008,
      "learning_rate": 1.037978134773335e-05,
      "loss": 1.0701,
      "step": 5386
    },
    {
      "epoch": 50.34895372826407,
      "grad_norm": 0.4956270158290863,
      "learning_rate": 1.0376756639934096e-05,
      "loss": 1.0978,
      "step": 5387
    },
    {
      "epoch": 50.358384910109045,
      "grad_norm": 0.5152869820594788,
      "learning_rate": 1.0373731897616525e-05,
      "loss": 1.0466,
      "step": 5388
    },
    {
      "epoch": 50.367816091954026,
      "grad_norm": 0.48005402088165283,
      "learning_rate": 1.0370707121057762e-05,
      "loss": 1.0873,
      "step": 5389
    },
    {
      "epoch": 50.377247273799,
      "grad_norm": 0.5146732926368713,
      "learning_rate": 1.0367682310534935e-05,
      "loss": 1.0481,
      "step": 5390
    },
    {
      "epoch": 50.386678455643974,
      "grad_norm": 0.5423011779785156,
      "learning_rate": 1.0364657466325175e-05,
      "loss": 1.0707,
      "step": 5391
    },
    {
      "epoch": 50.39610963748895,
      "grad_norm": 0.5568698048591614,
      "learning_rate": 1.0361632588705618e-05,
      "loss": 1.0793,
      "step": 5392
    },
    {
      "epoch": 50.40554081933392,
      "grad_norm": 0.4977085590362549,
      "learning_rate": 1.0358607677953404e-05,
      "loss": 1.0477,
      "step": 5393
    },
    {
      "epoch": 50.414972001178896,
      "grad_norm": 0.5078830718994141,
      "learning_rate": 1.0355582734345675e-05,
      "loss": 1.0773,
      "step": 5394
    },
    {
      "epoch": 50.42440318302387,
      "grad_norm": 0.5104547739028931,
      "learning_rate": 1.0352557758159571e-05,
      "loss": 1.0562,
      "step": 5395
    },
    {
      "epoch": 50.43383436486885,
      "grad_norm": 0.4782545864582062,
      "learning_rate": 1.0349532749672245e-05,
      "loss": 1.0749,
      "step": 5396
    },
    {
      "epoch": 50.443265546713825,
      "grad_norm": 0.5542824268341064,
      "learning_rate": 1.034650770916084e-05,
      "loss": 1.0296,
      "step": 5397
    },
    {
      "epoch": 50.4526967285588,
      "grad_norm": 0.49301832914352417,
      "learning_rate": 1.0343482636902517e-05,
      "loss": 1.0872,
      "step": 5398
    },
    {
      "epoch": 50.46212791040377,
      "grad_norm": 0.618270993232727,
      "learning_rate": 1.0340457533174426e-05,
      "loss": 1.0713,
      "step": 5399
    },
    {
      "epoch": 50.47155909224875,
      "grad_norm": 0.49088647961616516,
      "learning_rate": 1.033743239825373e-05,
      "loss": 1.0324,
      "step": 5400
    },
    {
      "epoch": 50.48099027409372,
      "grad_norm": 0.5132675170898438,
      "learning_rate": 1.0334407232417589e-05,
      "loss": 1.0616,
      "step": 5401
    },
    {
      "epoch": 50.490421455938694,
      "grad_norm": 0.49039238691329956,
      "learning_rate": 1.0331382035943167e-05,
      "loss": 1.0972,
      "step": 5402
    },
    {
      "epoch": 50.499852637783675,
      "grad_norm": 0.5031256675720215,
      "learning_rate": 1.0328356809107634e-05,
      "loss": 1.0821,
      "step": 5403
    },
    {
      "epoch": 50.50928381962865,
      "grad_norm": 0.5184370279312134,
      "learning_rate": 1.032533155218816e-05,
      "loss": 1.0746,
      "step": 5404
    },
    {
      "epoch": 50.51871500147362,
      "grad_norm": 0.5199331045150757,
      "learning_rate": 1.0322306265461917e-05,
      "loss": 1.0473,
      "step": 5405
    },
    {
      "epoch": 50.5281461833186,
      "grad_norm": 0.5039031505584717,
      "learning_rate": 1.0319280949206076e-05,
      "loss": 1.0333,
      "step": 5406
    },
    {
      "epoch": 50.53757736516357,
      "grad_norm": 0.5397680401802063,
      "learning_rate": 1.0316255603697825e-05,
      "loss": 1.0965,
      "step": 5407
    },
    {
      "epoch": 50.547008547008545,
      "grad_norm": 0.48619234561920166,
      "learning_rate": 1.031323022921434e-05,
      "loss": 1.0701,
      "step": 5408
    },
    {
      "epoch": 50.55643972885352,
      "grad_norm": 0.5202988982200623,
      "learning_rate": 1.03102048260328e-05,
      "loss": 1.0582,
      "step": 5409
    },
    {
      "epoch": 50.5658709106985,
      "grad_norm": 0.5124340653419495,
      "learning_rate": 1.0307179394430402e-05,
      "loss": 1.0785,
      "step": 5410
    },
    {
      "epoch": 50.575302092543474,
      "grad_norm": 0.5781117081642151,
      "learning_rate": 1.0304153934684328e-05,
      "loss": 1.0565,
      "step": 5411
    },
    {
      "epoch": 50.58473327438845,
      "grad_norm": 0.526603102684021,
      "learning_rate": 1.030112844707177e-05,
      "loss": 1.0538,
      "step": 5412
    },
    {
      "epoch": 50.59416445623342,
      "grad_norm": 0.4845718741416931,
      "learning_rate": 1.0298102931869925e-05,
      "loss": 1.0444,
      "step": 5413
    },
    {
      "epoch": 50.603595638078396,
      "grad_norm": 0.5774330496788025,
      "learning_rate": 1.0295077389355987e-05,
      "loss": 1.0396,
      "step": 5414
    },
    {
      "epoch": 50.61302681992337,
      "grad_norm": 0.5268517732620239,
      "learning_rate": 1.0292051819807159e-05,
      "loss": 1.0643,
      "step": 5415
    },
    {
      "epoch": 50.622458001768344,
      "grad_norm": 0.5174977779388428,
      "learning_rate": 1.0289026223500638e-05,
      "loss": 1.1021,
      "step": 5416
    },
    {
      "epoch": 50.631889183613325,
      "grad_norm": 0.5075930953025818,
      "learning_rate": 1.0286000600713628e-05,
      "loss": 1.0627,
      "step": 5417
    },
    {
      "epoch": 50.6413203654583,
      "grad_norm": 0.4938851594924927,
      "learning_rate": 1.028297495172334e-05,
      "loss": 1.0529,
      "step": 5418
    },
    {
      "epoch": 50.65075154730327,
      "grad_norm": 0.5131641030311584,
      "learning_rate": 1.027994927680698e-05,
      "loss": 1.0887,
      "step": 5419
    },
    {
      "epoch": 50.66018272914825,
      "grad_norm": 0.49240410327911377,
      "learning_rate": 1.0276923576241762e-05,
      "loss": 1.0673,
      "step": 5420
    },
    {
      "epoch": 50.66961391099322,
      "grad_norm": 0.5328530669212341,
      "learning_rate": 1.0273897850304892e-05,
      "loss": 1.0087,
      "step": 5421
    },
    {
      "epoch": 50.679045092838194,
      "grad_norm": 0.515124499797821,
      "learning_rate": 1.0270872099273597e-05,
      "loss": 1.0507,
      "step": 5422
    },
    {
      "epoch": 50.68847627468317,
      "grad_norm": 0.5308626890182495,
      "learning_rate": 1.0267846323425088e-05,
      "loss": 1.0614,
      "step": 5423
    },
    {
      "epoch": 50.69790745652815,
      "grad_norm": 0.5350085496902466,
      "learning_rate": 1.0264820523036587e-05,
      "loss": 1.0466,
      "step": 5424
    },
    {
      "epoch": 50.70733863837312,
      "grad_norm": 0.49900686740875244,
      "learning_rate": 1.0261794698385318e-05,
      "loss": 1.0075,
      "step": 5425
    },
    {
      "epoch": 50.7167698202181,
      "grad_norm": 0.48881107568740845,
      "learning_rate": 1.0258768849748506e-05,
      "loss": 1.0644,
      "step": 5426
    },
    {
      "epoch": 50.72620100206307,
      "grad_norm": 0.541140615940094,
      "learning_rate": 1.0255742977403379e-05,
      "loss": 1.058,
      "step": 5427
    },
    {
      "epoch": 50.735632183908045,
      "grad_norm": 0.49771812558174133,
      "learning_rate": 1.0252717081627162e-05,
      "loss": 1.0649,
      "step": 5428
    },
    {
      "epoch": 50.74506336575302,
      "grad_norm": 0.4841480255126953,
      "learning_rate": 1.024969116269709e-05,
      "loss": 1.0875,
      "step": 5429
    },
    {
      "epoch": 50.75449454759799,
      "grad_norm": 0.5142260789871216,
      "learning_rate": 1.0246665220890394e-05,
      "loss": 1.1059,
      "step": 5430
    },
    {
      "epoch": 50.763925729442974,
      "grad_norm": 0.49563363194465637,
      "learning_rate": 1.0243639256484317e-05,
      "loss": 1.061,
      "step": 5431
    },
    {
      "epoch": 50.77335691128795,
      "grad_norm": 0.5154733657836914,
      "learning_rate": 1.0240613269756093e-05,
      "loss": 1.0729,
      "step": 5432
    },
    {
      "epoch": 50.78278809313292,
      "grad_norm": 0.489881694316864,
      "learning_rate": 1.0237587260982955e-05,
      "loss": 1.0612,
      "step": 5433
    },
    {
      "epoch": 50.792219274977896,
      "grad_norm": 0.5234990119934082,
      "learning_rate": 1.0234561230442154e-05,
      "loss": 1.0391,
      "step": 5434
    },
    {
      "epoch": 50.80165045682287,
      "grad_norm": 0.5580355525016785,
      "learning_rate": 1.023153517841093e-05,
      "loss": 1.0088,
      "step": 5435
    },
    {
      "epoch": 50.811081638667844,
      "grad_norm": 0.5891701579093933,
      "learning_rate": 1.0228509105166532e-05,
      "loss": 1.0439,
      "step": 5436
    },
    {
      "epoch": 50.82051282051282,
      "grad_norm": 0.5229194760322571,
      "learning_rate": 1.0225483010986203e-05,
      "loss": 1.0776,
      "step": 5437
    },
    {
      "epoch": 50.8299440023578,
      "grad_norm": 0.5279148817062378,
      "learning_rate": 1.0222456896147198e-05,
      "loss": 1.069,
      "step": 5438
    },
    {
      "epoch": 50.83937518420277,
      "grad_norm": 0.4970405697822571,
      "learning_rate": 1.021943076092676e-05,
      "loss": 1.0632,
      "step": 5439
    },
    {
      "epoch": 50.84880636604775,
      "grad_norm": 0.5307050347328186,
      "learning_rate": 1.0216404605602152e-05,
      "loss": 1.0733,
      "step": 5440
    },
    {
      "epoch": 50.85823754789272,
      "grad_norm": 0.4898170530796051,
      "learning_rate": 1.0213378430450627e-05,
      "loss": 1.0515,
      "step": 5441
    },
    {
      "epoch": 50.867668729737694,
      "grad_norm": 0.4837368428707123,
      "learning_rate": 1.021035223574944e-05,
      "loss": 1.0352,
      "step": 5442
    },
    {
      "epoch": 50.87709991158267,
      "grad_norm": 0.5484749674797058,
      "learning_rate": 1.0207326021775849e-05,
      "loss": 1.0233,
      "step": 5443
    },
    {
      "epoch": 50.88653109342764,
      "grad_norm": 0.5061496496200562,
      "learning_rate": 1.0204299788807119e-05,
      "loss": 1.0609,
      "step": 5444
    },
    {
      "epoch": 50.89596227527262,
      "grad_norm": 0.5010334253311157,
      "learning_rate": 1.0201273537120508e-05,
      "loss": 1.0711,
      "step": 5445
    },
    {
      "epoch": 50.9053934571176,
      "grad_norm": 0.5260738134384155,
      "learning_rate": 1.0198247266993284e-05,
      "loss": 1.0794,
      "step": 5446
    },
    {
      "epoch": 50.91482463896257,
      "grad_norm": 0.5349136590957642,
      "learning_rate": 1.019522097870271e-05,
      "loss": 1.0351,
      "step": 5447
    },
    {
      "epoch": 50.924255820807545,
      "grad_norm": 0.4900454580783844,
      "learning_rate": 1.0192194672526055e-05,
      "loss": 1.0739,
      "step": 5448
    },
    {
      "epoch": 50.93368700265252,
      "grad_norm": 0.5779296159744263,
      "learning_rate": 1.0189168348740588e-05,
      "loss": 1.0358,
      "step": 5449
    },
    {
      "epoch": 50.94311818449749,
      "grad_norm": 0.5230627655982971,
      "learning_rate": 1.0186142007623577e-05,
      "loss": 1.0554,
      "step": 5450
    },
    {
      "epoch": 50.95254936634247,
      "grad_norm": 0.5536296963691711,
      "learning_rate": 1.01831156494523e-05,
      "loss": 1.0896,
      "step": 5451
    },
    {
      "epoch": 50.96198054818745,
      "grad_norm": 0.5193456411361694,
      "learning_rate": 1.0180089274504026e-05,
      "loss": 1.0574,
      "step": 5452
    },
    {
      "epoch": 50.97141173003242,
      "grad_norm": 0.4756775498390198,
      "learning_rate": 1.0177062883056033e-05,
      "loss": 1.0474,
      "step": 5453
    },
    {
      "epoch": 50.980842911877396,
      "grad_norm": 0.4796759784221649,
      "learning_rate": 1.0174036475385596e-05,
      "loss": 1.0637,
      "step": 5454
    },
    {
      "epoch": 50.99027409372237,
      "grad_norm": 0.5434620976448059,
      "learning_rate": 1.0171010051769997e-05,
      "loss": 1.0822,
      "step": 5455
    },
    {
      "epoch": 50.999705275567344,
      "grad_norm": 0.5172832012176514,
      "learning_rate": 1.0167983612486512e-05,
      "loss": 1.0456,
      "step": 5456
    },
    {
      "epoch": 51.0,
      "grad_norm": 4.0999040603637695,
      "learning_rate": 1.0164957157812425e-05,
      "loss": 0.7622,
      "step": 5457
    },
    {
      "epoch": 51.009431181844974,
      "grad_norm": 0.5207782983779907,
      "learning_rate": 1.0161930688025018e-05,
      "loss": 1.0781,
      "step": 5458
    },
    {
      "epoch": 51.01886236368995,
      "grad_norm": 0.5155892372131348,
      "learning_rate": 1.0158904203401574e-05,
      "loss": 1.0132,
      "step": 5459
    },
    {
      "epoch": 51.02829354553492,
      "grad_norm": 0.47541001439094543,
      "learning_rate": 1.0155877704219378e-05,
      "loss": 1.0342,
      "step": 5460
    },
    {
      "epoch": 51.0377247273799,
      "grad_norm": 0.47352808713912964,
      "learning_rate": 1.0152851190755721e-05,
      "loss": 1.0757,
      "step": 5461
    },
    {
      "epoch": 51.04715590922488,
      "grad_norm": 0.5003646612167358,
      "learning_rate": 1.0149824663287889e-05,
      "loss": 1.0752,
      "step": 5462
    },
    {
      "epoch": 51.05658709106985,
      "grad_norm": 0.5022345781326294,
      "learning_rate": 1.0146798122093167e-05,
      "loss": 1.0878,
      "step": 5463
    },
    {
      "epoch": 51.066018272914825,
      "grad_norm": 0.5281558632850647,
      "learning_rate": 1.0143771567448852e-05,
      "loss": 1.0546,
      "step": 5464
    },
    {
      "epoch": 51.0754494547598,
      "grad_norm": 0.5191981792449951,
      "learning_rate": 1.0140744999632234e-05,
      "loss": 1.0677,
      "step": 5465
    },
    {
      "epoch": 51.08488063660477,
      "grad_norm": 0.5176687836647034,
      "learning_rate": 1.0137718418920605e-05,
      "loss": 1.0419,
      "step": 5466
    },
    {
      "epoch": 51.09431181844975,
      "grad_norm": 0.602813184261322,
      "learning_rate": 1.013469182559126e-05,
      "loss": 1.0773,
      "step": 5467
    },
    {
      "epoch": 51.10374300029473,
      "grad_norm": 0.520394504070282,
      "learning_rate": 1.0131665219921497e-05,
      "loss": 0.9792,
      "step": 5468
    },
    {
      "epoch": 51.1131741821397,
      "grad_norm": 0.5227801203727722,
      "learning_rate": 1.0128638602188608e-05,
      "loss": 1.0806,
      "step": 5469
    },
    {
      "epoch": 51.122605363984675,
      "grad_norm": 0.514519453048706,
      "learning_rate": 1.0125611972669894e-05,
      "loss": 1.0282,
      "step": 5470
    },
    {
      "epoch": 51.13203654582965,
      "grad_norm": 0.5113250017166138,
      "learning_rate": 1.0122585331642651e-05,
      "loss": 1.0498,
      "step": 5471
    },
    {
      "epoch": 51.14146772767462,
      "grad_norm": 0.524357259273529,
      "learning_rate": 1.0119558679384182e-05,
      "loss": 1.0394,
      "step": 5472
    },
    {
      "epoch": 51.1508989095196,
      "grad_norm": 0.5439786314964294,
      "learning_rate": 1.0116532016171785e-05,
      "loss": 1.0908,
      "step": 5473
    },
    {
      "epoch": 51.16033009136457,
      "grad_norm": 0.5940637588500977,
      "learning_rate": 1.0113505342282764e-05,
      "loss": 1.065,
      "step": 5474
    },
    {
      "epoch": 51.16976127320955,
      "grad_norm": 0.5042744278907776,
      "learning_rate": 1.0110478657994422e-05,
      "loss": 1.0766,
      "step": 5475
    },
    {
      "epoch": 51.179192455054526,
      "grad_norm": 0.5285367965698242,
      "learning_rate": 1.0107451963584061e-05,
      "loss": 1.0283,
      "step": 5476
    },
    {
      "epoch": 51.1886236368995,
      "grad_norm": 0.5487282276153564,
      "learning_rate": 1.0104425259328985e-05,
      "loss": 1.0468,
      "step": 5477
    },
    {
      "epoch": 51.198054818744474,
      "grad_norm": 0.5397911071777344,
      "learning_rate": 1.0101398545506505e-05,
      "loss": 1.0409,
      "step": 5478
    },
    {
      "epoch": 51.20748600058945,
      "grad_norm": 0.4977778494358063,
      "learning_rate": 1.0098371822393921e-05,
      "loss": 1.0736,
      "step": 5479
    },
    {
      "epoch": 51.21691718243442,
      "grad_norm": 0.587604820728302,
      "learning_rate": 1.0095345090268543e-05,
      "loss": 1.0618,
      "step": 5480
    },
    {
      "epoch": 51.226348364279396,
      "grad_norm": 0.4968056082725525,
      "learning_rate": 1.0092318349407681e-05,
      "loss": 1.0834,
      "step": 5481
    },
    {
      "epoch": 51.23577954612438,
      "grad_norm": 0.49810588359832764,
      "learning_rate": 1.0089291600088642e-05,
      "loss": 1.0442,
      "step": 5482
    },
    {
      "epoch": 51.24521072796935,
      "grad_norm": 0.5354220867156982,
      "learning_rate": 1.0086264842588738e-05,
      "loss": 1.051,
      "step": 5483
    },
    {
      "epoch": 51.254641909814325,
      "grad_norm": 0.5330519676208496,
      "learning_rate": 1.0083238077185273e-05,
      "loss": 1.0556,
      "step": 5484
    },
    {
      "epoch": 51.2640730916593,
      "grad_norm": 0.5494718551635742,
      "learning_rate": 1.0080211304155564e-05,
      "loss": 1.0841,
      "step": 5485
    },
    {
      "epoch": 51.27350427350427,
      "grad_norm": 0.5061254501342773,
      "learning_rate": 1.0077184523776926e-05,
      "loss": 1.0399,
      "step": 5486
    },
    {
      "epoch": 51.28293545534925,
      "grad_norm": 0.5095997452735901,
      "learning_rate": 1.0074157736326666e-05,
      "loss": 1.0446,
      "step": 5487
    },
    {
      "epoch": 51.29236663719422,
      "grad_norm": 0.5539581179618835,
      "learning_rate": 1.0071130942082095e-05,
      "loss": 1.1165,
      "step": 5488
    },
    {
      "epoch": 51.3017978190392,
      "grad_norm": 0.5021702647209167,
      "learning_rate": 1.0068104141320534e-05,
      "loss": 1.0982,
      "step": 5489
    },
    {
      "epoch": 51.311229000884175,
      "grad_norm": 0.49035313725471497,
      "learning_rate": 1.0065077334319295e-05,
      "loss": 1.0279,
      "step": 5490
    },
    {
      "epoch": 51.32066018272915,
      "grad_norm": 0.5136646032333374,
      "learning_rate": 1.0062050521355689e-05,
      "loss": 1.0642,
      "step": 5491
    },
    {
      "epoch": 51.33009136457412,
      "grad_norm": 0.5245873928070068,
      "learning_rate": 1.0059023702707037e-05,
      "loss": 1.0254,
      "step": 5492
    },
    {
      "epoch": 51.3395225464191,
      "grad_norm": 0.499143123626709,
      "learning_rate": 1.0055996878650652e-05,
      "loss": 1.0827,
      "step": 5493
    },
    {
      "epoch": 51.34895372826407,
      "grad_norm": 0.5752935409545898,
      "learning_rate": 1.005297004946385e-05,
      "loss": 1.0541,
      "step": 5494
    },
    {
      "epoch": 51.358384910109045,
      "grad_norm": 0.521465539932251,
      "learning_rate": 1.0049943215423953e-05,
      "loss": 1.0684,
      "step": 5495
    },
    {
      "epoch": 51.367816091954026,
      "grad_norm": 0.5002906918525696,
      "learning_rate": 1.0046916376808274e-05,
      "loss": 1.0735,
      "step": 5496
    },
    {
      "epoch": 51.377247273799,
      "grad_norm": 0.5254576206207275,
      "learning_rate": 1.0043889533894129e-05,
      "loss": 1.091,
      "step": 5497
    },
    {
      "epoch": 51.386678455643974,
      "grad_norm": 0.5052348375320435,
      "learning_rate": 1.0040862686958843e-05,
      "loss": 1.086,
      "step": 5498
    },
    {
      "epoch": 51.39610963748895,
      "grad_norm": 0.5788342356681824,
      "learning_rate": 1.0037835836279728e-05,
      "loss": 1.0636,
      "step": 5499
    },
    {
      "epoch": 51.40554081933392,
      "grad_norm": 0.5302203893661499,
      "learning_rate": 1.003480898213411e-05,
      "loss": 1.0622,
      "step": 5500
    },
    {
      "epoch": 51.414972001178896,
      "grad_norm": 0.5003389716148376,
      "learning_rate": 1.0031782124799304e-05,
      "loss": 1.0668,
      "step": 5501
    },
    {
      "epoch": 51.42440318302387,
      "grad_norm": 0.4856675863265991,
      "learning_rate": 1.002875526455263e-05,
      "loss": 1.0584,
      "step": 5502
    },
    {
      "epoch": 51.43383436486885,
      "grad_norm": 0.5384105443954468,
      "learning_rate": 1.0025728401671404e-05,
      "loss": 1.0974,
      "step": 5503
    },
    {
      "epoch": 51.443265546713825,
      "grad_norm": 0.47388261556625366,
      "learning_rate": 1.0022701536432954e-05,
      "loss": 1.0304,
      "step": 5504
    },
    {
      "epoch": 51.4526967285588,
      "grad_norm": 0.5470851063728333,
      "learning_rate": 1.0019674669114599e-05,
      "loss": 1.0758,
      "step": 5505
    },
    {
      "epoch": 51.46212791040377,
      "grad_norm": 0.536262035369873,
      "learning_rate": 1.0016647799993654e-05,
      "loss": 1.0829,
      "step": 5506
    },
    {
      "epoch": 51.47155909224875,
      "grad_norm": 0.5256814956665039,
      "learning_rate": 1.0013620929347443e-05,
      "loss": 1.026,
      "step": 5507
    },
    {
      "epoch": 51.48099027409372,
      "grad_norm": 0.4841298460960388,
      "learning_rate": 1.0010594057453288e-05,
      "loss": 1.0669,
      "step": 5508
    },
    {
      "epoch": 51.490421455938694,
      "grad_norm": 0.5224629044532776,
      "learning_rate": 1.0007567184588512e-05,
      "loss": 1.0622,
      "step": 5509
    },
    {
      "epoch": 51.499852637783675,
      "grad_norm": 0.5019553899765015,
      "learning_rate": 1.0004540311030427e-05,
      "loss": 1.0248,
      "step": 5510
    },
    {
      "epoch": 51.50928381962865,
      "grad_norm": 0.5203858017921448,
      "learning_rate": 1.0001513437056363e-05,
      "loss": 1.0482,
      "step": 5511
    },
    {
      "epoch": 51.51871500147362,
      "grad_norm": 0.4995206892490387,
      "learning_rate": 9.998486562943639e-06,
      "loss": 1.0764,
      "step": 5512
    },
    {
      "epoch": 51.5281461833186,
      "grad_norm": 0.5423513650894165,
      "learning_rate": 9.995459688969576e-06,
      "loss": 1.0543,
      "step": 5513
    },
    {
      "epoch": 51.53757736516357,
      "grad_norm": 0.5092490911483765,
      "learning_rate": 9.992432815411495e-06,
      "loss": 1.055,
      "step": 5514
    },
    {
      "epoch": 51.547008547008545,
      "grad_norm": 0.5613613724708557,
      "learning_rate": 9.989405942546717e-06,
      "loss": 1.043,
      "step": 5515
    },
    {
      "epoch": 51.55643972885352,
      "grad_norm": 0.5147056579589844,
      "learning_rate": 9.986379070652557e-06,
      "loss": 1.0673,
      "step": 5516
    },
    {
      "epoch": 51.5658709106985,
      "grad_norm": 0.5297311544418335,
      "learning_rate": 9.983352200006348e-06,
      "loss": 1.0832,
      "step": 5517
    },
    {
      "epoch": 51.575302092543474,
      "grad_norm": 0.5060404539108276,
      "learning_rate": 9.980325330885405e-06,
      "loss": 1.0753,
      "step": 5518
    },
    {
      "epoch": 51.58473327438845,
      "grad_norm": 0.5014229416847229,
      "learning_rate": 9.977298463567047e-06,
      "loss": 1.0939,
      "step": 5519
    },
    {
      "epoch": 51.59416445623342,
      "grad_norm": 0.5419350266456604,
      "learning_rate": 9.974271598328598e-06,
      "loss": 1.0866,
      "step": 5520
    },
    {
      "epoch": 51.603595638078396,
      "grad_norm": 0.49826568365097046,
      "learning_rate": 9.971244735447377e-06,
      "loss": 1.0561,
      "step": 5521
    },
    {
      "epoch": 51.61302681992337,
      "grad_norm": 0.5182000994682312,
      "learning_rate": 9.968217875200701e-06,
      "loss": 1.0326,
      "step": 5522
    },
    {
      "epoch": 51.622458001768344,
      "grad_norm": 0.5368418097496033,
      "learning_rate": 9.965191017865891e-06,
      "loss": 1.0903,
      "step": 5523
    },
    {
      "epoch": 51.631889183613325,
      "grad_norm": 0.5541606545448303,
      "learning_rate": 9.962164163720272e-06,
      "loss": 1.0226,
      "step": 5524
    },
    {
      "epoch": 51.6413203654583,
      "grad_norm": 0.5526098608970642,
      "learning_rate": 9.95913731304116e-06,
      "loss": 1.0837,
      "step": 5525
    },
    {
      "epoch": 51.65075154730327,
      "grad_norm": 0.5121545791625977,
      "learning_rate": 9.956110466105873e-06,
      "loss": 1.0609,
      "step": 5526
    },
    {
      "epoch": 51.66018272914825,
      "grad_norm": 0.5319232940673828,
      "learning_rate": 9.95308362319173e-06,
      "loss": 1.0403,
      "step": 5527
    },
    {
      "epoch": 51.66961391099322,
      "grad_norm": 0.5332357883453369,
      "learning_rate": 9.950056784576049e-06,
      "loss": 1.0739,
      "step": 5528
    },
    {
      "epoch": 51.679045092838194,
      "grad_norm": 0.5424566268920898,
      "learning_rate": 9.947029950536153e-06,
      "loss": 1.0615,
      "step": 5529
    },
    {
      "epoch": 51.68847627468317,
      "grad_norm": 0.5344222784042358,
      "learning_rate": 9.944003121349353e-06,
      "loss": 1.0448,
      "step": 5530
    },
    {
      "epoch": 51.69790745652815,
      "grad_norm": 0.5485187768936157,
      "learning_rate": 9.940976297292967e-06,
      "loss": 1.06,
      "step": 5531
    },
    {
      "epoch": 51.70733863837312,
      "grad_norm": 0.5055370926856995,
      "learning_rate": 9.937949478644313e-06,
      "loss": 1.0178,
      "step": 5532
    },
    {
      "epoch": 51.7167698202181,
      "grad_norm": 0.5324234366416931,
      "learning_rate": 9.934922665680709e-06,
      "loss": 1.0353,
      "step": 5533
    },
    {
      "epoch": 51.72620100206307,
      "grad_norm": 0.534980058670044,
      "learning_rate": 9.93189585867947e-06,
      "loss": 1.0979,
      "step": 5534
    },
    {
      "epoch": 51.735632183908045,
      "grad_norm": 0.5041662454605103,
      "learning_rate": 9.928869057917907e-06,
      "loss": 1.067,
      "step": 5535
    },
    {
      "epoch": 51.74506336575302,
      "grad_norm": 0.4882258176803589,
      "learning_rate": 9.925842263673339e-06,
      "loss": 1.0527,
      "step": 5536
    },
    {
      "epoch": 51.75449454759799,
      "grad_norm": 0.5170309543609619,
      "learning_rate": 9.922815476223079e-06,
      "loss": 1.0369,
      "step": 5537
    },
    {
      "epoch": 51.763925729442974,
      "grad_norm": 0.5164111852645874,
      "learning_rate": 9.919788695844434e-06,
      "loss": 1.1042,
      "step": 5538
    },
    {
      "epoch": 51.77335691128795,
      "grad_norm": 0.5327208638191223,
      "learning_rate": 9.916761922814728e-06,
      "loss": 1.0499,
      "step": 5539
    },
    {
      "epoch": 51.78278809313292,
      "grad_norm": 0.524311363697052,
      "learning_rate": 9.913735157411267e-06,
      "loss": 1.0442,
      "step": 5540
    },
    {
      "epoch": 51.792219274977896,
      "grad_norm": 0.5413409471511841,
      "learning_rate": 9.910708399911361e-06,
      "loss": 1.0675,
      "step": 5541
    },
    {
      "epoch": 51.80165045682287,
      "grad_norm": 0.5779257416725159,
      "learning_rate": 9.907681650592322e-06,
      "loss": 1.0844,
      "step": 5542
    },
    {
      "epoch": 51.811081638667844,
      "grad_norm": 0.49447116255760193,
      "learning_rate": 9.90465490973146e-06,
      "loss": 1.0659,
      "step": 5543
    },
    {
      "epoch": 51.82051282051282,
      "grad_norm": 0.5272562503814697,
      "learning_rate": 9.901628177606084e-06,
      "loss": 1.0645,
      "step": 5544
    },
    {
      "epoch": 51.8299440023578,
      "grad_norm": 0.5435131192207336,
      "learning_rate": 9.898601454493502e-06,
      "loss": 1.0989,
      "step": 5545
    },
    {
      "epoch": 51.83937518420277,
      "grad_norm": 0.48605504631996155,
      "learning_rate": 9.895574740671016e-06,
      "loss": 1.0674,
      "step": 5546
    },
    {
      "epoch": 51.84880636604775,
      "grad_norm": 0.5382292866706848,
      "learning_rate": 9.892548036415942e-06,
      "loss": 1.0674,
      "step": 5547
    },
    {
      "epoch": 51.85823754789272,
      "grad_norm": 0.5117332935333252,
      "learning_rate": 9.889521342005582e-06,
      "loss": 1.0222,
      "step": 5548
    },
    {
      "epoch": 51.867668729737694,
      "grad_norm": 0.5343919992446899,
      "learning_rate": 9.88649465771724e-06,
      "loss": 1.0556,
      "step": 5549
    },
    {
      "epoch": 51.87709991158267,
      "grad_norm": 0.5485385060310364,
      "learning_rate": 9.883467983828219e-06,
      "loss": 1.0824,
      "step": 5550
    },
    {
      "epoch": 51.88653109342764,
      "grad_norm": 0.5021466016769409,
      "learning_rate": 9.880441320615823e-06,
      "loss": 1.0757,
      "step": 5551
    },
    {
      "epoch": 51.89596227527262,
      "grad_norm": 0.5307148694992065,
      "learning_rate": 9.877414668357354e-06,
      "loss": 1.0531,
      "step": 5552
    },
    {
      "epoch": 51.9053934571176,
      "grad_norm": 0.5574308037757874,
      "learning_rate": 9.874388027330108e-06,
      "loss": 1.0569,
      "step": 5553
    },
    {
      "epoch": 51.91482463896257,
      "grad_norm": 0.4737466275691986,
      "learning_rate": 9.871361397811394e-06,
      "loss": 1.0482,
      "step": 5554
    },
    {
      "epoch": 51.924255820807545,
      "grad_norm": 0.5246309041976929,
      "learning_rate": 9.868334780078506e-06,
      "loss": 1.0542,
      "step": 5555
    },
    {
      "epoch": 51.93368700265252,
      "grad_norm": 0.544723391532898,
      "learning_rate": 9.865308174408741e-06,
      "loss": 1.0514,
      "step": 5556
    },
    {
      "epoch": 51.94311818449749,
      "grad_norm": 0.5261297225952148,
      "learning_rate": 9.862281581079398e-06,
      "loss": 1.0682,
      "step": 5557
    },
    {
      "epoch": 51.95254936634247,
      "grad_norm": 0.48740050196647644,
      "learning_rate": 9.859255000367767e-06,
      "loss": 1.0548,
      "step": 5558
    },
    {
      "epoch": 51.96198054818745,
      "grad_norm": 0.5706350803375244,
      "learning_rate": 9.856228432551151e-06,
      "loss": 1.0919,
      "step": 5559
    },
    {
      "epoch": 51.97141173003242,
      "grad_norm": 0.5354637503623962,
      "learning_rate": 9.853201877906836e-06,
      "loss": 1.0412,
      "step": 5560
    },
    {
      "epoch": 51.980842911877396,
      "grad_norm": 0.5122441053390503,
      "learning_rate": 9.850175336712116e-06,
      "loss": 1.0319,
      "step": 5561
    },
    {
      "epoch": 51.99027409372237,
      "grad_norm": 0.4918249845504761,
      "learning_rate": 9.847148809244282e-06,
      "loss": 1.1028,
      "step": 5562
    },
    {
      "epoch": 51.999705275567344,
      "grad_norm": 0.496656209230423,
      "learning_rate": 9.844122295780623e-06,
      "loss": 1.051,
      "step": 5563
    },
    {
      "epoch": 52.0,
      "grad_norm": 2.731598377227783,
      "learning_rate": 9.84109579659843e-06,
      "loss": 0.6317,
      "step": 5564
    },
    {
      "epoch": 52.009431181844974,
      "grad_norm": 0.5449686646461487,
      "learning_rate": 9.838069311974986e-06,
      "loss": 1.0083,
      "step": 5565
    },
    {
      "epoch": 52.01886236368995,
      "grad_norm": 0.48736754059791565,
      "learning_rate": 9.83504284218758e-06,
      "loss": 1.1052,
      "step": 5566
    },
    {
      "epoch": 52.02829354553492,
      "grad_norm": 0.49074459075927734,
      "learning_rate": 9.832016387513493e-06,
      "loss": 1.0684,
      "step": 5567
    },
    {
      "epoch": 52.0377247273799,
      "grad_norm": 0.5103950500488281,
      "learning_rate": 9.828989948230005e-06,
      "loss": 1.0872,
      "step": 5568
    },
    {
      "epoch": 52.04715590922488,
      "grad_norm": 0.5034614205360413,
      "learning_rate": 9.825963524614405e-06,
      "loss": 1.0565,
      "step": 5569
    },
    {
      "epoch": 52.05658709106985,
      "grad_norm": 0.49342742562294006,
      "learning_rate": 9.822937116943969e-06,
      "loss": 1.0518,
      "step": 5570
    },
    {
      "epoch": 52.066018272914825,
      "grad_norm": 0.5025242567062378,
      "learning_rate": 9.819910725495977e-06,
      "loss": 1.0437,
      "step": 5571
    },
    {
      "epoch": 52.0754494547598,
      "grad_norm": 0.4991946220397949,
      "learning_rate": 9.816884350547704e-06,
      "loss": 1.0477,
      "step": 5572
    },
    {
      "epoch": 52.08488063660477,
      "grad_norm": 0.5190257430076599,
      "learning_rate": 9.813857992376427e-06,
      "loss": 1.0527,
      "step": 5573
    },
    {
      "epoch": 52.09431181844975,
      "grad_norm": 0.5510295629501343,
      "learning_rate": 9.810831651259419e-06,
      "loss": 1.0263,
      "step": 5574
    },
    {
      "epoch": 52.10374300029473,
      "grad_norm": 0.5330488085746765,
      "learning_rate": 9.807805327473945e-06,
      "loss": 1.1069,
      "step": 5575
    },
    {
      "epoch": 52.1131741821397,
      "grad_norm": 0.5255746841430664,
      "learning_rate": 9.804779021297292e-06,
      "loss": 1.0755,
      "step": 5576
    },
    {
      "epoch": 52.122605363984675,
      "grad_norm": 0.5921114087104797,
      "learning_rate": 9.801752733006719e-06,
      "loss": 1.0415,
      "step": 5577
    },
    {
      "epoch": 52.13203654582965,
      "grad_norm": 0.47013047337532043,
      "learning_rate": 9.798726462879495e-06,
      "loss": 1.055,
      "step": 5578
    },
    {
      "epoch": 52.14146772767462,
      "grad_norm": 0.5479335784912109,
      "learning_rate": 9.795700211192885e-06,
      "loss": 1.0428,
      "step": 5579
    },
    {
      "epoch": 52.1508989095196,
      "grad_norm": 0.5289661288261414,
      "learning_rate": 9.792673978224154e-06,
      "loss": 1.0442,
      "step": 5580
    },
    {
      "epoch": 52.16033009136457,
      "grad_norm": 0.5091844201087952,
      "learning_rate": 9.789647764250565e-06,
      "loss": 1.0225,
      "step": 5581
    },
    {
      "epoch": 52.16976127320955,
      "grad_norm": 0.5160965323448181,
      "learning_rate": 9.786621569549378e-06,
      "loss": 1.0713,
      "step": 5582
    },
    {
      "epoch": 52.179192455054526,
      "grad_norm": 0.5308802127838135,
      "learning_rate": 9.783595394397848e-06,
      "loss": 1.0666,
      "step": 5583
    },
    {
      "epoch": 52.1886236368995,
      "grad_norm": 0.5425565838813782,
      "learning_rate": 9.780569239073241e-06,
      "loss": 1.0945,
      "step": 5584
    },
    {
      "epoch": 52.198054818744474,
      "grad_norm": 0.5112029314041138,
      "learning_rate": 9.777543103852807e-06,
      "loss": 1.0489,
      "step": 5585
    },
    {
      "epoch": 52.20748600058945,
      "grad_norm": 0.5048484206199646,
      "learning_rate": 9.7745169890138e-06,
      "loss": 1.0445,
      "step": 5586
    },
    {
      "epoch": 52.21691718243442,
      "grad_norm": 0.5253485441207886,
      "learning_rate": 9.771490894833473e-06,
      "loss": 1.0199,
      "step": 5587
    },
    {
      "epoch": 52.226348364279396,
      "grad_norm": 0.5234382152557373,
      "learning_rate": 9.768464821589073e-06,
      "loss": 1.0593,
      "step": 5588
    },
    {
      "epoch": 52.23577954612438,
      "grad_norm": 0.5316420197486877,
      "learning_rate": 9.76543876955785e-06,
      "loss": 1.0789,
      "step": 5589
    },
    {
      "epoch": 52.24521072796935,
      "grad_norm": 0.551650881767273,
      "learning_rate": 9.762412739017045e-06,
      "loss": 1.0841,
      "step": 5590
    },
    {
      "epoch": 52.254641909814325,
      "grad_norm": 0.49283403158187866,
      "learning_rate": 9.75938673024391e-06,
      "loss": 1.0452,
      "step": 5591
    },
    {
      "epoch": 52.2640730916593,
      "grad_norm": 0.5402761697769165,
      "learning_rate": 9.756360743515686e-06,
      "loss": 1.0764,
      "step": 5592
    },
    {
      "epoch": 52.27350427350427,
      "grad_norm": 0.5385963916778564,
      "learning_rate": 9.753334779109607e-06,
      "loss": 1.0496,
      "step": 5593
    },
    {
      "epoch": 52.28293545534925,
      "grad_norm": 0.492952823638916,
      "learning_rate": 9.750308837302913e-06,
      "loss": 1.0624,
      "step": 5594
    },
    {
      "epoch": 52.29236663719422,
      "grad_norm": 0.5110093355178833,
      "learning_rate": 9.747282918372842e-06,
      "loss": 1.0643,
      "step": 5595
    },
    {
      "epoch": 52.3017978190392,
      "grad_norm": 0.49987274408340454,
      "learning_rate": 9.744257022596626e-06,
      "loss": 1.068,
      "step": 5596
    },
    {
      "epoch": 52.311229000884175,
      "grad_norm": 0.5337361693382263,
      "learning_rate": 9.741231150251499e-06,
      "loss": 1.0555,
      "step": 5597
    },
    {
      "epoch": 52.32066018272915,
      "grad_norm": 0.5286296010017395,
      "learning_rate": 9.738205301614685e-06,
      "loss": 1.0163,
      "step": 5598
    },
    {
      "epoch": 52.33009136457412,
      "grad_norm": 0.5104942321777344,
      "learning_rate": 9.735179476963414e-06,
      "loss": 1.0449,
      "step": 5599
    },
    {
      "epoch": 52.3395225464191,
      "grad_norm": 0.5342395901679993,
      "learning_rate": 9.732153676574915e-06,
      "loss": 1.0665,
      "step": 5600
    },
    {
      "epoch": 52.34895372826407,
      "grad_norm": 0.5671058893203735,
      "learning_rate": 9.729127900726407e-06,
      "loss": 1.1083,
      "step": 5601
    },
    {
      "epoch": 52.358384910109045,
      "grad_norm": 0.5118476748466492,
      "learning_rate": 9.72610214969511e-06,
      "loss": 1.0444,
      "step": 5602
    },
    {
      "epoch": 52.367816091954026,
      "grad_norm": 0.5201494693756104,
      "learning_rate": 9.723076423758243e-06,
      "loss": 1.068,
      "step": 5603
    },
    {
      "epoch": 52.377247273799,
      "grad_norm": 0.5581445693969727,
      "learning_rate": 9.720050723193026e-06,
      "loss": 1.0454,
      "step": 5604
    },
    {
      "epoch": 52.386678455643974,
      "grad_norm": 0.5421144366264343,
      "learning_rate": 9.717025048276662e-06,
      "loss": 1.0681,
      "step": 5605
    },
    {
      "epoch": 52.39610963748895,
      "grad_norm": 0.5369462370872498,
      "learning_rate": 9.713999399286374e-06,
      "loss": 1.0814,
      "step": 5606
    },
    {
      "epoch": 52.40554081933392,
      "grad_norm": 0.5116209387779236,
      "learning_rate": 9.710973776499365e-06,
      "loss": 1.0803,
      "step": 5607
    },
    {
      "epoch": 52.414972001178896,
      "grad_norm": 0.5011294484138489,
      "learning_rate": 9.707948180192845e-06,
      "loss": 1.0798,
      "step": 5608
    },
    {
      "epoch": 52.42440318302387,
      "grad_norm": 0.536965012550354,
      "learning_rate": 9.704922610644015e-06,
      "loss": 1.0224,
      "step": 5609
    },
    {
      "epoch": 52.43383436486885,
      "grad_norm": 0.5142120718955994,
      "learning_rate": 9.701897068130078e-06,
      "loss": 1.0813,
      "step": 5610
    },
    {
      "epoch": 52.443265546713825,
      "grad_norm": 0.5284756422042847,
      "learning_rate": 9.698871552928233e-06,
      "loss": 1.0531,
      "step": 5611
    },
    {
      "epoch": 52.4526967285588,
      "grad_norm": 0.4914690852165222,
      "learning_rate": 9.695846065315677e-06,
      "loss": 1.062,
      "step": 5612
    },
    {
      "epoch": 52.46212791040377,
      "grad_norm": 0.490444153547287,
      "learning_rate": 9.6928206055696e-06,
      "loss": 1.1224,
      "step": 5613
    },
    {
      "epoch": 52.47155909224875,
      "grad_norm": 0.5209439396858215,
      "learning_rate": 9.6897951739672e-06,
      "loss": 1.0199,
      "step": 5614
    },
    {
      "epoch": 52.48099027409372,
      "grad_norm": 0.5006964802742004,
      "learning_rate": 9.686769770785664e-06,
      "loss": 1.0594,
      "step": 5615
    },
    {
      "epoch": 52.490421455938694,
      "grad_norm": 0.4994186758995056,
      "learning_rate": 9.683744396302178e-06,
      "loss": 1.0369,
      "step": 5616
    },
    {
      "epoch": 52.499852637783675,
      "grad_norm": 0.5439783334732056,
      "learning_rate": 9.680719050793927e-06,
      "loss": 1.0763,
      "step": 5617
    },
    {
      "epoch": 52.50928381962865,
      "grad_norm": 0.513504683971405,
      "learning_rate": 9.67769373453809e-06,
      "loss": 1.0687,
      "step": 5618
    },
    {
      "epoch": 52.51871500147362,
      "grad_norm": 0.4945533573627472,
      "learning_rate": 9.674668447811845e-06,
      "loss": 1.0597,
      "step": 5619
    },
    {
      "epoch": 52.5281461833186,
      "grad_norm": 0.5150675773620605,
      "learning_rate": 9.671643190892366e-06,
      "loss": 1.0767,
      "step": 5620
    },
    {
      "epoch": 52.53757736516357,
      "grad_norm": 0.5339319109916687,
      "learning_rate": 9.668617964056833e-06,
      "loss": 1.0483,
      "step": 5621
    },
    {
      "epoch": 52.547008547008545,
      "grad_norm": 0.5330395102500916,
      "learning_rate": 9.665592767582413e-06,
      "loss": 1.0383,
      "step": 5622
    },
    {
      "epoch": 52.55643972885352,
      "grad_norm": 0.5455740690231323,
      "learning_rate": 9.662567601746273e-06,
      "loss": 1.0782,
      "step": 5623
    },
    {
      "epoch": 52.5658709106985,
      "grad_norm": 0.5218794941902161,
      "learning_rate": 9.659542466825578e-06,
      "loss": 1.0357,
      "step": 5624
    },
    {
      "epoch": 52.575302092543474,
      "grad_norm": 0.5085427165031433,
      "learning_rate": 9.656517363097486e-06,
      "loss": 1.0621,
      "step": 5625
    },
    {
      "epoch": 52.58473327438845,
      "grad_norm": 0.5141758322715759,
      "learning_rate": 9.653492290839162e-06,
      "loss": 1.0891,
      "step": 5626
    },
    {
      "epoch": 52.59416445623342,
      "grad_norm": 0.5497328042984009,
      "learning_rate": 9.65046725032776e-06,
      "loss": 1.0424,
      "step": 5627
    },
    {
      "epoch": 52.603595638078396,
      "grad_norm": 0.5513441562652588,
      "learning_rate": 9.64744224184043e-06,
      "loss": 1.0605,
      "step": 5628
    },
    {
      "epoch": 52.61302681992337,
      "grad_norm": 0.534328043460846,
      "learning_rate": 9.644417265654327e-06,
      "loss": 1.0483,
      "step": 5629
    },
    {
      "epoch": 52.622458001768344,
      "grad_norm": 0.5505013465881348,
      "learning_rate": 9.641392322046597e-06,
      "loss": 1.0632,
      "step": 5630
    },
    {
      "epoch": 52.631889183613325,
      "grad_norm": 0.5407119393348694,
      "learning_rate": 9.638367411294383e-06,
      "loss": 1.0388,
      "step": 5631
    },
    {
      "epoch": 52.6413203654583,
      "grad_norm": 0.5263705849647522,
      "learning_rate": 9.635342533674829e-06,
      "loss": 1.0773,
      "step": 5632
    },
    {
      "epoch": 52.65075154730327,
      "grad_norm": 0.5245714783668518,
      "learning_rate": 9.63231768946507e-06,
      "loss": 1.0592,
      "step": 5633
    },
    {
      "epoch": 52.66018272914825,
      "grad_norm": 0.5345708727836609,
      "learning_rate": 9.629292878942243e-06,
      "loss": 1.0626,
      "step": 5634
    },
    {
      "epoch": 52.66961391099322,
      "grad_norm": 0.537239670753479,
      "learning_rate": 9.626268102383477e-06,
      "loss": 1.03,
      "step": 5635
    },
    {
      "epoch": 52.679045092838194,
      "grad_norm": 0.5316862463951111,
      "learning_rate": 9.623243360065905e-06,
      "loss": 1.0592,
      "step": 5636
    },
    {
      "epoch": 52.68847627468317,
      "grad_norm": 0.5333071947097778,
      "learning_rate": 9.620218652266653e-06,
      "loss": 1.0512,
      "step": 5637
    },
    {
      "epoch": 52.69790745652815,
      "grad_norm": 0.5167201161384583,
      "learning_rate": 9.617193979262841e-06,
      "loss": 1.0537,
      "step": 5638
    },
    {
      "epoch": 52.70733863837312,
      "grad_norm": 0.49109023809432983,
      "learning_rate": 9.614169341331593e-06,
      "loss": 1.0616,
      "step": 5639
    },
    {
      "epoch": 52.7167698202181,
      "grad_norm": 0.5225271582603455,
      "learning_rate": 9.61114473875002e-06,
      "loss": 1.0733,
      "step": 5640
    },
    {
      "epoch": 52.72620100206307,
      "grad_norm": 0.5432351231575012,
      "learning_rate": 9.608120171795238e-06,
      "loss": 1.0071,
      "step": 5641
    },
    {
      "epoch": 52.735632183908045,
      "grad_norm": 0.535321831703186,
      "learning_rate": 9.605095640744355e-06,
      "loss": 1.0296,
      "step": 5642
    },
    {
      "epoch": 52.74506336575302,
      "grad_norm": 0.5909218192100525,
      "learning_rate": 9.602071145874476e-06,
      "loss": 1.0749,
      "step": 5643
    },
    {
      "epoch": 52.75449454759799,
      "grad_norm": 0.5403082966804504,
      "learning_rate": 9.599046687462709e-06,
      "loss": 1.1057,
      "step": 5644
    },
    {
      "epoch": 52.763925729442974,
      "grad_norm": 0.516707718372345,
      "learning_rate": 9.596022265786155e-06,
      "loss": 1.0636,
      "step": 5645
    },
    {
      "epoch": 52.77335691128795,
      "grad_norm": 0.4938562214374542,
      "learning_rate": 9.592997881121905e-06,
      "loss": 1.0474,
      "step": 5646
    },
    {
      "epoch": 52.78278809313292,
      "grad_norm": 0.5041198134422302,
      "learning_rate": 9.589973533747055e-06,
      "loss": 1.0599,
      "step": 5647
    },
    {
      "epoch": 52.792219274977896,
      "grad_norm": 0.5497931838035583,
      "learning_rate": 9.586949223938693e-06,
      "loss": 1.0885,
      "step": 5648
    },
    {
      "epoch": 52.80165045682287,
      "grad_norm": 0.5148288011550903,
      "learning_rate": 9.583924951973908e-06,
      "loss": 1.0769,
      "step": 5649
    },
    {
      "epoch": 52.811081638667844,
      "grad_norm": 0.47550028562545776,
      "learning_rate": 9.580900718129776e-06,
      "loss": 1.027,
      "step": 5650
    },
    {
      "epoch": 52.82051282051282,
      "grad_norm": 0.5552061200141907,
      "learning_rate": 9.577876522683386e-06,
      "loss": 1.083,
      "step": 5651
    },
    {
      "epoch": 52.8299440023578,
      "grad_norm": 0.5193747878074646,
      "learning_rate": 9.574852365911808e-06,
      "loss": 1.0882,
      "step": 5652
    },
    {
      "epoch": 52.83937518420277,
      "grad_norm": 0.5257030129432678,
      "learning_rate": 9.571828248092117e-06,
      "loss": 1.0538,
      "step": 5653
    },
    {
      "epoch": 52.84880636604775,
      "grad_norm": 0.5130683183670044,
      "learning_rate": 9.56880416950138e-06,
      "loss": 1.051,
      "step": 5654
    },
    {
      "epoch": 52.85823754789272,
      "grad_norm": 0.5669764876365662,
      "learning_rate": 9.56578013041666e-06,
      "loss": 1.038,
      "step": 5655
    },
    {
      "epoch": 52.867668729737694,
      "grad_norm": 0.5162306427955627,
      "learning_rate": 9.562756131115022e-06,
      "loss": 1.0626,
      "step": 5656
    },
    {
      "epoch": 52.87709991158267,
      "grad_norm": 0.5683175325393677,
      "learning_rate": 9.559732171873524e-06,
      "loss": 1.0778,
      "step": 5657
    },
    {
      "epoch": 52.88653109342764,
      "grad_norm": 0.5549982190132141,
      "learning_rate": 9.556708252969215e-06,
      "loss": 1.0745,
      "step": 5658
    },
    {
      "epoch": 52.89596227527262,
      "grad_norm": 0.5457572340965271,
      "learning_rate": 9.553684374679154e-06,
      "loss": 1.0601,
      "step": 5659
    },
    {
      "epoch": 52.9053934571176,
      "grad_norm": 0.5259625315666199,
      "learning_rate": 9.550660537280377e-06,
      "loss": 1.0686,
      "step": 5660
    },
    {
      "epoch": 52.91482463896257,
      "grad_norm": 0.5101200938224792,
      "learning_rate": 9.547636741049937e-06,
      "loss": 1.1139,
      "step": 5661
    },
    {
      "epoch": 52.924255820807545,
      "grad_norm": 0.49903494119644165,
      "learning_rate": 9.544612986264868e-06,
      "loss": 1.0469,
      "step": 5662
    },
    {
      "epoch": 52.93368700265252,
      "grad_norm": 0.4974430501461029,
      "learning_rate": 9.541589273202207e-06,
      "loss": 1.0692,
      "step": 5663
    },
    {
      "epoch": 52.94311818449749,
      "grad_norm": 0.5225063562393188,
      "learning_rate": 9.538565602138986e-06,
      "loss": 1.061,
      "step": 5664
    },
    {
      "epoch": 52.95254936634247,
      "grad_norm": 0.5351470112800598,
      "learning_rate": 9.535541973352227e-06,
      "loss": 1.0144,
      "step": 5665
    },
    {
      "epoch": 52.96198054818745,
      "grad_norm": 0.5385006666183472,
      "learning_rate": 9.532518387118961e-06,
      "loss": 1.0469,
      "step": 5666
    },
    {
      "epoch": 52.97141173003242,
      "grad_norm": 0.5058990120887756,
      "learning_rate": 9.529494843716208e-06,
      "loss": 1.0524,
      "step": 5667
    },
    {
      "epoch": 52.980842911877396,
      "grad_norm": 0.5162661075592041,
      "learning_rate": 9.52647134342098e-06,
      "loss": 1.0685,
      "step": 5668
    },
    {
      "epoch": 52.99027409372237,
      "grad_norm": 0.5203683376312256,
      "learning_rate": 9.523447886510293e-06,
      "loss": 1.0626,
      "step": 5669
    },
    {
      "epoch": 52.999705275567344,
      "grad_norm": 0.5423789620399475,
      "learning_rate": 9.520424473261152e-06,
      "loss": 1.0401,
      "step": 5670
    },
    {
      "epoch": 53.0,
      "grad_norm": 3.169309377670288,
      "learning_rate": 9.51740110395056e-06,
      "loss": 0.5191,
      "step": 5671
    },
    {
      "epoch": 53.009431181844974,
      "grad_norm": 0.49669450521469116,
      "learning_rate": 9.514377778855521e-06,
      "loss": 1.0405,
      "step": 5672
    },
    {
      "epoch": 53.01886236368995,
      "grad_norm": 0.5375789999961853,
      "learning_rate": 9.511354498253024e-06,
      "loss": 1.0646,
      "step": 5673
    },
    {
      "epoch": 53.02829354553492,
      "grad_norm": 0.5511997938156128,
      "learning_rate": 9.50833126242007e-06,
      "loss": 1.0268,
      "step": 5674
    },
    {
      "epoch": 53.0377247273799,
      "grad_norm": 0.5298737287521362,
      "learning_rate": 9.505308071633645e-06,
      "loss": 1.0168,
      "step": 5675
    },
    {
      "epoch": 53.04715590922488,
      "grad_norm": 0.48024260997772217,
      "learning_rate": 9.502284926170729e-06,
      "loss": 1.0518,
      "step": 5676
    },
    {
      "epoch": 53.05658709106985,
      "grad_norm": 0.5133756995201111,
      "learning_rate": 9.499261826308303e-06,
      "loss": 1.0418,
      "step": 5677
    },
    {
      "epoch": 53.066018272914825,
      "grad_norm": 0.5246779322624207,
      "learning_rate": 9.496238772323345e-06,
      "loss": 1.0504,
      "step": 5678
    },
    {
      "epoch": 53.0754494547598,
      "grad_norm": 0.5331218838691711,
      "learning_rate": 9.493215764492822e-06,
      "loss": 1.0863,
      "step": 5679
    },
    {
      "epoch": 53.08488063660477,
      "grad_norm": 0.5105348825454712,
      "learning_rate": 9.490192803093698e-06,
      "loss": 1.1202,
      "step": 5680
    },
    {
      "epoch": 53.09431181844975,
      "grad_norm": 0.486935019493103,
      "learning_rate": 9.487169888402946e-06,
      "loss": 1.0651,
      "step": 5681
    },
    {
      "epoch": 53.10374300029473,
      "grad_norm": 0.5620782375335693,
      "learning_rate": 9.48414702069752e-06,
      "loss": 1.0649,
      "step": 5682
    },
    {
      "epoch": 53.1131741821397,
      "grad_norm": 0.5009765028953552,
      "learning_rate": 9.481124200254371e-06,
      "loss": 1.0532,
      "step": 5683
    },
    {
      "epoch": 53.122605363984675,
      "grad_norm": 0.4879922568798065,
      "learning_rate": 9.478101427350452e-06,
      "loss": 1.0512,
      "step": 5684
    },
    {
      "epoch": 53.13203654582965,
      "grad_norm": 0.5290483236312866,
      "learning_rate": 9.475078702262707e-06,
      "loss": 1.0175,
      "step": 5685
    },
    {
      "epoch": 53.14146772767462,
      "grad_norm": 0.5283721685409546,
      "learning_rate": 9.472056025268077e-06,
      "loss": 1.0691,
      "step": 5686
    },
    {
      "epoch": 53.1508989095196,
      "grad_norm": 0.5047444701194763,
      "learning_rate": 9.469033396643498e-06,
      "loss": 1.0291,
      "step": 5687
    },
    {
      "epoch": 53.16033009136457,
      "grad_norm": 0.5468389987945557,
      "learning_rate": 9.466010816665906e-06,
      "loss": 1.0539,
      "step": 5688
    },
    {
      "epoch": 53.16976127320955,
      "grad_norm": 0.4929649233818054,
      "learning_rate": 9.462988285612227e-06,
      "loss": 1.072,
      "step": 5689
    },
    {
      "epoch": 53.179192455054526,
      "grad_norm": 0.5628200769424438,
      "learning_rate": 9.459965803759381e-06,
      "loss": 1.0426,
      "step": 5690
    },
    {
      "epoch": 53.1886236368995,
      "grad_norm": 0.514982283115387,
      "learning_rate": 9.456943371384288e-06,
      "loss": 1.0301,
      "step": 5691
    },
    {
      "epoch": 53.198054818744474,
      "grad_norm": 0.5989962816238403,
      "learning_rate": 9.453920988763866e-06,
      "loss": 1.0303,
      "step": 5692
    },
    {
      "epoch": 53.20748600058945,
      "grad_norm": 0.5558977127075195,
      "learning_rate": 9.450898656175022e-06,
      "loss": 1.0716,
      "step": 5693
    },
    {
      "epoch": 53.21691718243442,
      "grad_norm": 0.508676290512085,
      "learning_rate": 9.447876373894662e-06,
      "loss": 1.0557,
      "step": 5694
    },
    {
      "epoch": 53.226348364279396,
      "grad_norm": 0.5222520232200623,
      "learning_rate": 9.444854142199684e-06,
      "loss": 1.0246,
      "step": 5695
    },
    {
      "epoch": 53.23577954612438,
      "grad_norm": 0.5316934585571289,
      "learning_rate": 9.441831961366986e-06,
      "loss": 1.0494,
      "step": 5696
    },
    {
      "epoch": 53.24521072796935,
      "grad_norm": 0.5433635711669922,
      "learning_rate": 9.438809831673458e-06,
      "loss": 1.0415,
      "step": 5697
    },
    {
      "epoch": 53.254641909814325,
      "grad_norm": 0.5076116323471069,
      "learning_rate": 9.43578775339599e-06,
      "loss": 1.0477,
      "step": 5698
    },
    {
      "epoch": 53.2640730916593,
      "grad_norm": 0.5379910469055176,
      "learning_rate": 9.432765726811462e-06,
      "loss": 1.0519,
      "step": 5699
    },
    {
      "epoch": 53.27350427350427,
      "grad_norm": 0.5416173338890076,
      "learning_rate": 9.429743752196748e-06,
      "loss": 1.0694,
      "step": 5700
    },
    {
      "epoch": 53.28293545534925,
      "grad_norm": 0.5370873212814331,
      "learning_rate": 9.426721829828724e-06,
      "loss": 1.0905,
      "step": 5701
    },
    {
      "epoch": 53.29236663719422,
      "grad_norm": 0.5586090683937073,
      "learning_rate": 9.423699959984252e-06,
      "loss": 1.0439,
      "step": 5702
    },
    {
      "epoch": 53.3017978190392,
      "grad_norm": 0.5672237873077393,
      "learning_rate": 9.420678142940202e-06,
      "loss": 1.0626,
      "step": 5703
    },
    {
      "epoch": 53.311229000884175,
      "grad_norm": 0.5218707919120789,
      "learning_rate": 9.41765637897343e-06,
      "loss": 1.0503,
      "step": 5704
    },
    {
      "epoch": 53.32066018272915,
      "grad_norm": 0.5333723425865173,
      "learning_rate": 9.414634668360787e-06,
      "loss": 1.0584,
      "step": 5705
    },
    {
      "epoch": 53.33009136457412,
      "grad_norm": 0.5285005569458008,
      "learning_rate": 9.411613011379122e-06,
      "loss": 1.0335,
      "step": 5706
    },
    {
      "epoch": 53.3395225464191,
      "grad_norm": 0.5009337663650513,
      "learning_rate": 9.408591408305278e-06,
      "loss": 1.0472,
      "step": 5707
    },
    {
      "epoch": 53.34895372826407,
      "grad_norm": 0.5245755314826965,
      "learning_rate": 9.405569859416095e-06,
      "loss": 1.0846,
      "step": 5708
    },
    {
      "epoch": 53.358384910109045,
      "grad_norm": 0.5228574872016907,
      "learning_rate": 9.402548364988403e-06,
      "loss": 1.034,
      "step": 5709
    },
    {
      "epoch": 53.367816091954026,
      "grad_norm": 0.5251649618148804,
      "learning_rate": 9.399526925299029e-06,
      "loss": 1.0514,
      "step": 5710
    },
    {
      "epoch": 53.377247273799,
      "grad_norm": 0.5167180299758911,
      "learning_rate": 9.396505540624802e-06,
      "loss": 1.0371,
      "step": 5711
    },
    {
      "epoch": 53.386678455643974,
      "grad_norm": 0.5675376653671265,
      "learning_rate": 9.393484211242538e-06,
      "loss": 1.0464,
      "step": 5712
    },
    {
      "epoch": 53.39610963748895,
      "grad_norm": 0.5046192407608032,
      "learning_rate": 9.39046293742905e-06,
      "loss": 1.0796,
      "step": 5713
    },
    {
      "epoch": 53.40554081933392,
      "grad_norm": 0.506959080696106,
      "learning_rate": 9.387441719461147e-06,
      "loss": 1.0607,
      "step": 5714
    },
    {
      "epoch": 53.414972001178896,
      "grad_norm": 0.5269845128059387,
      "learning_rate": 9.38442055761563e-06,
      "loss": 1.0446,
      "step": 5715
    },
    {
      "epoch": 53.42440318302387,
      "grad_norm": 0.5327683091163635,
      "learning_rate": 9.381399452169298e-06,
      "loss": 1.0936,
      "step": 5716
    },
    {
      "epoch": 53.43383436486885,
      "grad_norm": 0.5809013247489929,
      "learning_rate": 9.378378403398938e-06,
      "loss": 1.1039,
      "step": 5717
    },
    {
      "epoch": 53.443265546713825,
      "grad_norm": 0.5263189673423767,
      "learning_rate": 9.375357411581346e-06,
      "loss": 1.0394,
      "step": 5718
    },
    {
      "epoch": 53.4526967285588,
      "grad_norm": 0.5204119682312012,
      "learning_rate": 9.372336476993304e-06,
      "loss": 1.0549,
      "step": 5719
    },
    {
      "epoch": 53.46212791040377,
      "grad_norm": 0.48880255222320557,
      "learning_rate": 9.369315599911586e-06,
      "loss": 1.0965,
      "step": 5720
    },
    {
      "epoch": 53.47155909224875,
      "grad_norm": 0.5253767371177673,
      "learning_rate": 9.366294780612963e-06,
      "loss": 1.0524,
      "step": 5721
    },
    {
      "epoch": 53.48099027409372,
      "grad_norm": 0.5263446569442749,
      "learning_rate": 9.3632740193742e-06,
      "loss": 1.0493,
      "step": 5722
    },
    {
      "epoch": 53.490421455938694,
      "grad_norm": 0.5194963812828064,
      "learning_rate": 9.360253316472064e-06,
      "loss": 1.0467,
      "step": 5723
    },
    {
      "epoch": 53.499852637783675,
      "grad_norm": 0.48090723156929016,
      "learning_rate": 9.357232672183305e-06,
      "loss": 1.0971,
      "step": 5724
    },
    {
      "epoch": 53.50928381962865,
      "grad_norm": 0.4875473380088806,
      "learning_rate": 9.354212086784677e-06,
      "loss": 1.057,
      "step": 5725
    },
    {
      "epoch": 53.51871500147362,
      "grad_norm": 0.4876112639904022,
      "learning_rate": 9.351191560552923e-06,
      "loss": 1.0694,
      "step": 5726
    },
    {
      "epoch": 53.5281461833186,
      "grad_norm": 0.5345636606216431,
      "learning_rate": 9.348171093764782e-06,
      "loss": 1.0697,
      "step": 5727
    },
    {
      "epoch": 53.53757736516357,
      "grad_norm": 0.5384696125984192,
      "learning_rate": 9.345150686696993e-06,
      "loss": 1.0719,
      "step": 5728
    },
    {
      "epoch": 53.547008547008545,
      "grad_norm": 0.522564172744751,
      "learning_rate": 9.34213033962628e-06,
      "loss": 1.0602,
      "step": 5729
    },
    {
      "epoch": 53.55643972885352,
      "grad_norm": 0.5388616919517517,
      "learning_rate": 9.339110052829368e-06,
      "loss": 1.0512,
      "step": 5730
    },
    {
      "epoch": 53.5658709106985,
      "grad_norm": 0.5101093649864197,
      "learning_rate": 9.336089826582973e-06,
      "loss": 1.0321,
      "step": 5731
    },
    {
      "epoch": 53.575302092543474,
      "grad_norm": 0.5099874138832092,
      "learning_rate": 9.333069661163806e-06,
      "loss": 1.0653,
      "step": 5732
    },
    {
      "epoch": 53.58473327438845,
      "grad_norm": 0.5195062160491943,
      "learning_rate": 9.33004955684858e-06,
      "loss": 1.0811,
      "step": 5733
    },
    {
      "epoch": 53.59416445623342,
      "grad_norm": 0.494078665971756,
      "learning_rate": 9.32702951391399e-06,
      "loss": 1.079,
      "step": 5734
    },
    {
      "epoch": 53.603595638078396,
      "grad_norm": 0.5434499382972717,
      "learning_rate": 9.324009532636734e-06,
      "loss": 1.0189,
      "step": 5735
    },
    {
      "epoch": 53.61302681992337,
      "grad_norm": 0.5490091443061829,
      "learning_rate": 9.3209896132935e-06,
      "loss": 1.0758,
      "step": 5736
    },
    {
      "epoch": 53.622458001768344,
      "grad_norm": 0.5031123757362366,
      "learning_rate": 9.317969756160976e-06,
      "loss": 1.0585,
      "step": 5737
    },
    {
      "epoch": 53.631889183613325,
      "grad_norm": 0.517433762550354,
      "learning_rate": 9.314949961515834e-06,
      "loss": 1.0756,
      "step": 5738
    },
    {
      "epoch": 53.6413203654583,
      "grad_norm": 0.5076907277107239,
      "learning_rate": 9.31193022963475e-06,
      "loss": 1.086,
      "step": 5739
    },
    {
      "epoch": 53.65075154730327,
      "grad_norm": 0.544492244720459,
      "learning_rate": 9.308910560794387e-06,
      "loss": 1.0646,
      "step": 5740
    },
    {
      "epoch": 53.66018272914825,
      "grad_norm": 0.5231742262840271,
      "learning_rate": 9.305890955271414e-06,
      "loss": 1.071,
      "step": 5741
    },
    {
      "epoch": 53.66961391099322,
      "grad_norm": 0.4819384813308716,
      "learning_rate": 9.302871413342482e-06,
      "loss": 1.0451,
      "step": 5742
    },
    {
      "epoch": 53.679045092838194,
      "grad_norm": 0.5263423919677734,
      "learning_rate": 9.29985193528424e-06,
      "loss": 1.0768,
      "step": 5743
    },
    {
      "epoch": 53.68847627468317,
      "grad_norm": 0.5145605206489563,
      "learning_rate": 9.296832521373333e-06,
      "loss": 1.0648,
      "step": 5744
    },
    {
      "epoch": 53.69790745652815,
      "grad_norm": 0.5225722193717957,
      "learning_rate": 9.293813171886395e-06,
      "loss": 1.0598,
      "step": 5745
    },
    {
      "epoch": 53.70733863837312,
      "grad_norm": 0.5284746885299683,
      "learning_rate": 9.290793887100062e-06,
      "loss": 1.0633,
      "step": 5746
    },
    {
      "epoch": 53.7167698202181,
      "grad_norm": 0.531528651714325,
      "learning_rate": 9.287774667290955e-06,
      "loss": 1.0111,
      "step": 5747
    },
    {
      "epoch": 53.72620100206307,
      "grad_norm": 0.5443147420883179,
      "learning_rate": 9.2847555127357e-06,
      "loss": 1.0164,
      "step": 5748
    },
    {
      "epoch": 53.735632183908045,
      "grad_norm": 0.5598362684249878,
      "learning_rate": 9.281736423710908e-06,
      "loss": 1.0725,
      "step": 5749
    },
    {
      "epoch": 53.74506336575302,
      "grad_norm": 0.5385303497314453,
      "learning_rate": 9.278717400493188e-06,
      "loss": 1.0556,
      "step": 5750
    },
    {
      "epoch": 53.75449454759799,
      "grad_norm": 0.5030642747879028,
      "learning_rate": 9.275698443359141e-06,
      "loss": 1.0754,
      "step": 5751
    },
    {
      "epoch": 53.763925729442974,
      "grad_norm": 0.5363554954528809,
      "learning_rate": 9.272679552585361e-06,
      "loss": 1.0448,
      "step": 5752
    },
    {
      "epoch": 53.77335691128795,
      "grad_norm": 0.525601863861084,
      "learning_rate": 9.26966072844844e-06,
      "loss": 1.0781,
      "step": 5753
    },
    {
      "epoch": 53.78278809313292,
      "grad_norm": 0.5105060338973999,
      "learning_rate": 9.266641971224963e-06,
      "loss": 1.1096,
      "step": 5754
    },
    {
      "epoch": 53.792219274977896,
      "grad_norm": 0.5327514410018921,
      "learning_rate": 9.263623281191503e-06,
      "loss": 1.0734,
      "step": 5755
    },
    {
      "epoch": 53.80165045682287,
      "grad_norm": 0.4892890155315399,
      "learning_rate": 9.260604658624637e-06,
      "loss": 1.0551,
      "step": 5756
    },
    {
      "epoch": 53.811081638667844,
      "grad_norm": 0.5368897318840027,
      "learning_rate": 9.257586103800924e-06,
      "loss": 1.0676,
      "step": 5757
    },
    {
      "epoch": 53.82051282051282,
      "grad_norm": 0.5651063919067383,
      "learning_rate": 9.25456761699693e-06,
      "loss": 1.0522,
      "step": 5758
    },
    {
      "epoch": 53.8299440023578,
      "grad_norm": 0.49600720405578613,
      "learning_rate": 9.251549198489203e-06,
      "loss": 1.0621,
      "step": 5759
    },
    {
      "epoch": 53.83937518420277,
      "grad_norm": 0.5379823446273804,
      "learning_rate": 9.24853084855429e-06,
      "loss": 1.0793,
      "step": 5760
    },
    {
      "epoch": 53.84880636604775,
      "grad_norm": 0.48520758748054504,
      "learning_rate": 9.245512567468733e-06,
      "loss": 1.076,
      "step": 5761
    },
    {
      "epoch": 53.85823754789272,
      "grad_norm": 0.47866836190223694,
      "learning_rate": 9.242494355509061e-06,
      "loss": 1.0678,
      "step": 5762
    },
    {
      "epoch": 53.867668729737694,
      "grad_norm": 0.6002085208892822,
      "learning_rate": 9.239476212951809e-06,
      "loss": 1.1055,
      "step": 5763
    },
    {
      "epoch": 53.87709991158267,
      "grad_norm": 0.4915625751018524,
      "learning_rate": 9.236458140073495e-06,
      "loss": 1.0323,
      "step": 5764
    },
    {
      "epoch": 53.88653109342764,
      "grad_norm": 0.5174686312675476,
      "learning_rate": 9.233440137150633e-06,
      "loss": 1.0864,
      "step": 5765
    },
    {
      "epoch": 53.89596227527262,
      "grad_norm": 0.5183276534080505,
      "learning_rate": 9.23042220445973e-06,
      "loss": 1.037,
      "step": 5766
    },
    {
      "epoch": 53.9053934571176,
      "grad_norm": 0.5219012498855591,
      "learning_rate": 9.227404342277293e-06,
      "loss": 1.0481,
      "step": 5767
    },
    {
      "epoch": 53.91482463896257,
      "grad_norm": 0.530572772026062,
      "learning_rate": 9.224386550879813e-06,
      "loss": 1.0798,
      "step": 5768
    },
    {
      "epoch": 53.924255820807545,
      "grad_norm": 0.5191041231155396,
      "learning_rate": 9.221368830543781e-06,
      "loss": 1.0546,
      "step": 5769
    },
    {
      "epoch": 53.93368700265252,
      "grad_norm": 0.5456739068031311,
      "learning_rate": 9.218351181545673e-06,
      "loss": 1.0535,
      "step": 5770
    },
    {
      "epoch": 53.94311818449749,
      "grad_norm": 0.49429601430892944,
      "learning_rate": 9.215333604161977e-06,
      "loss": 1.0389,
      "step": 5771
    },
    {
      "epoch": 53.95254936634247,
      "grad_norm": 0.5612103343009949,
      "learning_rate": 9.212316098669156e-06,
      "loss": 1.084,
      "step": 5772
    },
    {
      "epoch": 53.96198054818745,
      "grad_norm": 0.5220736861228943,
      "learning_rate": 9.209298665343673e-06,
      "loss": 1.0725,
      "step": 5773
    },
    {
      "epoch": 53.97141173003242,
      "grad_norm": 0.553401529788971,
      "learning_rate": 9.206281304461985e-06,
      "loss": 1.0818,
      "step": 5774
    },
    {
      "epoch": 53.980842911877396,
      "grad_norm": 0.5583834648132324,
      "learning_rate": 9.203264016300541e-06,
      "loss": 1.0631,
      "step": 5775
    },
    {
      "epoch": 53.99027409372237,
      "grad_norm": 0.5231133103370667,
      "learning_rate": 9.200246801135784e-06,
      "loss": 1.0569,
      "step": 5776
    },
    {
      "epoch": 53.999705275567344,
      "grad_norm": 0.49654051661491394,
      "learning_rate": 9.197229659244145e-06,
      "loss": 1.052,
      "step": 5777
    },
    {
      "epoch": 54.0,
      "grad_norm": 2.560697317123413,
      "learning_rate": 9.194212590902065e-06,
      "loss": 0.4618,
      "step": 5778
    },
    {
      "epoch": 54.009431181844974,
      "grad_norm": 0.5146988034248352,
      "learning_rate": 9.19119559638596e-06,
      "loss": 1.054,
      "step": 5779
    },
    {
      "epoch": 54.01886236368995,
      "grad_norm": 0.5246128439903259,
      "learning_rate": 9.188178675972246e-06,
      "loss": 1.0478,
      "step": 5780
    },
    {
      "epoch": 54.02829354553492,
      "grad_norm": 0.4797438681125641,
      "learning_rate": 9.185161829937334e-06,
      "loss": 1.0675,
      "step": 5781
    },
    {
      "epoch": 54.0377247273799,
      "grad_norm": 0.5148853659629822,
      "learning_rate": 9.182145058557625e-06,
      "loss": 1.0888,
      "step": 5782
    },
    {
      "epoch": 54.04715590922488,
      "grad_norm": 0.5139723420143127,
      "learning_rate": 9.179128362109512e-06,
      "loss": 1.0408,
      "step": 5783
    },
    {
      "epoch": 54.05658709106985,
      "grad_norm": 0.5288711786270142,
      "learning_rate": 9.176111740869389e-06,
      "loss": 1.0609,
      "step": 5784
    },
    {
      "epoch": 54.066018272914825,
      "grad_norm": 0.5122700929641724,
      "learning_rate": 9.173095195113634e-06,
      "loss": 1.0002,
      "step": 5785
    },
    {
      "epoch": 54.0754494547598,
      "grad_norm": 0.5671810507774353,
      "learning_rate": 9.170078725118625e-06,
      "loss": 1.058,
      "step": 5786
    },
    {
      "epoch": 54.08488063660477,
      "grad_norm": 0.5573017001152039,
      "learning_rate": 9.16706233116073e-06,
      "loss": 1.071,
      "step": 5787
    },
    {
      "epoch": 54.09431181844975,
      "grad_norm": 0.5404378771781921,
      "learning_rate": 9.164046013516304e-06,
      "loss": 1.0369,
      "step": 5788
    },
    {
      "epoch": 54.10374300029473,
      "grad_norm": 0.5395581126213074,
      "learning_rate": 9.161029772461708e-06,
      "loss": 1.0542,
      "step": 5789
    },
    {
      "epoch": 54.1131741821397,
      "grad_norm": 0.5097588896751404,
      "learning_rate": 9.158013608273289e-06,
      "loss": 1.0546,
      "step": 5790
    },
    {
      "epoch": 54.122605363984675,
      "grad_norm": 0.5013967156410217,
      "learning_rate": 9.154997521227382e-06,
      "loss": 1.0562,
      "step": 5791
    },
    {
      "epoch": 54.13203654582965,
      "grad_norm": 0.5486441850662231,
      "learning_rate": 9.151981511600322e-06,
      "loss": 1.0734,
      "step": 5792
    },
    {
      "epoch": 54.14146772767462,
      "grad_norm": 0.526380717754364,
      "learning_rate": 9.148965579668433e-06,
      "loss": 1.0305,
      "step": 5793
    },
    {
      "epoch": 54.1508989095196,
      "grad_norm": 0.5774216055870056,
      "learning_rate": 9.14594972570804e-06,
      "loss": 1.0665,
      "step": 5794
    },
    {
      "epoch": 54.16033009136457,
      "grad_norm": 0.5163137316703796,
      "learning_rate": 9.142933949995449e-06,
      "loss": 1.0611,
      "step": 5795
    },
    {
      "epoch": 54.16976127320955,
      "grad_norm": 0.5099332928657532,
      "learning_rate": 9.139918252806968e-06,
      "loss": 1.0756,
      "step": 5796
    },
    {
      "epoch": 54.179192455054526,
      "grad_norm": 0.5811510682106018,
      "learning_rate": 9.13690263441889e-06,
      "loss": 1.0629,
      "step": 5797
    },
    {
      "epoch": 54.1886236368995,
      "grad_norm": 0.5039187669754028,
      "learning_rate": 9.133887095107507e-06,
      "loss": 1.035,
      "step": 5798
    },
    {
      "epoch": 54.198054818744474,
      "grad_norm": 0.5444594025611877,
      "learning_rate": 9.130871635149097e-06,
      "loss": 1.0608,
      "step": 5799
    },
    {
      "epoch": 54.20748600058945,
      "grad_norm": 0.5080097913742065,
      "learning_rate": 9.127856254819946e-06,
      "loss": 1.0584,
      "step": 5800
    },
    {
      "epoch": 54.21691718243442,
      "grad_norm": 0.5237571001052856,
      "learning_rate": 9.124840954396315e-06,
      "loss": 1.063,
      "step": 5801
    },
    {
      "epoch": 54.226348364279396,
      "grad_norm": 0.533413290977478,
      "learning_rate": 9.121825734154467e-06,
      "loss": 1.073,
      "step": 5802
    },
    {
      "epoch": 54.23577954612438,
      "grad_norm": 0.5907029509544373,
      "learning_rate": 9.118810594370651e-06,
      "loss": 1.017,
      "step": 5803
    },
    {
      "epoch": 54.24521072796935,
      "grad_norm": 0.5205554962158203,
      "learning_rate": 9.11579553532112e-06,
      "loss": 1.0639,
      "step": 5804
    },
    {
      "epoch": 54.254641909814325,
      "grad_norm": 0.5382220149040222,
      "learning_rate": 9.11278055728211e-06,
      "loss": 1.0565,
      "step": 5805
    },
    {
      "epoch": 54.2640730916593,
      "grad_norm": 0.5179026126861572,
      "learning_rate": 9.10976566052985e-06,
      "loss": 1.0497,
      "step": 5806
    },
    {
      "epoch": 54.27350427350427,
      "grad_norm": 0.5189934372901917,
      "learning_rate": 9.10675084534056e-06,
      "loss": 1.0581,
      "step": 5807
    },
    {
      "epoch": 54.28293545534925,
      "grad_norm": 0.5506795644760132,
      "learning_rate": 9.103736111990467e-06,
      "loss": 1.0102,
      "step": 5808
    },
    {
      "epoch": 54.29236663719422,
      "grad_norm": 0.5040019750595093,
      "learning_rate": 9.100721460755776e-06,
      "loss": 1.0535,
      "step": 5809
    },
    {
      "epoch": 54.3017978190392,
      "grad_norm": 0.5770531296730042,
      "learning_rate": 9.097706891912687e-06,
      "loss": 1.0326,
      "step": 5810
    },
    {
      "epoch": 54.311229000884175,
      "grad_norm": 0.5286648869514465,
      "learning_rate": 9.094692405737394e-06,
      "loss": 1.0596,
      "step": 5811
    },
    {
      "epoch": 54.32066018272915,
      "grad_norm": 0.5533154606819153,
      "learning_rate": 9.091678002506082e-06,
      "loss": 1.0922,
      "step": 5812
    },
    {
      "epoch": 54.33009136457412,
      "grad_norm": 0.507971465587616,
      "learning_rate": 9.088663682494929e-06,
      "loss": 1.0898,
      "step": 5813
    },
    {
      "epoch": 54.3395225464191,
      "grad_norm": 0.5620585680007935,
      "learning_rate": 9.085649445980106e-06,
      "loss": 1.0415,
      "step": 5814
    },
    {
      "epoch": 54.34895372826407,
      "grad_norm": 0.5281839370727539,
      "learning_rate": 9.082635293237781e-06,
      "loss": 1.0562,
      "step": 5815
    },
    {
      "epoch": 54.358384910109045,
      "grad_norm": 0.5218442678451538,
      "learning_rate": 9.079621224544109e-06,
      "loss": 1.0179,
      "step": 5816
    },
    {
      "epoch": 54.367816091954026,
      "grad_norm": 0.5236942172050476,
      "learning_rate": 9.076607240175233e-06,
      "loss": 1.0621,
      "step": 5817
    },
    {
      "epoch": 54.377247273799,
      "grad_norm": 0.5256877541542053,
      "learning_rate": 9.073593340407298e-06,
      "loss": 1.0332,
      "step": 5818
    },
    {
      "epoch": 54.386678455643974,
      "grad_norm": 0.545066773891449,
      "learning_rate": 9.07057952551643e-06,
      "loss": 1.0532,
      "step": 5819
    },
    {
      "epoch": 54.39610963748895,
      "grad_norm": 0.5240388512611389,
      "learning_rate": 9.067565795778762e-06,
      "loss": 1.0131,
      "step": 5820
    },
    {
      "epoch": 54.40554081933392,
      "grad_norm": 0.5244606137275696,
      "learning_rate": 9.064552151470408e-06,
      "loss": 1.0586,
      "step": 5821
    },
    {
      "epoch": 54.414972001178896,
      "grad_norm": 0.5988397002220154,
      "learning_rate": 9.06153859286747e-06,
      "loss": 1.0934,
      "step": 5822
    },
    {
      "epoch": 54.42440318302387,
      "grad_norm": 0.5386291146278381,
      "learning_rate": 9.058525120246062e-06,
      "loss": 1.0578,
      "step": 5823
    },
    {
      "epoch": 54.43383436486885,
      "grad_norm": 0.5078942775726318,
      "learning_rate": 9.055511733882268e-06,
      "loss": 1.0606,
      "step": 5824
    },
    {
      "epoch": 54.443265546713825,
      "grad_norm": 0.5183533430099487,
      "learning_rate": 9.052498434052177e-06,
      "loss": 1.103,
      "step": 5825
    },
    {
      "epoch": 54.4526967285588,
      "grad_norm": 0.535627543926239,
      "learning_rate": 9.049485221031866e-06,
      "loss": 1.0552,
      "step": 5826
    },
    {
      "epoch": 54.46212791040377,
      "grad_norm": 0.5475397706031799,
      "learning_rate": 9.046472095097404e-06,
      "loss": 1.0404,
      "step": 5827
    },
    {
      "epoch": 54.47155909224875,
      "grad_norm": 0.5173704624176025,
      "learning_rate": 9.043459056524855e-06,
      "loss": 1.0626,
      "step": 5828
    },
    {
      "epoch": 54.48099027409372,
      "grad_norm": 0.49645936489105225,
      "learning_rate": 9.040446105590265e-06,
      "loss": 1.0526,
      "step": 5829
    },
    {
      "epoch": 54.490421455938694,
      "grad_norm": 0.5407021641731262,
      "learning_rate": 9.037433242569688e-06,
      "loss": 1.0499,
      "step": 5830
    },
    {
      "epoch": 54.499852637783675,
      "grad_norm": 0.5350737571716309,
      "learning_rate": 9.034420467739162e-06,
      "loss": 1.0696,
      "step": 5831
    },
    {
      "epoch": 54.50928381962865,
      "grad_norm": 0.5265600085258484,
      "learning_rate": 9.031407781374711e-06,
      "loss": 1.0718,
      "step": 5832
    },
    {
      "epoch": 54.51871500147362,
      "grad_norm": 0.5038821697235107,
      "learning_rate": 9.028395183752357e-06,
      "loss": 1.0772,
      "step": 5833
    },
    {
      "epoch": 54.5281461833186,
      "grad_norm": 0.5420429110527039,
      "learning_rate": 9.025382675148116e-06,
      "loss": 1.0783,
      "step": 5834
    },
    {
      "epoch": 54.53757736516357,
      "grad_norm": 0.5307786464691162,
      "learning_rate": 9.022370255837992e-06,
      "loss": 1.0594,
      "step": 5835
    },
    {
      "epoch": 54.547008547008545,
      "grad_norm": 0.524941086769104,
      "learning_rate": 9.019357926097982e-06,
      "loss": 1.0329,
      "step": 5836
    },
    {
      "epoch": 54.55643972885352,
      "grad_norm": 0.5467795133590698,
      "learning_rate": 9.01634568620407e-06,
      "loss": 1.1134,
      "step": 5837
    },
    {
      "epoch": 54.5658709106985,
      "grad_norm": 0.5482788681983948,
      "learning_rate": 9.013333536432243e-06,
      "loss": 1.0468,
      "step": 5838
    },
    {
      "epoch": 54.575302092543474,
      "grad_norm": 0.5270993113517761,
      "learning_rate": 9.010321477058471e-06,
      "loss": 1.0181,
      "step": 5839
    },
    {
      "epoch": 54.58473327438845,
      "grad_norm": 0.4963623583316803,
      "learning_rate": 9.00730950835872e-06,
      "loss": 1.059,
      "step": 5840
    },
    {
      "epoch": 54.59416445623342,
      "grad_norm": 0.5415675044059753,
      "learning_rate": 9.004297630608941e-06,
      "loss": 1.0504,
      "step": 5841
    },
    {
      "epoch": 54.603595638078396,
      "grad_norm": 0.5096120238304138,
      "learning_rate": 9.001285844085083e-06,
      "loss": 1.0483,
      "step": 5842
    },
    {
      "epoch": 54.61302681992337,
      "grad_norm": 0.5207280516624451,
      "learning_rate": 8.998274149063086e-06,
      "loss": 1.0697,
      "step": 5843
    },
    {
      "epoch": 54.622458001768344,
      "grad_norm": 0.48859360814094543,
      "learning_rate": 8.995262545818878e-06,
      "loss": 1.0992,
      "step": 5844
    },
    {
      "epoch": 54.631889183613325,
      "grad_norm": 0.507891833782196,
      "learning_rate": 8.992251034628384e-06,
      "loss": 1.0831,
      "step": 5845
    },
    {
      "epoch": 54.6413203654583,
      "grad_norm": 0.5096641182899475,
      "learning_rate": 8.989239615767518e-06,
      "loss": 1.0851,
      "step": 5846
    },
    {
      "epoch": 54.65075154730327,
      "grad_norm": 0.4786377251148224,
      "learning_rate": 8.986228289512184e-06,
      "loss": 1.0884,
      "step": 5847
    },
    {
      "epoch": 54.66018272914825,
      "grad_norm": 0.5387715697288513,
      "learning_rate": 8.983217056138276e-06,
      "loss": 1.0813,
      "step": 5848
    },
    {
      "epoch": 54.66961391099322,
      "grad_norm": 0.5287502408027649,
      "learning_rate": 8.980205915921687e-06,
      "loss": 0.9933,
      "step": 5849
    },
    {
      "epoch": 54.679045092838194,
      "grad_norm": 0.5206340551376343,
      "learning_rate": 8.977194869138293e-06,
      "loss": 1.0953,
      "step": 5850
    },
    {
      "epoch": 54.68847627468317,
      "grad_norm": 0.5323344469070435,
      "learning_rate": 8.974183916063967e-06,
      "loss": 1.0452,
      "step": 5851
    },
    {
      "epoch": 54.69790745652815,
      "grad_norm": 0.4936918318271637,
      "learning_rate": 8.97117305697457e-06,
      "loss": 1.058,
      "step": 5852
    },
    {
      "epoch": 54.70733863837312,
      "grad_norm": 0.49309268593788147,
      "learning_rate": 8.968162292145959e-06,
      "loss": 1.062,
      "step": 5853
    },
    {
      "epoch": 54.7167698202181,
      "grad_norm": 0.48892685770988464,
      "learning_rate": 8.965151621853974e-06,
      "loss": 1.0762,
      "step": 5854
    },
    {
      "epoch": 54.72620100206307,
      "grad_norm": 0.5410017371177673,
      "learning_rate": 8.962141046374458e-06,
      "loss": 1.0189,
      "step": 5855
    },
    {
      "epoch": 54.735632183908045,
      "grad_norm": 0.5161365866661072,
      "learning_rate": 8.959130565983233e-06,
      "loss": 1.0028,
      "step": 5856
    },
    {
      "epoch": 54.74506336575302,
      "grad_norm": 0.5722760558128357,
      "learning_rate": 8.956120180956124e-06,
      "loss": 1.0388,
      "step": 5857
    },
    {
      "epoch": 54.75449454759799,
      "grad_norm": 0.4956693649291992,
      "learning_rate": 8.953109891568939e-06,
      "loss": 1.0957,
      "step": 5858
    },
    {
      "epoch": 54.763925729442974,
      "grad_norm": 0.4875361919403076,
      "learning_rate": 8.950099698097474e-06,
      "loss": 1.0735,
      "step": 5859
    },
    {
      "epoch": 54.77335691128795,
      "grad_norm": 0.5108743906021118,
      "learning_rate": 8.94708960081753e-06,
      "loss": 1.0643,
      "step": 5860
    },
    {
      "epoch": 54.78278809313292,
      "grad_norm": 0.515095055103302,
      "learning_rate": 8.94407960000489e-06,
      "loss": 1.0716,
      "step": 5861
    },
    {
      "epoch": 54.792219274977896,
      "grad_norm": 0.5329799652099609,
      "learning_rate": 8.941069695935326e-06,
      "loss": 1.0193,
      "step": 5862
    },
    {
      "epoch": 54.80165045682287,
      "grad_norm": 0.5767807960510254,
      "learning_rate": 8.938059888884606e-06,
      "loss": 1.0458,
      "step": 5863
    },
    {
      "epoch": 54.811081638667844,
      "grad_norm": 0.5162017345428467,
      "learning_rate": 8.935050179128486e-06,
      "loss": 1.0435,
      "step": 5864
    },
    {
      "epoch": 54.82051282051282,
      "grad_norm": 0.5236385464668274,
      "learning_rate": 8.932040566942719e-06,
      "loss": 1.0529,
      "step": 5865
    },
    {
      "epoch": 54.8299440023578,
      "grad_norm": 0.5615944266319275,
      "learning_rate": 8.929031052603042e-06,
      "loss": 1.0363,
      "step": 5866
    },
    {
      "epoch": 54.83937518420277,
      "grad_norm": 0.5175338387489319,
      "learning_rate": 8.926021636385179e-06,
      "loss": 1.0928,
      "step": 5867
    },
    {
      "epoch": 54.84880636604775,
      "grad_norm": 0.5192197561264038,
      "learning_rate": 8.923012318564863e-06,
      "loss": 1.0788,
      "step": 5868
    },
    {
      "epoch": 54.85823754789272,
      "grad_norm": 0.5545268654823303,
      "learning_rate": 8.920003099417803e-06,
      "loss": 1.0821,
      "step": 5869
    },
    {
      "epoch": 54.867668729737694,
      "grad_norm": 0.5274239778518677,
      "learning_rate": 8.9169939792197e-06,
      "loss": 0.9994,
      "step": 5870
    },
    {
      "epoch": 54.87709991158267,
      "grad_norm": 0.48417916893959045,
      "learning_rate": 8.91398495824625e-06,
      "loss": 1.0552,
      "step": 5871
    },
    {
      "epoch": 54.88653109342764,
      "grad_norm": 0.496992826461792,
      "learning_rate": 8.910976036773138e-06,
      "loss": 1.0655,
      "step": 5872
    },
    {
      "epoch": 54.89596227527262,
      "grad_norm": 0.5171954035758972,
      "learning_rate": 8.907967215076043e-06,
      "loss": 1.0876,
      "step": 5873
    },
    {
      "epoch": 54.9053934571176,
      "grad_norm": 0.5215822458267212,
      "learning_rate": 8.904958493430626e-06,
      "loss": 1.083,
      "step": 5874
    },
    {
      "epoch": 54.91482463896257,
      "grad_norm": 0.521144449710846,
      "learning_rate": 8.901949872112552e-06,
      "loss": 1.0911,
      "step": 5875
    },
    {
      "epoch": 54.924255820807545,
      "grad_norm": 0.5319280028343201,
      "learning_rate": 8.898941351397467e-06,
      "loss": 1.0539,
      "step": 5876
    },
    {
      "epoch": 54.93368700265252,
      "grad_norm": 0.528145968914032,
      "learning_rate": 8.895932931561013e-06,
      "loss": 1.0731,
      "step": 5877
    },
    {
      "epoch": 54.94311818449749,
      "grad_norm": 0.5183759331703186,
      "learning_rate": 8.892924612878815e-06,
      "loss": 1.0651,
      "step": 5878
    },
    {
      "epoch": 54.95254936634247,
      "grad_norm": 0.5839059352874756,
      "learning_rate": 8.8899163956265e-06,
      "loss": 1.0571,
      "step": 5879
    },
    {
      "epoch": 54.96198054818745,
      "grad_norm": 0.5298061370849609,
      "learning_rate": 8.886908280079675e-06,
      "loss": 1.1067,
      "step": 5880
    },
    {
      "epoch": 54.97141173003242,
      "grad_norm": 0.5371148586273193,
      "learning_rate": 8.883900266513948e-06,
      "loss": 1.0664,
      "step": 5881
    },
    {
      "epoch": 54.980842911877396,
      "grad_norm": 0.5173529982566833,
      "learning_rate": 8.880892355204904e-06,
      "loss": 1.0686,
      "step": 5882
    },
    {
      "epoch": 54.99027409372237,
      "grad_norm": 0.5162965059280396,
      "learning_rate": 8.877884546428137e-06,
      "loss": 1.0552,
      "step": 5883
    },
    {
      "epoch": 54.999705275567344,
      "grad_norm": 0.5513011813163757,
      "learning_rate": 8.874876840459214e-06,
      "loss": 1.0411,
      "step": 5884
    },
    {
      "epoch": 55.0,
      "grad_norm": 2.7399191856384277,
      "learning_rate": 8.871869237573701e-06,
      "loss": 0.4805,
      "step": 5885
    },
    {
      "epoch": 55.009431181844974,
      "grad_norm": 0.49521201848983765,
      "learning_rate": 8.868861738047158e-06,
      "loss": 1.0682,
      "step": 5886
    },
    {
      "epoch": 55.01886236368995,
      "grad_norm": 0.5578422546386719,
      "learning_rate": 8.865854342155127e-06,
      "loss": 1.0977,
      "step": 5887
    },
    {
      "epoch": 55.02829354553492,
      "grad_norm": 0.5141503214836121,
      "learning_rate": 8.862847050173147e-06,
      "loss": 1.0643,
      "step": 5888
    },
    {
      "epoch": 55.0377247273799,
      "grad_norm": 0.5755935907363892,
      "learning_rate": 8.859839862376742e-06,
      "loss": 1.0495,
      "step": 5889
    },
    {
      "epoch": 55.04715590922488,
      "grad_norm": 0.5492014288902283,
      "learning_rate": 8.856832779041428e-06,
      "loss": 1.0135,
      "step": 5890
    },
    {
      "epoch": 55.05658709106985,
      "grad_norm": 0.4779031574726105,
      "learning_rate": 8.853825800442721e-06,
      "loss": 1.0858,
      "step": 5891
    },
    {
      "epoch": 55.066018272914825,
      "grad_norm": 0.5091177821159363,
      "learning_rate": 8.850818926856115e-06,
      "loss": 1.0672,
      "step": 5892
    },
    {
      "epoch": 55.0754494547598,
      "grad_norm": 0.5056696534156799,
      "learning_rate": 8.847812158557098e-06,
      "loss": 1.0675,
      "step": 5893
    },
    {
      "epoch": 55.08488063660477,
      "grad_norm": 0.5236080288887024,
      "learning_rate": 8.84480549582115e-06,
      "loss": 1.0776,
      "step": 5894
    },
    {
      "epoch": 55.09431181844975,
      "grad_norm": 0.5442259907722473,
      "learning_rate": 8.841798938923739e-06,
      "loss": 1.0142,
      "step": 5895
    },
    {
      "epoch": 55.10374300029473,
      "grad_norm": 0.5194990634918213,
      "learning_rate": 8.838792488140326e-06,
      "loss": 1.0389,
      "step": 5896
    },
    {
      "epoch": 55.1131741821397,
      "grad_norm": 0.5342520475387573,
      "learning_rate": 8.835786143746358e-06,
      "loss": 1.0495,
      "step": 5897
    },
    {
      "epoch": 55.122605363984675,
      "grad_norm": 0.5438886880874634,
      "learning_rate": 8.832779906017281e-06,
      "loss": 1.0702,
      "step": 5898
    },
    {
      "epoch": 55.13203654582965,
      "grad_norm": 0.5396651029586792,
      "learning_rate": 8.829773775228524e-06,
      "loss": 1.0909,
      "step": 5899
    },
    {
      "epoch": 55.14146772767462,
      "grad_norm": 0.5561047792434692,
      "learning_rate": 8.826767751655504e-06,
      "loss": 1.0567,
      "step": 5900
    },
    {
      "epoch": 55.1508989095196,
      "grad_norm": 0.5345202684402466,
      "learning_rate": 8.823761835573635e-06,
      "loss": 1.0146,
      "step": 5901
    },
    {
      "epoch": 55.16033009136457,
      "grad_norm": 0.5230197906494141,
      "learning_rate": 8.820756027258318e-06,
      "loss": 1.0661,
      "step": 5902
    },
    {
      "epoch": 55.16976127320955,
      "grad_norm": 0.5240851640701294,
      "learning_rate": 8.817750326984944e-06,
      "loss": 1.0862,
      "step": 5903
    },
    {
      "epoch": 55.179192455054526,
      "grad_norm": 0.5484163761138916,
      "learning_rate": 8.814744735028887e-06,
      "loss": 1.0912,
      "step": 5904
    },
    {
      "epoch": 55.1886236368995,
      "grad_norm": 0.4965091645717621,
      "learning_rate": 8.811739251665529e-06,
      "loss": 1.0518,
      "step": 5905
    },
    {
      "epoch": 55.198054818744474,
      "grad_norm": 0.5471274256706238,
      "learning_rate": 8.80873387717023e-06,
      "loss": 1.091,
      "step": 5906
    },
    {
      "epoch": 55.20748600058945,
      "grad_norm": 0.5516201853752136,
      "learning_rate": 8.805728611818338e-06,
      "loss": 1.0793,
      "step": 5907
    },
    {
      "epoch": 55.21691718243442,
      "grad_norm": 0.5428614616394043,
      "learning_rate": 8.802723455885195e-06,
      "loss": 1.022,
      "step": 5908
    },
    {
      "epoch": 55.226348364279396,
      "grad_norm": 0.55422443151474,
      "learning_rate": 8.799718409646134e-06,
      "loss": 1.0599,
      "step": 5909
    },
    {
      "epoch": 55.23577954612438,
      "grad_norm": 0.5391206741333008,
      "learning_rate": 8.796713473376472e-06,
      "loss": 1.0765,
      "step": 5910
    },
    {
      "epoch": 55.24521072796935,
      "grad_norm": 0.5462409853935242,
      "learning_rate": 8.79370864735152e-06,
      "loss": 1.0793,
      "step": 5911
    },
    {
      "epoch": 55.254641909814325,
      "grad_norm": 0.5687711238861084,
      "learning_rate": 8.790703931846587e-06,
      "loss": 1.0486,
      "step": 5912
    },
    {
      "epoch": 55.2640730916593,
      "grad_norm": 0.4966203570365906,
      "learning_rate": 8.78769932713696e-06,
      "loss": 1.057,
      "step": 5913
    },
    {
      "epoch": 55.27350427350427,
      "grad_norm": 0.5266944169998169,
      "learning_rate": 8.784694833497919e-06,
      "loss": 1.0757,
      "step": 5914
    },
    {
      "epoch": 55.28293545534925,
      "grad_norm": 0.5328086614608765,
      "learning_rate": 8.781690451204736e-06,
      "loss": 1.0485,
      "step": 5915
    },
    {
      "epoch": 55.29236663719422,
      "grad_norm": 0.5152223110198975,
      "learning_rate": 8.778686180532667e-06,
      "loss": 1.0503,
      "step": 5916
    },
    {
      "epoch": 55.3017978190392,
      "grad_norm": 0.5489837527275085,
      "learning_rate": 8.775682021756967e-06,
      "loss": 1.0276,
      "step": 5917
    },
    {
      "epoch": 55.311229000884175,
      "grad_norm": 0.5321356654167175,
      "learning_rate": 8.772677975152876e-06,
      "loss": 1.0182,
      "step": 5918
    },
    {
      "epoch": 55.32066018272915,
      "grad_norm": 0.5319206118583679,
      "learning_rate": 8.769674040995621e-06,
      "loss": 1.0257,
      "step": 5919
    },
    {
      "epoch": 55.33009136457412,
      "grad_norm": 0.5204092264175415,
      "learning_rate": 8.766670219560425e-06,
      "loss": 1.0843,
      "step": 5920
    },
    {
      "epoch": 55.3395225464191,
      "grad_norm": 0.5085121393203735,
      "learning_rate": 8.763666511122492e-06,
      "loss": 1.0292,
      "step": 5921
    },
    {
      "epoch": 55.34895372826407,
      "grad_norm": 0.5132620334625244,
      "learning_rate": 8.760662915957026e-06,
      "loss": 1.0471,
      "step": 5922
    },
    {
      "epoch": 55.358384910109045,
      "grad_norm": 0.5753227472305298,
      "learning_rate": 8.757659434339216e-06,
      "loss": 1.0626,
      "step": 5923
    },
    {
      "epoch": 55.367816091954026,
      "grad_norm": 0.4916653037071228,
      "learning_rate": 8.754656066544235e-06,
      "loss": 1.0599,
      "step": 5924
    },
    {
      "epoch": 55.377247273799,
      "grad_norm": 0.530719518661499,
      "learning_rate": 8.751652812847253e-06,
      "loss": 1.0378,
      "step": 5925
    },
    {
      "epoch": 55.386678455643974,
      "grad_norm": 0.5349663496017456,
      "learning_rate": 8.748649673523422e-06,
      "loss": 1.0772,
      "step": 5926
    },
    {
      "epoch": 55.39610963748895,
      "grad_norm": 0.5290753841400146,
      "learning_rate": 8.7456466488479e-06,
      "loss": 1.0532,
      "step": 5927
    },
    {
      "epoch": 55.40554081933392,
      "grad_norm": 0.5078790187835693,
      "learning_rate": 8.742643739095816e-06,
      "loss": 1.0454,
      "step": 5928
    },
    {
      "epoch": 55.414972001178896,
      "grad_norm": 0.5362856388092041,
      "learning_rate": 8.739640944542295e-06,
      "loss": 1.0617,
      "step": 5929
    },
    {
      "epoch": 55.42440318302387,
      "grad_norm": 0.5847488641738892,
      "learning_rate": 8.736638265462456e-06,
      "loss": 1.069,
      "step": 5930
    },
    {
      "epoch": 55.43383436486885,
      "grad_norm": 0.53973788022995,
      "learning_rate": 8.7336357021314e-06,
      "loss": 1.0624,
      "step": 5931
    },
    {
      "epoch": 55.443265546713825,
      "grad_norm": 0.5535175800323486,
      "learning_rate": 8.730633254824222e-06,
      "loss": 1.0436,
      "step": 5932
    },
    {
      "epoch": 55.4526967285588,
      "grad_norm": 0.5180513262748718,
      "learning_rate": 8.727630923816002e-06,
      "loss": 1.0288,
      "step": 5933
    },
    {
      "epoch": 55.46212791040377,
      "grad_norm": 0.5301783084869385,
      "learning_rate": 8.724628709381815e-06,
      "loss": 1.0841,
      "step": 5934
    },
    {
      "epoch": 55.47155909224875,
      "grad_norm": 0.5002820491790771,
      "learning_rate": 8.721626611796725e-06,
      "loss": 1.0448,
      "step": 5935
    },
    {
      "epoch": 55.48099027409372,
      "grad_norm": 0.47942662239074707,
      "learning_rate": 8.718624631335784e-06,
      "loss": 1.0664,
      "step": 5936
    },
    {
      "epoch": 55.490421455938694,
      "grad_norm": 0.5332059860229492,
      "learning_rate": 8.715622768274028e-06,
      "loss": 1.0395,
      "step": 5937
    },
    {
      "epoch": 55.499852637783675,
      "grad_norm": 0.5847426056861877,
      "learning_rate": 8.712621022886489e-06,
      "loss": 1.054,
      "step": 5938
    },
    {
      "epoch": 55.50928381962865,
      "grad_norm": 0.5426549315452576,
      "learning_rate": 8.709619395448186e-06,
      "loss": 1.0128,
      "step": 5939
    },
    {
      "epoch": 55.51871500147362,
      "grad_norm": 0.49403220415115356,
      "learning_rate": 8.706617886234127e-06,
      "loss": 1.0765,
      "step": 5940
    },
    {
      "epoch": 55.5281461833186,
      "grad_norm": 0.4906383752822876,
      "learning_rate": 8.703616495519305e-06,
      "loss": 1.0829,
      "step": 5941
    },
    {
      "epoch": 55.53757736516357,
      "grad_norm": 0.49969038367271423,
      "learning_rate": 8.700615223578716e-06,
      "loss": 1.0925,
      "step": 5942
    },
    {
      "epoch": 55.547008547008545,
      "grad_norm": 0.570443332195282,
      "learning_rate": 8.697614070687328e-06,
      "loss": 1.0373,
      "step": 5943
    },
    {
      "epoch": 55.55643972885352,
      "grad_norm": 0.551507294178009,
      "learning_rate": 8.694613037120108e-06,
      "loss": 1.0875,
      "step": 5944
    },
    {
      "epoch": 55.5658709106985,
      "grad_norm": 0.5169046521186829,
      "learning_rate": 8.69161212315201e-06,
      "loss": 1.0796,
      "step": 5945
    },
    {
      "epoch": 55.575302092543474,
      "grad_norm": 0.5321174263954163,
      "learning_rate": 8.688611329057977e-06,
      "loss": 1.0843,
      "step": 5946
    },
    {
      "epoch": 55.58473327438845,
      "grad_norm": 0.5495566129684448,
      "learning_rate": 8.685610655112936e-06,
      "loss": 1.0917,
      "step": 5947
    },
    {
      "epoch": 55.59416445623342,
      "grad_norm": 0.5561806559562683,
      "learning_rate": 8.682610101591813e-06,
      "loss": 1.0674,
      "step": 5948
    },
    {
      "epoch": 55.603595638078396,
      "grad_norm": 0.5149860382080078,
      "learning_rate": 8.679609668769517e-06,
      "loss": 1.0481,
      "step": 5949
    },
    {
      "epoch": 55.61302681992337,
      "grad_norm": 0.5422797799110413,
      "learning_rate": 8.676609356920946e-06,
      "loss": 1.053,
      "step": 5950
    },
    {
      "epoch": 55.622458001768344,
      "grad_norm": 0.5286881327629089,
      "learning_rate": 8.673609166320988e-06,
      "loss": 1.0472,
      "step": 5951
    },
    {
      "epoch": 55.631889183613325,
      "grad_norm": 0.47990095615386963,
      "learning_rate": 8.670609097244518e-06,
      "loss": 1.0549,
      "step": 5952
    },
    {
      "epoch": 55.6413203654583,
      "grad_norm": 0.5505739450454712,
      "learning_rate": 8.667609149966405e-06,
      "loss": 1.037,
      "step": 5953
    },
    {
      "epoch": 55.65075154730327,
      "grad_norm": 0.5318058729171753,
      "learning_rate": 8.6646093247615e-06,
      "loss": 1.0886,
      "step": 5954
    },
    {
      "epoch": 55.66018272914825,
      "grad_norm": 0.5547525882720947,
      "learning_rate": 8.661609621904646e-06,
      "loss": 1.0334,
      "step": 5955
    },
    {
      "epoch": 55.66961391099322,
      "grad_norm": 0.5302714705467224,
      "learning_rate": 8.65861004167067e-06,
      "loss": 1.0552,
      "step": 5956
    },
    {
      "epoch": 55.679045092838194,
      "grad_norm": 0.5356714129447937,
      "learning_rate": 8.655610584334403e-06,
      "loss": 1.0784,
      "step": 5957
    },
    {
      "epoch": 55.68847627468317,
      "grad_norm": 0.5224717855453491,
      "learning_rate": 8.65261125017065e-06,
      "loss": 1.0767,
      "step": 5958
    },
    {
      "epoch": 55.69790745652815,
      "grad_norm": 0.5099815130233765,
      "learning_rate": 8.649612039454207e-06,
      "loss": 1.0355,
      "step": 5959
    },
    {
      "epoch": 55.70733863837312,
      "grad_norm": 0.5108827352523804,
      "learning_rate": 8.646612952459863e-06,
      "loss": 1.044,
      "step": 5960
    },
    {
      "epoch": 55.7167698202181,
      "grad_norm": 0.5044381618499756,
      "learning_rate": 8.643613989462391e-06,
      "loss": 1.0688,
      "step": 5961
    },
    {
      "epoch": 55.72620100206307,
      "grad_norm": 0.5137985944747925,
      "learning_rate": 8.640615150736558e-06,
      "loss": 1.0494,
      "step": 5962
    },
    {
      "epoch": 55.735632183908045,
      "grad_norm": 0.5724047422409058,
      "learning_rate": 8.637616436557114e-06,
      "loss": 1.0327,
      "step": 5963
    },
    {
      "epoch": 55.74506336575302,
      "grad_norm": 0.5484999418258667,
      "learning_rate": 8.634617847198796e-06,
      "loss": 1.0513,
      "step": 5964
    },
    {
      "epoch": 55.75449454759799,
      "grad_norm": 0.5158854126930237,
      "learning_rate": 8.631619382936346e-06,
      "loss": 1.0541,
      "step": 5965
    },
    {
      "epoch": 55.763925729442974,
      "grad_norm": 0.5465616583824158,
      "learning_rate": 8.628621044044471e-06,
      "loss": 1.0932,
      "step": 5966
    },
    {
      "epoch": 55.77335691128795,
      "grad_norm": 0.5445591807365417,
      "learning_rate": 8.625622830797885e-06,
      "loss": 1.0538,
      "step": 5967
    },
    {
      "epoch": 55.78278809313292,
      "grad_norm": 0.5325587391853333,
      "learning_rate": 8.622624743471279e-06,
      "loss": 1.0587,
      "step": 5968
    },
    {
      "epoch": 55.792219274977896,
      "grad_norm": 0.49460113048553467,
      "learning_rate": 8.619626782339338e-06,
      "loss": 1.0738,
      "step": 5969
    },
    {
      "epoch": 55.80165045682287,
      "grad_norm": 0.4629790782928467,
      "learning_rate": 8.616628947676734e-06,
      "loss": 1.063,
      "step": 5970
    },
    {
      "epoch": 55.811081638667844,
      "grad_norm": 0.4928603172302246,
      "learning_rate": 8.613631239758124e-06,
      "loss": 1.0398,
      "step": 5971
    },
    {
      "epoch": 55.82051282051282,
      "grad_norm": 0.5277243256568909,
      "learning_rate": 8.610633658858165e-06,
      "loss": 1.0477,
      "step": 5972
    },
    {
      "epoch": 55.8299440023578,
      "grad_norm": 0.5352298021316528,
      "learning_rate": 8.607636205251491e-06,
      "loss": 1.1176,
      "step": 5973
    },
    {
      "epoch": 55.83937518420277,
      "grad_norm": 0.554198682308197,
      "learning_rate": 8.604638879212726e-06,
      "loss": 1.0194,
      "step": 5974
    },
    {
      "epoch": 55.84880636604775,
      "grad_norm": 0.4935629665851593,
      "learning_rate": 8.601641681016483e-06,
      "loss": 1.0581,
      "step": 5975
    },
    {
      "epoch": 55.85823754789272,
      "grad_norm": 0.5401833653450012,
      "learning_rate": 8.59864461093737e-06,
      "loss": 1.0553,
      "step": 5976
    },
    {
      "epoch": 55.867668729737694,
      "grad_norm": 0.5161696076393127,
      "learning_rate": 8.59564766924997e-06,
      "loss": 1.0479,
      "step": 5977
    },
    {
      "epoch": 55.87709991158267,
      "grad_norm": 0.5120900869369507,
      "learning_rate": 8.592650856228869e-06,
      "loss": 1.035,
      "step": 5978
    },
    {
      "epoch": 55.88653109342764,
      "grad_norm": 0.5466859936714172,
      "learning_rate": 8.589654172148626e-06,
      "loss": 1.0719,
      "step": 5979
    },
    {
      "epoch": 55.89596227527262,
      "grad_norm": 0.5593411922454834,
      "learning_rate": 8.586657617283805e-06,
      "loss": 1.0609,
      "step": 5980
    },
    {
      "epoch": 55.9053934571176,
      "grad_norm": 0.5317162275314331,
      "learning_rate": 8.583661191908943e-06,
      "loss": 1.0802,
      "step": 5981
    },
    {
      "epoch": 55.91482463896257,
      "grad_norm": 0.4925059378147125,
      "learning_rate": 8.580664896298574e-06,
      "loss": 1.0586,
      "step": 5982
    },
    {
      "epoch": 55.924255820807545,
      "grad_norm": 0.5257766246795654,
      "learning_rate": 8.577668730727218e-06,
      "loss": 1.0825,
      "step": 5983
    },
    {
      "epoch": 55.93368700265252,
      "grad_norm": 0.5344334840774536,
      "learning_rate": 8.574672695469382e-06,
      "loss": 1.0496,
      "step": 5984
    },
    {
      "epoch": 55.94311818449749,
      "grad_norm": 0.512913703918457,
      "learning_rate": 8.571676790799564e-06,
      "loss": 1.048,
      "step": 5985
    },
    {
      "epoch": 55.95254936634247,
      "grad_norm": 0.5532868504524231,
      "learning_rate": 8.568681016992241e-06,
      "loss": 1.0476,
      "step": 5986
    },
    {
      "epoch": 55.96198054818745,
      "grad_norm": 0.5022493600845337,
      "learning_rate": 8.56568537432189e-06,
      "loss": 1.0338,
      "step": 5987
    },
    {
      "epoch": 55.97141173003242,
      "grad_norm": 0.5326250791549683,
      "learning_rate": 8.562689863062971e-06,
      "loss": 1.0489,
      "step": 5988
    },
    {
      "epoch": 55.980842911877396,
      "grad_norm": 0.5021722316741943,
      "learning_rate": 8.559694483489934e-06,
      "loss": 1.0622,
      "step": 5989
    },
    {
      "epoch": 55.99027409372237,
      "grad_norm": 0.49792811274528503,
      "learning_rate": 8.55669923587721e-06,
      "loss": 1.064,
      "step": 5990
    },
    {
      "epoch": 55.999705275567344,
      "grad_norm": 0.5069727897644043,
      "learning_rate": 8.553704120499224e-06,
      "loss": 1.0655,
      "step": 5991
    },
    {
      "epoch": 56.0,
      "grad_norm": 2.8100199699401855,
      "learning_rate": 8.550709137630388e-06,
      "loss": 0.6744,
      "step": 5992
    },
    {
      "epoch": 56.009431181844974,
      "grad_norm": 0.5298910737037659,
      "learning_rate": 8.5477142875451e-06,
      "loss": 1.0525,
      "step": 5993
    },
    {
      "epoch": 56.01886236368995,
      "grad_norm": 0.5367228984832764,
      "learning_rate": 8.544719570517746e-06,
      "loss": 1.0463,
      "step": 5994
    },
    {
      "epoch": 56.02829354553492,
      "grad_norm": 0.5347153544425964,
      "learning_rate": 8.541724986822705e-06,
      "loss": 1.0606,
      "step": 5995
    },
    {
      "epoch": 56.0377247273799,
      "grad_norm": 0.5049179792404175,
      "learning_rate": 8.538730536734338e-06,
      "loss": 1.0319,
      "step": 5996
    },
    {
      "epoch": 56.04715590922488,
      "grad_norm": 0.5251964926719666,
      "learning_rate": 8.535736220526998e-06,
      "loss": 1.0406,
      "step": 5997
    },
    {
      "epoch": 56.05658709106985,
      "grad_norm": 0.5067630410194397,
      "learning_rate": 8.532742038475018e-06,
      "loss": 1.0422,
      "step": 5998
    },
    {
      "epoch": 56.066018272914825,
      "grad_norm": 0.5214529037475586,
      "learning_rate": 8.529747990852729e-06,
      "loss": 1.0729,
      "step": 5999
    },
    {
      "epoch": 56.0754494547598,
      "grad_norm": 0.5645402073860168,
      "learning_rate": 8.526754077934441e-06,
      "loss": 1.0745,
      "step": 6000
    },
    {
      "epoch": 56.08488063660477,
      "grad_norm": 0.5313451290130615,
      "learning_rate": 8.523760299994452e-06,
      "loss": 1.0582,
      "step": 6001
    },
    {
      "epoch": 56.09431181844975,
      "grad_norm": 0.5255727767944336,
      "learning_rate": 8.52076665730706e-06,
      "loss": 1.0597,
      "step": 6002
    },
    {
      "epoch": 56.10374300029473,
      "grad_norm": 0.533767580986023,
      "learning_rate": 8.51777315014654e-06,
      "loss": 1.0551,
      "step": 6003
    },
    {
      "epoch": 56.1131741821397,
      "grad_norm": 0.5127114653587341,
      "learning_rate": 8.51477977878715e-06,
      "loss": 1.0841,
      "step": 6004
    },
    {
      "epoch": 56.122605363984675,
      "grad_norm": 0.5087685585021973,
      "learning_rate": 8.511786543503146e-06,
      "loss": 1.0198,
      "step": 6005
    },
    {
      "epoch": 56.13203654582965,
      "grad_norm": 0.5167328119277954,
      "learning_rate": 8.508793444568766e-06,
      "loss": 1.0673,
      "step": 6006
    },
    {
      "epoch": 56.14146772767462,
      "grad_norm": 0.537898063659668,
      "learning_rate": 8.505800482258238e-06,
      "loss": 1.061,
      "step": 6007
    },
    {
      "epoch": 56.1508989095196,
      "grad_norm": 0.5170820355415344,
      "learning_rate": 8.502807656845772e-06,
      "loss": 1.0558,
      "step": 6008
    },
    {
      "epoch": 56.16033009136457,
      "grad_norm": 0.5478394627571106,
      "learning_rate": 8.499814968605573e-06,
      "loss": 1.0506,
      "step": 6009
    },
    {
      "epoch": 56.16976127320955,
      "grad_norm": 0.5402225852012634,
      "learning_rate": 8.496822417811832e-06,
      "loss": 1.0996,
      "step": 6010
    },
    {
      "epoch": 56.179192455054526,
      "grad_norm": 0.5296114087104797,
      "learning_rate": 8.493830004738723e-06,
      "loss": 1.0338,
      "step": 6011
    },
    {
      "epoch": 56.1886236368995,
      "grad_norm": 0.543692409992218,
      "learning_rate": 8.490837729660411e-06,
      "loss": 1.0593,
      "step": 6012
    },
    {
      "epoch": 56.198054818744474,
      "grad_norm": 0.526919960975647,
      "learning_rate": 8.487845592851043e-06,
      "loss": 1.0048,
      "step": 6013
    },
    {
      "epoch": 56.20748600058945,
      "grad_norm": 0.5360121130943298,
      "learning_rate": 8.484853594584766e-06,
      "loss": 1.0575,
      "step": 6014
    },
    {
      "epoch": 56.21691718243442,
      "grad_norm": 0.5304307341575623,
      "learning_rate": 8.4818617351357e-06,
      "loss": 1.0461,
      "step": 6015
    },
    {
      "epoch": 56.226348364279396,
      "grad_norm": 0.5934832692146301,
      "learning_rate": 8.478870014777956e-06,
      "loss": 1.0509,
      "step": 6016
    },
    {
      "epoch": 56.23577954612438,
      "grad_norm": 0.5461078882217407,
      "learning_rate": 8.47587843378564e-06,
      "loss": 1.0271,
      "step": 6017
    },
    {
      "epoch": 56.24521072796935,
      "grad_norm": 0.5286596417427063,
      "learning_rate": 8.472886992432836e-06,
      "loss": 1.0468,
      "step": 6018
    },
    {
      "epoch": 56.254641909814325,
      "grad_norm": 0.5044885277748108,
      "learning_rate": 8.469895690993622e-06,
      "loss": 1.0835,
      "step": 6019
    },
    {
      "epoch": 56.2640730916593,
      "grad_norm": 0.5180712342262268,
      "learning_rate": 8.466904529742058e-06,
      "loss": 1.0935,
      "step": 6020
    },
    {
      "epoch": 56.27350427350427,
      "grad_norm": 0.5124825239181519,
      "learning_rate": 8.463913508952194e-06,
      "loss": 1.0693,
      "step": 6021
    },
    {
      "epoch": 56.28293545534925,
      "grad_norm": 0.49046579003334045,
      "learning_rate": 8.460922628898064e-06,
      "loss": 1.0632,
      "step": 6022
    },
    {
      "epoch": 56.29236663719422,
      "grad_norm": 0.5262624025344849,
      "learning_rate": 8.457931889853692e-06,
      "loss": 1.037,
      "step": 6023
    },
    {
      "epoch": 56.3017978190392,
      "grad_norm": 0.5231098532676697,
      "learning_rate": 8.454941292093093e-06,
      "loss": 1.0276,
      "step": 6024
    },
    {
      "epoch": 56.311229000884175,
      "grad_norm": 0.5487034916877747,
      "learning_rate": 8.45195083589026e-06,
      "loss": 1.0618,
      "step": 6025
    },
    {
      "epoch": 56.32066018272915,
      "grad_norm": 0.5659156441688538,
      "learning_rate": 8.448960521519179e-06,
      "loss": 1.033,
      "step": 6026
    },
    {
      "epoch": 56.33009136457412,
      "grad_norm": 0.5627676844596863,
      "learning_rate": 8.445970349253822e-06,
      "loss": 1.0708,
      "step": 6027
    },
    {
      "epoch": 56.3395225464191,
      "grad_norm": 0.5126030445098877,
      "learning_rate": 8.442980319368147e-06,
      "loss": 1.0897,
      "step": 6028
    },
    {
      "epoch": 56.34895372826407,
      "grad_norm": 0.5052067637443542,
      "learning_rate": 8.439990432136101e-06,
      "loss": 1.0941,
      "step": 6029
    },
    {
      "epoch": 56.358384910109045,
      "grad_norm": 0.513999342918396,
      "learning_rate": 8.437000687831615e-06,
      "loss": 1.0695,
      "step": 6030
    },
    {
      "epoch": 56.367816091954026,
      "grad_norm": 0.5180615186691284,
      "learning_rate": 8.434011086728602e-06,
      "loss": 1.057,
      "step": 6031
    },
    {
      "epoch": 56.377247273799,
      "grad_norm": 0.5542859435081482,
      "learning_rate": 8.431021629100982e-06,
      "loss": 1.0605,
      "step": 6032
    },
    {
      "epoch": 56.386678455643974,
      "grad_norm": 0.48270025849342346,
      "learning_rate": 8.42803231522264e-06,
      "loss": 1.0105,
      "step": 6033
    },
    {
      "epoch": 56.39610963748895,
      "grad_norm": 0.5647696256637573,
      "learning_rate": 8.425043145367455e-06,
      "loss": 1.0677,
      "step": 6034
    },
    {
      "epoch": 56.40554081933392,
      "grad_norm": 0.5773271918296814,
      "learning_rate": 8.422054119809298e-06,
      "loss": 1.0362,
      "step": 6035
    },
    {
      "epoch": 56.414972001178896,
      "grad_norm": 0.5036468505859375,
      "learning_rate": 8.419065238822018e-06,
      "loss": 1.062,
      "step": 6036
    },
    {
      "epoch": 56.42440318302387,
      "grad_norm": 0.5220641493797302,
      "learning_rate": 8.41607650267946e-06,
      "loss": 1.0702,
      "step": 6037
    },
    {
      "epoch": 56.43383436486885,
      "grad_norm": 0.5357199907302856,
      "learning_rate": 8.413087911655442e-06,
      "loss": 1.0468,
      "step": 6038
    },
    {
      "epoch": 56.443265546713825,
      "grad_norm": 0.5001274347305298,
      "learning_rate": 8.410099466023789e-06,
      "loss": 1.0693,
      "step": 6039
    },
    {
      "epoch": 56.4526967285588,
      "grad_norm": 0.5449792146682739,
      "learning_rate": 8.407111166058297e-06,
      "loss": 1.0858,
      "step": 6040
    },
    {
      "epoch": 56.46212791040377,
      "grad_norm": 0.5383220911026001,
      "learning_rate": 8.404123012032753e-06,
      "loss": 1.0407,
      "step": 6041
    },
    {
      "epoch": 56.47155909224875,
      "grad_norm": 0.5282198190689087,
      "learning_rate": 8.401135004220928e-06,
      "loss": 1.0398,
      "step": 6042
    },
    {
      "epoch": 56.48099027409372,
      "grad_norm": 0.5481658577919006,
      "learning_rate": 8.398147142896586e-06,
      "loss": 1.0836,
      "step": 6043
    },
    {
      "epoch": 56.490421455938694,
      "grad_norm": 0.5240108966827393,
      "learning_rate": 8.395159428333472e-06,
      "loss": 1.0425,
      "step": 6044
    },
    {
      "epoch": 56.499852637783675,
      "grad_norm": 0.5385614037513733,
      "learning_rate": 8.39217186080532e-06,
      "loss": 1.0329,
      "step": 6045
    },
    {
      "epoch": 56.50928381962865,
      "grad_norm": 0.5198280811309814,
      "learning_rate": 8.389184440585848e-06,
      "loss": 1.0468,
      "step": 6046
    },
    {
      "epoch": 56.51871500147362,
      "grad_norm": 0.5375166535377502,
      "learning_rate": 8.386197167948766e-06,
      "loss": 1.0681,
      "step": 6047
    },
    {
      "epoch": 56.5281461833186,
      "grad_norm": 0.5439737439155579,
      "learning_rate": 8.383210043167767e-06,
      "loss": 1.0755,
      "step": 6048
    },
    {
      "epoch": 56.53757736516357,
      "grad_norm": 0.5471686720848083,
      "learning_rate": 8.380223066516524e-06,
      "loss": 1.0104,
      "step": 6049
    },
    {
      "epoch": 56.547008547008545,
      "grad_norm": 0.5645154118537903,
      "learning_rate": 8.377236238268711e-06,
      "loss": 1.0433,
      "step": 6050
    },
    {
      "epoch": 56.55643972885352,
      "grad_norm": 0.5069594383239746,
      "learning_rate": 8.374249558697978e-06,
      "loss": 1.0619,
      "step": 6051
    },
    {
      "epoch": 56.5658709106985,
      "grad_norm": 0.5016111135482788,
      "learning_rate": 8.37126302807796e-06,
      "loss": 1.0727,
      "step": 6052
    },
    {
      "epoch": 56.575302092543474,
      "grad_norm": 0.5217251181602478,
      "learning_rate": 8.368276646682283e-06,
      "loss": 1.0901,
      "step": 6053
    },
    {
      "epoch": 56.58473327438845,
      "grad_norm": 0.5086727142333984,
      "learning_rate": 8.365290414784559e-06,
      "loss": 1.0673,
      "step": 6054
    },
    {
      "epoch": 56.59416445623342,
      "grad_norm": 0.5168671011924744,
      "learning_rate": 8.362304332658389e-06,
      "loss": 1.0541,
      "step": 6055
    },
    {
      "epoch": 56.603595638078396,
      "grad_norm": 0.518144965171814,
      "learning_rate": 8.359318400577353e-06,
      "loss": 1.0685,
      "step": 6056
    },
    {
      "epoch": 56.61302681992337,
      "grad_norm": 0.5185763835906982,
      "learning_rate": 8.356332618815021e-06,
      "loss": 1.0915,
      "step": 6057
    },
    {
      "epoch": 56.622458001768344,
      "grad_norm": 0.5089523792266846,
      "learning_rate": 8.353346987644952e-06,
      "loss": 1.0547,
      "step": 6058
    },
    {
      "epoch": 56.631889183613325,
      "grad_norm": 0.5288191437721252,
      "learning_rate": 8.350361507340688e-06,
      "loss": 1.082,
      "step": 6059
    },
    {
      "epoch": 56.6413203654583,
      "grad_norm": 0.5395674705505371,
      "learning_rate": 8.347376178175754e-06,
      "loss": 1.0448,
      "step": 6060
    },
    {
      "epoch": 56.65075154730327,
      "grad_norm": 0.5759127140045166,
      "learning_rate": 8.344391000423663e-06,
      "loss": 1.0165,
      "step": 6061
    },
    {
      "epoch": 56.66018272914825,
      "grad_norm": 0.5406954884529114,
      "learning_rate": 8.341405974357927e-06,
      "loss": 1.0564,
      "step": 6062
    },
    {
      "epoch": 56.66961391099322,
      "grad_norm": 0.549105703830719,
      "learning_rate": 8.338421100252025e-06,
      "loss": 1.0386,
      "step": 6063
    },
    {
      "epoch": 56.679045092838194,
      "grad_norm": 0.5264132618904114,
      "learning_rate": 8.335436378379433e-06,
      "loss": 1.0267,
      "step": 6064
    },
    {
      "epoch": 56.68847627468317,
      "grad_norm": 0.5076972246170044,
      "learning_rate": 8.332451809013607e-06,
      "loss": 1.0241,
      "step": 6065
    },
    {
      "epoch": 56.69790745652815,
      "grad_norm": 0.5203927755355835,
      "learning_rate": 8.329467392427996e-06,
      "loss": 1.0652,
      "step": 6066
    },
    {
      "epoch": 56.70733863837312,
      "grad_norm": 0.5466968417167664,
      "learning_rate": 8.326483128896028e-06,
      "loss": 1.0679,
      "step": 6067
    },
    {
      "epoch": 56.7167698202181,
      "grad_norm": 0.5269997715950012,
      "learning_rate": 8.323499018691118e-06,
      "loss": 1.0696,
      "step": 6068
    },
    {
      "epoch": 56.72620100206307,
      "grad_norm": 0.5107957720756531,
      "learning_rate": 8.320515062086679e-06,
      "loss": 1.0213,
      "step": 6069
    },
    {
      "epoch": 56.735632183908045,
      "grad_norm": 0.5192292928695679,
      "learning_rate": 8.31753125935609e-06,
      "loss": 1.0407,
      "step": 6070
    },
    {
      "epoch": 56.74506336575302,
      "grad_norm": 0.567288875579834,
      "learning_rate": 8.314547610772734e-06,
      "loss": 1.0353,
      "step": 6071
    },
    {
      "epoch": 56.75449454759799,
      "grad_norm": 0.5541257262229919,
      "learning_rate": 8.311564116609965e-06,
      "loss": 1.0681,
      "step": 6072
    },
    {
      "epoch": 56.763925729442974,
      "grad_norm": 0.5410045981407166,
      "learning_rate": 8.308580777141136e-06,
      "loss": 1.0881,
      "step": 6073
    },
    {
      "epoch": 56.77335691128795,
      "grad_norm": 0.5459619164466858,
      "learning_rate": 8.305597592639574e-06,
      "loss": 1.0645,
      "step": 6074
    },
    {
      "epoch": 56.78278809313292,
      "grad_norm": 0.5168424248695374,
      "learning_rate": 8.302614563378597e-06,
      "loss": 1.0401,
      "step": 6075
    },
    {
      "epoch": 56.792219274977896,
      "grad_norm": 0.5321065783500671,
      "learning_rate": 8.299631689631512e-06,
      "loss": 1.0376,
      "step": 6076
    },
    {
      "epoch": 56.80165045682287,
      "grad_norm": 0.5126962661743164,
      "learning_rate": 8.296648971671612e-06,
      "loss": 1.0894,
      "step": 6077
    },
    {
      "epoch": 56.811081638667844,
      "grad_norm": 0.4973420798778534,
      "learning_rate": 8.293666409772168e-06,
      "loss": 1.0411,
      "step": 6078
    },
    {
      "epoch": 56.82051282051282,
      "grad_norm": 0.5269895195960999,
      "learning_rate": 8.290684004206443e-06,
      "loss": 1.0595,
      "step": 6079
    },
    {
      "epoch": 56.8299440023578,
      "grad_norm": 0.515717089176178,
      "learning_rate": 8.287701755247683e-06,
      "loss": 1.0579,
      "step": 6080
    },
    {
      "epoch": 56.83937518420277,
      "grad_norm": 0.4901271164417267,
      "learning_rate": 8.28471966316912e-06,
      "loss": 1.0962,
      "step": 6081
    },
    {
      "epoch": 56.84880636604775,
      "grad_norm": 0.5688396692276001,
      "learning_rate": 8.281737728243976e-06,
      "loss": 1.0312,
      "step": 6082
    },
    {
      "epoch": 56.85823754789272,
      "grad_norm": 0.5081729292869568,
      "learning_rate": 8.27875595074545e-06,
      "loss": 1.0752,
      "step": 6083
    },
    {
      "epoch": 56.867668729737694,
      "grad_norm": 0.5067452788352966,
      "learning_rate": 8.275774330946735e-06,
      "loss": 1.097,
      "step": 6084
    },
    {
      "epoch": 56.87709991158267,
      "grad_norm": 0.5525562763214111,
      "learning_rate": 8.272792869121006e-06,
      "loss": 1.0572,
      "step": 6085
    },
    {
      "epoch": 56.88653109342764,
      "grad_norm": 0.5559465289115906,
      "learning_rate": 8.269811565541424e-06,
      "loss": 1.0562,
      "step": 6086
    },
    {
      "epoch": 56.89596227527262,
      "grad_norm": 0.5078199505805969,
      "learning_rate": 8.266830420481132e-06,
      "loss": 1.0629,
      "step": 6087
    },
    {
      "epoch": 56.9053934571176,
      "grad_norm": 0.5182626247406006,
      "learning_rate": 8.263849434213263e-06,
      "loss": 1.0683,
      "step": 6088
    },
    {
      "epoch": 56.91482463896257,
      "grad_norm": 0.510762631893158,
      "learning_rate": 8.260868607010935e-06,
      "loss": 1.0955,
      "step": 6089
    },
    {
      "epoch": 56.924255820807545,
      "grad_norm": 0.5140824913978577,
      "learning_rate": 8.257887939147249e-06,
      "loss": 1.0586,
      "step": 6090
    },
    {
      "epoch": 56.93368700265252,
      "grad_norm": 0.5118964910507202,
      "learning_rate": 8.254907430895288e-06,
      "loss": 1.0454,
      "step": 6091
    },
    {
      "epoch": 56.94311818449749,
      "grad_norm": 0.49658170342445374,
      "learning_rate": 8.251927082528136e-06,
      "loss": 1.0994,
      "step": 6092
    },
    {
      "epoch": 56.95254936634247,
      "grad_norm": 0.489571213722229,
      "learning_rate": 8.248946894318845e-06,
      "loss": 1.0625,
      "step": 6093
    },
    {
      "epoch": 56.96198054818745,
      "grad_norm": 0.5455926060676575,
      "learning_rate": 8.24596686654046e-06,
      "loss": 1.0687,
      "step": 6094
    },
    {
      "epoch": 56.97141173003242,
      "grad_norm": 0.5195016860961914,
      "learning_rate": 8.242986999466012e-06,
      "loss": 1.0816,
      "step": 6095
    },
    {
      "epoch": 56.980842911877396,
      "grad_norm": 0.5278772711753845,
      "learning_rate": 8.240007293368511e-06,
      "loss": 1.0824,
      "step": 6096
    },
    {
      "epoch": 56.99027409372237,
      "grad_norm": 0.5080971717834473,
      "learning_rate": 8.23702774852096e-06,
      "loss": 1.0374,
      "step": 6097
    },
    {
      "epoch": 56.999705275567344,
      "grad_norm": 0.524503231048584,
      "learning_rate": 8.234048365196338e-06,
      "loss": 1.0901,
      "step": 6098
    },
    {
      "epoch": 57.0,
      "grad_norm": 3.087158203125,
      "learning_rate": 8.231069143667627e-06,
      "loss": 0.2948,
      "step": 6099
    },
    {
      "epoch": 57.009431181844974,
      "grad_norm": 0.5165675282478333,
      "learning_rate": 8.228090084207773e-06,
      "loss": 1.0165,
      "step": 6100
    },
    {
      "epoch": 57.01886236368995,
      "grad_norm": 0.5331564545631409,
      "learning_rate": 8.22511118708972e-06,
      "loss": 1.0923,
      "step": 6101
    },
    {
      "epoch": 57.02829354553492,
      "grad_norm": 0.5231208801269531,
      "learning_rate": 8.222132452586392e-06,
      "loss": 1.052,
      "step": 6102
    },
    {
      "epoch": 57.0377247273799,
      "grad_norm": 0.5044244527816772,
      "learning_rate": 8.2191538809707e-06,
      "loss": 1.0646,
      "step": 6103
    },
    {
      "epoch": 57.04715590922488,
      "grad_norm": 0.5219925045967102,
      "learning_rate": 8.21617547251554e-06,
      "loss": 1.0368,
      "step": 6104
    },
    {
      "epoch": 57.05658709106985,
      "grad_norm": 0.4929598867893219,
      "learning_rate": 8.213197227493791e-06,
      "loss": 1.0703,
      "step": 6105
    },
    {
      "epoch": 57.066018272914825,
      "grad_norm": 0.5087897777557373,
      "learning_rate": 8.21021914617832e-06,
      "loss": 1.0707,
      "step": 6106
    },
    {
      "epoch": 57.0754494547598,
      "grad_norm": 0.5275622010231018,
      "learning_rate": 8.20724122884198e-06,
      "loss": 1.103,
      "step": 6107
    },
    {
      "epoch": 57.08488063660477,
      "grad_norm": 0.4977910816669464,
      "learning_rate": 8.204263475757606e-06,
      "loss": 1.0522,
      "step": 6108
    },
    {
      "epoch": 57.09431181844975,
      "grad_norm": 0.5310483574867249,
      "learning_rate": 8.201285887198018e-06,
      "loss": 1.077,
      "step": 6109
    },
    {
      "epoch": 57.10374300029473,
      "grad_norm": 0.5253736972808838,
      "learning_rate": 8.198308463436018e-06,
      "loss": 1.0277,
      "step": 6110
    },
    {
      "epoch": 57.1131741821397,
      "grad_norm": 0.5484007000923157,
      "learning_rate": 8.195331204744403e-06,
      "loss": 1.0401,
      "step": 6111
    },
    {
      "epoch": 57.122605363984675,
      "grad_norm": 0.5158185958862305,
      "learning_rate": 8.192354111395946e-06,
      "loss": 1.0108,
      "step": 6112
    },
    {
      "epoch": 57.13203654582965,
      "grad_norm": 0.5180564522743225,
      "learning_rate": 8.189377183663404e-06,
      "loss": 1.0553,
      "step": 6113
    },
    {
      "epoch": 57.14146772767462,
      "grad_norm": 0.5072965621948242,
      "learning_rate": 8.186400421819528e-06,
      "loss": 1.0528,
      "step": 6114
    },
    {
      "epoch": 57.1508989095196,
      "grad_norm": 0.5205309987068176,
      "learning_rate": 8.183423826137042e-06,
      "loss": 1.0637,
      "step": 6115
    },
    {
      "epoch": 57.16033009136457,
      "grad_norm": 0.5002706050872803,
      "learning_rate": 8.180447396888667e-06,
      "loss": 1.0756,
      "step": 6116
    },
    {
      "epoch": 57.16976127320955,
      "grad_norm": 0.5605603456497192,
      "learning_rate": 8.177471134347098e-06,
      "loss": 1.0845,
      "step": 6117
    },
    {
      "epoch": 57.179192455054526,
      "grad_norm": 0.5252959728240967,
      "learning_rate": 8.17449503878502e-06,
      "loss": 1.0203,
      "step": 6118
    },
    {
      "epoch": 57.1886236368995,
      "grad_norm": 0.5271444320678711,
      "learning_rate": 8.171519110475104e-06,
      "loss": 1.0362,
      "step": 6119
    },
    {
      "epoch": 57.198054818744474,
      "grad_norm": 0.487072616815567,
      "learning_rate": 8.16854334969e-06,
      "loss": 1.0459,
      "step": 6120
    },
    {
      "epoch": 57.20748600058945,
      "grad_norm": 0.5170709490776062,
      "learning_rate": 8.165567756702343e-06,
      "loss": 1.0664,
      "step": 6121
    },
    {
      "epoch": 57.21691718243442,
      "grad_norm": 0.5503613948822021,
      "learning_rate": 8.162592331784767e-06,
      "loss": 1.0609,
      "step": 6122
    },
    {
      "epoch": 57.226348364279396,
      "grad_norm": 0.5175973176956177,
      "learning_rate": 8.159617075209874e-06,
      "loss": 1.0436,
      "step": 6123
    },
    {
      "epoch": 57.23577954612438,
      "grad_norm": 0.5229600071907043,
      "learning_rate": 8.156641987250254e-06,
      "loss": 1.0558,
      "step": 6124
    },
    {
      "epoch": 57.24521072796935,
      "grad_norm": 0.4990667998790741,
      "learning_rate": 8.153667068178484e-06,
      "loss": 1.0574,
      "step": 6125
    },
    {
      "epoch": 57.254641909814325,
      "grad_norm": 0.5186494588851929,
      "learning_rate": 8.150692318267125e-06,
      "loss": 1.0681,
      "step": 6126
    },
    {
      "epoch": 57.2640730916593,
      "grad_norm": 0.5363268852233887,
      "learning_rate": 8.147717737788728e-06,
      "loss": 1.0654,
      "step": 6127
    },
    {
      "epoch": 57.27350427350427,
      "grad_norm": 0.5061169862747192,
      "learning_rate": 8.14474332701581e-06,
      "loss": 1.0875,
      "step": 6128
    },
    {
      "epoch": 57.28293545534925,
      "grad_norm": 0.5344192385673523,
      "learning_rate": 8.1417690862209e-06,
      "loss": 1.0792,
      "step": 6129
    },
    {
      "epoch": 57.29236663719422,
      "grad_norm": 0.5154991149902344,
      "learning_rate": 8.138795015676493e-06,
      "loss": 1.0456,
      "step": 6130
    },
    {
      "epoch": 57.3017978190392,
      "grad_norm": 0.5861847996711731,
      "learning_rate": 8.135821115655068e-06,
      "loss": 1.0614,
      "step": 6131
    },
    {
      "epoch": 57.311229000884175,
      "grad_norm": 0.5340208411216736,
      "learning_rate": 8.132847386429095e-06,
      "loss": 1.0878,
      "step": 6132
    },
    {
      "epoch": 57.32066018272915,
      "grad_norm": 0.5448281168937683,
      "learning_rate": 8.12987382827103e-06,
      "loss": 1.0526,
      "step": 6133
    },
    {
      "epoch": 57.33009136457412,
      "grad_norm": 0.5642796158790588,
      "learning_rate": 8.126900441453301e-06,
      "loss": 1.036,
      "step": 6134
    },
    {
      "epoch": 57.3395225464191,
      "grad_norm": 0.5791909694671631,
      "learning_rate": 8.123927226248332e-06,
      "loss": 1.0735,
      "step": 6135
    },
    {
      "epoch": 57.34895372826407,
      "grad_norm": 0.5043903589248657,
      "learning_rate": 8.120954182928533e-06,
      "loss": 1.0458,
      "step": 6136
    },
    {
      "epoch": 57.358384910109045,
      "grad_norm": 0.5190753936767578,
      "learning_rate": 8.11798131176629e-06,
      "loss": 1.0582,
      "step": 6137
    },
    {
      "epoch": 57.367816091954026,
      "grad_norm": 0.5086060166358948,
      "learning_rate": 8.115008613033976e-06,
      "loss": 1.056,
      "step": 6138
    },
    {
      "epoch": 57.377247273799,
      "grad_norm": 0.5420635342597961,
      "learning_rate": 8.112036087003948e-06,
      "loss": 1.0437,
      "step": 6139
    },
    {
      "epoch": 57.386678455643974,
      "grad_norm": 0.5441256761550903,
      "learning_rate": 8.10906373394855e-06,
      "loss": 1.0803,
      "step": 6140
    },
    {
      "epoch": 57.39610963748895,
      "grad_norm": 0.5651511549949646,
      "learning_rate": 8.106091554140103e-06,
      "loss": 1.0721,
      "step": 6141
    },
    {
      "epoch": 57.40554081933392,
      "grad_norm": 0.5455741882324219,
      "learning_rate": 8.103119547850924e-06,
      "loss": 1.0828,
      "step": 6142
    },
    {
      "epoch": 57.414972001178896,
      "grad_norm": 0.5302711129188538,
      "learning_rate": 8.100147715353303e-06,
      "loss": 1.0977,
      "step": 6143
    },
    {
      "epoch": 57.42440318302387,
      "grad_norm": 0.5283301472663879,
      "learning_rate": 8.09717605691952e-06,
      "loss": 1.0397,
      "step": 6144
    },
    {
      "epoch": 57.43383436486885,
      "grad_norm": 0.5062282085418701,
      "learning_rate": 8.094204572821837e-06,
      "loss": 1.0606,
      "step": 6145
    },
    {
      "epoch": 57.443265546713825,
      "grad_norm": 0.5351219773292542,
      "learning_rate": 8.091233263332497e-06,
      "loss": 1.0417,
      "step": 6146
    },
    {
      "epoch": 57.4526967285588,
      "grad_norm": 0.5230951309204102,
      "learning_rate": 8.088262128723737e-06,
      "loss": 1.066,
      "step": 6147
    },
    {
      "epoch": 57.46212791040377,
      "grad_norm": 0.5202221274375916,
      "learning_rate": 8.085291169267768e-06,
      "loss": 1.0676,
      "step": 6148
    },
    {
      "epoch": 57.47155909224875,
      "grad_norm": 0.5408967137336731,
      "learning_rate": 8.082320385236787e-06,
      "loss": 1.035,
      "step": 6149
    },
    {
      "epoch": 57.48099027409372,
      "grad_norm": 0.5451923608779907,
      "learning_rate": 8.079349776902976e-06,
      "loss": 1.0562,
      "step": 6150
    },
    {
      "epoch": 57.490421455938694,
      "grad_norm": 0.5000051856040955,
      "learning_rate": 8.076379344538504e-06,
      "loss": 1.0832,
      "step": 6151
    },
    {
      "epoch": 57.499852637783675,
      "grad_norm": 0.5632057785987854,
      "learning_rate": 8.07340908841552e-06,
      "loss": 1.0819,
      "step": 6152
    },
    {
      "epoch": 57.50928381962865,
      "grad_norm": 0.4980539083480835,
      "learning_rate": 8.070439008806158e-06,
      "loss": 1.085,
      "step": 6153
    },
    {
      "epoch": 57.51871500147362,
      "grad_norm": 0.5413488745689392,
      "learning_rate": 8.067469105982535e-06,
      "loss": 1.0473,
      "step": 6154
    },
    {
      "epoch": 57.5281461833186,
      "grad_norm": 0.516371488571167,
      "learning_rate": 8.064499380216752e-06,
      "loss": 1.0801,
      "step": 6155
    },
    {
      "epoch": 57.53757736516357,
      "grad_norm": 0.5373052358627319,
      "learning_rate": 8.061529831780896e-06,
      "loss": 1.1003,
      "step": 6156
    },
    {
      "epoch": 57.547008547008545,
      "grad_norm": 0.5463024377822876,
      "learning_rate": 8.058560460947034e-06,
      "loss": 1.0601,
      "step": 6157
    },
    {
      "epoch": 57.55643972885352,
      "grad_norm": 0.5356425046920776,
      "learning_rate": 8.055591267987218e-06,
      "loss": 1.0867,
      "step": 6158
    },
    {
      "epoch": 57.5658709106985,
      "grad_norm": 0.5094578266143799,
      "learning_rate": 8.05262225317349e-06,
      "loss": 1.0491,
      "step": 6159
    },
    {
      "epoch": 57.575302092543474,
      "grad_norm": 0.533044159412384,
      "learning_rate": 8.049653416777864e-06,
      "loss": 1.024,
      "step": 6160
    },
    {
      "epoch": 57.58473327438845,
      "grad_norm": 0.5146127939224243,
      "learning_rate": 8.046684759072347e-06,
      "loss": 1.0483,
      "step": 6161
    },
    {
      "epoch": 57.59416445623342,
      "grad_norm": 0.5280177593231201,
      "learning_rate": 8.043716280328928e-06,
      "loss": 1.0295,
      "step": 6162
    },
    {
      "epoch": 57.603595638078396,
      "grad_norm": 0.5033844113349915,
      "learning_rate": 8.040747980819575e-06,
      "loss": 1.0591,
      "step": 6163
    },
    {
      "epoch": 57.61302681992337,
      "grad_norm": 0.5085586309432983,
      "learning_rate": 8.037779860816242e-06,
      "loss": 1.0581,
      "step": 6164
    },
    {
      "epoch": 57.622458001768344,
      "grad_norm": 0.5322694182395935,
      "learning_rate": 8.034811920590864e-06,
      "loss": 1.0359,
      "step": 6165
    },
    {
      "epoch": 57.631889183613325,
      "grad_norm": 0.4970359206199646,
      "learning_rate": 8.031844160415372e-06,
      "loss": 1.0449,
      "step": 6166
    },
    {
      "epoch": 57.6413203654583,
      "grad_norm": 0.5204771161079407,
      "learning_rate": 8.028876580561667e-06,
      "loss": 1.0632,
      "step": 6167
    },
    {
      "epoch": 57.65075154730327,
      "grad_norm": 0.5658107399940491,
      "learning_rate": 8.025909181301638e-06,
      "loss": 1.0179,
      "step": 6168
    },
    {
      "epoch": 57.66018272914825,
      "grad_norm": 0.5616507530212402,
      "learning_rate": 8.022941962907156e-06,
      "loss": 1.0711,
      "step": 6169
    },
    {
      "epoch": 57.66961391099322,
      "grad_norm": 0.5133965611457825,
      "learning_rate": 8.019974925650075e-06,
      "loss": 1.1133,
      "step": 6170
    },
    {
      "epoch": 57.679045092838194,
      "grad_norm": 0.5703948736190796,
      "learning_rate": 8.017008069802237e-06,
      "loss": 1.0599,
      "step": 6171
    },
    {
      "epoch": 57.68847627468317,
      "grad_norm": 0.5196036696434021,
      "learning_rate": 8.014041395635461e-06,
      "loss": 1.1041,
      "step": 6172
    },
    {
      "epoch": 57.69790745652815,
      "grad_norm": 0.545894980430603,
      "learning_rate": 8.011074903421556e-06,
      "loss": 1.0579,
      "step": 6173
    },
    {
      "epoch": 57.70733863837312,
      "grad_norm": 0.5600831508636475,
      "learning_rate": 8.00810859343231e-06,
      "loss": 1.1042,
      "step": 6174
    },
    {
      "epoch": 57.7167698202181,
      "grad_norm": 0.5162600874900818,
      "learning_rate": 8.005142465939496e-06,
      "loss": 1.0307,
      "step": 6175
    },
    {
      "epoch": 57.72620100206307,
      "grad_norm": 0.5151146650314331,
      "learning_rate": 8.002176521214869e-06,
      "loss": 1.0391,
      "step": 6176
    },
    {
      "epoch": 57.735632183908045,
      "grad_norm": 0.5833246111869812,
      "learning_rate": 7.999210759530165e-06,
      "loss": 1.0365,
      "step": 6177
    },
    {
      "epoch": 57.74506336575302,
      "grad_norm": 0.5019382238388062,
      "learning_rate": 7.996245181157111e-06,
      "loss": 1.0308,
      "step": 6178
    },
    {
      "epoch": 57.75449454759799,
      "grad_norm": 0.5083023905754089,
      "learning_rate": 7.993279786367412e-06,
      "loss": 1.0383,
      "step": 6179
    },
    {
      "epoch": 57.763925729442974,
      "grad_norm": 0.5358641743659973,
      "learning_rate": 7.990314575432751e-06,
      "loss": 1.0397,
      "step": 6180
    },
    {
      "epoch": 57.77335691128795,
      "grad_norm": 0.5348503589630127,
      "learning_rate": 7.987349548624805e-06,
      "loss": 1.0503,
      "step": 6181
    },
    {
      "epoch": 57.78278809313292,
      "grad_norm": 0.5525509715080261,
      "learning_rate": 7.984384706215226e-06,
      "loss": 1.0849,
      "step": 6182
    },
    {
      "epoch": 57.792219274977896,
      "grad_norm": 0.5598542094230652,
      "learning_rate": 7.981420048475654e-06,
      "loss": 0.9992,
      "step": 6183
    },
    {
      "epoch": 57.80165045682287,
      "grad_norm": 0.512935221195221,
      "learning_rate": 7.97845557567771e-06,
      "loss": 1.0439,
      "step": 6184
    },
    {
      "epoch": 57.811081638667844,
      "grad_norm": 0.5292835831642151,
      "learning_rate": 7.975491288092997e-06,
      "loss": 1.051,
      "step": 6185
    },
    {
      "epoch": 57.82051282051282,
      "grad_norm": 0.526441752910614,
      "learning_rate": 7.972527185993101e-06,
      "loss": 1.0537,
      "step": 6186
    },
    {
      "epoch": 57.8299440023578,
      "grad_norm": 0.5613176226615906,
      "learning_rate": 7.969563269649594e-06,
      "loss": 1.0747,
      "step": 6187
    },
    {
      "epoch": 57.83937518420277,
      "grad_norm": 0.5098854303359985,
      "learning_rate": 7.966599539334023e-06,
      "loss": 1.0332,
      "step": 6188
    },
    {
      "epoch": 57.84880636604775,
      "grad_norm": 0.4996834397315979,
      "learning_rate": 7.963635995317933e-06,
      "loss": 1.0657,
      "step": 6189
    },
    {
      "epoch": 57.85823754789272,
      "grad_norm": 0.5315017700195312,
      "learning_rate": 7.96067263787284e-06,
      "loss": 1.0372,
      "step": 6190
    },
    {
      "epoch": 57.867668729737694,
      "grad_norm": 0.5192263126373291,
      "learning_rate": 7.957709467270243e-06,
      "loss": 1.0605,
      "step": 6191
    },
    {
      "epoch": 57.87709991158267,
      "grad_norm": 0.5234838128089905,
      "learning_rate": 7.95474648378163e-06,
      "loss": 1.0595,
      "step": 6192
    },
    {
      "epoch": 57.88653109342764,
      "grad_norm": 0.533340334892273,
      "learning_rate": 7.951783687678468e-06,
      "loss": 1.0699,
      "step": 6193
    },
    {
      "epoch": 57.89596227527262,
      "grad_norm": 0.5704857110977173,
      "learning_rate": 7.948821079232204e-06,
      "loss": 1.0475,
      "step": 6194
    },
    {
      "epoch": 57.9053934571176,
      "grad_norm": 0.5198240280151367,
      "learning_rate": 7.94585865871427e-06,
      "loss": 1.063,
      "step": 6195
    },
    {
      "epoch": 57.91482463896257,
      "grad_norm": 0.5396211743354797,
      "learning_rate": 7.942896426396091e-06,
      "loss": 1.0835,
      "step": 6196
    },
    {
      "epoch": 57.924255820807545,
      "grad_norm": 0.5568252205848694,
      "learning_rate": 7.93993438254906e-06,
      "loss": 1.0358,
      "step": 6197
    },
    {
      "epoch": 57.93368700265252,
      "grad_norm": 0.5360851883888245,
      "learning_rate": 7.936972527444559e-06,
      "loss": 1.0694,
      "step": 6198
    },
    {
      "epoch": 57.94311818449749,
      "grad_norm": 0.5220085978507996,
      "learning_rate": 7.934010861353951e-06,
      "loss": 1.0777,
      "step": 6199
    },
    {
      "epoch": 57.95254936634247,
      "grad_norm": 0.5624609589576721,
      "learning_rate": 7.931049384548585e-06,
      "loss": 1.0225,
      "step": 6200
    },
    {
      "epoch": 57.96198054818745,
      "grad_norm": 0.5394035577774048,
      "learning_rate": 7.92808809729979e-06,
      "loss": 1.0526,
      "step": 6201
    },
    {
      "epoch": 57.97141173003242,
      "grad_norm": 0.5072095394134521,
      "learning_rate": 7.925126999878876e-06,
      "loss": 1.0397,
      "step": 6202
    },
    {
      "epoch": 57.980842911877396,
      "grad_norm": 0.5145644545555115,
      "learning_rate": 7.922166092557137e-06,
      "loss": 1.0461,
      "step": 6203
    },
    {
      "epoch": 57.99027409372237,
      "grad_norm": 0.5634089112281799,
      "learning_rate": 7.919205375605855e-06,
      "loss": 1.0118,
      "step": 6204
    },
    {
      "epoch": 57.999705275567344,
      "grad_norm": 0.53557288646698,
      "learning_rate": 7.916244849296288e-06,
      "loss": 1.0417,
      "step": 6205
    },
    {
      "epoch": 58.0,
      "grad_norm": 3.736193895339966,
      "learning_rate": 7.913284513899679e-06,
      "loss": 0.8231,
      "step": 6206
    },
    {
      "epoch": 58.009431181844974,
      "grad_norm": 0.5610001683235168,
      "learning_rate": 7.91032436968725e-06,
      "loss": 1.0255,
      "step": 6207
    },
    {
      "epoch": 58.01886236368995,
      "grad_norm": 0.5411713123321533,
      "learning_rate": 7.907364416930213e-06,
      "loss": 1.0527,
      "step": 6208
    },
    {
      "epoch": 58.02829354553492,
      "grad_norm": 0.5128193497657776,
      "learning_rate": 7.904404655899754e-06,
      "loss": 1.0721,
      "step": 6209
    },
    {
      "epoch": 58.0377247273799,
      "grad_norm": 0.5285453200340271,
      "learning_rate": 7.901445086867044e-06,
      "loss": 1.0752,
      "step": 6210
    },
    {
      "epoch": 58.04715590922488,
      "grad_norm": 0.5169559717178345,
      "learning_rate": 7.898485710103244e-06,
      "loss": 1.0474,
      "step": 6211
    },
    {
      "epoch": 58.05658709106985,
      "grad_norm": 0.524869441986084,
      "learning_rate": 7.895526525879484e-06,
      "loss": 1.0473,
      "step": 6212
    },
    {
      "epoch": 58.066018272914825,
      "grad_norm": 0.5324226021766663,
      "learning_rate": 7.892567534466891e-06,
      "loss": 1.0632,
      "step": 6213
    },
    {
      "epoch": 58.0754494547598,
      "grad_norm": 0.5204309821128845,
      "learning_rate": 7.889608736136561e-06,
      "loss": 1.0503,
      "step": 6214
    },
    {
      "epoch": 58.08488063660477,
      "grad_norm": 0.5165776610374451,
      "learning_rate": 7.886650131159581e-06,
      "loss": 1.0729,
      "step": 6215
    },
    {
      "epoch": 58.09431181844975,
      "grad_norm": 0.48676031827926636,
      "learning_rate": 7.883691719807015e-06,
      "loss": 1.0643,
      "step": 6216
    },
    {
      "epoch": 58.10374300029473,
      "grad_norm": 0.535576343536377,
      "learning_rate": 7.880733502349916e-06,
      "loss": 1.1129,
      "step": 6217
    },
    {
      "epoch": 58.1131741821397,
      "grad_norm": 0.5166003704071045,
      "learning_rate": 7.877775479059305e-06,
      "loss": 1.0669,
      "step": 6218
    },
    {
      "epoch": 58.122605363984675,
      "grad_norm": 0.5053492188453674,
      "learning_rate": 7.874817650206207e-06,
      "loss": 1.0207,
      "step": 6219
    },
    {
      "epoch": 58.13203654582965,
      "grad_norm": 0.4888259172439575,
      "learning_rate": 7.871860016061612e-06,
      "loss": 1.043,
      "step": 6220
    },
    {
      "epoch": 58.14146772767462,
      "grad_norm": 0.5045031905174255,
      "learning_rate": 7.868902576896497e-06,
      "loss": 1.0689,
      "step": 6221
    },
    {
      "epoch": 58.1508989095196,
      "grad_norm": 0.5057181715965271,
      "learning_rate": 7.865945332981825e-06,
      "loss": 1.0182,
      "step": 6222
    },
    {
      "epoch": 58.16033009136457,
      "grad_norm": 0.5104877352714539,
      "learning_rate": 7.862988284588533e-06,
      "loss": 1.0977,
      "step": 6223
    },
    {
      "epoch": 58.16976127320955,
      "grad_norm": 0.4853367507457733,
      "learning_rate": 7.860031431987548e-06,
      "loss": 1.0576,
      "step": 6224
    },
    {
      "epoch": 58.179192455054526,
      "grad_norm": 0.5125201940536499,
      "learning_rate": 7.85707477544977e-06,
      "loss": 1.0908,
      "step": 6225
    },
    {
      "epoch": 58.1886236368995,
      "grad_norm": 0.5325952768325806,
      "learning_rate": 7.854118315246096e-06,
      "loss": 1.0475,
      "step": 6226
    },
    {
      "epoch": 58.198054818744474,
      "grad_norm": 0.5048444867134094,
      "learning_rate": 7.851162051647391e-06,
      "loss": 1.0672,
      "step": 6227
    },
    {
      "epoch": 58.20748600058945,
      "grad_norm": 0.5638138055801392,
      "learning_rate": 7.848205984924508e-06,
      "loss": 1.0236,
      "step": 6228
    },
    {
      "epoch": 58.21691718243442,
      "grad_norm": 0.5160195827484131,
      "learning_rate": 7.84525011534828e-06,
      "loss": 1.0748,
      "step": 6229
    },
    {
      "epoch": 58.226348364279396,
      "grad_norm": 0.5490133762359619,
      "learning_rate": 7.842294443189525e-06,
      "loss": 1.0624,
      "step": 6230
    },
    {
      "epoch": 58.23577954612438,
      "grad_norm": 0.5607509613037109,
      "learning_rate": 7.839338968719036e-06,
      "loss": 1.0927,
      "step": 6231
    },
    {
      "epoch": 58.24521072796935,
      "grad_norm": 0.5063537955284119,
      "learning_rate": 7.836383692207597e-06,
      "loss": 1.0554,
      "step": 6232
    },
    {
      "epoch": 58.254641909814325,
      "grad_norm": 0.5260264873504639,
      "learning_rate": 7.833428613925962e-06,
      "loss": 1.0733,
      "step": 6233
    },
    {
      "epoch": 58.2640730916593,
      "grad_norm": 0.5313825607299805,
      "learning_rate": 7.830473734144885e-06,
      "loss": 1.066,
      "step": 6234
    },
    {
      "epoch": 58.27350427350427,
      "grad_norm": 0.5428470969200134,
      "learning_rate": 7.827519053135088e-06,
      "loss": 1.0526,
      "step": 6235
    },
    {
      "epoch": 58.28293545534925,
      "grad_norm": 0.5252209901809692,
      "learning_rate": 7.824564571167275e-06,
      "loss": 1.0572,
      "step": 6236
    },
    {
      "epoch": 58.29236663719422,
      "grad_norm": 0.5091292858123779,
      "learning_rate": 7.821610288512135e-06,
      "loss": 1.0591,
      "step": 6237
    },
    {
      "epoch": 58.3017978190392,
      "grad_norm": 0.5246278047561646,
      "learning_rate": 7.818656205440338e-06,
      "loss": 1.0755,
      "step": 6238
    },
    {
      "epoch": 58.311229000884175,
      "grad_norm": 0.5110326409339905,
      "learning_rate": 7.815702322222539e-06,
      "loss": 1.0507,
      "step": 6239
    },
    {
      "epoch": 58.32066018272915,
      "grad_norm": 0.49807974696159363,
      "learning_rate": 7.812748639129368e-06,
      "loss": 1.0404,
      "step": 6240
    },
    {
      "epoch": 58.33009136457412,
      "grad_norm": 0.5191144943237305,
      "learning_rate": 7.809795156431445e-06,
      "loss": 1.0298,
      "step": 6241
    },
    {
      "epoch": 58.3395225464191,
      "grad_norm": 0.5272935628890991,
      "learning_rate": 7.806841874399364e-06,
      "loss": 1.0522,
      "step": 6242
    },
    {
      "epoch": 58.34895372826407,
      "grad_norm": 0.5491692423820496,
      "learning_rate": 7.803888793303701e-06,
      "loss": 1.0333,
      "step": 6243
    },
    {
      "epoch": 58.358384910109045,
      "grad_norm": 0.5394987463951111,
      "learning_rate": 7.800935913415025e-06,
      "loss": 1.0562,
      "step": 6244
    },
    {
      "epoch": 58.367816091954026,
      "grad_norm": 0.5253018140792847,
      "learning_rate": 7.797983235003873e-06,
      "loss": 1.0505,
      "step": 6245
    },
    {
      "epoch": 58.377247273799,
      "grad_norm": 0.5317978262901306,
      "learning_rate": 7.795030758340765e-06,
      "loss": 1.0401,
      "step": 6246
    },
    {
      "epoch": 58.386678455643974,
      "grad_norm": 0.5320402383804321,
      "learning_rate": 7.792078483696209e-06,
      "loss": 1.0593,
      "step": 6247
    },
    {
      "epoch": 58.39610963748895,
      "grad_norm": 0.4988127648830414,
      "learning_rate": 7.789126411340688e-06,
      "loss": 1.051,
      "step": 6248
    },
    {
      "epoch": 58.40554081933392,
      "grad_norm": 0.5442821979522705,
      "learning_rate": 7.78617454154468e-06,
      "loss": 1.0838,
      "step": 6249
    },
    {
      "epoch": 58.414972001178896,
      "grad_norm": 0.5315388441085815,
      "learning_rate": 7.783222874578623e-06,
      "loss": 1.0454,
      "step": 6250
    },
    {
      "epoch": 58.42440318302387,
      "grad_norm": 0.5222861766815186,
      "learning_rate": 7.780271410712955e-06,
      "loss": 1.0461,
      "step": 6251
    },
    {
      "epoch": 58.43383436486885,
      "grad_norm": 0.5214449167251587,
      "learning_rate": 7.777320150218087e-06,
      "loss": 1.0083,
      "step": 6252
    },
    {
      "epoch": 58.443265546713825,
      "grad_norm": 0.5155392289161682,
      "learning_rate": 7.77436909336441e-06,
      "loss": 1.0784,
      "step": 6253
    },
    {
      "epoch": 58.4526967285588,
      "grad_norm": 0.5968552827835083,
      "learning_rate": 7.771418240422297e-06,
      "loss": 1.0424,
      "step": 6254
    },
    {
      "epoch": 58.46212791040377,
      "grad_norm": 0.5161944627761841,
      "learning_rate": 7.768467591662105e-06,
      "loss": 1.0874,
      "step": 6255
    },
    {
      "epoch": 58.47155909224875,
      "grad_norm": 0.5735148787498474,
      "learning_rate": 7.765517147354178e-06,
      "loss": 1.0539,
      "step": 6256
    },
    {
      "epoch": 58.48099027409372,
      "grad_norm": 0.5275980830192566,
      "learning_rate": 7.76256690776883e-06,
      "loss": 1.043,
      "step": 6257
    },
    {
      "epoch": 58.490421455938694,
      "grad_norm": 0.5540984272956848,
      "learning_rate": 7.759616873176361e-06,
      "loss": 1.0203,
      "step": 6258
    },
    {
      "epoch": 58.499852637783675,
      "grad_norm": 0.5212201476097107,
      "learning_rate": 7.756667043847053e-06,
      "loss": 1.0678,
      "step": 6259
    },
    {
      "epoch": 58.50928381962865,
      "grad_norm": 0.5171808004379272,
      "learning_rate": 7.753717420051167e-06,
      "loss": 1.0385,
      "step": 6260
    },
    {
      "epoch": 58.51871500147362,
      "grad_norm": 0.5537300705909729,
      "learning_rate": 7.750768002058947e-06,
      "loss": 1.023,
      "step": 6261
    },
    {
      "epoch": 58.5281461833186,
      "grad_norm": 0.5457785725593567,
      "learning_rate": 7.747818790140614e-06,
      "loss": 1.0754,
      "step": 6262
    },
    {
      "epoch": 58.53757736516357,
      "grad_norm": 0.5513458847999573,
      "learning_rate": 7.744869784566383e-06,
      "loss": 1.087,
      "step": 6263
    },
    {
      "epoch": 58.547008547008545,
      "grad_norm": 0.5470026731491089,
      "learning_rate": 7.741920985606436e-06,
      "loss": 1.0729,
      "step": 6264
    },
    {
      "epoch": 58.55643972885352,
      "grad_norm": 0.5249820351600647,
      "learning_rate": 7.73897239353094e-06,
      "loss": 1.0438,
      "step": 6265
    },
    {
      "epoch": 58.5658709106985,
      "grad_norm": 0.5561424493789673,
      "learning_rate": 7.736024008610043e-06,
      "loss": 1.0486,
      "step": 6266
    },
    {
      "epoch": 58.575302092543474,
      "grad_norm": 0.5380720496177673,
      "learning_rate": 7.733075831113878e-06,
      "loss": 1.0717,
      "step": 6267
    },
    {
      "epoch": 58.58473327438845,
      "grad_norm": 0.5774859189987183,
      "learning_rate": 7.730127861312557e-06,
      "loss": 1.039,
      "step": 6268
    },
    {
      "epoch": 58.59416445623342,
      "grad_norm": 0.5953109264373779,
      "learning_rate": 7.727180099476166e-06,
      "loss": 1.0649,
      "step": 6269
    },
    {
      "epoch": 58.603595638078396,
      "grad_norm": 0.55363529920578,
      "learning_rate": 7.724232545874781e-06,
      "loss": 1.0321,
      "step": 6270
    },
    {
      "epoch": 58.61302681992337,
      "grad_norm": 0.5397446751594543,
      "learning_rate": 7.72128520077846e-06,
      "loss": 1.0574,
      "step": 6271
    },
    {
      "epoch": 58.622458001768344,
      "grad_norm": 0.5527923107147217,
      "learning_rate": 7.718338064457235e-06,
      "loss": 1.0275,
      "step": 6272
    },
    {
      "epoch": 58.631889183613325,
      "grad_norm": 0.5849611163139343,
      "learning_rate": 7.71539113718112e-06,
      "loss": 1.081,
      "step": 6273
    },
    {
      "epoch": 58.6413203654583,
      "grad_norm": 0.5295896530151367,
      "learning_rate": 7.71244441922011e-06,
      "loss": 1.0933,
      "step": 6274
    },
    {
      "epoch": 58.65075154730327,
      "grad_norm": 0.5434001088142395,
      "learning_rate": 7.70949791084419e-06,
      "loss": 1.0128,
      "step": 6275
    },
    {
      "epoch": 58.66018272914825,
      "grad_norm": 0.5109572410583496,
      "learning_rate": 7.70655161232331e-06,
      "loss": 1.0699,
      "step": 6276
    },
    {
      "epoch": 58.66961391099322,
      "grad_norm": 0.5866798758506775,
      "learning_rate": 7.703605523927412e-06,
      "loss": 1.0674,
      "step": 6277
    },
    {
      "epoch": 58.679045092838194,
      "grad_norm": 0.4896767735481262,
      "learning_rate": 7.700659645926418e-06,
      "loss": 1.0334,
      "step": 6278
    },
    {
      "epoch": 58.68847627468317,
      "grad_norm": 0.4830530881881714,
      "learning_rate": 7.697713978590222e-06,
      "loss": 1.0924,
      "step": 6279
    },
    {
      "epoch": 58.69790745652815,
      "grad_norm": 0.5637204647064209,
      "learning_rate": 7.694768522188711e-06,
      "loss": 1.0605,
      "step": 6280
    },
    {
      "epoch": 58.70733863837312,
      "grad_norm": 0.5267741680145264,
      "learning_rate": 7.691823276991747e-06,
      "loss": 1.0469,
      "step": 6281
    },
    {
      "epoch": 58.7167698202181,
      "grad_norm": 0.536346435546875,
      "learning_rate": 7.688878243269169e-06,
      "loss": 1.1013,
      "step": 6282
    },
    {
      "epoch": 58.72620100206307,
      "grad_norm": 0.530522882938385,
      "learning_rate": 7.6859334212908e-06,
      "loss": 1.0513,
      "step": 6283
    },
    {
      "epoch": 58.735632183908045,
      "grad_norm": 0.5699220895767212,
      "learning_rate": 7.68298881132645e-06,
      "loss": 1.0115,
      "step": 6284
    },
    {
      "epoch": 58.74506336575302,
      "grad_norm": 0.5488550662994385,
      "learning_rate": 7.680044413645888e-06,
      "loss": 1.0817,
      "step": 6285
    },
    {
      "epoch": 58.75449454759799,
      "grad_norm": 0.5513995885848999,
      "learning_rate": 7.677100228518894e-06,
      "loss": 1.0329,
      "step": 6286
    },
    {
      "epoch": 58.763925729442974,
      "grad_norm": 0.5321407318115234,
      "learning_rate": 7.67415625621521e-06,
      "loss": 0.9962,
      "step": 6287
    },
    {
      "epoch": 58.77335691128795,
      "grad_norm": 0.5570985078811646,
      "learning_rate": 7.671212497004559e-06,
      "loss": 1.0501,
      "step": 6288
    },
    {
      "epoch": 58.78278809313292,
      "grad_norm": 0.5302680730819702,
      "learning_rate": 7.668268951156647e-06,
      "loss": 1.0632,
      "step": 6289
    },
    {
      "epoch": 58.792219274977896,
      "grad_norm": 0.5306194424629211,
      "learning_rate": 7.665325618941163e-06,
      "loss": 1.0843,
      "step": 6290
    },
    {
      "epoch": 58.80165045682287,
      "grad_norm": 0.5466466546058655,
      "learning_rate": 7.662382500627774e-06,
      "loss": 1.0858,
      "step": 6291
    },
    {
      "epoch": 58.811081638667844,
      "grad_norm": 0.5180985331535339,
      "learning_rate": 7.65943959648612e-06,
      "loss": 1.0265,
      "step": 6292
    },
    {
      "epoch": 58.82051282051282,
      "grad_norm": 0.5164006352424622,
      "learning_rate": 7.65649690678584e-06,
      "loss": 1.0682,
      "step": 6293
    },
    {
      "epoch": 58.8299440023578,
      "grad_norm": 0.517412543296814,
      "learning_rate": 7.653554431796539e-06,
      "loss": 1.0827,
      "step": 6294
    },
    {
      "epoch": 58.83937518420277,
      "grad_norm": 0.5461874604225159,
      "learning_rate": 7.650612171787803e-06,
      "loss": 1.058,
      "step": 6295
    },
    {
      "epoch": 58.84880636604775,
      "grad_norm": 0.5069615840911865,
      "learning_rate": 7.647670127029202e-06,
      "loss": 1.0348,
      "step": 6296
    },
    {
      "epoch": 58.85823754789272,
      "grad_norm": 0.55958092212677,
      "learning_rate": 7.644728297790285e-06,
      "loss": 1.084,
      "step": 6297
    },
    {
      "epoch": 58.867668729737694,
      "grad_norm": 0.5470202565193176,
      "learning_rate": 7.641786684340582e-06,
      "loss": 1.0488,
      "step": 6298
    },
    {
      "epoch": 58.87709991158267,
      "grad_norm": 0.5374450087547302,
      "learning_rate": 7.638845286949602e-06,
      "loss": 1.0956,
      "step": 6299
    },
    {
      "epoch": 58.88653109342764,
      "grad_norm": 0.4755570590496063,
      "learning_rate": 7.635904105886832e-06,
      "loss": 1.0579,
      "step": 6300
    },
    {
      "epoch": 58.89596227527262,
      "grad_norm": 0.5473742485046387,
      "learning_rate": 7.63296314142175e-06,
      "loss": 1.0325,
      "step": 6301
    },
    {
      "epoch": 58.9053934571176,
      "grad_norm": 0.5431772470474243,
      "learning_rate": 7.630022393823798e-06,
      "loss": 1.0585,
      "step": 6302
    },
    {
      "epoch": 58.91482463896257,
      "grad_norm": 0.5250309109687805,
      "learning_rate": 7.6270818633624125e-06,
      "loss": 1.0257,
      "step": 6303
    },
    {
      "epoch": 58.924255820807545,
      "grad_norm": 0.5525118112564087,
      "learning_rate": 7.624141550307e-06,
      "loss": 1.0496,
      "step": 6304
    },
    {
      "epoch": 58.93368700265252,
      "grad_norm": 0.5182052254676819,
      "learning_rate": 7.62120145492695e-06,
      "loss": 1.0333,
      "step": 6305
    },
    {
      "epoch": 58.94311818449749,
      "grad_norm": 0.5085103511810303,
      "learning_rate": 7.618261577491637e-06,
      "loss": 1.0778,
      "step": 6306
    },
    {
      "epoch": 58.95254936634247,
      "grad_norm": 0.518723726272583,
      "learning_rate": 7.615321918270408e-06,
      "loss": 1.0104,
      "step": 6307
    },
    {
      "epoch": 58.96198054818745,
      "grad_norm": 0.5530791878700256,
      "learning_rate": 7.612382477532598e-06,
      "loss": 1.0765,
      "step": 6308
    },
    {
      "epoch": 58.97141173003242,
      "grad_norm": 0.5615153908729553,
      "learning_rate": 7.609443255547513e-06,
      "loss": 1.0609,
      "step": 6309
    },
    {
      "epoch": 58.980842911877396,
      "grad_norm": 0.5465691685676575,
      "learning_rate": 7.6065042525844465e-06,
      "loss": 1.0599,
      "step": 6310
    },
    {
      "epoch": 58.99027409372237,
      "grad_norm": 0.5374999642372131,
      "learning_rate": 7.6035654689126696e-06,
      "loss": 1.0678,
      "step": 6311
    },
    {
      "epoch": 58.999705275567344,
      "grad_norm": 0.5306398868560791,
      "learning_rate": 7.6006269048014294e-06,
      "loss": 1.0884,
      "step": 6312
    },
    {
      "epoch": 59.0,
      "grad_norm": 4.581600666046143,
      "learning_rate": 7.597688560519959e-06,
      "loss": 0.5676,
      "step": 6313
    },
    {
      "epoch": 59.009431181844974,
      "grad_norm": 0.5362046957015991,
      "learning_rate": 7.594750436337467e-06,
      "loss": 1.0782,
      "step": 6314
    },
    {
      "epoch": 59.01886236368995,
      "grad_norm": 0.5484853982925415,
      "learning_rate": 7.5918125325231394e-06,
      "loss": 1.0474,
      "step": 6315
    },
    {
      "epoch": 59.02829354553492,
      "grad_norm": 0.49557143449783325,
      "learning_rate": 7.588874849346155e-06,
      "loss": 1.0489,
      "step": 6316
    },
    {
      "epoch": 59.0377247273799,
      "grad_norm": 0.5463293194770813,
      "learning_rate": 7.585937387075658e-06,
      "loss": 1.0569,
      "step": 6317
    },
    {
      "epoch": 59.04715590922488,
      "grad_norm": 0.5184001922607422,
      "learning_rate": 7.583000145980779e-06,
      "loss": 1.0696,
      "step": 6318
    },
    {
      "epoch": 59.05658709106985,
      "grad_norm": 0.5103827118873596,
      "learning_rate": 7.580063126330626e-06,
      "loss": 1.0436,
      "step": 6319
    },
    {
      "epoch": 59.066018272914825,
      "grad_norm": 0.5121135115623474,
      "learning_rate": 7.577126328394288e-06,
      "loss": 1.0531,
      "step": 6320
    },
    {
      "epoch": 59.0754494547598,
      "grad_norm": 0.5643466114997864,
      "learning_rate": 7.574189752440834e-06,
      "loss": 1.02,
      "step": 6321
    },
    {
      "epoch": 59.08488063660477,
      "grad_norm": 0.4992123544216156,
      "learning_rate": 7.571253398739308e-06,
      "loss": 1.0481,
      "step": 6322
    },
    {
      "epoch": 59.09431181844975,
      "grad_norm": 0.5384685397148132,
      "learning_rate": 7.5683172675587445e-06,
      "loss": 1.0197,
      "step": 6323
    },
    {
      "epoch": 59.10374300029473,
      "grad_norm": 0.5523490905761719,
      "learning_rate": 7.565381359168149e-06,
      "loss": 1.0482,
      "step": 6324
    },
    {
      "epoch": 59.1131741821397,
      "grad_norm": 0.5036211013793945,
      "learning_rate": 7.562445673836508e-06,
      "loss": 1.0403,
      "step": 6325
    },
    {
      "epoch": 59.122605363984675,
      "grad_norm": 0.5167815089225769,
      "learning_rate": 7.559510211832787e-06,
      "loss": 1.0631,
      "step": 6326
    },
    {
      "epoch": 59.13203654582965,
      "grad_norm": 0.5201616287231445,
      "learning_rate": 7.556574973425932e-06,
      "loss": 1.0835,
      "step": 6327
    },
    {
      "epoch": 59.14146772767462,
      "grad_norm": 0.5497385263442993,
      "learning_rate": 7.5536399588848685e-06,
      "loss": 1.0059,
      "step": 6328
    },
    {
      "epoch": 59.1508989095196,
      "grad_norm": 0.5410134196281433,
      "learning_rate": 7.550705168478503e-06,
      "loss": 1.0583,
      "step": 6329
    },
    {
      "epoch": 59.16033009136457,
      "grad_norm": 0.5183975100517273,
      "learning_rate": 7.547770602475715e-06,
      "loss": 1.0227,
      "step": 6330
    },
    {
      "epoch": 59.16976127320955,
      "grad_norm": 0.536953866481781,
      "learning_rate": 7.5448362611453766e-06,
      "loss": 1.0726,
      "step": 6331
    },
    {
      "epoch": 59.179192455054526,
      "grad_norm": 0.5595400333404541,
      "learning_rate": 7.541902144756326e-06,
      "loss": 1.0302,
      "step": 6332
    },
    {
      "epoch": 59.1886236368995,
      "grad_norm": 0.5242477059364319,
      "learning_rate": 7.538968253577388e-06,
      "loss": 1.0338,
      "step": 6333
    },
    {
      "epoch": 59.198054818744474,
      "grad_norm": 0.5235941410064697,
      "learning_rate": 7.536034587877364e-06,
      "loss": 1.0593,
      "step": 6334
    },
    {
      "epoch": 59.20748600058945,
      "grad_norm": 0.5192721486091614,
      "learning_rate": 7.533101147925033e-06,
      "loss": 1.0541,
      "step": 6335
    },
    {
      "epoch": 59.21691718243442,
      "grad_norm": 0.5295329689979553,
      "learning_rate": 7.530167933989161e-06,
      "loss": 1.0831,
      "step": 6336
    },
    {
      "epoch": 59.226348364279396,
      "grad_norm": 0.5427616834640503,
      "learning_rate": 7.527234946338482e-06,
      "loss": 1.0556,
      "step": 6337
    },
    {
      "epoch": 59.23577954612438,
      "grad_norm": 0.46982505917549133,
      "learning_rate": 7.524302185241721e-06,
      "loss": 1.0362,
      "step": 6338
    },
    {
      "epoch": 59.24521072796935,
      "grad_norm": 0.5294870734214783,
      "learning_rate": 7.5213696509675745e-06,
      "loss": 1.0327,
      "step": 6339
    },
    {
      "epoch": 59.254641909814325,
      "grad_norm": 0.5800275206565857,
      "learning_rate": 7.518437343784718e-06,
      "loss": 1.0454,
      "step": 6340
    },
    {
      "epoch": 59.2640730916593,
      "grad_norm": 0.5766883492469788,
      "learning_rate": 7.515505263961812e-06,
      "loss": 1.0517,
      "step": 6341
    },
    {
      "epoch": 59.27350427350427,
      "grad_norm": 0.5185865163803101,
      "learning_rate": 7.512573411767492e-06,
      "loss": 1.059,
      "step": 6342
    },
    {
      "epoch": 59.28293545534925,
      "grad_norm": 0.5608735680580139,
      "learning_rate": 7.509641787470372e-06,
      "loss": 1.0518,
      "step": 6343
    },
    {
      "epoch": 59.29236663719422,
      "grad_norm": 0.5518065094947815,
      "learning_rate": 7.506710391339047e-06,
      "loss": 1.0888,
      "step": 6344
    },
    {
      "epoch": 59.3017978190392,
      "grad_norm": 0.595057487487793,
      "learning_rate": 7.503779223642087e-06,
      "loss": 1.0309,
      "step": 6345
    },
    {
      "epoch": 59.311229000884175,
      "grad_norm": 0.5652115941047668,
      "learning_rate": 7.500848284648051e-06,
      "loss": 1.031,
      "step": 6346
    },
    {
      "epoch": 59.32066018272915,
      "grad_norm": 0.5513833165168762,
      "learning_rate": 7.49791757462547e-06,
      "loss": 1.0678,
      "step": 6347
    },
    {
      "epoch": 59.33009136457412,
      "grad_norm": 0.556298553943634,
      "learning_rate": 7.49498709384285e-06,
      "loss": 1.0648,
      "step": 6348
    },
    {
      "epoch": 59.3395225464191,
      "grad_norm": 0.533333957195282,
      "learning_rate": 7.492056842568685e-06,
      "loss": 1.0577,
      "step": 6349
    },
    {
      "epoch": 59.34895372826407,
      "grad_norm": 0.5210519433021545,
      "learning_rate": 7.489126821071441e-06,
      "loss": 1.0596,
      "step": 6350
    },
    {
      "epoch": 59.358384910109045,
      "grad_norm": 0.497834712266922,
      "learning_rate": 7.486197029619566e-06,
      "loss": 1.0422,
      "step": 6351
    },
    {
      "epoch": 59.367816091954026,
      "grad_norm": 0.5486376285552979,
      "learning_rate": 7.483267468481483e-06,
      "loss": 1.0626,
      "step": 6352
    },
    {
      "epoch": 59.377247273799,
      "grad_norm": 0.5659525394439697,
      "learning_rate": 7.480338137925607e-06,
      "loss": 1.045,
      "step": 6353
    },
    {
      "epoch": 59.386678455643974,
      "grad_norm": 0.5181379914283752,
      "learning_rate": 7.477409038220315e-06,
      "loss": 1.047,
      "step": 6354
    },
    {
      "epoch": 59.39610963748895,
      "grad_norm": 0.510043203830719,
      "learning_rate": 7.474480169633972e-06,
      "loss": 1.0621,
      "step": 6355
    },
    {
      "epoch": 59.40554081933392,
      "grad_norm": 0.4979747235774994,
      "learning_rate": 7.47155153243492e-06,
      "loss": 1.0956,
      "step": 6356
    },
    {
      "epoch": 59.414972001178896,
      "grad_norm": 0.5014898777008057,
      "learning_rate": 7.468623126891479e-06,
      "loss": 1.0818,
      "step": 6357
    },
    {
      "epoch": 59.42440318302387,
      "grad_norm": 0.5988479852676392,
      "learning_rate": 7.46569495327195e-06,
      "loss": 1.0545,
      "step": 6358
    },
    {
      "epoch": 59.43383436486885,
      "grad_norm": 0.5531277060508728,
      "learning_rate": 7.462767011844605e-06,
      "loss": 1.0039,
      "step": 6359
    },
    {
      "epoch": 59.443265546713825,
      "grad_norm": 0.48801037669181824,
      "learning_rate": 7.459839302877711e-06,
      "loss": 1.1108,
      "step": 6360
    },
    {
      "epoch": 59.4526967285588,
      "grad_norm": 0.5385178327560425,
      "learning_rate": 7.4569118266395014e-06,
      "loss": 1.0481,
      "step": 6361
    },
    {
      "epoch": 59.46212791040377,
      "grad_norm": 0.5546932816505432,
      "learning_rate": 7.453984583398186e-06,
      "loss": 1.0423,
      "step": 6362
    },
    {
      "epoch": 59.47155909224875,
      "grad_norm": 0.5412788987159729,
      "learning_rate": 7.45105757342196e-06,
      "loss": 1.0695,
      "step": 6363
    },
    {
      "epoch": 59.48099027409372,
      "grad_norm": 0.5175465941429138,
      "learning_rate": 7.448130796978996e-06,
      "loss": 1.0488,
      "step": 6364
    },
    {
      "epoch": 59.490421455938694,
      "grad_norm": 0.532203197479248,
      "learning_rate": 7.445204254337443e-06,
      "loss": 1.0727,
      "step": 6365
    },
    {
      "epoch": 59.499852637783675,
      "grad_norm": 0.5235519409179688,
      "learning_rate": 7.4422779457654305e-06,
      "loss": 1.0968,
      "step": 6366
    },
    {
      "epoch": 59.50928381962865,
      "grad_norm": 0.5125682353973389,
      "learning_rate": 7.439351871531063e-06,
      "loss": 1.05,
      "step": 6367
    },
    {
      "epoch": 59.51871500147362,
      "grad_norm": 0.5280140042304993,
      "learning_rate": 7.436426031902433e-06,
      "loss": 1.0273,
      "step": 6368
    },
    {
      "epoch": 59.5281461833186,
      "grad_norm": 0.5244161486625671,
      "learning_rate": 7.433500427147601e-06,
      "loss": 1.0558,
      "step": 6369
    },
    {
      "epoch": 59.53757736516357,
      "grad_norm": 0.4978717565536499,
      "learning_rate": 7.43057505753461e-06,
      "loss": 1.0679,
      "step": 6370
    },
    {
      "epoch": 59.547008547008545,
      "grad_norm": 0.5182172656059265,
      "learning_rate": 7.427649923331479e-06,
      "loss": 1.0371,
      "step": 6371
    },
    {
      "epoch": 59.55643972885352,
      "grad_norm": 0.5353640913963318,
      "learning_rate": 7.424725024806214e-06,
      "loss": 1.0291,
      "step": 6372
    },
    {
      "epoch": 59.5658709106985,
      "grad_norm": 0.5465663075447083,
      "learning_rate": 7.421800362226787e-06,
      "loss": 1.0704,
      "step": 6373
    },
    {
      "epoch": 59.575302092543474,
      "grad_norm": 0.4962323009967804,
      "learning_rate": 7.418875935861159e-06,
      "loss": 1.0532,
      "step": 6374
    },
    {
      "epoch": 59.58473327438845,
      "grad_norm": 0.5600813627243042,
      "learning_rate": 7.415951745977262e-06,
      "loss": 1.0611,
      "step": 6375
    },
    {
      "epoch": 59.59416445623342,
      "grad_norm": 0.593010425567627,
      "learning_rate": 7.41302779284301e-06,
      "loss": 1.0657,
      "step": 6376
    },
    {
      "epoch": 59.603595638078396,
      "grad_norm": 0.4930415749549866,
      "learning_rate": 7.410104076726296e-06,
      "loss": 1.0687,
      "step": 6377
    },
    {
      "epoch": 59.61302681992337,
      "grad_norm": 0.5598917603492737,
      "learning_rate": 7.407180597894992e-06,
      "loss": 1.084,
      "step": 6378
    },
    {
      "epoch": 59.622458001768344,
      "grad_norm": 0.5829513072967529,
      "learning_rate": 7.404257356616941e-06,
      "loss": 1.0788,
      "step": 6379
    },
    {
      "epoch": 59.631889183613325,
      "grad_norm": 0.5524988174438477,
      "learning_rate": 7.4013343531599725e-06,
      "loss": 1.0498,
      "step": 6380
    },
    {
      "epoch": 59.6413203654583,
      "grad_norm": 0.5384781956672668,
      "learning_rate": 7.398411587791889e-06,
      "loss": 1.0358,
      "step": 6381
    },
    {
      "epoch": 59.65075154730327,
      "grad_norm": 0.4994521141052246,
      "learning_rate": 7.395489060780472e-06,
      "loss": 1.0836,
      "step": 6382
    },
    {
      "epoch": 59.66018272914825,
      "grad_norm": 0.536689281463623,
      "learning_rate": 7.392566772393488e-06,
      "loss": 1.0878,
      "step": 6383
    },
    {
      "epoch": 59.66961391099322,
      "grad_norm": 0.5004431009292603,
      "learning_rate": 7.389644722898674e-06,
      "loss": 1.0776,
      "step": 6384
    },
    {
      "epoch": 59.679045092838194,
      "grad_norm": 0.5217136144638062,
      "learning_rate": 7.386722912563745e-06,
      "loss": 1.0298,
      "step": 6385
    },
    {
      "epoch": 59.68847627468317,
      "grad_norm": 0.5350216627120972,
      "learning_rate": 7.383801341656398e-06,
      "loss": 1.0176,
      "step": 6386
    },
    {
      "epoch": 59.69790745652815,
      "grad_norm": 0.5392008423805237,
      "learning_rate": 7.380880010444307e-06,
      "loss": 1.0168,
      "step": 6387
    },
    {
      "epoch": 59.70733863837312,
      "grad_norm": 0.5508739352226257,
      "learning_rate": 7.377958919195121e-06,
      "loss": 1.0904,
      "step": 6388
    },
    {
      "epoch": 59.7167698202181,
      "grad_norm": 0.5320081114768982,
      "learning_rate": 7.375038068176468e-06,
      "loss": 1.0911,
      "step": 6389
    },
    {
      "epoch": 59.72620100206307,
      "grad_norm": 0.5042601823806763,
      "learning_rate": 7.37211745765596e-06,
      "loss": 1.0328,
      "step": 6390
    },
    {
      "epoch": 59.735632183908045,
      "grad_norm": 0.5253401398658752,
      "learning_rate": 7.3691970879011836e-06,
      "loss": 1.0675,
      "step": 6391
    },
    {
      "epoch": 59.74506336575302,
      "grad_norm": 0.5618439316749573,
      "learning_rate": 7.366276959179698e-06,
      "loss": 1.0564,
      "step": 6392
    },
    {
      "epoch": 59.75449454759799,
      "grad_norm": 0.5234008431434631,
      "learning_rate": 7.363357071759045e-06,
      "loss": 1.0422,
      "step": 6393
    },
    {
      "epoch": 59.763925729442974,
      "grad_norm": 0.5805023312568665,
      "learning_rate": 7.360437425906743e-06,
      "loss": 1.0095,
      "step": 6394
    },
    {
      "epoch": 59.77335691128795,
      "grad_norm": 0.5220911502838135,
      "learning_rate": 7.357518021890293e-06,
      "loss": 1.081,
      "step": 6395
    },
    {
      "epoch": 59.78278809313292,
      "grad_norm": 0.5724973678588867,
      "learning_rate": 7.354598859977165e-06,
      "loss": 1.0309,
      "step": 6396
    },
    {
      "epoch": 59.792219274977896,
      "grad_norm": 0.5436739921569824,
      "learning_rate": 7.351679940434811e-06,
      "loss": 1.1056,
      "step": 6397
    },
    {
      "epoch": 59.80165045682287,
      "grad_norm": 0.5539133548736572,
      "learning_rate": 7.348761263530666e-06,
      "loss": 1.056,
      "step": 6398
    },
    {
      "epoch": 59.811081638667844,
      "grad_norm": 0.5293292999267578,
      "learning_rate": 7.3458428295321386e-06,
      "loss": 1.0294,
      "step": 6399
    },
    {
      "epoch": 59.82051282051282,
      "grad_norm": 0.5383824706077576,
      "learning_rate": 7.342924638706612e-06,
      "loss": 1.0561,
      "step": 6400
    },
    {
      "epoch": 59.8299440023578,
      "grad_norm": 0.5151990056037903,
      "learning_rate": 7.340006691321449e-06,
      "loss": 1.063,
      "step": 6401
    },
    {
      "epoch": 59.83937518420277,
      "grad_norm": 0.5288830995559692,
      "learning_rate": 7.337088987643992e-06,
      "loss": 1.056,
      "step": 6402
    },
    {
      "epoch": 59.84880636604775,
      "grad_norm": 0.5498705506324768,
      "learning_rate": 7.334171527941561e-06,
      "loss": 1.0271,
      "step": 6403
    },
    {
      "epoch": 59.85823754789272,
      "grad_norm": 0.5785712003707886,
      "learning_rate": 7.331254312481451e-06,
      "loss": 1.0175,
      "step": 6404
    },
    {
      "epoch": 59.867668729737694,
      "grad_norm": 0.5229136943817139,
      "learning_rate": 7.3283373415309385e-06,
      "loss": 1.0563,
      "step": 6405
    },
    {
      "epoch": 59.87709991158267,
      "grad_norm": 0.5074726939201355,
      "learning_rate": 7.325420615357275e-06,
      "loss": 1.0589,
      "step": 6406
    },
    {
      "epoch": 59.88653109342764,
      "grad_norm": 0.538543701171875,
      "learning_rate": 7.322504134227687e-06,
      "loss": 1.0319,
      "step": 6407
    },
    {
      "epoch": 59.89596227527262,
      "grad_norm": 0.5367515087127686,
      "learning_rate": 7.319587898409386e-06,
      "loss": 1.0553,
      "step": 6408
    },
    {
      "epoch": 59.9053934571176,
      "grad_norm": 0.4985877275466919,
      "learning_rate": 7.316671908169555e-06,
      "loss": 1.0854,
      "step": 6409
    },
    {
      "epoch": 59.91482463896257,
      "grad_norm": 0.5167328715324402,
      "learning_rate": 7.313756163775354e-06,
      "loss": 1.0485,
      "step": 6410
    },
    {
      "epoch": 59.924255820807545,
      "grad_norm": 0.5685916543006897,
      "learning_rate": 7.310840665493926e-06,
      "loss": 1.0549,
      "step": 6411
    },
    {
      "epoch": 59.93368700265252,
      "grad_norm": 0.5012941956520081,
      "learning_rate": 7.30792541359238e-06,
      "loss": 1.0578,
      "step": 6412
    },
    {
      "epoch": 59.94311818449749,
      "grad_norm": 0.4832373559474945,
      "learning_rate": 7.3050104083378205e-06,
      "loss": 1.0377,
      "step": 6413
    },
    {
      "epoch": 59.95254936634247,
      "grad_norm": 0.6072092056274414,
      "learning_rate": 7.302095649997316e-06,
      "loss": 1.0331,
      "step": 6414
    },
    {
      "epoch": 59.96198054818745,
      "grad_norm": 0.5337529182434082,
      "learning_rate": 7.299181138837914e-06,
      "loss": 1.0911,
      "step": 6415
    },
    {
      "epoch": 59.97141173003242,
      "grad_norm": 0.5424013137817383,
      "learning_rate": 7.296266875126643e-06,
      "loss": 1.0954,
      "step": 6416
    },
    {
      "epoch": 59.980842911877396,
      "grad_norm": 0.550925076007843,
      "learning_rate": 7.2933528591305054e-06,
      "loss": 1.0491,
      "step": 6417
    },
    {
      "epoch": 59.99027409372237,
      "grad_norm": 0.5087856650352478,
      "learning_rate": 7.290439091116483e-06,
      "loss": 1.0659,
      "step": 6418
    },
    {
      "epoch": 59.999705275567344,
      "grad_norm": 0.5221232175827026,
      "learning_rate": 7.28752557135153e-06,
      "loss": 1.0669,
      "step": 6419
    },
    {
      "epoch": 60.0,
      "grad_norm": 2.1932992935180664,
      "learning_rate": 7.284612300102589e-06,
      "loss": 0.9629,
      "step": 6420
    },
    {
      "epoch": 60.009431181844974,
      "grad_norm": 0.5224208831787109,
      "learning_rate": 7.2816992776365714e-06,
      "loss": 1.0353,
      "step": 6421
    },
    {
      "epoch": 60.01886236368995,
      "grad_norm": 0.5340388417243958,
      "learning_rate": 7.2787865042203665e-06,
      "loss": 1.0845,
      "step": 6422
    },
    {
      "epoch": 60.02829354553492,
      "grad_norm": 0.559378981590271,
      "learning_rate": 7.27587398012084e-06,
      "loss": 1.0511,
      "step": 6423
    },
    {
      "epoch": 60.0377247273799,
      "grad_norm": 0.5257744789123535,
      "learning_rate": 7.27296170560484e-06,
      "loss": 1.063,
      "step": 6424
    },
    {
      "epoch": 60.04715590922488,
      "grad_norm": 0.5498398542404175,
      "learning_rate": 7.270049680939182e-06,
      "loss": 1.0728,
      "step": 6425
    },
    {
      "epoch": 60.05658709106985,
      "grad_norm": 0.5004584789276123,
      "learning_rate": 7.267137906390671e-06,
      "loss": 1.0796,
      "step": 6426
    },
    {
      "epoch": 60.066018272914825,
      "grad_norm": 0.5519447326660156,
      "learning_rate": 7.2642263822260765e-06,
      "loss": 1.0547,
      "step": 6427
    },
    {
      "epoch": 60.0754494547598,
      "grad_norm": 0.5223137736320496,
      "learning_rate": 7.261315108712157e-06,
      "loss": 1.0297,
      "step": 6428
    },
    {
      "epoch": 60.08488063660477,
      "grad_norm": 0.5036760568618774,
      "learning_rate": 7.2584040861156425e-06,
      "loss": 1.0408,
      "step": 6429
    },
    {
      "epoch": 60.09431181844975,
      "grad_norm": 0.566261351108551,
      "learning_rate": 7.255493314703237e-06,
      "loss": 1.0108,
      "step": 6430
    },
    {
      "epoch": 60.10374300029473,
      "grad_norm": 0.5327765941619873,
      "learning_rate": 7.252582794741625e-06,
      "loss": 1.0234,
      "step": 6431
    },
    {
      "epoch": 60.1131741821397,
      "grad_norm": 0.5616782307624817,
      "learning_rate": 7.249672526497469e-06,
      "loss": 1.0804,
      "step": 6432
    },
    {
      "epoch": 60.122605363984675,
      "grad_norm": 0.5335140228271484,
      "learning_rate": 7.246762510237404e-06,
      "loss": 1.02,
      "step": 6433
    },
    {
      "epoch": 60.13203654582965,
      "grad_norm": 0.5468832850456238,
      "learning_rate": 7.243852746228044e-06,
      "loss": 1.0507,
      "step": 6434
    },
    {
      "epoch": 60.14146772767462,
      "grad_norm": 0.5084440112113953,
      "learning_rate": 7.240943234735986e-06,
      "loss": 1.0776,
      "step": 6435
    },
    {
      "epoch": 60.1508989095196,
      "grad_norm": 0.5839678645133972,
      "learning_rate": 7.238033976027797e-06,
      "loss": 1.015,
      "step": 6436
    },
    {
      "epoch": 60.16033009136457,
      "grad_norm": 0.5195721387863159,
      "learning_rate": 7.235124970370018e-06,
      "loss": 1.0886,
      "step": 6437
    },
    {
      "epoch": 60.16976127320955,
      "grad_norm": 0.565828263759613,
      "learning_rate": 7.232216218029175e-06,
      "loss": 1.017,
      "step": 6438
    },
    {
      "epoch": 60.179192455054526,
      "grad_norm": 0.581069827079773,
      "learning_rate": 7.229307719271766e-06,
      "loss": 1.0747,
      "step": 6439
    },
    {
      "epoch": 60.1886236368995,
      "grad_norm": 0.5233818888664246,
      "learning_rate": 7.226399474364268e-06,
      "loss": 1.0809,
      "step": 6440
    },
    {
      "epoch": 60.198054818744474,
      "grad_norm": 0.5093714594841003,
      "learning_rate": 7.2234914835731305e-06,
      "loss": 1.064,
      "step": 6441
    },
    {
      "epoch": 60.20748600058945,
      "grad_norm": 0.5475778579711914,
      "learning_rate": 7.22058374716478e-06,
      "loss": 1.0713,
      "step": 6442
    },
    {
      "epoch": 60.21691718243442,
      "grad_norm": 0.49183058738708496,
      "learning_rate": 7.217676265405631e-06,
      "loss": 1.0588,
      "step": 6443
    },
    {
      "epoch": 60.226348364279396,
      "grad_norm": 0.5196900963783264,
      "learning_rate": 7.21476903856206e-06,
      "loss": 1.0567,
      "step": 6444
    },
    {
      "epoch": 60.23577954612438,
      "grad_norm": 0.5054160356521606,
      "learning_rate": 7.2118620669004305e-06,
      "loss": 1.0604,
      "step": 6445
    },
    {
      "epoch": 60.24521072796935,
      "grad_norm": 0.5766957402229309,
      "learning_rate": 7.2089553506870724e-06,
      "loss": 1.0058,
      "step": 6446
    },
    {
      "epoch": 60.254641909814325,
      "grad_norm": 0.534250795841217,
      "learning_rate": 7.206048890188303e-06,
      "loss": 1.0807,
      "step": 6447
    },
    {
      "epoch": 60.2640730916593,
      "grad_norm": 0.5294831395149231,
      "learning_rate": 7.203142685670408e-06,
      "loss": 1.0287,
      "step": 6448
    },
    {
      "epoch": 60.27350427350427,
      "grad_norm": 0.5408264398574829,
      "learning_rate": 7.20023673739965e-06,
      "loss": 1.0499,
      "step": 6449
    },
    {
      "epoch": 60.28293545534925,
      "grad_norm": 0.5390182137489319,
      "learning_rate": 7.197331045642282e-06,
      "loss": 1.0877,
      "step": 6450
    },
    {
      "epoch": 60.29236663719422,
      "grad_norm": 0.5560164451599121,
      "learning_rate": 7.194425610664514e-06,
      "loss": 1.0349,
      "step": 6451
    },
    {
      "epoch": 60.3017978190392,
      "grad_norm": 0.527864396572113,
      "learning_rate": 7.191520432732541e-06,
      "loss": 1.0691,
      "step": 6452
    },
    {
      "epoch": 60.311229000884175,
      "grad_norm": 0.4781847298145294,
      "learning_rate": 7.18861551211254e-06,
      "loss": 1.0703,
      "step": 6453
    },
    {
      "epoch": 60.32066018272915,
      "grad_norm": 0.5149919986724854,
      "learning_rate": 7.185710849070653e-06,
      "loss": 1.0421,
      "step": 6454
    },
    {
      "epoch": 60.33009136457412,
      "grad_norm": 0.6049079298973083,
      "learning_rate": 7.182806443873005e-06,
      "loss": 1.0427,
      "step": 6455
    },
    {
      "epoch": 60.3395225464191,
      "grad_norm": 0.5363622903823853,
      "learning_rate": 7.179902296785699e-06,
      "loss": 1.0819,
      "step": 6456
    },
    {
      "epoch": 60.34895372826407,
      "grad_norm": 0.5123053193092346,
      "learning_rate": 7.176998408074808e-06,
      "loss": 1.0712,
      "step": 6457
    },
    {
      "epoch": 60.358384910109045,
      "grad_norm": 0.5301723480224609,
      "learning_rate": 7.174094778006391e-06,
      "loss": 1.0891,
      "step": 6458
    },
    {
      "epoch": 60.367816091954026,
      "grad_norm": 0.5079798698425293,
      "learning_rate": 7.171191406846476e-06,
      "loss": 1.0304,
      "step": 6459
    },
    {
      "epoch": 60.377247273799,
      "grad_norm": 0.51124507188797,
      "learning_rate": 7.168288294861066e-06,
      "loss": 1.0833,
      "step": 6460
    },
    {
      "epoch": 60.386678455643974,
      "grad_norm": 0.5860523581504822,
      "learning_rate": 7.165385442316146e-06,
      "loss": 1.0187,
      "step": 6461
    },
    {
      "epoch": 60.39610963748895,
      "grad_norm": 0.5132669806480408,
      "learning_rate": 7.1624828494776745e-06,
      "loss": 1.05,
      "step": 6462
    },
    {
      "epoch": 60.40554081933392,
      "grad_norm": 0.5386208295822144,
      "learning_rate": 7.159580516611582e-06,
      "loss": 1.0531,
      "step": 6463
    },
    {
      "epoch": 60.414972001178896,
      "grad_norm": 0.5244811177253723,
      "learning_rate": 7.1566784439837825e-06,
      "loss": 1.0256,
      "step": 6464
    },
    {
      "epoch": 60.42440318302387,
      "grad_norm": 0.5193495750427246,
      "learning_rate": 7.153776631860163e-06,
      "loss": 1.085,
      "step": 6465
    },
    {
      "epoch": 60.43383436486885,
      "grad_norm": 0.5338623523712158,
      "learning_rate": 7.150875080506587e-06,
      "loss": 1.0362,
      "step": 6466
    },
    {
      "epoch": 60.443265546713825,
      "grad_norm": 0.5247768759727478,
      "learning_rate": 7.147973790188894e-06,
      "loss": 1.0823,
      "step": 6467
    },
    {
      "epoch": 60.4526967285588,
      "grad_norm": 0.5317199230194092,
      "learning_rate": 7.145072761172896e-06,
      "loss": 1.0306,
      "step": 6468
    },
    {
      "epoch": 60.46212791040377,
      "grad_norm": 0.5611997246742249,
      "learning_rate": 7.142171993724388e-06,
      "loss": 1.0815,
      "step": 6469
    },
    {
      "epoch": 60.47155909224875,
      "grad_norm": 0.5306034684181213,
      "learning_rate": 7.139271488109136e-06,
      "loss": 1.0902,
      "step": 6470
    },
    {
      "epoch": 60.48099027409372,
      "grad_norm": 0.514187216758728,
      "learning_rate": 7.136371244592881e-06,
      "loss": 1.0172,
      "step": 6471
    },
    {
      "epoch": 60.490421455938694,
      "grad_norm": 0.5639464855194092,
      "learning_rate": 7.133471263441347e-06,
      "loss": 1.0391,
      "step": 6472
    },
    {
      "epoch": 60.499852637783675,
      "grad_norm": 0.5520392060279846,
      "learning_rate": 7.130571544920224e-06,
      "loss": 1.0172,
      "step": 6473
    },
    {
      "epoch": 60.50928381962865,
      "grad_norm": 0.560368001461029,
      "learning_rate": 7.127672089295189e-06,
      "loss": 1.0561,
      "step": 6474
    },
    {
      "epoch": 60.51871500147362,
      "grad_norm": 0.5285862684249878,
      "learning_rate": 7.124772896831886e-06,
      "loss": 1.0625,
      "step": 6475
    },
    {
      "epoch": 60.5281461833186,
      "grad_norm": 0.5741782784461975,
      "learning_rate": 7.121873967795938e-06,
      "loss": 1.0424,
      "step": 6476
    },
    {
      "epoch": 60.53757736516357,
      "grad_norm": 0.5365079641342163,
      "learning_rate": 7.118975302452944e-06,
      "loss": 1.0802,
      "step": 6477
    },
    {
      "epoch": 60.547008547008545,
      "grad_norm": 0.5113306641578674,
      "learning_rate": 7.11607690106848e-06,
      "loss": 1.055,
      "step": 6478
    },
    {
      "epoch": 60.55643972885352,
      "grad_norm": 0.5187752842903137,
      "learning_rate": 7.1131787639080905e-06,
      "loss": 1.03,
      "step": 6479
    },
    {
      "epoch": 60.5658709106985,
      "grad_norm": 0.5428563952445984,
      "learning_rate": 7.110280891237312e-06,
      "loss": 1.0187,
      "step": 6480
    },
    {
      "epoch": 60.575302092543474,
      "grad_norm": 0.5477873086929321,
      "learning_rate": 7.107383283321641e-06,
      "loss": 1.0801,
      "step": 6481
    },
    {
      "epoch": 60.58473327438845,
      "grad_norm": 0.5476918816566467,
      "learning_rate": 7.104485940426556e-06,
      "loss": 1.053,
      "step": 6482
    },
    {
      "epoch": 60.59416445623342,
      "grad_norm": 0.5430654287338257,
      "learning_rate": 7.10158886281751e-06,
      "loss": 1.0292,
      "step": 6483
    },
    {
      "epoch": 60.603595638078396,
      "grad_norm": 0.5364115238189697,
      "learning_rate": 7.098692050759934e-06,
      "loss": 1.1031,
      "step": 6484
    },
    {
      "epoch": 60.61302681992337,
      "grad_norm": 0.5423590540885925,
      "learning_rate": 7.095795504519232e-06,
      "loss": 1.0195,
      "step": 6485
    },
    {
      "epoch": 60.622458001768344,
      "grad_norm": 0.5415520071983337,
      "learning_rate": 7.0928992243607785e-06,
      "loss": 1.0486,
      "step": 6486
    },
    {
      "epoch": 60.631889183613325,
      "grad_norm": 0.5270563364028931,
      "learning_rate": 7.0900032105499415e-06,
      "loss": 1.0689,
      "step": 6487
    },
    {
      "epoch": 60.6413203654583,
      "grad_norm": 0.5515185594558716,
      "learning_rate": 7.087107463352046e-06,
      "loss": 1.0428,
      "step": 6488
    },
    {
      "epoch": 60.65075154730327,
      "grad_norm": 0.5136374831199646,
      "learning_rate": 7.0842119830324005e-06,
      "loss": 1.0561,
      "step": 6489
    },
    {
      "epoch": 60.66018272914825,
      "grad_norm": 0.5265337228775024,
      "learning_rate": 7.081316769856288e-06,
      "loss": 1.1227,
      "step": 6490
    },
    {
      "epoch": 60.66961391099322,
      "grad_norm": 0.4949459433555603,
      "learning_rate": 7.078421824088968e-06,
      "loss": 1.0623,
      "step": 6491
    },
    {
      "epoch": 60.679045092838194,
      "grad_norm": 0.547958254814148,
      "learning_rate": 7.075527145995671e-06,
      "loss": 1.0654,
      "step": 6492
    },
    {
      "epoch": 60.68847627468317,
      "grad_norm": 0.5524824857711792,
      "learning_rate": 7.07263273584161e-06,
      "loss": 1.0478,
      "step": 6493
    },
    {
      "epoch": 60.69790745652815,
      "grad_norm": 0.5154569149017334,
      "learning_rate": 7.069738593891964e-06,
      "loss": 1.0845,
      "step": 6494
    },
    {
      "epoch": 60.70733863837312,
      "grad_norm": 0.5496973991394043,
      "learning_rate": 7.066844720411903e-06,
      "loss": 1.0267,
      "step": 6495
    },
    {
      "epoch": 60.7167698202181,
      "grad_norm": 0.514122724533081,
      "learning_rate": 7.063951115666557e-06,
      "loss": 1.0275,
      "step": 6496
    },
    {
      "epoch": 60.72620100206307,
      "grad_norm": 0.5010601878166199,
      "learning_rate": 7.061057779921038e-06,
      "loss": 1.0363,
      "step": 6497
    },
    {
      "epoch": 60.735632183908045,
      "grad_norm": 0.5120615363121033,
      "learning_rate": 7.0581647134404315e-06,
      "loss": 1.0359,
      "step": 6498
    },
    {
      "epoch": 60.74506336575302,
      "grad_norm": 0.5748307108879089,
      "learning_rate": 7.055271916489799e-06,
      "loss": 1.0448,
      "step": 6499
    },
    {
      "epoch": 60.75449454759799,
      "grad_norm": 0.5345858931541443,
      "learning_rate": 7.05237938933418e-06,
      "loss": 1.0533,
      "step": 6500
    },
    {
      "epoch": 60.763925729442974,
      "grad_norm": 0.5545861124992371,
      "learning_rate": 7.049487132238584e-06,
      "loss": 1.0397,
      "step": 6501
    },
    {
      "epoch": 60.77335691128795,
      "grad_norm": 0.48533347249031067,
      "learning_rate": 7.046595145468002e-06,
      "loss": 1.0711,
      "step": 6502
    },
    {
      "epoch": 60.78278809313292,
      "grad_norm": 0.562223494052887,
      "learning_rate": 7.043703429287394e-06,
      "loss": 1.0664,
      "step": 6503
    },
    {
      "epoch": 60.792219274977896,
      "grad_norm": 0.5445960760116577,
      "learning_rate": 7.040811983961699e-06,
      "loss": 1.0267,
      "step": 6504
    },
    {
      "epoch": 60.80165045682287,
      "grad_norm": 0.5471762418746948,
      "learning_rate": 7.037920809755831e-06,
      "loss": 1.0729,
      "step": 6505
    },
    {
      "epoch": 60.811081638667844,
      "grad_norm": 0.5407757759094238,
      "learning_rate": 7.035029906934678e-06,
      "loss": 1.0185,
      "step": 6506
    },
    {
      "epoch": 60.82051282051282,
      "grad_norm": 0.5248540043830872,
      "learning_rate": 7.032139275763103e-06,
      "loss": 1.0497,
      "step": 6507
    },
    {
      "epoch": 60.8299440023578,
      "grad_norm": 0.5460631847381592,
      "learning_rate": 7.029248916505947e-06,
      "loss": 1.0101,
      "step": 6508
    },
    {
      "epoch": 60.83937518420277,
      "grad_norm": 0.5435373187065125,
      "learning_rate": 7.026358829428015e-06,
      "loss": 1.0449,
      "step": 6509
    },
    {
      "epoch": 60.84880636604775,
      "grad_norm": 0.4937424063682556,
      "learning_rate": 7.023469014794109e-06,
      "loss": 1.0477,
      "step": 6510
    },
    {
      "epoch": 60.85823754789272,
      "grad_norm": 0.5350887179374695,
      "learning_rate": 7.020579472868985e-06,
      "loss": 1.0452,
      "step": 6511
    },
    {
      "epoch": 60.867668729737694,
      "grad_norm": 0.5315582752227783,
      "learning_rate": 7.017690203917383e-06,
      "loss": 1.0418,
      "step": 6512
    },
    {
      "epoch": 60.87709991158267,
      "grad_norm": 0.5565363764762878,
      "learning_rate": 7.014801208204018e-06,
      "loss": 1.0573,
      "step": 6513
    },
    {
      "epoch": 60.88653109342764,
      "grad_norm": 0.5347456336021423,
      "learning_rate": 7.011912485993578e-06,
      "loss": 1.0764,
      "step": 6514
    },
    {
      "epoch": 60.89596227527262,
      "grad_norm": 0.546295702457428,
      "learning_rate": 7.009024037550726e-06,
      "loss": 1.0716,
      "step": 6515
    },
    {
      "epoch": 60.9053934571176,
      "grad_norm": 0.5310891270637512,
      "learning_rate": 7.0061358631401e-06,
      "loss": 1.0273,
      "step": 6516
    },
    {
      "epoch": 60.91482463896257,
      "grad_norm": 0.5426504611968994,
      "learning_rate": 7.003247963026316e-06,
      "loss": 1.0545,
      "step": 6517
    },
    {
      "epoch": 60.924255820807545,
      "grad_norm": 0.538324236869812,
      "learning_rate": 7.0003603374739636e-06,
      "loss": 1.095,
      "step": 6518
    },
    {
      "epoch": 60.93368700265252,
      "grad_norm": 0.5099160671234131,
      "learning_rate": 6.9974729867476035e-06,
      "loss": 1.049,
      "step": 6519
    },
    {
      "epoch": 60.94311818449749,
      "grad_norm": 0.5324874520301819,
      "learning_rate": 6.994585911111773e-06,
      "loss": 1.0795,
      "step": 6520
    },
    {
      "epoch": 60.95254936634247,
      "grad_norm": 0.5487691760063171,
      "learning_rate": 6.991699110830988e-06,
      "loss": 1.1135,
      "step": 6521
    },
    {
      "epoch": 60.96198054818745,
      "grad_norm": 0.5122429728507996,
      "learning_rate": 6.988812586169736e-06,
      "loss": 1.0639,
      "step": 6522
    },
    {
      "epoch": 60.97141173003242,
      "grad_norm": 0.5327801704406738,
      "learning_rate": 6.985926337392475e-06,
      "loss": 1.0846,
      "step": 6523
    },
    {
      "epoch": 60.980842911877396,
      "grad_norm": 0.5566310286521912,
      "learning_rate": 6.983040364763641e-06,
      "loss": 1.0607,
      "step": 6524
    },
    {
      "epoch": 60.99027409372237,
      "grad_norm": 0.662681519985199,
      "learning_rate": 6.980154668547656e-06,
      "loss": 1.0298,
      "step": 6525
    },
    {
      "epoch": 60.999705275567344,
      "grad_norm": 0.5537471771240234,
      "learning_rate": 6.9772692490089e-06,
      "loss": 1.0611,
      "step": 6526
    },
    {
      "epoch": 61.0,
      "grad_norm": 2.98694109916687,
      "learning_rate": 6.974384106411734e-06,
      "loss": 0.5227,
      "step": 6527
    },
    {
      "epoch": 61.009431181844974,
      "grad_norm": 0.5231776237487793,
      "learning_rate": 6.971499241020495e-06,
      "loss": 1.0197,
      "step": 6528
    },
    {
      "epoch": 61.01886236368995,
      "grad_norm": 0.5811578035354614,
      "learning_rate": 6.968614653099493e-06,
      "loss": 1.0382,
      "step": 6529
    },
    {
      "epoch": 61.02829354553492,
      "grad_norm": 0.5392147302627563,
      "learning_rate": 6.965730342913011e-06,
      "loss": 1.0944,
      "step": 6530
    },
    {
      "epoch": 61.0377247273799,
      "grad_norm": 0.5488380193710327,
      "learning_rate": 6.962846310725309e-06,
      "loss": 1.0468,
      "step": 6531
    },
    {
      "epoch": 61.04715590922488,
      "grad_norm": 0.5160514116287231,
      "learning_rate": 6.959962556800626e-06,
      "loss": 1.0493,
      "step": 6532
    },
    {
      "epoch": 61.05658709106985,
      "grad_norm": 0.5068212747573853,
      "learning_rate": 6.957079081403168e-06,
      "loss": 1.041,
      "step": 6533
    },
    {
      "epoch": 61.066018272914825,
      "grad_norm": 0.5584080815315247,
      "learning_rate": 6.954195884797115e-06,
      "loss": 1.0324,
      "step": 6534
    },
    {
      "epoch": 61.0754494547598,
      "grad_norm": 0.5392678380012512,
      "learning_rate": 6.951312967246626e-06,
      "loss": 1.0373,
      "step": 6535
    },
    {
      "epoch": 61.08488063660477,
      "grad_norm": 0.5650491714477539,
      "learning_rate": 6.948430329015836e-06,
      "loss": 1.0767,
      "step": 6536
    },
    {
      "epoch": 61.09431181844975,
      "grad_norm": 0.6392878293991089,
      "learning_rate": 6.945547970368849e-06,
      "loss": 1.0432,
      "step": 6537
    },
    {
      "epoch": 61.10374300029473,
      "grad_norm": 0.5067226886749268,
      "learning_rate": 6.9426658915697445e-06,
      "loss": 1.0613,
      "step": 6538
    },
    {
      "epoch": 61.1131741821397,
      "grad_norm": 0.5644367337226868,
      "learning_rate": 6.939784092882576e-06,
      "loss": 1.0195,
      "step": 6539
    },
    {
      "epoch": 61.122605363984675,
      "grad_norm": 0.5306713581085205,
      "learning_rate": 6.936902574571378e-06,
      "loss": 1.0587,
      "step": 6540
    },
    {
      "epoch": 61.13203654582965,
      "grad_norm": 0.5300576090812683,
      "learning_rate": 6.934021336900154e-06,
      "loss": 1.0669,
      "step": 6541
    },
    {
      "epoch": 61.14146772767462,
      "grad_norm": 0.59269118309021,
      "learning_rate": 6.931140380132879e-06,
      "loss": 1.0693,
      "step": 6542
    },
    {
      "epoch": 61.1508989095196,
      "grad_norm": 0.5370554327964783,
      "learning_rate": 6.928259704533508e-06,
      "loss": 1.0344,
      "step": 6543
    },
    {
      "epoch": 61.16033009136457,
      "grad_norm": 0.5079147815704346,
      "learning_rate": 6.925379310365965e-06,
      "loss": 1.0371,
      "step": 6544
    },
    {
      "epoch": 61.16976127320955,
      "grad_norm": 0.5825839638710022,
      "learning_rate": 6.9224991978941514e-06,
      "loss": 1.0562,
      "step": 6545
    },
    {
      "epoch": 61.179192455054526,
      "grad_norm": 0.547640860080719,
      "learning_rate": 6.919619367381938e-06,
      "loss": 1.0374,
      "step": 6546
    },
    {
      "epoch": 61.1886236368995,
      "grad_norm": 0.5415665507316589,
      "learning_rate": 6.916739819093184e-06,
      "loss": 1.0436,
      "step": 6547
    },
    {
      "epoch": 61.198054818744474,
      "grad_norm": 0.6037842631340027,
      "learning_rate": 6.913860553291705e-06,
      "loss": 1.0602,
      "step": 6548
    },
    {
      "epoch": 61.20748600058945,
      "grad_norm": 0.5258744359016418,
      "learning_rate": 6.910981570241302e-06,
      "loss": 1.086,
      "step": 6549
    },
    {
      "epoch": 61.21691718243442,
      "grad_norm": 0.5339576601982117,
      "learning_rate": 6.908102870205744e-06,
      "loss": 1.0618,
      "step": 6550
    },
    {
      "epoch": 61.226348364279396,
      "grad_norm": 0.5418860912322998,
      "learning_rate": 6.905224453448778e-06,
      "loss": 1.1048,
      "step": 6551
    },
    {
      "epoch": 61.23577954612438,
      "grad_norm": 0.5168221592903137,
      "learning_rate": 6.9023463202341236e-06,
      "loss": 1.0346,
      "step": 6552
    },
    {
      "epoch": 61.24521072796935,
      "grad_norm": 0.6090732216835022,
      "learning_rate": 6.8994684708254725e-06,
      "loss": 1.0548,
      "step": 6553
    },
    {
      "epoch": 61.254641909814325,
      "grad_norm": 0.5761947631835938,
      "learning_rate": 6.89659090548649e-06,
      "loss": 1.0472,
      "step": 6554
    },
    {
      "epoch": 61.2640730916593,
      "grad_norm": 0.5327228903770447,
      "learning_rate": 6.893713624480826e-06,
      "loss": 1.032,
      "step": 6555
    },
    {
      "epoch": 61.27350427350427,
      "grad_norm": 0.532010018825531,
      "learning_rate": 6.890836628072091e-06,
      "loss": 1.0684,
      "step": 6556
    },
    {
      "epoch": 61.28293545534925,
      "grad_norm": 0.5865113139152527,
      "learning_rate": 6.8879599165238744e-06,
      "loss": 1.0403,
      "step": 6557
    },
    {
      "epoch": 61.29236663719422,
      "grad_norm": 0.5403202772140503,
      "learning_rate": 6.88508349009974e-06,
      "loss": 1.0498,
      "step": 6558
    },
    {
      "epoch": 61.3017978190392,
      "grad_norm": 0.5220891237258911,
      "learning_rate": 6.882207349063226e-06,
      "loss": 1.0438,
      "step": 6559
    },
    {
      "epoch": 61.311229000884175,
      "grad_norm": 0.5218414068222046,
      "learning_rate": 6.879331493677839e-06,
      "loss": 1.0659,
      "step": 6560
    },
    {
      "epoch": 61.32066018272915,
      "grad_norm": 0.5169461965560913,
      "learning_rate": 6.876455924207068e-06,
      "loss": 1.0493,
      "step": 6561
    },
    {
      "epoch": 61.33009136457412,
      "grad_norm": 0.5532362461090088,
      "learning_rate": 6.873580640914374e-06,
      "loss": 1.0686,
      "step": 6562
    },
    {
      "epoch": 61.3395225464191,
      "grad_norm": 0.5015151500701904,
      "learning_rate": 6.870705644063187e-06,
      "loss": 1.0585,
      "step": 6563
    },
    {
      "epoch": 61.34895372826407,
      "grad_norm": 0.5730898380279541,
      "learning_rate": 6.8678309339169115e-06,
      "loss": 1.0614,
      "step": 6564
    },
    {
      "epoch": 61.358384910109045,
      "grad_norm": 0.5616095066070557,
      "learning_rate": 6.864956510738927e-06,
      "loss": 1.0673,
      "step": 6565
    },
    {
      "epoch": 61.367816091954026,
      "grad_norm": 0.5513656735420227,
      "learning_rate": 6.862082374792591e-06,
      "loss": 1.0649,
      "step": 6566
    },
    {
      "epoch": 61.377247273799,
      "grad_norm": 0.5746060013771057,
      "learning_rate": 6.85920852634123e-06,
      "loss": 1.0876,
      "step": 6567
    },
    {
      "epoch": 61.386678455643974,
      "grad_norm": 0.5266503095626831,
      "learning_rate": 6.856334965648144e-06,
      "loss": 1.053,
      "step": 6568
    },
    {
      "epoch": 61.39610963748895,
      "grad_norm": 0.564558207988739,
      "learning_rate": 6.853461692976605e-06,
      "loss": 1.0037,
      "step": 6569
    },
    {
      "epoch": 61.40554081933392,
      "grad_norm": 0.619155764579773,
      "learning_rate": 6.850588708589865e-06,
      "loss": 1.0631,
      "step": 6570
    },
    {
      "epoch": 61.414972001178896,
      "grad_norm": 0.5543873310089111,
      "learning_rate": 6.847716012751145e-06,
      "loss": 1.0188,
      "step": 6571
    },
    {
      "epoch": 61.42440318302387,
      "grad_norm": 0.5147726535797119,
      "learning_rate": 6.844843605723642e-06,
      "loss": 1.0592,
      "step": 6572
    },
    {
      "epoch": 61.43383436486885,
      "grad_norm": 0.5266843438148499,
      "learning_rate": 6.8419714877705225e-06,
      "loss": 1.088,
      "step": 6573
    },
    {
      "epoch": 61.443265546713825,
      "grad_norm": 0.5200413465499878,
      "learning_rate": 6.839099659154931e-06,
      "loss": 1.037,
      "step": 6574
    },
    {
      "epoch": 61.4526967285588,
      "grad_norm": 0.5601297616958618,
      "learning_rate": 6.836228120139981e-06,
      "loss": 1.0679,
      "step": 6575
    },
    {
      "epoch": 61.46212791040377,
      "grad_norm": 0.5377551317214966,
      "learning_rate": 6.833356870988759e-06,
      "loss": 1.0357,
      "step": 6576
    },
    {
      "epoch": 61.47155909224875,
      "grad_norm": 0.5074375867843628,
      "learning_rate": 6.830485911964337e-06,
      "loss": 1.0823,
      "step": 6577
    },
    {
      "epoch": 61.48099027409372,
      "grad_norm": 0.5338512659072876,
      "learning_rate": 6.827615243329746e-06,
      "loss": 1.0482,
      "step": 6578
    },
    {
      "epoch": 61.490421455938694,
      "grad_norm": 0.5275405645370483,
      "learning_rate": 6.824744865347996e-06,
      "loss": 1.0441,
      "step": 6579
    },
    {
      "epoch": 61.499852637783675,
      "grad_norm": 0.5735391974449158,
      "learning_rate": 6.821874778282072e-06,
      "loss": 1.0379,
      "step": 6580
    },
    {
      "epoch": 61.50928381962865,
      "grad_norm": 0.5784497857093811,
      "learning_rate": 6.819004982394928e-06,
      "loss": 1.029,
      "step": 6581
    },
    {
      "epoch": 61.51871500147362,
      "grad_norm": 0.5358495712280273,
      "learning_rate": 6.816135477949492e-06,
      "loss": 1.0472,
      "step": 6582
    },
    {
      "epoch": 61.5281461833186,
      "grad_norm": 0.4825313091278076,
      "learning_rate": 6.813266265208672e-06,
      "loss": 1.0937,
      "step": 6583
    },
    {
      "epoch": 61.53757736516357,
      "grad_norm": 0.536797046661377,
      "learning_rate": 6.8103973444353366e-06,
      "loss": 1.0483,
      "step": 6584
    },
    {
      "epoch": 61.547008547008545,
      "grad_norm": 0.5187873840332031,
      "learning_rate": 6.8075287158923455e-06,
      "loss": 1.0535,
      "step": 6585
    },
    {
      "epoch": 61.55643972885352,
      "grad_norm": 0.517396867275238,
      "learning_rate": 6.8046603798425155e-06,
      "loss": 1.0328,
      "step": 6586
    },
    {
      "epoch": 61.5658709106985,
      "grad_norm": 0.5210546255111694,
      "learning_rate": 6.801792336548645e-06,
      "loss": 1.0499,
      "step": 6587
    },
    {
      "epoch": 61.575302092543474,
      "grad_norm": 0.5391873717308044,
      "learning_rate": 6.798924586273501e-06,
      "loss": 1.0334,
      "step": 6588
    },
    {
      "epoch": 61.58473327438845,
      "grad_norm": 0.5406864285469055,
      "learning_rate": 6.7960571292798265e-06,
      "loss": 1.0506,
      "step": 6589
    },
    {
      "epoch": 61.59416445623342,
      "grad_norm": 0.5581021308898926,
      "learning_rate": 6.793189965830337e-06,
      "loss": 1.0698,
      "step": 6590
    },
    {
      "epoch": 61.603595638078396,
      "grad_norm": 0.5080016851425171,
      "learning_rate": 6.790323096187718e-06,
      "loss": 1.0829,
      "step": 6591
    },
    {
      "epoch": 61.61302681992337,
      "grad_norm": 0.48817986249923706,
      "learning_rate": 6.787456520614637e-06,
      "loss": 1.0686,
      "step": 6592
    },
    {
      "epoch": 61.622458001768344,
      "grad_norm": 0.6158440113067627,
      "learning_rate": 6.784590239373727e-06,
      "loss": 1.0713,
      "step": 6593
    },
    {
      "epoch": 61.631889183613325,
      "grad_norm": 0.5186137557029724,
      "learning_rate": 6.781724252727595e-06,
      "loss": 1.0785,
      "step": 6594
    },
    {
      "epoch": 61.6413203654583,
      "grad_norm": 0.5131083726882935,
      "learning_rate": 6.778858560938822e-06,
      "loss": 1.0406,
      "step": 6595
    },
    {
      "epoch": 61.65075154730327,
      "grad_norm": 0.5203525424003601,
      "learning_rate": 6.775993164269958e-06,
      "loss": 1.0547,
      "step": 6596
    },
    {
      "epoch": 61.66018272914825,
      "grad_norm": 0.5469070672988892,
      "learning_rate": 6.773128062983535e-06,
      "loss": 1.0307,
      "step": 6597
    },
    {
      "epoch": 61.66961391099322,
      "grad_norm": 0.5153897404670715,
      "learning_rate": 6.770263257342049e-06,
      "loss": 1.0594,
      "step": 6598
    },
    {
      "epoch": 61.679045092838194,
      "grad_norm": 0.5357221961021423,
      "learning_rate": 6.767398747607976e-06,
      "loss": 1.0665,
      "step": 6599
    },
    {
      "epoch": 61.68847627468317,
      "grad_norm": 0.48523110151290894,
      "learning_rate": 6.76453453404376e-06,
      "loss": 1.0694,
      "step": 6600
    },
    {
      "epoch": 61.69790745652815,
      "grad_norm": 0.5243363976478577,
      "learning_rate": 6.761670616911816e-06,
      "loss": 1.0673,
      "step": 6601
    },
    {
      "epoch": 61.70733863837312,
      "grad_norm": 0.4959283769130707,
      "learning_rate": 6.758806996474541e-06,
      "loss": 1.0588,
      "step": 6602
    },
    {
      "epoch": 61.7167698202181,
      "grad_norm": 0.5479819774627686,
      "learning_rate": 6.755943672994296e-06,
      "loss": 1.055,
      "step": 6603
    },
    {
      "epoch": 61.72620100206307,
      "grad_norm": 0.5192367434501648,
      "learning_rate": 6.753080646733416e-06,
      "loss": 1.0647,
      "step": 6604
    },
    {
      "epoch": 61.735632183908045,
      "grad_norm": 0.545669436454773,
      "learning_rate": 6.750217917954214e-06,
      "loss": 1.0363,
      "step": 6605
    },
    {
      "epoch": 61.74506336575302,
      "grad_norm": 0.5311061143875122,
      "learning_rate": 6.747355486918966e-06,
      "loss": 1.0762,
      "step": 6606
    },
    {
      "epoch": 61.75449454759799,
      "grad_norm": 0.5511078238487244,
      "learning_rate": 6.744493353889936e-06,
      "loss": 1.0791,
      "step": 6607
    },
    {
      "epoch": 61.763925729442974,
      "grad_norm": 0.5212148427963257,
      "learning_rate": 6.741631519129346e-06,
      "loss": 1.0886,
      "step": 6608
    },
    {
      "epoch": 61.77335691128795,
      "grad_norm": 0.5261622071266174,
      "learning_rate": 6.738769982899398e-06,
      "loss": 1.0615,
      "step": 6609
    },
    {
      "epoch": 61.78278809313292,
      "grad_norm": 0.5441513657569885,
      "learning_rate": 6.735908745462265e-06,
      "loss": 1.0863,
      "step": 6610
    },
    {
      "epoch": 61.792219274977896,
      "grad_norm": 0.5883972644805908,
      "learning_rate": 6.733047807080092e-06,
      "loss": 1.0617,
      "step": 6611
    },
    {
      "epoch": 61.80165045682287,
      "grad_norm": 0.5239925384521484,
      "learning_rate": 6.730187168014996e-06,
      "loss": 1.0868,
      "step": 6612
    },
    {
      "epoch": 61.811081638667844,
      "grad_norm": 0.4890758991241455,
      "learning_rate": 6.727326828529067e-06,
      "loss": 1.0299,
      "step": 6613
    },
    {
      "epoch": 61.82051282051282,
      "grad_norm": 0.5888767242431641,
      "learning_rate": 6.724466788884374e-06,
      "loss": 1.0426,
      "step": 6614
    },
    {
      "epoch": 61.8299440023578,
      "grad_norm": 0.5817403197288513,
      "learning_rate": 6.72160704934295e-06,
      "loss": 1.0067,
      "step": 6615
    },
    {
      "epoch": 61.83937518420277,
      "grad_norm": 0.5665786266326904,
      "learning_rate": 6.718747610166801e-06,
      "loss": 1.0392,
      "step": 6616
    },
    {
      "epoch": 61.84880636604775,
      "grad_norm": 0.5497019290924072,
      "learning_rate": 6.715888471617911e-06,
      "loss": 1.0499,
      "step": 6617
    },
    {
      "epoch": 61.85823754789272,
      "grad_norm": 0.5417355298995972,
      "learning_rate": 6.7130296339582324e-06,
      "loss": 1.0679,
      "step": 6618
    },
    {
      "epoch": 61.867668729737694,
      "grad_norm": 0.5331780314445496,
      "learning_rate": 6.710171097449689e-06,
      "loss": 1.0289,
      "step": 6619
    },
    {
      "epoch": 61.87709991158267,
      "grad_norm": 0.5803101658821106,
      "learning_rate": 6.707312862354183e-06,
      "loss": 1.0725,
      "step": 6620
    },
    {
      "epoch": 61.88653109342764,
      "grad_norm": 0.5733679533004761,
      "learning_rate": 6.704454928933576e-06,
      "loss": 0.9862,
      "step": 6621
    },
    {
      "epoch": 61.89596227527262,
      "grad_norm": 0.530989408493042,
      "learning_rate": 6.701597297449722e-06,
      "loss": 1.0439,
      "step": 6622
    },
    {
      "epoch": 61.9053934571176,
      "grad_norm": 0.5353664755821228,
      "learning_rate": 6.698739968164432e-06,
      "loss": 1.0847,
      "step": 6623
    },
    {
      "epoch": 61.91482463896257,
      "grad_norm": 0.5883890986442566,
      "learning_rate": 6.695882941339493e-06,
      "loss": 1.0588,
      "step": 6624
    },
    {
      "epoch": 61.924255820807545,
      "grad_norm": 0.5170300602912903,
      "learning_rate": 6.693026217236666e-06,
      "loss": 1.0455,
      "step": 6625
    },
    {
      "epoch": 61.93368700265252,
      "grad_norm": 0.5339601039886475,
      "learning_rate": 6.690169796117682e-06,
      "loss": 1.0997,
      "step": 6626
    },
    {
      "epoch": 61.94311818449749,
      "grad_norm": 0.5426146984100342,
      "learning_rate": 6.687313678244243e-06,
      "loss": 1.0697,
      "step": 6627
    },
    {
      "epoch": 61.95254936634247,
      "grad_norm": 0.5448604822158813,
      "learning_rate": 6.684457863878027e-06,
      "loss": 1.068,
      "step": 6628
    },
    {
      "epoch": 61.96198054818745,
      "grad_norm": 0.578607976436615,
      "learning_rate": 6.681602353280687e-06,
      "loss": 1.0236,
      "step": 6629
    },
    {
      "epoch": 61.97141173003242,
      "grad_norm": 0.5380234718322754,
      "learning_rate": 6.678747146713841e-06,
      "loss": 1.0436,
      "step": 6630
    },
    {
      "epoch": 61.980842911877396,
      "grad_norm": 0.5318371653556824,
      "learning_rate": 6.675892244439082e-06,
      "loss": 1.0685,
      "step": 6631
    },
    {
      "epoch": 61.99027409372237,
      "grad_norm": 0.5496357083320618,
      "learning_rate": 6.673037646717972e-06,
      "loss": 1.0421,
      "step": 6632
    },
    {
      "epoch": 61.999705275567344,
      "grad_norm": 0.5183234214782715,
      "learning_rate": 6.670183353812054e-06,
      "loss": 1.0516,
      "step": 6633
    },
    {
      "epoch": 62.0,
      "grad_norm": 4.384145259857178,
      "learning_rate": 6.667329365982835e-06,
      "loss": 0.54,
      "step": 6634
    },
    {
      "epoch": 62.009431181844974,
      "grad_norm": 0.5335506796836853,
      "learning_rate": 6.664475683491797e-06,
      "loss": 1.0692,
      "step": 6635
    },
    {
      "epoch": 62.01886236368995,
      "grad_norm": 0.5161835551261902,
      "learning_rate": 6.661622306600389e-06,
      "loss": 1.0493,
      "step": 6636
    },
    {
      "epoch": 62.02829354553492,
      "grad_norm": 0.5444179177284241,
      "learning_rate": 6.6587692355700405e-06,
      "loss": 1.0643,
      "step": 6637
    },
    {
      "epoch": 62.0377247273799,
      "grad_norm": 0.5752491354942322,
      "learning_rate": 6.655916470662152e-06,
      "loss": 1.0766,
      "step": 6638
    },
    {
      "epoch": 62.04715590922488,
      "grad_norm": 0.5440525412559509,
      "learning_rate": 6.653064012138088e-06,
      "loss": 1.0555,
      "step": 6639
    },
    {
      "epoch": 62.05658709106985,
      "grad_norm": 0.5572242140769958,
      "learning_rate": 6.65021186025919e-06,
      "loss": 1.0469,
      "step": 6640
    },
    {
      "epoch": 62.066018272914825,
      "grad_norm": 0.5359500050544739,
      "learning_rate": 6.6473600152867744e-06,
      "loss": 1.0782,
      "step": 6641
    },
    {
      "epoch": 62.0754494547598,
      "grad_norm": 0.558677077293396,
      "learning_rate": 6.644508477482123e-06,
      "loss": 1.0201,
      "step": 6642
    },
    {
      "epoch": 62.08488063660477,
      "grad_norm": 0.5262476205825806,
      "learning_rate": 6.641657247106491e-06,
      "loss": 1.0447,
      "step": 6643
    },
    {
      "epoch": 62.09431181844975,
      "grad_norm": 0.5449245572090149,
      "learning_rate": 6.638806324421115e-06,
      "loss": 1.0662,
      "step": 6644
    },
    {
      "epoch": 62.10374300029473,
      "grad_norm": 0.5455673933029175,
      "learning_rate": 6.63595570968719e-06,
      "loss": 1.0544,
      "step": 6645
    },
    {
      "epoch": 62.1131741821397,
      "grad_norm": 0.5498037934303284,
      "learning_rate": 6.633105403165887e-06,
      "loss": 1.0648,
      "step": 6646
    },
    {
      "epoch": 62.122605363984675,
      "grad_norm": 0.48838961124420166,
      "learning_rate": 6.630255405118356e-06,
      "loss": 1.072,
      "step": 6647
    },
    {
      "epoch": 62.13203654582965,
      "grad_norm": 0.5973381996154785,
      "learning_rate": 6.627405715805706e-06,
      "loss": 1.0767,
      "step": 6648
    },
    {
      "epoch": 62.14146772767462,
      "grad_norm": 0.5067005753517151,
      "learning_rate": 6.62455633548903e-06,
      "loss": 1.033,
      "step": 6649
    },
    {
      "epoch": 62.1508989095196,
      "grad_norm": 0.5251211524009705,
      "learning_rate": 6.6217072644293844e-06,
      "loss": 1.0521,
      "step": 6650
    },
    {
      "epoch": 62.16033009136457,
      "grad_norm": 0.5529710054397583,
      "learning_rate": 6.618858502887796e-06,
      "loss": 1.041,
      "step": 6651
    },
    {
      "epoch": 62.16976127320955,
      "grad_norm": 0.504368782043457,
      "learning_rate": 6.616010051125277e-06,
      "loss": 1.0474,
      "step": 6652
    },
    {
      "epoch": 62.179192455054526,
      "grad_norm": 0.5359634160995483,
      "learning_rate": 6.613161909402797e-06,
      "loss": 1.0076,
      "step": 6653
    },
    {
      "epoch": 62.1886236368995,
      "grad_norm": 0.5386047959327698,
      "learning_rate": 6.610314077981301e-06,
      "loss": 1.047,
      "step": 6654
    },
    {
      "epoch": 62.198054818744474,
      "grad_norm": 0.5348336100578308,
      "learning_rate": 6.607466557121708e-06,
      "loss": 1.044,
      "step": 6655
    },
    {
      "epoch": 62.20748600058945,
      "grad_norm": 0.5286587476730347,
      "learning_rate": 6.604619347084905e-06,
      "loss": 1.059,
      "step": 6656
    },
    {
      "epoch": 62.21691718243442,
      "grad_norm": 0.5327780842781067,
      "learning_rate": 6.6017724481317535e-06,
      "loss": 1.0502,
      "step": 6657
    },
    {
      "epoch": 62.226348364279396,
      "grad_norm": 0.5527210831642151,
      "learning_rate": 6.5989258605230825e-06,
      "loss": 1.0662,
      "step": 6658
    },
    {
      "epoch": 62.23577954612438,
      "grad_norm": 0.5144258141517639,
      "learning_rate": 6.5960795845197e-06,
      "loss": 1.072,
      "step": 6659
    },
    {
      "epoch": 62.24521072796935,
      "grad_norm": 0.5373411178588867,
      "learning_rate": 6.59323362038238e-06,
      "loss": 1.0503,
      "step": 6660
    },
    {
      "epoch": 62.254641909814325,
      "grad_norm": 0.5537605285644531,
      "learning_rate": 6.590387968371869e-06,
      "loss": 1.0333,
      "step": 6661
    },
    {
      "epoch": 62.2640730916593,
      "grad_norm": 0.553065299987793,
      "learning_rate": 6.58754262874888e-06,
      "loss": 1.0277,
      "step": 6662
    },
    {
      "epoch": 62.27350427350427,
      "grad_norm": 0.524355411529541,
      "learning_rate": 6.584697601774108e-06,
      "loss": 1.0109,
      "step": 6663
    },
    {
      "epoch": 62.28293545534925,
      "grad_norm": 0.5545167922973633,
      "learning_rate": 6.581852887708211e-06,
      "loss": 0.9954,
      "step": 6664
    },
    {
      "epoch": 62.29236663719422,
      "grad_norm": 0.5528215765953064,
      "learning_rate": 6.579008486811822e-06,
      "loss": 1.0756,
      "step": 6665
    },
    {
      "epoch": 62.3017978190392,
      "grad_norm": 0.5577453374862671,
      "learning_rate": 6.576164399345539e-06,
      "loss": 1.0531,
      "step": 6666
    },
    {
      "epoch": 62.311229000884175,
      "grad_norm": 0.5310421586036682,
      "learning_rate": 6.573320625569942e-06,
      "loss": 1.0668,
      "step": 6667
    },
    {
      "epoch": 62.32066018272915,
      "grad_norm": 0.5468242168426514,
      "learning_rate": 6.570477165745577e-06,
      "loss": 1.025,
      "step": 6668
    },
    {
      "epoch": 62.33009136457412,
      "grad_norm": 0.5429399609565735,
      "learning_rate": 6.567634020132957e-06,
      "loss": 1.1139,
      "step": 6669
    },
    {
      "epoch": 62.3395225464191,
      "grad_norm": 0.57331782579422,
      "learning_rate": 6.564791188992572e-06,
      "loss": 1.0468,
      "step": 6670
    },
    {
      "epoch": 62.34895372826407,
      "grad_norm": 0.5310287475585938,
      "learning_rate": 6.561948672584882e-06,
      "loss": 1.1005,
      "step": 6671
    },
    {
      "epoch": 62.358384910109045,
      "grad_norm": 0.5235208868980408,
      "learning_rate": 6.559106471170317e-06,
      "loss": 1.0462,
      "step": 6672
    },
    {
      "epoch": 62.367816091954026,
      "grad_norm": 0.5463609099388123,
      "learning_rate": 6.5562645850092725e-06,
      "loss": 1.0822,
      "step": 6673
    },
    {
      "epoch": 62.377247273799,
      "grad_norm": 0.5053850412368774,
      "learning_rate": 6.553423014362132e-06,
      "loss": 1.0274,
      "step": 6674
    },
    {
      "epoch": 62.386678455643974,
      "grad_norm": 0.5544313192367554,
      "learning_rate": 6.550581759489232e-06,
      "loss": 1.0619,
      "step": 6675
    },
    {
      "epoch": 62.39610963748895,
      "grad_norm": 0.5361156463623047,
      "learning_rate": 6.547740820650891e-06,
      "loss": 1.0563,
      "step": 6676
    },
    {
      "epoch": 62.40554081933392,
      "grad_norm": 0.5207270979881287,
      "learning_rate": 6.544900198107393e-06,
      "loss": 1.0583,
      "step": 6677
    },
    {
      "epoch": 62.414972001178896,
      "grad_norm": 0.5645213723182678,
      "learning_rate": 6.542059892118994e-06,
      "loss": 1.0792,
      "step": 6678
    },
    {
      "epoch": 62.42440318302387,
      "grad_norm": 0.5537790060043335,
      "learning_rate": 6.539219902945924e-06,
      "loss": 1.0556,
      "step": 6679
    },
    {
      "epoch": 62.43383436486885,
      "grad_norm": 0.5556953549385071,
      "learning_rate": 6.536380230848379e-06,
      "loss": 1.0417,
      "step": 6680
    },
    {
      "epoch": 62.443265546713825,
      "grad_norm": 0.525249183177948,
      "learning_rate": 6.5335408760865285e-06,
      "loss": 1.086,
      "step": 6681
    },
    {
      "epoch": 62.4526967285588,
      "grad_norm": 0.48537591099739075,
      "learning_rate": 6.530701838920518e-06,
      "loss": 1.0385,
      "step": 6682
    },
    {
      "epoch": 62.46212791040377,
      "grad_norm": 0.549577534198761,
      "learning_rate": 6.527863119610457e-06,
      "loss": 1.0764,
      "step": 6683
    },
    {
      "epoch": 62.47155909224875,
      "grad_norm": 0.5732659697532654,
      "learning_rate": 6.525024718416428e-06,
      "loss": 1.0186,
      "step": 6684
    },
    {
      "epoch": 62.48099027409372,
      "grad_norm": 0.5256980061531067,
      "learning_rate": 6.522186635598483e-06,
      "loss": 1.0694,
      "step": 6685
    },
    {
      "epoch": 62.490421455938694,
      "grad_norm": 0.6006115674972534,
      "learning_rate": 6.519348871416647e-06,
      "loss": 1.0423,
      "step": 6686
    },
    {
      "epoch": 62.499852637783675,
      "grad_norm": 0.5288941264152527,
      "learning_rate": 6.516511426130916e-06,
      "loss": 1.0578,
      "step": 6687
    },
    {
      "epoch": 62.50928381962865,
      "grad_norm": 0.4984540045261383,
      "learning_rate": 6.5136743000012514e-06,
      "loss": 1.0904,
      "step": 6688
    },
    {
      "epoch": 62.51871500147362,
      "grad_norm": 0.5354170799255371,
      "learning_rate": 6.5108374932875964e-06,
      "loss": 1.0204,
      "step": 6689
    },
    {
      "epoch": 62.5281461833186,
      "grad_norm": 0.5223137736320496,
      "learning_rate": 6.508001006249854e-06,
      "loss": 1.049,
      "step": 6690
    },
    {
      "epoch": 62.53757736516357,
      "grad_norm": 0.593636155128479,
      "learning_rate": 6.505164839147905e-06,
      "loss": 1.076,
      "step": 6691
    },
    {
      "epoch": 62.547008547008545,
      "grad_norm": 0.5321584939956665,
      "learning_rate": 6.5023289922415955e-06,
      "loss": 1.0467,
      "step": 6692
    },
    {
      "epoch": 62.55643972885352,
      "grad_norm": 0.5633756518363953,
      "learning_rate": 6.499493465790745e-06,
      "loss": 1.0211,
      "step": 6693
    },
    {
      "epoch": 62.5658709106985,
      "grad_norm": 0.5381914973258972,
      "learning_rate": 6.496658260055145e-06,
      "loss": 1.0742,
      "step": 6694
    },
    {
      "epoch": 62.575302092543474,
      "grad_norm": 0.5603837370872498,
      "learning_rate": 6.493823375294557e-06,
      "loss": 1.0301,
      "step": 6695
    },
    {
      "epoch": 62.58473327438845,
      "grad_norm": 0.5504440665245056,
      "learning_rate": 6.490988811768707e-06,
      "loss": 1.0763,
      "step": 6696
    },
    {
      "epoch": 62.59416445623342,
      "grad_norm": 0.5523439645767212,
      "learning_rate": 6.488154569737304e-06,
      "loss": 1.0209,
      "step": 6697
    },
    {
      "epoch": 62.603595638078396,
      "grad_norm": 0.5188002586364746,
      "learning_rate": 6.485320649460013e-06,
      "loss": 1.0707,
      "step": 6698
    },
    {
      "epoch": 62.61302681992337,
      "grad_norm": 0.5267020463943481,
      "learning_rate": 6.482487051196482e-06,
      "loss": 1.0707,
      "step": 6699
    },
    {
      "epoch": 62.622458001768344,
      "grad_norm": 0.5252809524536133,
      "learning_rate": 6.479653775206325e-06,
      "loss": 1.024,
      "step": 6700
    },
    {
      "epoch": 62.631889183613325,
      "grad_norm": 0.5441931486129761,
      "learning_rate": 6.476820821749123e-06,
      "loss": 1.0897,
      "step": 6701
    },
    {
      "epoch": 62.6413203654583,
      "grad_norm": 0.5366168022155762,
      "learning_rate": 6.473988191084431e-06,
      "loss": 1.0715,
      "step": 6702
    },
    {
      "epoch": 62.65075154730327,
      "grad_norm": 0.5508703589439392,
      "learning_rate": 6.4711558834717695e-06,
      "loss": 1.0361,
      "step": 6703
    },
    {
      "epoch": 62.66018272914825,
      "grad_norm": 0.5269337892532349,
      "learning_rate": 6.468323899170641e-06,
      "loss": 1.0759,
      "step": 6704
    },
    {
      "epoch": 62.66961391099322,
      "grad_norm": 0.5511062145233154,
      "learning_rate": 6.465492238440508e-06,
      "loss": 1.0864,
      "step": 6705
    },
    {
      "epoch": 62.679045092838194,
      "grad_norm": 0.5282919406890869,
      "learning_rate": 6.462660901540807e-06,
      "loss": 1.0461,
      "step": 6706
    },
    {
      "epoch": 62.68847627468317,
      "grad_norm": 0.5545432567596436,
      "learning_rate": 6.4598298887309415e-06,
      "loss": 1.0537,
      "step": 6707
    },
    {
      "epoch": 62.69790745652815,
      "grad_norm": 0.5174610614776611,
      "learning_rate": 6.4569992002702906e-06,
      "loss": 1.0607,
      "step": 6708
    },
    {
      "epoch": 62.70733863837312,
      "grad_norm": 0.5184136033058167,
      "learning_rate": 6.4541688364182e-06,
      "loss": 1.0166,
      "step": 6709
    },
    {
      "epoch": 62.7167698202181,
      "grad_norm": 0.5366177558898926,
      "learning_rate": 6.451338797433983e-06,
      "loss": 1.0711,
      "step": 6710
    },
    {
      "epoch": 62.72620100206307,
      "grad_norm": 0.5109651684761047,
      "learning_rate": 6.448509083576934e-06,
      "loss": 1.0885,
      "step": 6711
    },
    {
      "epoch": 62.735632183908045,
      "grad_norm": 0.5311932563781738,
      "learning_rate": 6.445679695106306e-06,
      "loss": 1.0469,
      "step": 6712
    },
    {
      "epoch": 62.74506336575302,
      "grad_norm": 0.5429024696350098,
      "learning_rate": 6.44285063228133e-06,
      "loss": 1.0594,
      "step": 6713
    },
    {
      "epoch": 62.75449454759799,
      "grad_norm": 0.5619844794273376,
      "learning_rate": 6.440021895361199e-06,
      "loss": 1.072,
      "step": 6714
    },
    {
      "epoch": 62.763925729442974,
      "grad_norm": 0.5157431960105896,
      "learning_rate": 6.437193484605085e-06,
      "loss": 1.0399,
      "step": 6715
    },
    {
      "epoch": 62.77335691128795,
      "grad_norm": 0.5176275372505188,
      "learning_rate": 6.434365400272124e-06,
      "loss": 1.091,
      "step": 6716
    },
    {
      "epoch": 62.78278809313292,
      "grad_norm": 0.5264784097671509,
      "learning_rate": 6.431537642621426e-06,
      "loss": 1.0474,
      "step": 6717
    },
    {
      "epoch": 62.792219274977896,
      "grad_norm": 0.554391086101532,
      "learning_rate": 6.428710211912061e-06,
      "loss": 1.0069,
      "step": 6718
    },
    {
      "epoch": 62.80165045682287,
      "grad_norm": 0.4984785318374634,
      "learning_rate": 6.42588310840309e-06,
      "loss": 1.0056,
      "step": 6719
    },
    {
      "epoch": 62.811081638667844,
      "grad_norm": 0.5747044086456299,
      "learning_rate": 6.423056332353525e-06,
      "loss": 1.0521,
      "step": 6720
    },
    {
      "epoch": 62.82051282051282,
      "grad_norm": 0.5550630688667297,
      "learning_rate": 6.420229884022355e-06,
      "loss": 1.0872,
      "step": 6721
    },
    {
      "epoch": 62.8299440023578,
      "grad_norm": 0.5215076208114624,
      "learning_rate": 6.417403763668537e-06,
      "loss": 1.056,
      "step": 6722
    },
    {
      "epoch": 62.83937518420277,
      "grad_norm": 0.5245978832244873,
      "learning_rate": 6.414577971551001e-06,
      "loss": 1.0739,
      "step": 6723
    },
    {
      "epoch": 62.84880636604775,
      "grad_norm": 0.5269624590873718,
      "learning_rate": 6.411752507928643e-06,
      "loss": 1.0498,
      "step": 6724
    },
    {
      "epoch": 62.85823754789272,
      "grad_norm": 0.5957425832748413,
      "learning_rate": 6.408927373060331e-06,
      "loss": 1.0402,
      "step": 6725
    },
    {
      "epoch": 62.867668729737694,
      "grad_norm": 0.5555205345153809,
      "learning_rate": 6.406102567204906e-06,
      "loss": 1.0345,
      "step": 6726
    },
    {
      "epoch": 62.87709991158267,
      "grad_norm": 0.5662789344787598,
      "learning_rate": 6.403278090621174e-06,
      "loss": 1.039,
      "step": 6727
    },
    {
      "epoch": 62.88653109342764,
      "grad_norm": 0.5539235472679138,
      "learning_rate": 6.400453943567914e-06,
      "loss": 1.0709,
      "step": 6728
    },
    {
      "epoch": 62.89596227527262,
      "grad_norm": 0.5552473068237305,
      "learning_rate": 6.397630126303869e-06,
      "loss": 1.0173,
      "step": 6729
    },
    {
      "epoch": 62.9053934571176,
      "grad_norm": 0.5478200912475586,
      "learning_rate": 6.394806639087761e-06,
      "loss": 1.0527,
      "step": 6730
    },
    {
      "epoch": 62.91482463896257,
      "grad_norm": 0.5167745351791382,
      "learning_rate": 6.391983482178277e-06,
      "loss": 1.0742,
      "step": 6731
    },
    {
      "epoch": 62.924255820807545,
      "grad_norm": 0.5023274421691895,
      "learning_rate": 6.38916065583407e-06,
      "loss": 1.0408,
      "step": 6732
    },
    {
      "epoch": 62.93368700265252,
      "grad_norm": 0.5560541749000549,
      "learning_rate": 6.386338160313766e-06,
      "loss": 1.026,
      "step": 6733
    },
    {
      "epoch": 62.94311818449749,
      "grad_norm": 0.5081763863563538,
      "learning_rate": 6.383515995875964e-06,
      "loss": 1.0719,
      "step": 6734
    },
    {
      "epoch": 62.95254936634247,
      "grad_norm": 0.5028923153877258,
      "learning_rate": 6.380694162779232e-06,
      "loss": 1.0682,
      "step": 6735
    },
    {
      "epoch": 62.96198054818745,
      "grad_norm": 0.5443663001060486,
      "learning_rate": 6.377872661282102e-06,
      "loss": 1.0724,
      "step": 6736
    },
    {
      "epoch": 62.97141173003242,
      "grad_norm": 0.4950157105922699,
      "learning_rate": 6.375051491643081e-06,
      "loss": 1.0479,
      "step": 6737
    },
    {
      "epoch": 62.980842911877396,
      "grad_norm": 0.5536915063858032,
      "learning_rate": 6.372230654120639e-06,
      "loss": 1.0611,
      "step": 6738
    },
    {
      "epoch": 62.99027409372237,
      "grad_norm": 0.527632474899292,
      "learning_rate": 6.369410148973226e-06,
      "loss": 1.0657,
      "step": 6739
    },
    {
      "epoch": 62.999705275567344,
      "grad_norm": 0.5366581082344055,
      "learning_rate": 6.366589976459249e-06,
      "loss": 1.0648,
      "step": 6740
    },
    {
      "epoch": 63.0,
      "grad_norm": 3.0927820205688477,
      "learning_rate": 6.3637701368371e-06,
      "loss": 0.5322,
      "step": 6741
    },
    {
      "epoch": 63.009431181844974,
      "grad_norm": 0.5264453887939453,
      "learning_rate": 6.360950630365126e-06,
      "loss": 1.0282,
      "step": 6742
    },
    {
      "epoch": 63.01886236368995,
      "grad_norm": 0.528849184513092,
      "learning_rate": 6.358131457301651e-06,
      "loss": 1.0428,
      "step": 6743
    },
    {
      "epoch": 63.02829354553492,
      "grad_norm": 0.5348801016807556,
      "learning_rate": 6.355312617904965e-06,
      "loss": 1.0572,
      "step": 6744
    },
    {
      "epoch": 63.0377247273799,
      "grad_norm": 0.5127131342887878,
      "learning_rate": 6.3524941124333315e-06,
      "loss": 1.0923,
      "step": 6745
    },
    {
      "epoch": 63.04715590922488,
      "grad_norm": 0.5320390462875366,
      "learning_rate": 6.34967594114498e-06,
      "loss": 1.0863,
      "step": 6746
    },
    {
      "epoch": 63.05658709106985,
      "grad_norm": 0.5880796313285828,
      "learning_rate": 6.3468581042981105e-06,
      "loss": 1.0022,
      "step": 6747
    },
    {
      "epoch": 63.066018272914825,
      "grad_norm": 0.5472208857536316,
      "learning_rate": 6.344040602150887e-06,
      "loss": 1.0308,
      "step": 6748
    },
    {
      "epoch": 63.0754494547598,
      "grad_norm": 0.4890401065349579,
      "learning_rate": 6.341223434961459e-06,
      "loss": 1.0502,
      "step": 6749
    },
    {
      "epoch": 63.08488063660477,
      "grad_norm": 0.5398758053779602,
      "learning_rate": 6.338406602987928e-06,
      "loss": 1.0617,
      "step": 6750
    },
    {
      "epoch": 63.09431181844975,
      "grad_norm": 0.5521430373191833,
      "learning_rate": 6.335590106488371e-06,
      "loss": 1.0726,
      "step": 6751
    },
    {
      "epoch": 63.10374300029473,
      "grad_norm": 0.5409716367721558,
      "learning_rate": 6.332773945720836e-06,
      "loss": 1.0002,
      "step": 6752
    },
    {
      "epoch": 63.1131741821397,
      "grad_norm": 0.4964320659637451,
      "learning_rate": 6.329958120943337e-06,
      "loss": 1.0706,
      "step": 6753
    },
    {
      "epoch": 63.122605363984675,
      "grad_norm": 0.5423842668533325,
      "learning_rate": 6.327142632413861e-06,
      "loss": 1.0523,
      "step": 6754
    },
    {
      "epoch": 63.13203654582965,
      "grad_norm": 0.5579737424850464,
      "learning_rate": 6.324327480390356e-06,
      "loss": 1.0289,
      "step": 6755
    },
    {
      "epoch": 63.14146772767462,
      "grad_norm": 0.5432571768760681,
      "learning_rate": 6.321512665130756e-06,
      "loss": 1.0523,
      "step": 6756
    },
    {
      "epoch": 63.1508989095196,
      "grad_norm": 0.5416097044944763,
      "learning_rate": 6.3186981868929465e-06,
      "loss": 1.0488,
      "step": 6757
    },
    {
      "epoch": 63.16033009136457,
      "grad_norm": 0.5175101161003113,
      "learning_rate": 6.315884045934791e-06,
      "loss": 1.0887,
      "step": 6758
    },
    {
      "epoch": 63.16976127320955,
      "grad_norm": 0.5312358736991882,
      "learning_rate": 6.31307024251412e-06,
      "loss": 1.0443,
      "step": 6759
    },
    {
      "epoch": 63.179192455054526,
      "grad_norm": 0.5919592380523682,
      "learning_rate": 6.31025677688873e-06,
      "loss": 1.0574,
      "step": 6760
    },
    {
      "epoch": 63.1886236368995,
      "grad_norm": 0.5426636338233948,
      "learning_rate": 6.307443649316395e-06,
      "loss": 1.0484,
      "step": 6761
    },
    {
      "epoch": 63.198054818744474,
      "grad_norm": 0.5253401398658752,
      "learning_rate": 6.30463086005485e-06,
      "loss": 1.0544,
      "step": 6762
    },
    {
      "epoch": 63.20748600058945,
      "grad_norm": 0.5198162794113159,
      "learning_rate": 6.3018184093618005e-06,
      "loss": 1.0782,
      "step": 6763
    },
    {
      "epoch": 63.21691718243442,
      "grad_norm": 0.5964500308036804,
      "learning_rate": 6.299006297494924e-06,
      "loss": 1.0759,
      "step": 6764
    },
    {
      "epoch": 63.226348364279396,
      "grad_norm": 0.5287726521492004,
      "learning_rate": 6.296194524711867e-06,
      "loss": 1.09,
      "step": 6765
    },
    {
      "epoch": 63.23577954612438,
      "grad_norm": 0.5143879652023315,
      "learning_rate": 6.293383091270242e-06,
      "loss": 1.0157,
      "step": 6766
    },
    {
      "epoch": 63.24521072796935,
      "grad_norm": 0.5309983491897583,
      "learning_rate": 6.290571997427632e-06,
      "loss": 1.0489,
      "step": 6767
    },
    {
      "epoch": 63.254641909814325,
      "grad_norm": 0.570847749710083,
      "learning_rate": 6.287761243441586e-06,
      "loss": 1.0149,
      "step": 6768
    },
    {
      "epoch": 63.2640730916593,
      "grad_norm": 0.5234587788581848,
      "learning_rate": 6.284950829569626e-06,
      "loss": 1.074,
      "step": 6769
    },
    {
      "epoch": 63.27350427350427,
      "grad_norm": 0.5079824924468994,
      "learning_rate": 6.282140756069237e-06,
      "loss": 1.0619,
      "step": 6770
    },
    {
      "epoch": 63.28293545534925,
      "grad_norm": 0.5158966183662415,
      "learning_rate": 6.279331023197885e-06,
      "loss": 1.0899,
      "step": 6771
    },
    {
      "epoch": 63.29236663719422,
      "grad_norm": 0.5427273511886597,
      "learning_rate": 6.276521631212995e-06,
      "loss": 1.0934,
      "step": 6772
    },
    {
      "epoch": 63.3017978190392,
      "grad_norm": 0.5301452875137329,
      "learning_rate": 6.273712580371958e-06,
      "loss": 1.0517,
      "step": 6773
    },
    {
      "epoch": 63.311229000884175,
      "grad_norm": 0.5228143334388733,
      "learning_rate": 6.270903870932141e-06,
      "loss": 1.0675,
      "step": 6774
    },
    {
      "epoch": 63.32066018272915,
      "grad_norm": 0.5803122520446777,
      "learning_rate": 6.268095503150878e-06,
      "loss": 1.0523,
      "step": 6775
    },
    {
      "epoch": 63.33009136457412,
      "grad_norm": 0.5463577508926392,
      "learning_rate": 6.265287477285469e-06,
      "loss": 1.0458,
      "step": 6776
    },
    {
      "epoch": 63.3395225464191,
      "grad_norm": 0.5074554085731506,
      "learning_rate": 6.262479793593186e-06,
      "loss": 1.0608,
      "step": 6777
    },
    {
      "epoch": 63.34895372826407,
      "grad_norm": 0.5413140058517456,
      "learning_rate": 6.259672452331261e-06,
      "loss": 1.0666,
      "step": 6778
    },
    {
      "epoch": 63.358384910109045,
      "grad_norm": 0.5409265756607056,
      "learning_rate": 6.256865453756913e-06,
      "loss": 1.0425,
      "step": 6779
    },
    {
      "epoch": 63.367816091954026,
      "grad_norm": 0.5484773516654968,
      "learning_rate": 6.254058798127312e-06,
      "loss": 1.0301,
      "step": 6780
    },
    {
      "epoch": 63.377247273799,
      "grad_norm": 0.5557829737663269,
      "learning_rate": 6.251252485699605e-06,
      "loss": 1.0157,
      "step": 6781
    },
    {
      "epoch": 63.386678455643974,
      "grad_norm": 0.5708958506584167,
      "learning_rate": 6.248446516730904e-06,
      "loss": 1.0605,
      "step": 6782
    },
    {
      "epoch": 63.39610963748895,
      "grad_norm": 0.5463111996650696,
      "learning_rate": 6.245640891478291e-06,
      "loss": 1.0835,
      "step": 6783
    },
    {
      "epoch": 63.40554081933392,
      "grad_norm": 0.5249557495117188,
      "learning_rate": 6.2428356101988155e-06,
      "loss": 1.0365,
      "step": 6784
    },
    {
      "epoch": 63.414972001178896,
      "grad_norm": 0.5349636077880859,
      "learning_rate": 6.240030673149494e-06,
      "loss": 1.0717,
      "step": 6785
    },
    {
      "epoch": 63.42440318302387,
      "grad_norm": 0.4986015558242798,
      "learning_rate": 6.2372260805873216e-06,
      "loss": 1.0597,
      "step": 6786
    },
    {
      "epoch": 63.43383436486885,
      "grad_norm": 0.5260382294654846,
      "learning_rate": 6.23442183276925e-06,
      "loss": 1.0425,
      "step": 6787
    },
    {
      "epoch": 63.443265546713825,
      "grad_norm": 0.5365006923675537,
      "learning_rate": 6.231617929952204e-06,
      "loss": 1.0303,
      "step": 6788
    },
    {
      "epoch": 63.4526967285588,
      "grad_norm": 0.5461073517799377,
      "learning_rate": 6.2288143723930746e-06,
      "loss": 1.0471,
      "step": 6789
    },
    {
      "epoch": 63.46212791040377,
      "grad_norm": 0.6534308195114136,
      "learning_rate": 6.2260111603487235e-06,
      "loss": 1.0356,
      "step": 6790
    },
    {
      "epoch": 63.47155909224875,
      "grad_norm": 0.5271276235580444,
      "learning_rate": 6.223208294075982e-06,
      "loss": 1.0727,
      "step": 6791
    },
    {
      "epoch": 63.48099027409372,
      "grad_norm": 0.5517271161079407,
      "learning_rate": 6.220405773831646e-06,
      "loss": 1.0127,
      "step": 6792
    },
    {
      "epoch": 63.490421455938694,
      "grad_norm": 0.5367189645767212,
      "learning_rate": 6.217603599872479e-06,
      "loss": 1.0333,
      "step": 6793
    },
    {
      "epoch": 63.499852637783675,
      "grad_norm": 0.5248900651931763,
      "learning_rate": 6.214801772455221e-06,
      "loss": 1.0277,
      "step": 6794
    },
    {
      "epoch": 63.50928381962865,
      "grad_norm": 0.5201401114463806,
      "learning_rate": 6.212000291836568e-06,
      "loss": 1.0946,
      "step": 6795
    },
    {
      "epoch": 63.51871500147362,
      "grad_norm": 0.5743617415428162,
      "learning_rate": 6.209199158273198e-06,
      "loss": 1.0879,
      "step": 6796
    },
    {
      "epoch": 63.5281461833186,
      "grad_norm": 0.5524697303771973,
      "learning_rate": 6.206398372021747e-06,
      "loss": 1.0802,
      "step": 6797
    },
    {
      "epoch": 63.53757736516357,
      "grad_norm": 0.5379385352134705,
      "learning_rate": 6.2035979333388185e-06,
      "loss": 1.0677,
      "step": 6798
    },
    {
      "epoch": 63.547008547008545,
      "grad_norm": 0.5187560319900513,
      "learning_rate": 6.200797842480992e-06,
      "loss": 1.0581,
      "step": 6799
    },
    {
      "epoch": 63.55643972885352,
      "grad_norm": 0.47811123728752136,
      "learning_rate": 6.197998099704805e-06,
      "loss": 1.0419,
      "step": 6800
    },
    {
      "epoch": 63.5658709106985,
      "grad_norm": 0.4994007349014282,
      "learning_rate": 6.195198705266778e-06,
      "loss": 1.0724,
      "step": 6801
    },
    {
      "epoch": 63.575302092543474,
      "grad_norm": 0.554157555103302,
      "learning_rate": 6.192399659423386e-06,
      "loss": 1.0992,
      "step": 6802
    },
    {
      "epoch": 63.58473327438845,
      "grad_norm": 0.523724377155304,
      "learning_rate": 6.189600962431076e-06,
      "loss": 1.0266,
      "step": 6803
    },
    {
      "epoch": 63.59416445623342,
      "grad_norm": 0.5588677525520325,
      "learning_rate": 6.186802614546264e-06,
      "loss": 1.0662,
      "step": 6804
    },
    {
      "epoch": 63.603595638078396,
      "grad_norm": 0.571531355381012,
      "learning_rate": 6.184004616025335e-06,
      "loss": 1.024,
      "step": 6805
    },
    {
      "epoch": 63.61302681992337,
      "grad_norm": 0.49950355291366577,
      "learning_rate": 6.18120696712464e-06,
      "loss": 1.0528,
      "step": 6806
    },
    {
      "epoch": 63.622458001768344,
      "grad_norm": 0.5174357295036316,
      "learning_rate": 6.178409668100498e-06,
      "loss": 1.0507,
      "step": 6807
    },
    {
      "epoch": 63.631889183613325,
      "grad_norm": 0.5768933296203613,
      "learning_rate": 6.175612719209193e-06,
      "loss": 1.0417,
      "step": 6808
    },
    {
      "epoch": 63.6413203654583,
      "grad_norm": 0.5146832466125488,
      "learning_rate": 6.172816120706988e-06,
      "loss": 1.0353,
      "step": 6809
    },
    {
      "epoch": 63.65075154730327,
      "grad_norm": 0.5491276383399963,
      "learning_rate": 6.170019872850104e-06,
      "loss": 1.0498,
      "step": 6810
    },
    {
      "epoch": 63.66018272914825,
      "grad_norm": 0.5113802552223206,
      "learning_rate": 6.167223975894731e-06,
      "loss": 1.0455,
      "step": 6811
    },
    {
      "epoch": 63.66961391099322,
      "grad_norm": 0.5414143800735474,
      "learning_rate": 6.164428430097029e-06,
      "loss": 1.0741,
      "step": 6812
    },
    {
      "epoch": 63.679045092838194,
      "grad_norm": 0.5323787927627563,
      "learning_rate": 6.161633235713123e-06,
      "loss": 1.0554,
      "step": 6813
    },
    {
      "epoch": 63.68847627468317,
      "grad_norm": 0.5768638253211975,
      "learning_rate": 6.15883839299911e-06,
      "loss": 1.0319,
      "step": 6814
    },
    {
      "epoch": 63.69790745652815,
      "grad_norm": 0.5920081734657288,
      "learning_rate": 6.156043902211049e-06,
      "loss": 1.0538,
      "step": 6815
    },
    {
      "epoch": 63.70733863837312,
      "grad_norm": 0.5364930033683777,
      "learning_rate": 6.153249763604975e-06,
      "loss": 1.0495,
      "step": 6816
    },
    {
      "epoch": 63.7167698202181,
      "grad_norm": 0.554073691368103,
      "learning_rate": 6.150455977436884e-06,
      "loss": 1.0331,
      "step": 6817
    },
    {
      "epoch": 63.72620100206307,
      "grad_norm": 0.49484390020370483,
      "learning_rate": 6.147662543962745e-06,
      "loss": 1.0419,
      "step": 6818
    },
    {
      "epoch": 63.735632183908045,
      "grad_norm": 0.526843786239624,
      "learning_rate": 6.144869463438485e-06,
      "loss": 1.0346,
      "step": 6819
    },
    {
      "epoch": 63.74506336575302,
      "grad_norm": 0.5406365990638733,
      "learning_rate": 6.1420767361200105e-06,
      "loss": 1.1097,
      "step": 6820
    },
    {
      "epoch": 63.75449454759799,
      "grad_norm": 0.5444060564041138,
      "learning_rate": 6.139284362263185e-06,
      "loss": 1.0348,
      "step": 6821
    },
    {
      "epoch": 63.763925729442974,
      "grad_norm": 0.4979515075683594,
      "learning_rate": 6.13649234212385e-06,
      "loss": 1.0754,
      "step": 6822
    },
    {
      "epoch": 63.77335691128795,
      "grad_norm": 0.546602725982666,
      "learning_rate": 6.133700675957807e-06,
      "loss": 1.0344,
      "step": 6823
    },
    {
      "epoch": 63.78278809313292,
      "grad_norm": 0.5380690693855286,
      "learning_rate": 6.130909364020829e-06,
      "loss": 1.0818,
      "step": 6824
    },
    {
      "epoch": 63.792219274977896,
      "grad_norm": 0.49821555614471436,
      "learning_rate": 6.128118406568655e-06,
      "loss": 1.0708,
      "step": 6825
    },
    {
      "epoch": 63.80165045682287,
      "grad_norm": 0.5462883710861206,
      "learning_rate": 6.125327803856989e-06,
      "loss": 1.0379,
      "step": 6826
    },
    {
      "epoch": 63.811081638667844,
      "grad_norm": 0.5132081508636475,
      "learning_rate": 6.1225375561415076e-06,
      "loss": 1.0221,
      "step": 6827
    },
    {
      "epoch": 63.82051282051282,
      "grad_norm": 0.5115802884101868,
      "learning_rate": 6.119747663677853e-06,
      "loss": 1.0481,
      "step": 6828
    },
    {
      "epoch": 63.8299440023578,
      "grad_norm": 0.5456043481826782,
      "learning_rate": 6.116958126721633e-06,
      "loss": 1.0884,
      "step": 6829
    },
    {
      "epoch": 63.83937518420277,
      "grad_norm": 0.5461821556091309,
      "learning_rate": 6.11416894552842e-06,
      "loss": 1.0665,
      "step": 6830
    },
    {
      "epoch": 63.84880636604775,
      "grad_norm": 0.5167234539985657,
      "learning_rate": 6.111380120353763e-06,
      "loss": 1.0579,
      "step": 6831
    },
    {
      "epoch": 63.85823754789272,
      "grad_norm": 0.5361011028289795,
      "learning_rate": 6.108591651453174e-06,
      "loss": 1.0475,
      "step": 6832
    },
    {
      "epoch": 63.867668729737694,
      "grad_norm": 0.538018524646759,
      "learning_rate": 6.1058035390821295e-06,
      "loss": 1.0579,
      "step": 6833
    },
    {
      "epoch": 63.87709991158267,
      "grad_norm": 0.5037674307823181,
      "learning_rate": 6.103015783496076e-06,
      "loss": 1.06,
      "step": 6834
    },
    {
      "epoch": 63.88653109342764,
      "grad_norm": 0.5366043448448181,
      "learning_rate": 6.100228384950425e-06,
      "loss": 1.0275,
      "step": 6835
    },
    {
      "epoch": 63.89596227527262,
      "grad_norm": 0.5053076148033142,
      "learning_rate": 6.097441343700559e-06,
      "loss": 1.0139,
      "step": 6836
    },
    {
      "epoch": 63.9053934571176,
      "grad_norm": 0.5316596031188965,
      "learning_rate": 6.094654660001822e-06,
      "loss": 1.0602,
      "step": 6837
    },
    {
      "epoch": 63.91482463896257,
      "grad_norm": 0.5305508971214294,
      "learning_rate": 6.091868334109534e-06,
      "loss": 1.0482,
      "step": 6838
    },
    {
      "epoch": 63.924255820807545,
      "grad_norm": 0.6103939414024353,
      "learning_rate": 6.0890823662789776e-06,
      "loss": 1.0249,
      "step": 6839
    },
    {
      "epoch": 63.93368700265252,
      "grad_norm": 0.5224563479423523,
      "learning_rate": 6.0862967567653995e-06,
      "loss": 1.1036,
      "step": 6840
    },
    {
      "epoch": 63.94311818449749,
      "grad_norm": 0.4871307909488678,
      "learning_rate": 6.0835115058240165e-06,
      "loss": 1.062,
      "step": 6841
    },
    {
      "epoch": 63.95254936634247,
      "grad_norm": 0.53387051820755,
      "learning_rate": 6.080726613710013e-06,
      "loss": 1.0724,
      "step": 6842
    },
    {
      "epoch": 63.96198054818745,
      "grad_norm": 0.5273730158805847,
      "learning_rate": 6.07794208067854e-06,
      "loss": 1.0344,
      "step": 6843
    },
    {
      "epoch": 63.97141173003242,
      "grad_norm": 0.5587930679321289,
      "learning_rate": 6.075157906984714e-06,
      "loss": 1.0309,
      "step": 6844
    },
    {
      "epoch": 63.980842911877396,
      "grad_norm": 0.5162415504455566,
      "learning_rate": 6.072374092883618e-06,
      "loss": 1.0696,
      "step": 6845
    },
    {
      "epoch": 63.99027409372237,
      "grad_norm": 0.530207097530365,
      "learning_rate": 6.0695906386303115e-06,
      "loss": 1.0319,
      "step": 6846
    },
    {
      "epoch": 63.999705275567344,
      "grad_norm": 0.5105500817298889,
      "learning_rate": 6.066807544479809e-06,
      "loss": 1.0816,
      "step": 6847
    },
    {
      "epoch": 64.0,
      "grad_norm": 3.910166025161743,
      "learning_rate": 6.064024810687098e-06,
      "loss": 0.2887,
      "step": 6848
    },
    {
      "epoch": 64.00943118184497,
      "grad_norm": 0.5294271111488342,
      "learning_rate": 6.061242437507131e-06,
      "loss": 1.0176,
      "step": 6849
    },
    {
      "epoch": 64.01886236368995,
      "grad_norm": 0.548961341381073,
      "learning_rate": 6.0584604251948265e-06,
      "loss": 1.0115,
      "step": 6850
    },
    {
      "epoch": 64.02829354553492,
      "grad_norm": 0.5395129919052124,
      "learning_rate": 6.0556787740050735e-06,
      "loss": 1.0607,
      "step": 6851
    },
    {
      "epoch": 64.0377247273799,
      "grad_norm": 0.5257405638694763,
      "learning_rate": 6.052897484192721e-06,
      "loss": 1.0994,
      "step": 6852
    },
    {
      "epoch": 64.04715590922487,
      "grad_norm": 0.5470574498176575,
      "learning_rate": 6.0501165560126e-06,
      "loss": 1.0393,
      "step": 6853
    },
    {
      "epoch": 64.05658709106984,
      "grad_norm": 0.5242436528205872,
      "learning_rate": 6.0473359897194915e-06,
      "loss": 1.0199,
      "step": 6854
    },
    {
      "epoch": 64.06601827291482,
      "grad_norm": 0.5484583973884583,
      "learning_rate": 6.0445557855681504e-06,
      "loss": 1.0503,
      "step": 6855
    },
    {
      "epoch": 64.0754494547598,
      "grad_norm": 0.5421627759933472,
      "learning_rate": 6.0417759438133e-06,
      "loss": 1.0579,
      "step": 6856
    },
    {
      "epoch": 64.08488063660478,
      "grad_norm": 0.5769106149673462,
      "learning_rate": 6.038996464709623e-06,
      "loss": 1.0227,
      "step": 6857
    },
    {
      "epoch": 64.09431181844975,
      "grad_norm": 0.5765407085418701,
      "learning_rate": 6.036217348511782e-06,
      "loss": 1.0811,
      "step": 6858
    },
    {
      "epoch": 64.10374300029473,
      "grad_norm": 0.5053533911705017,
      "learning_rate": 6.033438595474397e-06,
      "loss": 1.0805,
      "step": 6859
    },
    {
      "epoch": 64.1131741821397,
      "grad_norm": 0.5123759508132935,
      "learning_rate": 6.03066020585205e-06,
      "loss": 1.0667,
      "step": 6860
    },
    {
      "epoch": 64.12260536398468,
      "grad_norm": 0.5525862574577332,
      "learning_rate": 6.027882179899304e-06,
      "loss": 1.0745,
      "step": 6861
    },
    {
      "epoch": 64.13203654582965,
      "grad_norm": 0.5309796929359436,
      "learning_rate": 6.025104517870676e-06,
      "loss": 1.024,
      "step": 6862
    },
    {
      "epoch": 64.14146772767462,
      "grad_norm": 0.5084445476531982,
      "learning_rate": 6.022327220020658e-06,
      "loss": 1.0723,
      "step": 6863
    },
    {
      "epoch": 64.1508989095196,
      "grad_norm": 0.5527663230895996,
      "learning_rate": 6.019550286603703e-06,
      "loss": 1.0553,
      "step": 6864
    },
    {
      "epoch": 64.16033009136457,
      "grad_norm": 0.5356804132461548,
      "learning_rate": 6.016773717874234e-06,
      "loss": 1.0813,
      "step": 6865
    },
    {
      "epoch": 64.16976127320955,
      "grad_norm": 0.5382869839668274,
      "learning_rate": 6.013997514086636e-06,
      "loss": 1.0732,
      "step": 6866
    },
    {
      "epoch": 64.17919245505452,
      "grad_norm": 0.545336902141571,
      "learning_rate": 6.011221675495264e-06,
      "loss": 1.0481,
      "step": 6867
    },
    {
      "epoch": 64.1886236368995,
      "grad_norm": 0.4891982972621918,
      "learning_rate": 6.008446202354446e-06,
      "loss": 1.0567,
      "step": 6868
    },
    {
      "epoch": 64.19805481874447,
      "grad_norm": 0.5314447283744812,
      "learning_rate": 6.0056710949184636e-06,
      "loss": 1.0524,
      "step": 6869
    },
    {
      "epoch": 64.20748600058945,
      "grad_norm": 0.5572106242179871,
      "learning_rate": 6.002896353441574e-06,
      "loss": 1.0423,
      "step": 6870
    },
    {
      "epoch": 64.21691718243443,
      "grad_norm": 0.5222135782241821,
      "learning_rate": 6.0001219781779975e-06,
      "loss": 1.0587,
      "step": 6871
    },
    {
      "epoch": 64.2263483642794,
      "grad_norm": 0.5674095153808594,
      "learning_rate": 5.997347969381921e-06,
      "loss": 1.0197,
      "step": 6872
    },
    {
      "epoch": 64.23577954612438,
      "grad_norm": 0.5933526158332825,
      "learning_rate": 5.994574327307498e-06,
      "loss": 1.0509,
      "step": 6873
    },
    {
      "epoch": 64.24521072796935,
      "grad_norm": 0.5037246942520142,
      "learning_rate": 5.9918010522088495e-06,
      "loss": 1.0821,
      "step": 6874
    },
    {
      "epoch": 64.25464190981432,
      "grad_norm": 0.5535149574279785,
      "learning_rate": 5.9890281443400585e-06,
      "loss": 1.0143,
      "step": 6875
    },
    {
      "epoch": 64.2640730916593,
      "grad_norm": 0.532433807849884,
      "learning_rate": 5.986255603955184e-06,
      "loss": 1.0595,
      "step": 6876
    },
    {
      "epoch": 64.27350427350427,
      "grad_norm": 0.5446585416793823,
      "learning_rate": 5.983483431308242e-06,
      "loss": 1.0554,
      "step": 6877
    },
    {
      "epoch": 64.28293545534925,
      "grad_norm": 0.5495551824569702,
      "learning_rate": 5.98071162665322e-06,
      "loss": 1.0755,
      "step": 6878
    },
    {
      "epoch": 64.29236663719422,
      "grad_norm": 0.5307919979095459,
      "learning_rate": 5.977940190244067e-06,
      "loss": 1.0511,
      "step": 6879
    },
    {
      "epoch": 64.3017978190392,
      "grad_norm": 0.5143378973007202,
      "learning_rate": 5.975169122334702e-06,
      "loss": 1.0478,
      "step": 6880
    },
    {
      "epoch": 64.31122900088417,
      "grad_norm": 0.5969057679176331,
      "learning_rate": 5.972398423179009e-06,
      "loss": 1.0624,
      "step": 6881
    },
    {
      "epoch": 64.32066018272914,
      "grad_norm": 0.5392197966575623,
      "learning_rate": 5.969628093030837e-06,
      "loss": 1.0965,
      "step": 6882
    },
    {
      "epoch": 64.33009136457412,
      "grad_norm": 0.5747541189193726,
      "learning_rate": 5.966858132144006e-06,
      "loss": 1.026,
      "step": 6883
    },
    {
      "epoch": 64.3395225464191,
      "grad_norm": 0.5202687382698059,
      "learning_rate": 5.9640885407723015e-06,
      "loss": 1.0672,
      "step": 6884
    },
    {
      "epoch": 64.34895372826408,
      "grad_norm": 0.5169472694396973,
      "learning_rate": 5.961319319169466e-06,
      "loss": 1.0587,
      "step": 6885
    },
    {
      "epoch": 64.35838491010905,
      "grad_norm": 0.5331164598464966,
      "learning_rate": 5.958550467589218e-06,
      "loss": 1.0111,
      "step": 6886
    },
    {
      "epoch": 64.36781609195403,
      "grad_norm": 0.5608664751052856,
      "learning_rate": 5.955781986285236e-06,
      "loss": 1.064,
      "step": 6887
    },
    {
      "epoch": 64.377247273799,
      "grad_norm": 0.5634045004844666,
      "learning_rate": 5.953013875511173e-06,
      "loss": 1.0549,
      "step": 6888
    },
    {
      "epoch": 64.38667845564397,
      "grad_norm": 0.5164267420768738,
      "learning_rate": 5.950246135520638e-06,
      "loss": 1.07,
      "step": 6889
    },
    {
      "epoch": 64.39610963748895,
      "grad_norm": 0.5407065153121948,
      "learning_rate": 5.947478766567209e-06,
      "loss": 1.0539,
      "step": 6890
    },
    {
      "epoch": 64.40554081933392,
      "grad_norm": 0.5843902230262756,
      "learning_rate": 5.944711768904437e-06,
      "loss": 1.0836,
      "step": 6891
    },
    {
      "epoch": 64.4149720011789,
      "grad_norm": 0.547447681427002,
      "learning_rate": 5.941945142785826e-06,
      "loss": 1.0196,
      "step": 6892
    },
    {
      "epoch": 64.42440318302387,
      "grad_norm": 0.5311463475227356,
      "learning_rate": 5.9391788884648605e-06,
      "loss": 1.0605,
      "step": 6893
    },
    {
      "epoch": 64.43383436486884,
      "grad_norm": 0.576039731502533,
      "learning_rate": 5.936413006194982e-06,
      "loss": 1.0425,
      "step": 6894
    },
    {
      "epoch": 64.44326554671382,
      "grad_norm": 0.5195832848548889,
      "learning_rate": 5.933647496229597e-06,
      "loss": 1.0493,
      "step": 6895
    },
    {
      "epoch": 64.45269672855879,
      "grad_norm": 0.5903199911117554,
      "learning_rate": 5.930882358822084e-06,
      "loss": 1.057,
      "step": 6896
    },
    {
      "epoch": 64.46212791040377,
      "grad_norm": 0.520408034324646,
      "learning_rate": 5.928117594225778e-06,
      "loss": 1.09,
      "step": 6897
    },
    {
      "epoch": 64.47155909224875,
      "grad_norm": 0.5140337347984314,
      "learning_rate": 5.925353202693992e-06,
      "loss": 1.0762,
      "step": 6898
    },
    {
      "epoch": 64.48099027409373,
      "grad_norm": 0.5350922346115112,
      "learning_rate": 5.922589184479999e-06,
      "loss": 1.0839,
      "step": 6899
    },
    {
      "epoch": 64.4904214559387,
      "grad_norm": 0.4696202576160431,
      "learning_rate": 5.919825539837033e-06,
      "loss": 1.036,
      "step": 6900
    },
    {
      "epoch": 64.49985263778368,
      "grad_norm": 0.5918317437171936,
      "learning_rate": 5.917062269018301e-06,
      "loss": 1.0424,
      "step": 6901
    },
    {
      "epoch": 64.50928381962865,
      "grad_norm": 0.533429741859436,
      "learning_rate": 5.914299372276972e-06,
      "loss": 1.0632,
      "step": 6902
    },
    {
      "epoch": 64.51871500147362,
      "grad_norm": 0.5119151473045349,
      "learning_rate": 5.911536849866183e-06,
      "loss": 1.0748,
      "step": 6903
    },
    {
      "epoch": 64.5281461833186,
      "grad_norm": 0.5644817352294922,
      "learning_rate": 5.908774702039033e-06,
      "loss": 1.0344,
      "step": 6904
    },
    {
      "epoch": 64.53757736516357,
      "grad_norm": 0.5251531600952148,
      "learning_rate": 5.906012929048588e-06,
      "loss": 1.0235,
      "step": 6905
    },
    {
      "epoch": 64.54700854700855,
      "grad_norm": 0.5745527148246765,
      "learning_rate": 5.9032515311478855e-06,
      "loss": 1.0387,
      "step": 6906
    },
    {
      "epoch": 64.55643972885352,
      "grad_norm": 0.5811229348182678,
      "learning_rate": 5.900490508589922e-06,
      "loss": 1.0608,
      "step": 6907
    },
    {
      "epoch": 64.5658709106985,
      "grad_norm": 0.6242253184318542,
      "learning_rate": 5.897729861627662e-06,
      "loss": 1.0335,
      "step": 6908
    },
    {
      "epoch": 64.57530209254347,
      "grad_norm": 0.578890860080719,
      "learning_rate": 5.8949695905140324e-06,
      "loss": 1.0315,
      "step": 6909
    },
    {
      "epoch": 64.58473327438844,
      "grad_norm": 0.5214337110519409,
      "learning_rate": 5.892209695501929e-06,
      "loss": 1.0876,
      "step": 6910
    },
    {
      "epoch": 64.59416445623341,
      "grad_norm": 0.5248622894287109,
      "learning_rate": 5.8894501768442155e-06,
      "loss": 1.0174,
      "step": 6911
    },
    {
      "epoch": 64.6035956380784,
      "grad_norm": 0.5304977893829346,
      "learning_rate": 5.8866910347937116e-06,
      "loss": 1.0653,
      "step": 6912
    },
    {
      "epoch": 64.61302681992338,
      "grad_norm": 0.5503389835357666,
      "learning_rate": 5.883932269603217e-06,
      "loss": 1.0201,
      "step": 6913
    },
    {
      "epoch": 64.62245800176835,
      "grad_norm": 0.5076111555099487,
      "learning_rate": 5.881173881525485e-06,
      "loss": 1.0536,
      "step": 6914
    },
    {
      "epoch": 64.63188918361332,
      "grad_norm": 0.4840455651283264,
      "learning_rate": 5.8784158708132385e-06,
      "loss": 1.088,
      "step": 6915
    },
    {
      "epoch": 64.6413203654583,
      "grad_norm": 0.5584325194358826,
      "learning_rate": 5.875658237719166e-06,
      "loss": 1.0615,
      "step": 6916
    },
    {
      "epoch": 64.65075154730327,
      "grad_norm": 0.544089674949646,
      "learning_rate": 5.87290098249592e-06,
      "loss": 1.0353,
      "step": 6917
    },
    {
      "epoch": 64.66018272914825,
      "grad_norm": 0.5552452206611633,
      "learning_rate": 5.8701441053961185e-06,
      "loss": 1.0657,
      "step": 6918
    },
    {
      "epoch": 64.66961391099322,
      "grad_norm": 0.5697823166847229,
      "learning_rate": 5.867387606672348e-06,
      "loss": 1.0249,
      "step": 6919
    },
    {
      "epoch": 64.6790450928382,
      "grad_norm": 0.5220887660980225,
      "learning_rate": 5.864631486577154e-06,
      "loss": 1.0743,
      "step": 6920
    },
    {
      "epoch": 64.68847627468317,
      "grad_norm": 0.5549216270446777,
      "learning_rate": 5.861875745363057e-06,
      "loss": 1.0274,
      "step": 6921
    },
    {
      "epoch": 64.69790745652814,
      "grad_norm": 0.5109993815422058,
      "learning_rate": 5.859120383282534e-06,
      "loss": 1.0432,
      "step": 6922
    },
    {
      "epoch": 64.70733863837312,
      "grad_norm": 0.5332460999488831,
      "learning_rate": 5.856365400588029e-06,
      "loss": 1.0237,
      "step": 6923
    },
    {
      "epoch": 64.71676982021809,
      "grad_norm": 0.5468975901603699,
      "learning_rate": 5.8536107975319564e-06,
      "loss": 1.048,
      "step": 6924
    },
    {
      "epoch": 64.72620100206306,
      "grad_norm": 0.497039794921875,
      "learning_rate": 5.850856574366689e-06,
      "loss": 1.0553,
      "step": 6925
    },
    {
      "epoch": 64.73563218390805,
      "grad_norm": 0.5533066987991333,
      "learning_rate": 5.8481027313445694e-06,
      "loss": 1.044,
      "step": 6926
    },
    {
      "epoch": 64.74506336575303,
      "grad_norm": 0.5306147336959839,
      "learning_rate": 5.845349268717901e-06,
      "loss": 1.0882,
      "step": 6927
    },
    {
      "epoch": 64.754494547598,
      "grad_norm": 0.5355231761932373,
      "learning_rate": 5.842596186738957e-06,
      "loss": 1.0688,
      "step": 6928
    },
    {
      "epoch": 64.76392572944297,
      "grad_norm": 0.5385836958885193,
      "learning_rate": 5.839843485659975e-06,
      "loss": 1.0228,
      "step": 6929
    },
    {
      "epoch": 64.77335691128795,
      "grad_norm": 0.5632675886154175,
      "learning_rate": 5.8370911657331574e-06,
      "loss": 1.0237,
      "step": 6930
    },
    {
      "epoch": 64.78278809313292,
      "grad_norm": 0.5401067733764648,
      "learning_rate": 5.834339227210668e-06,
      "loss": 1.0751,
      "step": 6931
    },
    {
      "epoch": 64.7922192749779,
      "grad_norm": 0.5405423045158386,
      "learning_rate": 5.8315876703446405e-06,
      "loss": 1.0001,
      "step": 6932
    },
    {
      "epoch": 64.80165045682287,
      "grad_norm": 0.5633833408355713,
      "learning_rate": 5.828836495387169e-06,
      "loss": 1.0778,
      "step": 6933
    },
    {
      "epoch": 64.81108163866784,
      "grad_norm": 0.5333189368247986,
      "learning_rate": 5.826085702590314e-06,
      "loss": 1.0553,
      "step": 6934
    },
    {
      "epoch": 64.82051282051282,
      "grad_norm": 0.5573611855506897,
      "learning_rate": 5.823335292206109e-06,
      "loss": 1.0556,
      "step": 6935
    },
    {
      "epoch": 64.82994400235779,
      "grad_norm": 0.5665323734283447,
      "learning_rate": 5.820585264486542e-06,
      "loss": 1.04,
      "step": 6936
    },
    {
      "epoch": 64.83937518420277,
      "grad_norm": 0.5453238487243652,
      "learning_rate": 5.817835619683569e-06,
      "loss": 1.0694,
      "step": 6937
    },
    {
      "epoch": 64.84880636604774,
      "grad_norm": 0.5350841879844666,
      "learning_rate": 5.815086358049112e-06,
      "loss": 1.0105,
      "step": 6938
    },
    {
      "epoch": 64.85823754789271,
      "grad_norm": 0.5332770943641663,
      "learning_rate": 5.812337479835059e-06,
      "loss": 1.0627,
      "step": 6939
    },
    {
      "epoch": 64.8676687297377,
      "grad_norm": 0.5566542148590088,
      "learning_rate": 5.809588985293258e-06,
      "loss": 1.066,
      "step": 6940
    },
    {
      "epoch": 64.87709991158268,
      "grad_norm": 0.4988747537136078,
      "learning_rate": 5.8068408746755264e-06,
      "loss": 1.0863,
      "step": 6941
    },
    {
      "epoch": 64.88653109342765,
      "grad_norm": 0.5645219683647156,
      "learning_rate": 5.804093148233642e-06,
      "loss": 1.0684,
      "step": 6942
    },
    {
      "epoch": 64.89596227527262,
      "grad_norm": 0.5239999890327454,
      "learning_rate": 5.8013458062193605e-06,
      "loss": 1.0017,
      "step": 6943
    },
    {
      "epoch": 64.9053934571176,
      "grad_norm": 0.5087770819664001,
      "learning_rate": 5.7985988488843845e-06,
      "loss": 1.0717,
      "step": 6944
    },
    {
      "epoch": 64.91482463896257,
      "grad_norm": 0.5138173699378967,
      "learning_rate": 5.7958522764803905e-06,
      "loss": 1.0567,
      "step": 6945
    },
    {
      "epoch": 64.92425582080755,
      "grad_norm": 0.564908504486084,
      "learning_rate": 5.793106089259019e-06,
      "loss": 1.0679,
      "step": 6946
    },
    {
      "epoch": 64.93368700265252,
      "grad_norm": 0.5431555509567261,
      "learning_rate": 5.790360287471877e-06,
      "loss": 1.0484,
      "step": 6947
    },
    {
      "epoch": 64.9431181844975,
      "grad_norm": 0.548454761505127,
      "learning_rate": 5.787614871370531e-06,
      "loss": 1.0556,
      "step": 6948
    },
    {
      "epoch": 64.95254936634247,
      "grad_norm": 0.5645176768302917,
      "learning_rate": 5.7848698412065105e-06,
      "loss": 1.02,
      "step": 6949
    },
    {
      "epoch": 64.96198054818744,
      "grad_norm": 0.5160647034645081,
      "learning_rate": 5.7821251972313254e-06,
      "loss": 1.0651,
      "step": 6950
    },
    {
      "epoch": 64.97141173003241,
      "grad_norm": 0.5973486304283142,
      "learning_rate": 5.779380939696432e-06,
      "loss": 1.0309,
      "step": 6951
    },
    {
      "epoch": 64.98084291187739,
      "grad_norm": 0.5551409125328064,
      "learning_rate": 5.7766370688532604e-06,
      "loss": 1.1123,
      "step": 6952
    },
    {
      "epoch": 64.99027409372236,
      "grad_norm": 0.5203641057014465,
      "learning_rate": 5.773893584953202e-06,
      "loss": 1.0883,
      "step": 6953
    },
    {
      "epoch": 64.99970527556735,
      "grad_norm": 0.5218489766120911,
      "learning_rate": 5.771150488247615e-06,
      "loss": 1.0653,
      "step": 6954
    },
    {
      "epoch": 65.0,
      "grad_norm": 3.723162889480591,
      "learning_rate": 5.768407778987819e-06,
      "loss": 0.7241,
      "step": 6955
    },
    {
      "epoch": 65.00943118184497,
      "grad_norm": 0.5524440407752991,
      "learning_rate": 5.765665457425102e-06,
      "loss": 1.0147,
      "step": 6956
    },
    {
      "epoch": 65.01886236368995,
      "grad_norm": 0.5830124616622925,
      "learning_rate": 5.762923523810711e-06,
      "loss": 1.0642,
      "step": 6957
    },
    {
      "epoch": 65.02829354553492,
      "grad_norm": 0.5240907073020935,
      "learning_rate": 5.760181978395867e-06,
      "loss": 1.0272,
      "step": 6958
    },
    {
      "epoch": 65.0377247273799,
      "grad_norm": 0.5358644723892212,
      "learning_rate": 5.757440821431745e-06,
      "loss": 1.0634,
      "step": 6959
    },
    {
      "epoch": 65.04715590922487,
      "grad_norm": 0.5612937211990356,
      "learning_rate": 5.754700053169493e-06,
      "loss": 1.0545,
      "step": 6960
    },
    {
      "epoch": 65.05658709106984,
      "grad_norm": 0.5505442023277283,
      "learning_rate": 5.751959673860216e-06,
      "loss": 1.031,
      "step": 6961
    },
    {
      "epoch": 65.06601827291482,
      "grad_norm": 0.5129523277282715,
      "learning_rate": 5.749219683754987e-06,
      "loss": 1.0492,
      "step": 6962
    },
    {
      "epoch": 65.0754494547598,
      "grad_norm": 0.6027721166610718,
      "learning_rate": 5.746480083104844e-06,
      "loss": 1.0773,
      "step": 6963
    },
    {
      "epoch": 65.08488063660478,
      "grad_norm": 0.5115621089935303,
      "learning_rate": 5.743740872160783e-06,
      "loss": 1.0358,
      "step": 6964
    },
    {
      "epoch": 65.09431181844975,
      "grad_norm": 0.5197620391845703,
      "learning_rate": 5.741002051173779e-06,
      "loss": 1.0379,
      "step": 6965
    },
    {
      "epoch": 65.10374300029473,
      "grad_norm": 0.5021257996559143,
      "learning_rate": 5.738263620394755e-06,
      "loss": 1.0767,
      "step": 6966
    },
    {
      "epoch": 65.1131741821397,
      "grad_norm": 0.5925473570823669,
      "learning_rate": 5.735525580074609e-06,
      "loss": 1.0652,
      "step": 6967
    },
    {
      "epoch": 65.12260536398468,
      "grad_norm": 0.5359468460083008,
      "learning_rate": 5.7327879304641965e-06,
      "loss": 1.0733,
      "step": 6968
    },
    {
      "epoch": 65.13203654582965,
      "grad_norm": 0.5411875247955322,
      "learning_rate": 5.730050671814338e-06,
      "loss": 0.9946,
      "step": 6969
    },
    {
      "epoch": 65.14146772767462,
      "grad_norm": 0.5539634227752686,
      "learning_rate": 5.727313804375827e-06,
      "loss": 1.0933,
      "step": 6970
    },
    {
      "epoch": 65.1508989095196,
      "grad_norm": 0.5297803282737732,
      "learning_rate": 5.724577328399412e-06,
      "loss": 1.0811,
      "step": 6971
    },
    {
      "epoch": 65.16033009136457,
      "grad_norm": 0.5410375595092773,
      "learning_rate": 5.721841244135805e-06,
      "loss": 1.0945,
      "step": 6972
    },
    {
      "epoch": 65.16976127320955,
      "grad_norm": 0.52375328540802,
      "learning_rate": 5.719105551835687e-06,
      "loss": 1.0425,
      "step": 6973
    },
    {
      "epoch": 65.17919245505452,
      "grad_norm": 0.5089174509048462,
      "learning_rate": 5.7163702517496985e-06,
      "loss": 1.0442,
      "step": 6974
    },
    {
      "epoch": 65.1886236368995,
      "grad_norm": 0.5341485142707825,
      "learning_rate": 5.713635344128452e-06,
      "loss": 1.0527,
      "step": 6975
    },
    {
      "epoch": 65.19805481874447,
      "grad_norm": 0.5211697816848755,
      "learning_rate": 5.710900829222517e-06,
      "loss": 1.0555,
      "step": 6976
    },
    {
      "epoch": 65.20748600058945,
      "grad_norm": 0.5833621025085449,
      "learning_rate": 5.708166707282427e-06,
      "loss": 1.0323,
      "step": 6977
    },
    {
      "epoch": 65.21691718243443,
      "grad_norm": 0.5442790985107422,
      "learning_rate": 5.705432978558685e-06,
      "loss": 1.0603,
      "step": 6978
    },
    {
      "epoch": 65.2263483642794,
      "grad_norm": 0.5557539463043213,
      "learning_rate": 5.7026996433017455e-06,
      "loss": 1.0901,
      "step": 6979
    },
    {
      "epoch": 65.23577954612438,
      "grad_norm": 0.5238215327262878,
      "learning_rate": 5.699966701762047e-06,
      "loss": 1.0496,
      "step": 6980
    },
    {
      "epoch": 65.24521072796935,
      "grad_norm": 0.5763434767723083,
      "learning_rate": 5.6972341541899745e-06,
      "loss": 1.0488,
      "step": 6981
    },
    {
      "epoch": 65.25464190981432,
      "grad_norm": 0.5325907468795776,
      "learning_rate": 5.694502000835887e-06,
      "loss": 1.0903,
      "step": 6982
    },
    {
      "epoch": 65.2640730916593,
      "grad_norm": 0.5264800786972046,
      "learning_rate": 5.691770241950098e-06,
      "loss": 1.05,
      "step": 6983
    },
    {
      "epoch": 65.27350427350427,
      "grad_norm": 0.5470333695411682,
      "learning_rate": 5.6890388777828935e-06,
      "loss": 1.0303,
      "step": 6984
    },
    {
      "epoch": 65.28293545534925,
      "grad_norm": 0.5510003566741943,
      "learning_rate": 5.686307908584521e-06,
      "loss": 1.0614,
      "step": 6985
    },
    {
      "epoch": 65.29236663719422,
      "grad_norm": 0.5299981236457825,
      "learning_rate": 5.683577334605189e-06,
      "loss": 1.044,
      "step": 6986
    },
    {
      "epoch": 65.3017978190392,
      "grad_norm": 0.5637125968933105,
      "learning_rate": 5.680847156095069e-06,
      "loss": 1.0417,
      "step": 6987
    },
    {
      "epoch": 65.31122900088417,
      "grad_norm": 0.548669159412384,
      "learning_rate": 5.678117373304306e-06,
      "loss": 1.0459,
      "step": 6988
    },
    {
      "epoch": 65.32066018272914,
      "grad_norm": 0.5644062161445618,
      "learning_rate": 5.6753879864829995e-06,
      "loss": 1.0505,
      "step": 6989
    },
    {
      "epoch": 65.33009136457412,
      "grad_norm": 0.5419919490814209,
      "learning_rate": 5.672658995881214e-06,
      "loss": 1.0314,
      "step": 6990
    },
    {
      "epoch": 65.3395225464191,
      "grad_norm": 0.5526641011238098,
      "learning_rate": 5.669930401748977e-06,
      "loss": 1.0682,
      "step": 6991
    },
    {
      "epoch": 65.34895372826408,
      "grad_norm": 0.5210589170455933,
      "learning_rate": 5.667202204336286e-06,
      "loss": 1.0512,
      "step": 6992
    },
    {
      "epoch": 65.35838491010905,
      "grad_norm": 0.5246173143386841,
      "learning_rate": 5.664474403893092e-06,
      "loss": 1.0711,
      "step": 6993
    },
    {
      "epoch": 65.36781609195403,
      "grad_norm": 0.5214343667030334,
      "learning_rate": 5.661747000669315e-06,
      "loss": 1.0816,
      "step": 6994
    },
    {
      "epoch": 65.377247273799,
      "grad_norm": 0.5681782364845276,
      "learning_rate": 5.659019994914846e-06,
      "loss": 1.0935,
      "step": 6995
    },
    {
      "epoch": 65.38667845564397,
      "grad_norm": 0.5258205533027649,
      "learning_rate": 5.6562933868795276e-06,
      "loss": 1.0682,
      "step": 6996
    },
    {
      "epoch": 65.39610963748895,
      "grad_norm": 0.5599279403686523,
      "learning_rate": 5.65356717681317e-06,
      "loss": 1.0728,
      "step": 6997
    },
    {
      "epoch": 65.40554081933392,
      "grad_norm": 0.521424412727356,
      "learning_rate": 5.6508413649655505e-06,
      "loss": 1.035,
      "step": 6998
    },
    {
      "epoch": 65.4149720011789,
      "grad_norm": 0.5306124687194824,
      "learning_rate": 5.648115951586404e-06,
      "loss": 1.0427,
      "step": 6999
    },
    {
      "epoch": 65.42440318302387,
      "grad_norm": 0.5428833961486816,
      "learning_rate": 5.645390936925435e-06,
      "loss": 1.0243,
      "step": 7000
    },
    {
      "epoch": 65.43383436486884,
      "grad_norm": 0.5435571074485779,
      "learning_rate": 5.642666321232304e-06,
      "loss": 1.0595,
      "step": 7001
    },
    {
      "epoch": 65.44326554671382,
      "grad_norm": 0.5371835231781006,
      "learning_rate": 5.6399421047566395e-06,
      "loss": 1.0874,
      "step": 7002
    },
    {
      "epoch": 65.45269672855879,
      "grad_norm": 0.5237584710121155,
      "learning_rate": 5.637218287748041e-06,
      "loss": 1.0617,
      "step": 7003
    },
    {
      "epoch": 65.46212791040377,
      "grad_norm": 0.5049348473548889,
      "learning_rate": 5.634494870456057e-06,
      "loss": 1.0926,
      "step": 7004
    },
    {
      "epoch": 65.47155909224875,
      "grad_norm": 0.555419921875,
      "learning_rate": 5.631771853130208e-06,
      "loss": 1.036,
      "step": 7005
    },
    {
      "epoch": 65.48099027409373,
      "grad_norm": 0.5284885764122009,
      "learning_rate": 5.6290492360199765e-06,
      "loss": 1.04,
      "step": 7006
    },
    {
      "epoch": 65.4904214559387,
      "grad_norm": 0.5649023056030273,
      "learning_rate": 5.626327019374807e-06,
      "loss": 1.0329,
      "step": 7007
    },
    {
      "epoch": 65.49985263778368,
      "grad_norm": 0.54936283826828,
      "learning_rate": 5.623605203444107e-06,
      "loss": 1.0481,
      "step": 7008
    },
    {
      "epoch": 65.50928381962865,
      "grad_norm": 0.527906060218811,
      "learning_rate": 5.620883788477246e-06,
      "loss": 1.0647,
      "step": 7009
    },
    {
      "epoch": 65.51871500147362,
      "grad_norm": 0.5243217945098877,
      "learning_rate": 5.618162774723566e-06,
      "loss": 1.0673,
      "step": 7010
    },
    {
      "epoch": 65.5281461833186,
      "grad_norm": 0.5242462754249573,
      "learning_rate": 5.615442162432363e-06,
      "loss": 1.0649,
      "step": 7011
    },
    {
      "epoch": 65.53757736516357,
      "grad_norm": 0.6171452403068542,
      "learning_rate": 5.612721951852897e-06,
      "loss": 1.0488,
      "step": 7012
    },
    {
      "epoch": 65.54700854700855,
      "grad_norm": 0.5561549663543701,
      "learning_rate": 5.610002143234393e-06,
      "loss": 1.0497,
      "step": 7013
    },
    {
      "epoch": 65.55643972885352,
      "grad_norm": 0.5157666802406311,
      "learning_rate": 5.607282736826041e-06,
      "loss": 1.0701,
      "step": 7014
    },
    {
      "epoch": 65.5658709106985,
      "grad_norm": 0.5225009322166443,
      "learning_rate": 5.604563732876989e-06,
      "loss": 1.0739,
      "step": 7015
    },
    {
      "epoch": 65.57530209254347,
      "grad_norm": 0.5099774599075317,
      "learning_rate": 5.601845131636352e-06,
      "loss": 1.0466,
      "step": 7016
    },
    {
      "epoch": 65.58473327438844,
      "grad_norm": 0.5415313839912415,
      "learning_rate": 5.599126933353205e-06,
      "loss": 1.0442,
      "step": 7017
    },
    {
      "epoch": 65.59416445623341,
      "grad_norm": 0.5426389575004578,
      "learning_rate": 5.5964091382765954e-06,
      "loss": 1.0379,
      "step": 7018
    },
    {
      "epoch": 65.6035956380784,
      "grad_norm": 0.5170034766197205,
      "learning_rate": 5.593691746655522e-06,
      "loss": 1.0497,
      "step": 7019
    },
    {
      "epoch": 65.61302681992338,
      "grad_norm": 0.5067299604415894,
      "learning_rate": 5.590974758738952e-06,
      "loss": 1.0671,
      "step": 7020
    },
    {
      "epoch": 65.62245800176835,
      "grad_norm": 0.5246617197990417,
      "learning_rate": 5.588258174775816e-06,
      "loss": 1.1007,
      "step": 7021
    },
    {
      "epoch": 65.63188918361332,
      "grad_norm": 0.5406259894371033,
      "learning_rate": 5.5855419950150045e-06,
      "loss": 1.0034,
      "step": 7022
    },
    {
      "epoch": 65.6413203654583,
      "grad_norm": 0.5616956353187561,
      "learning_rate": 5.582826219705375e-06,
      "loss": 1.0288,
      "step": 7023
    },
    {
      "epoch": 65.65075154730327,
      "grad_norm": 0.5388810634613037,
      "learning_rate": 5.58011084909574e-06,
      "loss": 1.0726,
      "step": 7024
    },
    {
      "epoch": 65.66018272914825,
      "grad_norm": 0.5301604270935059,
      "learning_rate": 5.577395883434891e-06,
      "loss": 1.0468,
      "step": 7025
    },
    {
      "epoch": 65.66961391099322,
      "grad_norm": 0.5363073945045471,
      "learning_rate": 5.5746813229715645e-06,
      "loss": 1.0351,
      "step": 7026
    },
    {
      "epoch": 65.6790450928382,
      "grad_norm": 0.5326611995697021,
      "learning_rate": 5.5719671679544725e-06,
      "loss": 1.0265,
      "step": 7027
    },
    {
      "epoch": 65.68847627468317,
      "grad_norm": 0.5600500106811523,
      "learning_rate": 5.569253418632281e-06,
      "loss": 1.0432,
      "step": 7028
    },
    {
      "epoch": 65.69790745652814,
      "grad_norm": 0.5472760796546936,
      "learning_rate": 5.566540075253626e-06,
      "loss": 1.017,
      "step": 7029
    },
    {
      "epoch": 65.70733863837312,
      "grad_norm": 0.5252571702003479,
      "learning_rate": 5.563827138067102e-06,
      "loss": 1.0725,
      "step": 7030
    },
    {
      "epoch": 65.71676982021809,
      "grad_norm": 0.5011675357818604,
      "learning_rate": 5.561114607321261e-06,
      "loss": 1.088,
      "step": 7031
    },
    {
      "epoch": 65.72620100206306,
      "grad_norm": 0.5469350814819336,
      "learning_rate": 5.558402483264636e-06,
      "loss": 1.0486,
      "step": 7032
    },
    {
      "epoch": 65.73563218390805,
      "grad_norm": 0.5752447247505188,
      "learning_rate": 5.555690766145703e-06,
      "loss": 1.0399,
      "step": 7033
    },
    {
      "epoch": 65.74506336575303,
      "grad_norm": 0.5476040244102478,
      "learning_rate": 5.552979456212913e-06,
      "loss": 1.0706,
      "step": 7034
    },
    {
      "epoch": 65.754494547598,
      "grad_norm": 0.5259434580802917,
      "learning_rate": 5.55026855371467e-06,
      "loss": 1.0324,
      "step": 7035
    },
    {
      "epoch": 65.76392572944297,
      "grad_norm": 0.5317122936248779,
      "learning_rate": 5.5475580588993476e-06,
      "loss": 1.0768,
      "step": 7036
    },
    {
      "epoch": 65.77335691128795,
      "grad_norm": 0.5446083545684814,
      "learning_rate": 5.544847972015286e-06,
      "loss": 1.044,
      "step": 7037
    },
    {
      "epoch": 65.78278809313292,
      "grad_norm": 0.5584458112716675,
      "learning_rate": 5.542138293310777e-06,
      "loss": 1.0823,
      "step": 7038
    },
    {
      "epoch": 65.7922192749779,
      "grad_norm": 0.5382476449012756,
      "learning_rate": 5.539429023034082e-06,
      "loss": 1.0434,
      "step": 7039
    },
    {
      "epoch": 65.80165045682287,
      "grad_norm": 0.5762127637863159,
      "learning_rate": 5.536720161433423e-06,
      "loss": 1.0155,
      "step": 7040
    },
    {
      "epoch": 65.81108163866784,
      "grad_norm": 0.5340709090232849,
      "learning_rate": 5.534011708756981e-06,
      "loss": 1.0352,
      "step": 7041
    },
    {
      "epoch": 65.82051282051282,
      "grad_norm": 0.5442336201667786,
      "learning_rate": 5.531303665252913e-06,
      "loss": 1.0594,
      "step": 7042
    },
    {
      "epoch": 65.82994400235779,
      "grad_norm": 0.5236194729804993,
      "learning_rate": 5.528596031169323e-06,
      "loss": 1.0696,
      "step": 7043
    },
    {
      "epoch": 65.83937518420277,
      "grad_norm": 0.5147980451583862,
      "learning_rate": 5.525888806754285e-06,
      "loss": 1.0376,
      "step": 7044
    },
    {
      "epoch": 65.84880636604774,
      "grad_norm": 0.5189972519874573,
      "learning_rate": 5.5231819922558326e-06,
      "loss": 1.0399,
      "step": 7045
    },
    {
      "epoch": 65.85823754789271,
      "grad_norm": 0.5399712324142456,
      "learning_rate": 5.520475587921959e-06,
      "loss": 1.0577,
      "step": 7046
    },
    {
      "epoch": 65.8676687297377,
      "grad_norm": 0.5351850390434265,
      "learning_rate": 5.517769594000635e-06,
      "loss": 1.0544,
      "step": 7047
    },
    {
      "epoch": 65.87709991158268,
      "grad_norm": 0.5247694253921509,
      "learning_rate": 5.515064010739777e-06,
      "loss": 1.0411,
      "step": 7048
    },
    {
      "epoch": 65.88653109342765,
      "grad_norm": 0.5779396891593933,
      "learning_rate": 5.5123588383872685e-06,
      "loss": 1.0554,
      "step": 7049
    },
    {
      "epoch": 65.89596227527262,
      "grad_norm": 0.5254361033439636,
      "learning_rate": 5.509654077190957e-06,
      "loss": 1.0478,
      "step": 7050
    },
    {
      "epoch": 65.9053934571176,
      "grad_norm": 0.5641154050827026,
      "learning_rate": 5.506949727398654e-06,
      "loss": 1.0916,
      "step": 7051
    },
    {
      "epoch": 65.91482463896257,
      "grad_norm": 0.535772979259491,
      "learning_rate": 5.504245789258128e-06,
      "loss": 1.0814,
      "step": 7052
    },
    {
      "epoch": 65.92425582080755,
      "grad_norm": 0.5156680345535278,
      "learning_rate": 5.501542263017115e-06,
      "loss": 1.0554,
      "step": 7053
    },
    {
      "epoch": 65.93368700265252,
      "grad_norm": 0.5588589906692505,
      "learning_rate": 5.498839148923307e-06,
      "loss": 0.9862,
      "step": 7054
    },
    {
      "epoch": 65.9431181844975,
      "grad_norm": 0.5410871505737305,
      "learning_rate": 5.496136447224369e-06,
      "loss": 1.0635,
      "step": 7055
    },
    {
      "epoch": 65.95254936634247,
      "grad_norm": 0.5342828035354614,
      "learning_rate": 5.4934341581679194e-06,
      "loss": 1.0429,
      "step": 7056
    },
    {
      "epoch": 65.96198054818744,
      "grad_norm": 0.5534173846244812,
      "learning_rate": 5.4907322820015405e-06,
      "loss": 1.0829,
      "step": 7057
    },
    {
      "epoch": 65.97141173003241,
      "grad_norm": 0.5483890771865845,
      "learning_rate": 5.488030818972777e-06,
      "loss": 1.0337,
      "step": 7058
    },
    {
      "epoch": 65.98084291187739,
      "grad_norm": 0.5781923532485962,
      "learning_rate": 5.485329769329136e-06,
      "loss": 1.0657,
      "step": 7059
    },
    {
      "epoch": 65.99027409372236,
      "grad_norm": 0.550742506980896,
      "learning_rate": 5.482629133318087e-06,
      "loss": 1.0096,
      "step": 7060
    },
    {
      "epoch": 65.99970527556735,
      "grad_norm": 0.5299866199493408,
      "learning_rate": 5.479928911187058e-06,
      "loss": 1.0432,
      "step": 7061
    },
    {
      "epoch": 66.0,
      "grad_norm": 4.289499759674072,
      "learning_rate": 5.477229103183448e-06,
      "loss": 0.4123,
      "step": 7062
    },
    {
      "epoch": 66.00943118184497,
      "grad_norm": 0.551609456539154,
      "learning_rate": 5.4745297095546125e-06,
      "loss": 1.0465,
      "step": 7063
    },
    {
      "epoch": 66.01886236368995,
      "grad_norm": 0.5610839128494263,
      "learning_rate": 5.4718307305478645e-06,
      "loss": 1.053,
      "step": 7064
    },
    {
      "epoch": 66.02829354553492,
      "grad_norm": 0.5311167240142822,
      "learning_rate": 5.4691321664104864e-06,
      "loss": 1.0371,
      "step": 7065
    },
    {
      "epoch": 66.0377247273799,
      "grad_norm": 0.536185085773468,
      "learning_rate": 5.4664340173897215e-06,
      "loss": 1.0355,
      "step": 7066
    },
    {
      "epoch": 66.04715590922487,
      "grad_norm": 0.5304988026618958,
      "learning_rate": 5.463736283732768e-06,
      "loss": 1.0642,
      "step": 7067
    },
    {
      "epoch": 66.05658709106984,
      "grad_norm": 0.5183598399162292,
      "learning_rate": 5.461038965686796e-06,
      "loss": 1.0819,
      "step": 7068
    },
    {
      "epoch": 66.06601827291482,
      "grad_norm": 0.5427509546279907,
      "learning_rate": 5.458342063498927e-06,
      "loss": 1.0458,
      "step": 7069
    },
    {
      "epoch": 66.0754494547598,
      "grad_norm": 0.5625281929969788,
      "learning_rate": 5.4556455774162595e-06,
      "loss": 1.0715,
      "step": 7070
    },
    {
      "epoch": 66.08488063660478,
      "grad_norm": 0.5195581316947937,
      "learning_rate": 5.452949507685838e-06,
      "loss": 1.0457,
      "step": 7071
    },
    {
      "epoch": 66.09431181844975,
      "grad_norm": 0.5149087309837341,
      "learning_rate": 5.450253854554679e-06,
      "loss": 1.0697,
      "step": 7072
    },
    {
      "epoch": 66.10374300029473,
      "grad_norm": 0.5420824885368347,
      "learning_rate": 5.447558618269755e-06,
      "loss": 1.0281,
      "step": 7073
    },
    {
      "epoch": 66.1131741821397,
      "grad_norm": 0.5152126550674438,
      "learning_rate": 5.444863799078003e-06,
      "loss": 1.0353,
      "step": 7074
    },
    {
      "epoch": 66.12260536398468,
      "grad_norm": 0.5148575305938721,
      "learning_rate": 5.442169397226322e-06,
      "loss": 1.0503,
      "step": 7075
    },
    {
      "epoch": 66.13203654582965,
      "grad_norm": 0.5931857824325562,
      "learning_rate": 5.439475412961568e-06,
      "loss": 1.0707,
      "step": 7076
    },
    {
      "epoch": 66.14146772767462,
      "grad_norm": 0.5376324653625488,
      "learning_rate": 5.43678184653057e-06,
      "loss": 1.0586,
      "step": 7077
    },
    {
      "epoch": 66.1508989095196,
      "grad_norm": 0.503237247467041,
      "learning_rate": 5.4340886981801085e-06,
      "loss": 1.1053,
      "step": 7078
    },
    {
      "epoch": 66.16033009136457,
      "grad_norm": 0.5567411780357361,
      "learning_rate": 5.431395968156931e-06,
      "loss": 1.0754,
      "step": 7079
    },
    {
      "epoch": 66.16976127320955,
      "grad_norm": 0.5283187627792358,
      "learning_rate": 5.428703656707741e-06,
      "loss": 1.0364,
      "step": 7080
    },
    {
      "epoch": 66.17919245505452,
      "grad_norm": 0.5673940777778625,
      "learning_rate": 5.426011764079209e-06,
      "loss": 1.0746,
      "step": 7081
    },
    {
      "epoch": 66.1886236368995,
      "grad_norm": 0.5246716737747192,
      "learning_rate": 5.423320290517964e-06,
      "loss": 1.0559,
      "step": 7082
    },
    {
      "epoch": 66.19805481874447,
      "grad_norm": 0.5185544490814209,
      "learning_rate": 5.420629236270598e-06,
      "loss": 1.0303,
      "step": 7083
    },
    {
      "epoch": 66.20748600058945,
      "grad_norm": 0.5554834604263306,
      "learning_rate": 5.417938601583663e-06,
      "loss": 1.0822,
      "step": 7084
    },
    {
      "epoch": 66.21691718243443,
      "grad_norm": 0.5187700390815735,
      "learning_rate": 5.415248386703681e-06,
      "loss": 1.0432,
      "step": 7085
    },
    {
      "epoch": 66.2263483642794,
      "grad_norm": 0.5205309391021729,
      "learning_rate": 5.412558591877123e-06,
      "loss": 1.0291,
      "step": 7086
    },
    {
      "epoch": 66.23577954612438,
      "grad_norm": 0.5329354405403137,
      "learning_rate": 5.409869217350428e-06,
      "loss": 1.0336,
      "step": 7087
    },
    {
      "epoch": 66.24521072796935,
      "grad_norm": 0.548823356628418,
      "learning_rate": 5.407180263369996e-06,
      "loss": 1.0229,
      "step": 7088
    },
    {
      "epoch": 66.25464190981432,
      "grad_norm": 0.5114606618881226,
      "learning_rate": 5.404491730182187e-06,
      "loss": 1.0861,
      "step": 7089
    },
    {
      "epoch": 66.2640730916593,
      "grad_norm": 0.5408352613449097,
      "learning_rate": 5.4018036180333256e-06,
      "loss": 1.0342,
      "step": 7090
    },
    {
      "epoch": 66.27350427350427,
      "grad_norm": 0.5206696391105652,
      "learning_rate": 5.39911592716969e-06,
      "loss": 1.0485,
      "step": 7091
    },
    {
      "epoch": 66.28293545534925,
      "grad_norm": 0.5505309104919434,
      "learning_rate": 5.396428657837533e-06,
      "loss": 1.0159,
      "step": 7092
    },
    {
      "epoch": 66.29236663719422,
      "grad_norm": 0.5103888511657715,
      "learning_rate": 5.393741810283059e-06,
      "loss": 1.0406,
      "step": 7093
    },
    {
      "epoch": 66.3017978190392,
      "grad_norm": 0.5647343397140503,
      "learning_rate": 5.391055384752437e-06,
      "loss": 1.0354,
      "step": 7094
    },
    {
      "epoch": 66.31122900088417,
      "grad_norm": 0.5545752048492432,
      "learning_rate": 5.388369381491794e-06,
      "loss": 1.0582,
      "step": 7095
    },
    {
      "epoch": 66.32066018272914,
      "grad_norm": 0.5444351434707642,
      "learning_rate": 5.38568380074722e-06,
      "loss": 1.0489,
      "step": 7096
    },
    {
      "epoch": 66.33009136457412,
      "grad_norm": 0.5535855293273926,
      "learning_rate": 5.382998642764765e-06,
      "loss": 1.0316,
      "step": 7097
    },
    {
      "epoch": 66.3395225464191,
      "grad_norm": 0.538707435131073,
      "learning_rate": 5.380313907790451e-06,
      "loss": 1.0349,
      "step": 7098
    },
    {
      "epoch": 66.34895372826408,
      "grad_norm": 0.5770518183708191,
      "learning_rate": 5.377629596070246e-06,
      "loss": 1.045,
      "step": 7099
    },
    {
      "epoch": 66.35838491010905,
      "grad_norm": 0.5396744012832642,
      "learning_rate": 5.374945707850089e-06,
      "loss": 1.0193,
      "step": 7100
    },
    {
      "epoch": 66.36781609195403,
      "grad_norm": 0.5408554077148438,
      "learning_rate": 5.372262243375874e-06,
      "loss": 1.0454,
      "step": 7101
    },
    {
      "epoch": 66.377247273799,
      "grad_norm": 0.5384808778762817,
      "learning_rate": 5.369579202893455e-06,
      "loss": 1.0528,
      "step": 7102
    },
    {
      "epoch": 66.38667845564397,
      "grad_norm": 0.5487523674964905,
      "learning_rate": 5.366896586648662e-06,
      "loss": 1.0511,
      "step": 7103
    },
    {
      "epoch": 66.39610963748895,
      "grad_norm": 0.5280071496963501,
      "learning_rate": 5.36421439488727e-06,
      "loss": 1.0249,
      "step": 7104
    },
    {
      "epoch": 66.40554081933392,
      "grad_norm": 0.5410469770431519,
      "learning_rate": 5.361532627855021e-06,
      "loss": 1.0399,
      "step": 7105
    },
    {
      "epoch": 66.4149720011789,
      "grad_norm": 0.48483437299728394,
      "learning_rate": 5.358851285797615e-06,
      "loss": 1.0441,
      "step": 7106
    },
    {
      "epoch": 66.42440318302387,
      "grad_norm": 0.519551694393158,
      "learning_rate": 5.3561703689607155e-06,
      "loss": 1.0675,
      "step": 7107
    },
    {
      "epoch": 66.43383436486884,
      "grad_norm": 0.5544277429580688,
      "learning_rate": 5.353489877589953e-06,
      "loss": 1.0306,
      "step": 7108
    },
    {
      "epoch": 66.44326554671382,
      "grad_norm": 0.5424820780754089,
      "learning_rate": 5.350809811930909e-06,
      "loss": 1.0388,
      "step": 7109
    },
    {
      "epoch": 66.45269672855879,
      "grad_norm": 0.5796213746070862,
      "learning_rate": 5.348130172229131e-06,
      "loss": 1.042,
      "step": 7110
    },
    {
      "epoch": 66.46212791040377,
      "grad_norm": 0.543509840965271,
      "learning_rate": 5.345450958730127e-06,
      "loss": 1.0666,
      "step": 7111
    },
    {
      "epoch": 66.47155909224875,
      "grad_norm": 0.5413331985473633,
      "learning_rate": 5.342772171679364e-06,
      "loss": 1.0364,
      "step": 7112
    },
    {
      "epoch": 66.48099027409373,
      "grad_norm": 0.5291210412979126,
      "learning_rate": 5.340093811322275e-06,
      "loss": 1.0256,
      "step": 7113
    },
    {
      "epoch": 66.4904214559387,
      "grad_norm": 0.5378594994544983,
      "learning_rate": 5.337415877904244e-06,
      "loss": 1.0466,
      "step": 7114
    },
    {
      "epoch": 66.49985263778368,
      "grad_norm": 0.534914493560791,
      "learning_rate": 5.334738371670629e-06,
      "loss": 1.0202,
      "step": 7115
    },
    {
      "epoch": 66.50928381962865,
      "grad_norm": 0.5513430833816528,
      "learning_rate": 5.332061292866742e-06,
      "loss": 1.0492,
      "step": 7116
    },
    {
      "epoch": 66.51871500147362,
      "grad_norm": 0.5467360615730286,
      "learning_rate": 5.329384641737851e-06,
      "loss": 1.0531,
      "step": 7117
    },
    {
      "epoch": 66.5281461833186,
      "grad_norm": 0.5317665934562683,
      "learning_rate": 5.3267084185291965e-06,
      "loss": 1.0623,
      "step": 7118
    },
    {
      "epoch": 66.53757736516357,
      "grad_norm": 0.5189986228942871,
      "learning_rate": 5.324032623485967e-06,
      "loss": 1.0217,
      "step": 7119
    },
    {
      "epoch": 66.54700854700855,
      "grad_norm": 0.5556980967521667,
      "learning_rate": 5.321357256853321e-06,
      "loss": 1.0726,
      "step": 7120
    },
    {
      "epoch": 66.55643972885352,
      "grad_norm": 0.5772873759269714,
      "learning_rate": 5.318682318876372e-06,
      "loss": 1.0968,
      "step": 7121
    },
    {
      "epoch": 66.5658709106985,
      "grad_norm": 0.5690430998802185,
      "learning_rate": 5.316007809800201e-06,
      "loss": 1.0652,
      "step": 7122
    },
    {
      "epoch": 66.57530209254347,
      "grad_norm": 0.5466799736022949,
      "learning_rate": 5.313333729869845e-06,
      "loss": 1.035,
      "step": 7123
    },
    {
      "epoch": 66.58473327438844,
      "grad_norm": 0.6017068028450012,
      "learning_rate": 5.3106600793303e-06,
      "loss": 1.0704,
      "step": 7124
    },
    {
      "epoch": 66.59416445623341,
      "grad_norm": 0.5496611595153809,
      "learning_rate": 5.307986858426526e-06,
      "loss": 1.0417,
      "step": 7125
    },
    {
      "epoch": 66.6035956380784,
      "grad_norm": 0.5781610608100891,
      "learning_rate": 5.3053140674034445e-06,
      "loss": 1.0484,
      "step": 7126
    },
    {
      "epoch": 66.61302681992338,
      "grad_norm": 0.5290371775627136,
      "learning_rate": 5.302641706505932e-06,
      "loss": 1.0784,
      "step": 7127
    },
    {
      "epoch": 66.62245800176835,
      "grad_norm": 0.5272980332374573,
      "learning_rate": 5.299969775978832e-06,
      "loss": 1.0426,
      "step": 7128
    },
    {
      "epoch": 66.63188918361332,
      "grad_norm": 0.5262778997421265,
      "learning_rate": 5.297298276066942e-06,
      "loss": 1.0447,
      "step": 7129
    },
    {
      "epoch": 66.6413203654583,
      "grad_norm": 0.5523555278778076,
      "learning_rate": 5.294627207015031e-06,
      "loss": 1.068,
      "step": 7130
    },
    {
      "epoch": 66.65075154730327,
      "grad_norm": 0.5192275643348694,
      "learning_rate": 5.291956569067816e-06,
      "loss": 1.0794,
      "step": 7131
    },
    {
      "epoch": 66.66018272914825,
      "grad_norm": 0.5060542225837708,
      "learning_rate": 5.289286362469983e-06,
      "loss": 1.0231,
      "step": 7132
    },
    {
      "epoch": 66.66961391099322,
      "grad_norm": 0.5236855745315552,
      "learning_rate": 5.2866165874661735e-06,
      "loss": 1.0631,
      "step": 7133
    },
    {
      "epoch": 66.6790450928382,
      "grad_norm": 0.5000309944152832,
      "learning_rate": 5.283947244300991e-06,
      "loss": 1.0759,
      "step": 7134
    },
    {
      "epoch": 66.68847627468317,
      "grad_norm": 0.5698538422584534,
      "learning_rate": 5.281278333219001e-06,
      "loss": 1.0908,
      "step": 7135
    },
    {
      "epoch": 66.69790745652814,
      "grad_norm": 0.5690398812294006,
      "learning_rate": 5.278609854464725e-06,
      "loss": 1.0605,
      "step": 7136
    },
    {
      "epoch": 66.70733863837312,
      "grad_norm": 0.561882734298706,
      "learning_rate": 5.275941808282654e-06,
      "loss": 1.0693,
      "step": 7137
    },
    {
      "epoch": 66.71676982021809,
      "grad_norm": 0.5449117422103882,
      "learning_rate": 5.273274194917231e-06,
      "loss": 1.0717,
      "step": 7138
    },
    {
      "epoch": 66.72620100206306,
      "grad_norm": 0.5611288547515869,
      "learning_rate": 5.270607014612861e-06,
      "loss": 1.0671,
      "step": 7139
    },
    {
      "epoch": 66.73563218390805,
      "grad_norm": 0.6103129982948303,
      "learning_rate": 5.267940267613911e-06,
      "loss": 1.052,
      "step": 7140
    },
    {
      "epoch": 66.74506336575303,
      "grad_norm": 0.5377692580223083,
      "learning_rate": 5.265273954164708e-06,
      "loss": 1.0586,
      "step": 7141
    },
    {
      "epoch": 66.754494547598,
      "grad_norm": 0.5429003834724426,
      "learning_rate": 5.2626080745095366e-06,
      "loss": 1.026,
      "step": 7142
    },
    {
      "epoch": 66.76392572944297,
      "grad_norm": 0.5058746337890625,
      "learning_rate": 5.259942628892646e-06,
      "loss": 1.0757,
      "step": 7143
    },
    {
      "epoch": 66.77335691128795,
      "grad_norm": 0.575212836265564,
      "learning_rate": 5.257277617558239e-06,
      "loss": 1.04,
      "step": 7144
    },
    {
      "epoch": 66.78278809313292,
      "grad_norm": 0.5292394757270813,
      "learning_rate": 5.2546130407504885e-06,
      "loss": 1.0742,
      "step": 7145
    },
    {
      "epoch": 66.7922192749779,
      "grad_norm": 0.57379549741745,
      "learning_rate": 5.251948898713522e-06,
      "loss": 1.0665,
      "step": 7146
    },
    {
      "epoch": 66.80165045682287,
      "grad_norm": 0.5458274483680725,
      "learning_rate": 5.2492851916914246e-06,
      "loss": 1.0632,
      "step": 7147
    },
    {
      "epoch": 66.81108163866784,
      "grad_norm": 0.5070091485977173,
      "learning_rate": 5.246621919928245e-06,
      "loss": 1.0566,
      "step": 7148
    },
    {
      "epoch": 66.82051282051282,
      "grad_norm": 0.5334757566452026,
      "learning_rate": 5.243959083667991e-06,
      "loss": 1.0633,
      "step": 7149
    },
    {
      "epoch": 66.82994400235779,
      "grad_norm": 0.5742923021316528,
      "learning_rate": 5.241296683154633e-06,
      "loss": 1.0644,
      "step": 7150
    },
    {
      "epoch": 66.83937518420277,
      "grad_norm": 0.5352080464363098,
      "learning_rate": 5.238634718632092e-06,
      "loss": 1.0728,
      "step": 7151
    },
    {
      "epoch": 66.84880636604774,
      "grad_norm": 0.5329515337944031,
      "learning_rate": 5.235973190344267e-06,
      "loss": 1.0472,
      "step": 7152
    },
    {
      "epoch": 66.85823754789271,
      "grad_norm": 0.5207551121711731,
      "learning_rate": 5.2333120985349996e-06,
      "loss": 1.067,
      "step": 7153
    },
    {
      "epoch": 66.8676687297377,
      "grad_norm": 0.557356595993042,
      "learning_rate": 5.230651443448101e-06,
      "loss": 1.1091,
      "step": 7154
    },
    {
      "epoch": 66.87709991158268,
      "grad_norm": 0.5418485999107361,
      "learning_rate": 5.227991225327338e-06,
      "loss": 1.0678,
      "step": 7155
    },
    {
      "epoch": 66.88653109342765,
      "grad_norm": 0.5103540420532227,
      "learning_rate": 5.225331444416439e-06,
      "loss": 1.0244,
      "step": 7156
    },
    {
      "epoch": 66.89596227527262,
      "grad_norm": 0.555244505405426,
      "learning_rate": 5.222672100959092e-06,
      "loss": 1.0264,
      "step": 7157
    },
    {
      "epoch": 66.9053934571176,
      "grad_norm": 0.5568284392356873,
      "learning_rate": 5.220013195198942e-06,
      "loss": 1.0726,
      "step": 7158
    },
    {
      "epoch": 66.91482463896257,
      "grad_norm": 0.5264760255813599,
      "learning_rate": 5.217354727379605e-06,
      "loss": 1.0531,
      "step": 7159
    },
    {
      "epoch": 66.92425582080755,
      "grad_norm": 0.5531594157218933,
      "learning_rate": 5.214696697744643e-06,
      "loss": 1.0856,
      "step": 7160
    },
    {
      "epoch": 66.93368700265252,
      "grad_norm": 0.55204838514328,
      "learning_rate": 5.212039106537588e-06,
      "loss": 1.0749,
      "step": 7161
    },
    {
      "epoch": 66.9431181844975,
      "grad_norm": 0.5849895477294922,
      "learning_rate": 5.209381954001922e-06,
      "loss": 1.0229,
      "step": 7162
    },
    {
      "epoch": 66.95254936634247,
      "grad_norm": 0.5365465879440308,
      "learning_rate": 5.206725240381097e-06,
      "loss": 1.0241,
      "step": 7163
    },
    {
      "epoch": 66.96198054818744,
      "grad_norm": 0.5304112434387207,
      "learning_rate": 5.204068965918514e-06,
      "loss": 1.0441,
      "step": 7164
    },
    {
      "epoch": 66.97141173003241,
      "grad_norm": 0.5930317044258118,
      "learning_rate": 5.201413130857549e-06,
      "loss": 1.03,
      "step": 7165
    },
    {
      "epoch": 66.98084291187739,
      "grad_norm": 0.5082846283912659,
      "learning_rate": 5.198757735441524e-06,
      "loss": 1.0361,
      "step": 7166
    },
    {
      "epoch": 66.99027409372236,
      "grad_norm": 0.5192685723304749,
      "learning_rate": 5.196102779913727e-06,
      "loss": 1.068,
      "step": 7167
    },
    {
      "epoch": 66.99970527556735,
      "grad_norm": 0.5853013396263123,
      "learning_rate": 5.193448264517398e-06,
      "loss": 1.0349,
      "step": 7168
    },
    {
      "epoch": 67.0,
      "grad_norm": 3.617666482925415,
      "learning_rate": 5.190794189495752e-06,
      "loss": 0.2833,
      "step": 7169
    },
    {
      "epoch": 67.00943118184497,
      "grad_norm": 0.5362789034843445,
      "learning_rate": 5.18814055509195e-06,
      "loss": 1.0615,
      "step": 7170
    },
    {
      "epoch": 67.01886236368995,
      "grad_norm": 0.563062846660614,
      "learning_rate": 5.185487361549118e-06,
      "loss": 1.0929,
      "step": 7171
    },
    {
      "epoch": 67.02829354553492,
      "grad_norm": 0.5630174875259399,
      "learning_rate": 5.182834609110339e-06,
      "loss": 1.0419,
      "step": 7172
    },
    {
      "epoch": 67.0377247273799,
      "grad_norm": 0.5890123844146729,
      "learning_rate": 5.180182298018654e-06,
      "loss": 1.0237,
      "step": 7173
    },
    {
      "epoch": 67.04715590922487,
      "grad_norm": 0.525479257106781,
      "learning_rate": 5.177530428517077e-06,
      "loss": 1.071,
      "step": 7174
    },
    {
      "epoch": 67.05658709106984,
      "grad_norm": 0.5250880718231201,
      "learning_rate": 5.174879000848564e-06,
      "loss": 1.0373,
      "step": 7175
    },
    {
      "epoch": 67.06601827291482,
      "grad_norm": 0.5216183066368103,
      "learning_rate": 5.17222801525604e-06,
      "loss": 1.0528,
      "step": 7176
    },
    {
      "epoch": 67.0754494547598,
      "grad_norm": 0.5863989591598511,
      "learning_rate": 5.169577471982388e-06,
      "loss": 1.0604,
      "step": 7177
    },
    {
      "epoch": 67.08488063660478,
      "grad_norm": 0.5589621067047119,
      "learning_rate": 5.166927371270447e-06,
      "loss": 1.0825,
      "step": 7178
    },
    {
      "epoch": 67.09431181844975,
      "grad_norm": 0.5055732727050781,
      "learning_rate": 5.164277713363022e-06,
      "loss": 1.0842,
      "step": 7179
    },
    {
      "epoch": 67.10374300029473,
      "grad_norm": 0.5185938477516174,
      "learning_rate": 5.1616284985028704e-06,
      "loss": 1.0525,
      "step": 7180
    },
    {
      "epoch": 67.1131741821397,
      "grad_norm": 0.5279250144958496,
      "learning_rate": 5.158979726932712e-06,
      "loss": 1.038,
      "step": 7181
    },
    {
      "epoch": 67.12260536398468,
      "grad_norm": 0.6266822218894958,
      "learning_rate": 5.1563313988952315e-06,
      "loss": 1.0321,
      "step": 7182
    },
    {
      "epoch": 67.13203654582965,
      "grad_norm": 0.5530449748039246,
      "learning_rate": 5.1536835146330635e-06,
      "loss": 1.0067,
      "step": 7183
    },
    {
      "epoch": 67.14146772767462,
      "grad_norm": 0.5210346579551697,
      "learning_rate": 5.151036074388809e-06,
      "loss": 1.0254,
      "step": 7184
    },
    {
      "epoch": 67.1508989095196,
      "grad_norm": 0.5350829362869263,
      "learning_rate": 5.1483890784050225e-06,
      "loss": 1.0822,
      "step": 7185
    },
    {
      "epoch": 67.16033009136457,
      "grad_norm": 0.5382769107818604,
      "learning_rate": 5.145742526924223e-06,
      "loss": 1.0503,
      "step": 7186
    },
    {
      "epoch": 67.16976127320955,
      "grad_norm": 0.5249080657958984,
      "learning_rate": 5.143096420188887e-06,
      "loss": 1.0592,
      "step": 7187
    },
    {
      "epoch": 67.17919245505452,
      "grad_norm": 0.5414654016494751,
      "learning_rate": 5.140450758441444e-06,
      "loss": 1.0582,
      "step": 7188
    },
    {
      "epoch": 67.1886236368995,
      "grad_norm": 0.5340464115142822,
      "learning_rate": 5.137805541924299e-06,
      "loss": 1.0284,
      "step": 7189
    },
    {
      "epoch": 67.19805481874447,
      "grad_norm": 0.5483929514884949,
      "learning_rate": 5.1351607708798e-06,
      "loss": 1.0494,
      "step": 7190
    },
    {
      "epoch": 67.20748600058945,
      "grad_norm": 0.5230599045753479,
      "learning_rate": 5.13251644555026e-06,
      "loss": 1.0858,
      "step": 7191
    },
    {
      "epoch": 67.21691718243443,
      "grad_norm": 0.5539311766624451,
      "learning_rate": 5.129872566177954e-06,
      "loss": 1.044,
      "step": 7192
    },
    {
      "epoch": 67.2263483642794,
      "grad_norm": 0.5404910445213318,
      "learning_rate": 5.12722913300511e-06,
      "loss": 1.0471,
      "step": 7193
    },
    {
      "epoch": 67.23577954612438,
      "grad_norm": 0.5251473784446716,
      "learning_rate": 5.124586146273917e-06,
      "loss": 1.0863,
      "step": 7194
    },
    {
      "epoch": 67.24521072796935,
      "grad_norm": 0.5227991938591003,
      "learning_rate": 5.121943606226531e-06,
      "loss": 1.0755,
      "step": 7195
    },
    {
      "epoch": 67.25464190981432,
      "grad_norm": 0.5604990720748901,
      "learning_rate": 5.119301513105052e-06,
      "loss": 1.0305,
      "step": 7196
    },
    {
      "epoch": 67.2640730916593,
      "grad_norm": 0.5147777795791626,
      "learning_rate": 5.116659867151555e-06,
      "loss": 1.0597,
      "step": 7197
    },
    {
      "epoch": 67.27350427350427,
      "grad_norm": 0.5096336603164673,
      "learning_rate": 5.114018668608065e-06,
      "loss": 1.0561,
      "step": 7198
    },
    {
      "epoch": 67.28293545534925,
      "grad_norm": 0.4840923249721527,
      "learning_rate": 5.111377917716568e-06,
      "loss": 1.0542,
      "step": 7199
    },
    {
      "epoch": 67.29236663719422,
      "grad_norm": 0.5163644552230835,
      "learning_rate": 5.1087376147190075e-06,
      "loss": 1.0483,
      "step": 7200
    },
    {
      "epoch": 67.3017978190392,
      "grad_norm": 0.614344596862793,
      "learning_rate": 5.106097759857287e-06,
      "loss": 1.0443,
      "step": 7201
    },
    {
      "epoch": 67.31122900088417,
      "grad_norm": 0.5536653995513916,
      "learning_rate": 5.103458353373269e-06,
      "loss": 1.0845,
      "step": 7202
    },
    {
      "epoch": 67.32066018272914,
      "grad_norm": 0.5318751335144043,
      "learning_rate": 5.1008193955087725e-06,
      "loss": 1.0617,
      "step": 7203
    },
    {
      "epoch": 67.33009136457412,
      "grad_norm": 0.5213134288787842,
      "learning_rate": 5.0981808865055856e-06,
      "loss": 1.0487,
      "step": 7204
    },
    {
      "epoch": 67.3395225464191,
      "grad_norm": 0.592749834060669,
      "learning_rate": 5.095542826605442e-06,
      "loss": 1.0811,
      "step": 7205
    },
    {
      "epoch": 67.34895372826408,
      "grad_norm": 0.5703238248825073,
      "learning_rate": 5.092905216050041e-06,
      "loss": 0.9942,
      "step": 7206
    },
    {
      "epoch": 67.35838491010905,
      "grad_norm": 0.5303592681884766,
      "learning_rate": 5.09026805508104e-06,
      "loss": 1.0634,
      "step": 7207
    },
    {
      "epoch": 67.36781609195403,
      "grad_norm": 0.5258829593658447,
      "learning_rate": 5.087631343940053e-06,
      "loss": 1.0452,
      "step": 7208
    },
    {
      "epoch": 67.377247273799,
      "grad_norm": 0.6183742880821228,
      "learning_rate": 5.084995082868658e-06,
      "loss": 1.0325,
      "step": 7209
    },
    {
      "epoch": 67.38667845564397,
      "grad_norm": 0.5552372932434082,
      "learning_rate": 5.082359272108386e-06,
      "loss": 1.0327,
      "step": 7210
    },
    {
      "epoch": 67.39610963748895,
      "grad_norm": 0.5282596349716187,
      "learning_rate": 5.079723911900725e-06,
      "loss": 1.0756,
      "step": 7211
    },
    {
      "epoch": 67.40554081933392,
      "grad_norm": 0.5463420748710632,
      "learning_rate": 5.077089002487134e-06,
      "loss": 1.0618,
      "step": 7212
    },
    {
      "epoch": 67.4149720011789,
      "grad_norm": 0.5899983048439026,
      "learning_rate": 5.07445454410902e-06,
      "loss": 1.032,
      "step": 7213
    },
    {
      "epoch": 67.42440318302387,
      "grad_norm": 0.5749694108963013,
      "learning_rate": 5.07182053700775e-06,
      "loss": 1.0557,
      "step": 7214
    },
    {
      "epoch": 67.43383436486884,
      "grad_norm": 0.5622967481613159,
      "learning_rate": 5.06918698142465e-06,
      "loss": 1.0494,
      "step": 7215
    },
    {
      "epoch": 67.44326554671382,
      "grad_norm": 0.5349493622779846,
      "learning_rate": 5.066553877601006e-06,
      "loss": 1.0862,
      "step": 7216
    },
    {
      "epoch": 67.45269672855879,
      "grad_norm": 0.5445679426193237,
      "learning_rate": 5.0639212257780634e-06,
      "loss": 1.087,
      "step": 7217
    },
    {
      "epoch": 67.46212791040377,
      "grad_norm": 0.522389829158783,
      "learning_rate": 5.061289026197019e-06,
      "loss": 1.0857,
      "step": 7218
    },
    {
      "epoch": 67.47155909224875,
      "grad_norm": 0.5242837071418762,
      "learning_rate": 5.058657279099044e-06,
      "loss": 1.0452,
      "step": 7219
    },
    {
      "epoch": 67.48099027409373,
      "grad_norm": 0.5384493470191956,
      "learning_rate": 5.056025984725252e-06,
      "loss": 1.0745,
      "step": 7220
    },
    {
      "epoch": 67.4904214559387,
      "grad_norm": 0.5413469672203064,
      "learning_rate": 5.053395143316724e-06,
      "loss": 1.0279,
      "step": 7221
    },
    {
      "epoch": 67.49985263778368,
      "grad_norm": 0.5694699287414551,
      "learning_rate": 5.0507647551144944e-06,
      "loss": 1.0985,
      "step": 7222
    },
    {
      "epoch": 67.50928381962865,
      "grad_norm": 0.587784469127655,
      "learning_rate": 5.04813482035956e-06,
      "loss": 1.0095,
      "step": 7223
    },
    {
      "epoch": 67.51871500147362,
      "grad_norm": 0.573418378829956,
      "learning_rate": 5.0455053392928746e-06,
      "loss": 1.0737,
      "step": 7224
    },
    {
      "epoch": 67.5281461833186,
      "grad_norm": 0.5603944659233093,
      "learning_rate": 5.042876312155345e-06,
      "loss": 1.0659,
      "step": 7225
    },
    {
      "epoch": 67.53757736516357,
      "grad_norm": 0.5397571921348572,
      "learning_rate": 5.04024773918785e-06,
      "loss": 1.009,
      "step": 7226
    },
    {
      "epoch": 67.54700854700855,
      "grad_norm": 0.5584688782691956,
      "learning_rate": 5.037619620631215e-06,
      "loss": 1.0124,
      "step": 7227
    },
    {
      "epoch": 67.55643972885352,
      "grad_norm": 0.5432748198509216,
      "learning_rate": 5.034991956726229e-06,
      "loss": 1.0349,
      "step": 7228
    },
    {
      "epoch": 67.5658709106985,
      "grad_norm": 0.5564378499984741,
      "learning_rate": 5.032364747713636e-06,
      "loss": 1.0628,
      "step": 7229
    },
    {
      "epoch": 67.57530209254347,
      "grad_norm": 0.5319356322288513,
      "learning_rate": 5.029737993834136e-06,
      "loss": 1.0705,
      "step": 7230
    },
    {
      "epoch": 67.58473327438844,
      "grad_norm": 0.5091693997383118,
      "learning_rate": 5.027111695328399e-06,
      "loss": 1.0818,
      "step": 7231
    },
    {
      "epoch": 67.59416445623341,
      "grad_norm": 0.5156754851341248,
      "learning_rate": 5.024485852437042e-06,
      "loss": 1.0486,
      "step": 7232
    },
    {
      "epoch": 67.6035956380784,
      "grad_norm": 0.5160840153694153,
      "learning_rate": 5.021860465400645e-06,
      "loss": 1.045,
      "step": 7233
    },
    {
      "epoch": 67.61302681992338,
      "grad_norm": 0.5430681705474854,
      "learning_rate": 5.019235534459743e-06,
      "loss": 1.0331,
      "step": 7234
    },
    {
      "epoch": 67.62245800176835,
      "grad_norm": 0.5683885812759399,
      "learning_rate": 5.01661105985483e-06,
      "loss": 1.0627,
      "step": 7235
    },
    {
      "epoch": 67.63188918361332,
      "grad_norm": 0.5622347593307495,
      "learning_rate": 5.013987041826365e-06,
      "loss": 1.0469,
      "step": 7236
    },
    {
      "epoch": 67.6413203654583,
      "grad_norm": 0.5101044178009033,
      "learning_rate": 5.011363480614756e-06,
      "loss": 1.0525,
      "step": 7237
    },
    {
      "epoch": 67.65075154730327,
      "grad_norm": 0.5784048438072205,
      "learning_rate": 5.008740376460375e-06,
      "loss": 1.0763,
      "step": 7238
    },
    {
      "epoch": 67.66018272914825,
      "grad_norm": 0.5581756830215454,
      "learning_rate": 5.006117729603545e-06,
      "loss": 1.0483,
      "step": 7239
    },
    {
      "epoch": 67.66961391099322,
      "grad_norm": 0.5502898693084717,
      "learning_rate": 5.003495540284559e-06,
      "loss": 1.056,
      "step": 7240
    },
    {
      "epoch": 67.6790450928382,
      "grad_norm": 0.5561774373054504,
      "learning_rate": 5.0008738087436514e-06,
      "loss": 1.0182,
      "step": 7241
    },
    {
      "epoch": 67.68847627468317,
      "grad_norm": 0.5337404012680054,
      "learning_rate": 4.998252535221035e-06,
      "loss": 1.0303,
      "step": 7242
    },
    {
      "epoch": 67.69790745652814,
      "grad_norm": 0.5378963351249695,
      "learning_rate": 4.995631719956866e-06,
      "loss": 1.0033,
      "step": 7243
    },
    {
      "epoch": 67.70733863837312,
      "grad_norm": 0.5653398036956787,
      "learning_rate": 4.993011363191261e-06,
      "loss": 1.0361,
      "step": 7244
    },
    {
      "epoch": 67.71676982021809,
      "grad_norm": 0.6537736058235168,
      "learning_rate": 4.990391465164299e-06,
      "loss": 1.0219,
      "step": 7245
    },
    {
      "epoch": 67.72620100206306,
      "grad_norm": 0.516120970249176,
      "learning_rate": 4.987772026116011e-06,
      "loss": 1.025,
      "step": 7246
    },
    {
      "epoch": 67.73563218390805,
      "grad_norm": 0.5520797967910767,
      "learning_rate": 4.9851530462863914e-06,
      "loss": 1.0337,
      "step": 7247
    },
    {
      "epoch": 67.74506336575303,
      "grad_norm": 0.5764928460121155,
      "learning_rate": 4.982534525915384e-06,
      "loss": 1.0046,
      "step": 7248
    },
    {
      "epoch": 67.754494547598,
      "grad_norm": 0.5434316992759705,
      "learning_rate": 4.979916465242909e-06,
      "loss": 1.0534,
      "step": 7249
    },
    {
      "epoch": 67.76392572944297,
      "grad_norm": 0.5293575525283813,
      "learning_rate": 4.977298864508824e-06,
      "loss": 1.0603,
      "step": 7250
    },
    {
      "epoch": 67.77335691128795,
      "grad_norm": 0.5199255347251892,
      "learning_rate": 4.9746817239529534e-06,
      "loss": 1.039,
      "step": 7251
    },
    {
      "epoch": 67.78278809313292,
      "grad_norm": 0.5080100297927856,
      "learning_rate": 4.972065043815081e-06,
      "loss": 1.038,
      "step": 7252
    },
    {
      "epoch": 67.7922192749779,
      "grad_norm": 0.5719336271286011,
      "learning_rate": 4.969448824334943e-06,
      "loss": 1.0641,
      "step": 7253
    },
    {
      "epoch": 67.80165045682287,
      "grad_norm": 0.5741837620735168,
      "learning_rate": 4.96683306575224e-06,
      "loss": 1.0699,
      "step": 7254
    },
    {
      "epoch": 67.81108163866784,
      "grad_norm": 0.5905066728591919,
      "learning_rate": 4.964217768306625e-06,
      "loss": 1.0984,
      "step": 7255
    },
    {
      "epoch": 67.82051282051282,
      "grad_norm": 0.5667380690574646,
      "learning_rate": 4.961602932237707e-06,
      "loss": 1.063,
      "step": 7256
    },
    {
      "epoch": 67.82994400235779,
      "grad_norm": 0.5359117984771729,
      "learning_rate": 4.958988557785064e-06,
      "loss": 1.0836,
      "step": 7257
    },
    {
      "epoch": 67.83937518420277,
      "grad_norm": 0.5464984774589539,
      "learning_rate": 4.95637464518822e-06,
      "loss": 1.0338,
      "step": 7258
    },
    {
      "epoch": 67.84880636604774,
      "grad_norm": 0.5389001965522766,
      "learning_rate": 4.953761194686661e-06,
      "loss": 1.0218,
      "step": 7259
    },
    {
      "epoch": 67.85823754789271,
      "grad_norm": 0.5476914048194885,
      "learning_rate": 4.951148206519832e-06,
      "loss": 1.0701,
      "step": 7260
    },
    {
      "epoch": 67.8676687297377,
      "grad_norm": 0.5392157435417175,
      "learning_rate": 4.948535680927133e-06,
      "loss": 1.0622,
      "step": 7261
    },
    {
      "epoch": 67.87709991158268,
      "grad_norm": 0.5677380561828613,
      "learning_rate": 4.945923618147921e-06,
      "loss": 1.062,
      "step": 7262
    },
    {
      "epoch": 67.88653109342765,
      "grad_norm": 0.5460469722747803,
      "learning_rate": 4.9433120184215115e-06,
      "loss": 1.0748,
      "step": 7263
    },
    {
      "epoch": 67.89596227527262,
      "grad_norm": 0.6108200550079346,
      "learning_rate": 4.940700881987184e-06,
      "loss": 1.0117,
      "step": 7264
    },
    {
      "epoch": 67.9053934571176,
      "grad_norm": 0.5175607800483704,
      "learning_rate": 4.938090209084167e-06,
      "loss": 1.0634,
      "step": 7265
    },
    {
      "epoch": 67.91482463896257,
      "grad_norm": 0.548134982585907,
      "learning_rate": 4.935479999951649e-06,
      "loss": 1.069,
      "step": 7266
    },
    {
      "epoch": 67.92425582080755,
      "grad_norm": 0.5159659385681152,
      "learning_rate": 4.9328702548287765e-06,
      "loss": 1.0388,
      "step": 7267
    },
    {
      "epoch": 67.93368700265252,
      "grad_norm": 0.5407572984695435,
      "learning_rate": 4.930260973954655e-06,
      "loss": 1.0541,
      "step": 7268
    },
    {
      "epoch": 67.9431181844975,
      "grad_norm": 0.5456491708755493,
      "learning_rate": 4.927652157568343e-06,
      "loss": 1.0247,
      "step": 7269
    },
    {
      "epoch": 67.95254936634247,
      "grad_norm": 0.5216242074966431,
      "learning_rate": 4.925043805908859e-06,
      "loss": 1.0461,
      "step": 7270
    },
    {
      "epoch": 67.96198054818744,
      "grad_norm": 0.5599080324172974,
      "learning_rate": 4.922435919215184e-06,
      "loss": 1.0608,
      "step": 7271
    },
    {
      "epoch": 67.97141173003241,
      "grad_norm": 0.5134531259536743,
      "learning_rate": 4.919828497726249e-06,
      "loss": 1.0643,
      "step": 7272
    },
    {
      "epoch": 67.98084291187739,
      "grad_norm": 0.5595070719718933,
      "learning_rate": 4.917221541680947e-06,
      "loss": 1.1015,
      "step": 7273
    },
    {
      "epoch": 67.99027409372236,
      "grad_norm": 0.5270968675613403,
      "learning_rate": 4.914615051318123e-06,
      "loss": 1.0322,
      "step": 7274
    },
    {
      "epoch": 67.99970527556735,
      "grad_norm": 0.5824705958366394,
      "learning_rate": 4.912009026876584e-06,
      "loss": 1.1075,
      "step": 7275
    },
    {
      "epoch": 68.0,
      "grad_norm": 3.6420481204986572,
      "learning_rate": 4.909403468595094e-06,
      "loss": 0.6292,
      "step": 7276
    },
    {
      "epoch": 68.00943118184497,
      "grad_norm": 0.5638441443443298,
      "learning_rate": 4.9067983767123736e-06,
      "loss": 1.023,
      "step": 7277
    },
    {
      "epoch": 68.01886236368995,
      "grad_norm": 0.5195897817611694,
      "learning_rate": 4.904193751467096e-06,
      "loss": 1.063,
      "step": 7278
    },
    {
      "epoch": 68.02829354553492,
      "grad_norm": 0.5438133478164673,
      "learning_rate": 4.901589593097903e-06,
      "loss": 1.0376,
      "step": 7279
    },
    {
      "epoch": 68.0377247273799,
      "grad_norm": 0.5662636160850525,
      "learning_rate": 4.8989859018433835e-06,
      "loss": 1.0706,
      "step": 7280
    },
    {
      "epoch": 68.04715590922487,
      "grad_norm": 0.5432258248329163,
      "learning_rate": 4.896382677942088e-06,
      "loss": 1.0639,
      "step": 7281
    },
    {
      "epoch": 68.05658709106984,
      "grad_norm": 0.524152934551239,
      "learning_rate": 4.893779921632521e-06,
      "loss": 1.0981,
      "step": 7282
    },
    {
      "epoch": 68.06601827291482,
      "grad_norm": 0.553351879119873,
      "learning_rate": 4.891177633153148e-06,
      "loss": 1.086,
      "step": 7283
    },
    {
      "epoch": 68.0754494547598,
      "grad_norm": 0.5516313314437866,
      "learning_rate": 4.888575812742388e-06,
      "loss": 1.0657,
      "step": 7284
    },
    {
      "epoch": 68.08488063660478,
      "grad_norm": 0.5526984930038452,
      "learning_rate": 4.885974460638616e-06,
      "loss": 1.0199,
      "step": 7285
    },
    {
      "epoch": 68.09431181844975,
      "grad_norm": 0.5624210834503174,
      "learning_rate": 4.883373577080176e-06,
      "loss": 1.0092,
      "step": 7286
    },
    {
      "epoch": 68.10374300029473,
      "grad_norm": 0.5558414459228516,
      "learning_rate": 4.880773162305354e-06,
      "loss": 1.0444,
      "step": 7287
    },
    {
      "epoch": 68.1131741821397,
      "grad_norm": 0.557537317276001,
      "learning_rate": 4.8781732165524e-06,
      "loss": 1.0249,
      "step": 7288
    },
    {
      "epoch": 68.12260536398468,
      "grad_norm": 0.5450313091278076,
      "learning_rate": 4.8755737400595206e-06,
      "loss": 1.069,
      "step": 7289
    },
    {
      "epoch": 68.13203654582965,
      "grad_norm": 0.5561308264732361,
      "learning_rate": 4.872974733064879e-06,
      "loss": 1.0519,
      "step": 7290
    },
    {
      "epoch": 68.14146772767462,
      "grad_norm": 0.5495951175689697,
      "learning_rate": 4.870376195806591e-06,
      "loss": 1.0944,
      "step": 7291
    },
    {
      "epoch": 68.1508989095196,
      "grad_norm": 0.5424596071243286,
      "learning_rate": 4.867778128522741e-06,
      "loss": 1.0279,
      "step": 7292
    },
    {
      "epoch": 68.16033009136457,
      "grad_norm": 0.5178468227386475,
      "learning_rate": 4.86518053145136e-06,
      "loss": 1.0599,
      "step": 7293
    },
    {
      "epoch": 68.16976127320955,
      "grad_norm": 0.5179083347320557,
      "learning_rate": 4.862583404830439e-06,
      "loss": 1.0678,
      "step": 7294
    },
    {
      "epoch": 68.17919245505452,
      "grad_norm": 0.5807763934135437,
      "learning_rate": 4.859986748897926e-06,
      "loss": 1.0058,
      "step": 7295
    },
    {
      "epoch": 68.1886236368995,
      "grad_norm": 0.5228850841522217,
      "learning_rate": 4.857390563891721e-06,
      "loss": 1.0213,
      "step": 7296
    },
    {
      "epoch": 68.19805481874447,
      "grad_norm": 0.5062940716743469,
      "learning_rate": 4.854794850049693e-06,
      "loss": 1.0857,
      "step": 7297
    },
    {
      "epoch": 68.20748600058945,
      "grad_norm": 0.5309794545173645,
      "learning_rate": 4.852199607609659e-06,
      "loss": 1.0314,
      "step": 7298
    },
    {
      "epoch": 68.21691718243443,
      "grad_norm": 0.5615234375,
      "learning_rate": 4.849604836809393e-06,
      "loss": 1.0363,
      "step": 7299
    },
    {
      "epoch": 68.2263483642794,
      "grad_norm": 0.537670910358429,
      "learning_rate": 4.847010537886626e-06,
      "loss": 1.055,
      "step": 7300
    },
    {
      "epoch": 68.23577954612438,
      "grad_norm": 0.5522470474243164,
      "learning_rate": 4.844416711079044e-06,
      "loss": 1.026,
      "step": 7301
    },
    {
      "epoch": 68.24521072796935,
      "grad_norm": 0.5320113897323608,
      "learning_rate": 4.841823356624299e-06,
      "loss": 1.0735,
      "step": 7302
    },
    {
      "epoch": 68.25464190981432,
      "grad_norm": 0.5593902468681335,
      "learning_rate": 4.839230474759991e-06,
      "loss": 1.0295,
      "step": 7303
    },
    {
      "epoch": 68.2640730916593,
      "grad_norm": 0.5366432070732117,
      "learning_rate": 4.836638065723679e-06,
      "loss": 1.0633,
      "step": 7304
    },
    {
      "epoch": 68.27350427350427,
      "grad_norm": 0.5257582068443298,
      "learning_rate": 4.834046129752877e-06,
      "loss": 1.0399,
      "step": 7305
    },
    {
      "epoch": 68.28293545534925,
      "grad_norm": 0.5263242721557617,
      "learning_rate": 4.831454667085059e-06,
      "loss": 1.0234,
      "step": 7306
    },
    {
      "epoch": 68.29236663719422,
      "grad_norm": 0.5393709540367126,
      "learning_rate": 4.828863677957654e-06,
      "loss": 1.0492,
      "step": 7307
    },
    {
      "epoch": 68.3017978190392,
      "grad_norm": 0.5358707308769226,
      "learning_rate": 4.826273162608043e-06,
      "loss": 1.0554,
      "step": 7308
    },
    {
      "epoch": 68.31122900088417,
      "grad_norm": 0.5090704560279846,
      "learning_rate": 4.823683121273575e-06,
      "loss": 1.0998,
      "step": 7309
    },
    {
      "epoch": 68.32066018272914,
      "grad_norm": 0.5068532824516296,
      "learning_rate": 4.821093554191546e-06,
      "loss": 1.0666,
      "step": 7310
    },
    {
      "epoch": 68.33009136457412,
      "grad_norm": 0.5250847935676575,
      "learning_rate": 4.81850446159921e-06,
      "loss": 1.0528,
      "step": 7311
    },
    {
      "epoch": 68.3395225464191,
      "grad_norm": 0.5636516213417053,
      "learning_rate": 4.815915843733782e-06,
      "loss": 1.0658,
      "step": 7312
    },
    {
      "epoch": 68.34895372826408,
      "grad_norm": 0.5362898111343384,
      "learning_rate": 4.813327700832428e-06,
      "loss": 1.0541,
      "step": 7313
    },
    {
      "epoch": 68.35838491010905,
      "grad_norm": 0.5250839591026306,
      "learning_rate": 4.81074003313227e-06,
      "loss": 1.0652,
      "step": 7314
    },
    {
      "epoch": 68.36781609195403,
      "grad_norm": 0.55517578125,
      "learning_rate": 4.808152840870392e-06,
      "loss": 1.0534,
      "step": 7315
    },
    {
      "epoch": 68.377247273799,
      "grad_norm": 0.5839451551437378,
      "learning_rate": 4.805566124283834e-06,
      "loss": 1.041,
      "step": 7316
    },
    {
      "epoch": 68.38667845564397,
      "grad_norm": 0.6250501871109009,
      "learning_rate": 4.802979883609587e-06,
      "loss": 1.081,
      "step": 7317
    },
    {
      "epoch": 68.39610963748895,
      "grad_norm": 0.4908914864063263,
      "learning_rate": 4.800394119084605e-06,
      "loss": 1.1258,
      "step": 7318
    },
    {
      "epoch": 68.40554081933392,
      "grad_norm": 0.5563069581985474,
      "learning_rate": 4.797808830945791e-06,
      "loss": 1.0407,
      "step": 7319
    },
    {
      "epoch": 68.4149720011789,
      "grad_norm": 0.5125896334648132,
      "learning_rate": 4.79522401943001e-06,
      "loss": 1.0289,
      "step": 7320
    },
    {
      "epoch": 68.42440318302387,
      "grad_norm": 0.5436388850212097,
      "learning_rate": 4.7926396847740806e-06,
      "loss": 1.0208,
      "step": 7321
    },
    {
      "epoch": 68.43383436486884,
      "grad_norm": 0.5547723770141602,
      "learning_rate": 4.7900558272147805e-06,
      "loss": 1.0134,
      "step": 7322
    },
    {
      "epoch": 68.44326554671382,
      "grad_norm": 0.5396777391433716,
      "learning_rate": 4.787472446988837e-06,
      "loss": 1.0735,
      "step": 7323
    },
    {
      "epoch": 68.45269672855879,
      "grad_norm": 0.5495560765266418,
      "learning_rate": 4.784889544332945e-06,
      "loss": 1.0853,
      "step": 7324
    },
    {
      "epoch": 68.46212791040377,
      "grad_norm": 0.5817294120788574,
      "learning_rate": 4.7823071194837465e-06,
      "loss": 1.0469,
      "step": 7325
    },
    {
      "epoch": 68.47155909224875,
      "grad_norm": 0.5203184485435486,
      "learning_rate": 4.779725172677843e-06,
      "loss": 1.0584,
      "step": 7326
    },
    {
      "epoch": 68.48099027409373,
      "grad_norm": 0.5714230537414551,
      "learning_rate": 4.777143704151791e-06,
      "loss": 1.0889,
      "step": 7327
    },
    {
      "epoch": 68.4904214559387,
      "grad_norm": 0.5277750492095947,
      "learning_rate": 4.774562714142103e-06,
      "loss": 1.0503,
      "step": 7328
    },
    {
      "epoch": 68.49985263778368,
      "grad_norm": 0.5268943309783936,
      "learning_rate": 4.77198220288525e-06,
      "loss": 1.0465,
      "step": 7329
    },
    {
      "epoch": 68.50928381962865,
      "grad_norm": 0.5129525661468506,
      "learning_rate": 4.769402170617652e-06,
      "loss": 1.0443,
      "step": 7330
    },
    {
      "epoch": 68.51871500147362,
      "grad_norm": 0.5572322607040405,
      "learning_rate": 4.7668226175757e-06,
      "loss": 1.0306,
      "step": 7331
    },
    {
      "epoch": 68.5281461833186,
      "grad_norm": 0.5675819516181946,
      "learning_rate": 4.764243543995728e-06,
      "loss": 1.0814,
      "step": 7332
    },
    {
      "epoch": 68.53757736516357,
      "grad_norm": 0.5465179085731506,
      "learning_rate": 4.761664950114027e-06,
      "loss": 1.0245,
      "step": 7333
    },
    {
      "epoch": 68.54700854700855,
      "grad_norm": 0.5193597674369812,
      "learning_rate": 4.759086836166852e-06,
      "loss": 1.0423,
      "step": 7334
    },
    {
      "epoch": 68.55643972885352,
      "grad_norm": 0.5398731231689453,
      "learning_rate": 4.7565092023904045e-06,
      "loss": 1.0624,
      "step": 7335
    },
    {
      "epoch": 68.5658709106985,
      "grad_norm": 0.5116138458251953,
      "learning_rate": 4.753932049020847e-06,
      "loss": 1.0355,
      "step": 7336
    },
    {
      "epoch": 68.57530209254347,
      "grad_norm": 0.5743893384933472,
      "learning_rate": 4.7513553762943e-06,
      "loss": 1.0288,
      "step": 7337
    },
    {
      "epoch": 68.58473327438844,
      "grad_norm": 0.5437960028648376,
      "learning_rate": 4.748779184446831e-06,
      "loss": 1.0558,
      "step": 7338
    },
    {
      "epoch": 68.59416445623341,
      "grad_norm": 0.5190139412879944,
      "learning_rate": 4.746203473714479e-06,
      "loss": 1.0194,
      "step": 7339
    },
    {
      "epoch": 68.6035956380784,
      "grad_norm": 0.5121116042137146,
      "learning_rate": 4.743628244333226e-06,
      "loss": 1.0277,
      "step": 7340
    },
    {
      "epoch": 68.61302681992338,
      "grad_norm": 0.5834918022155762,
      "learning_rate": 4.741053496539012e-06,
      "loss": 1.0725,
      "step": 7341
    },
    {
      "epoch": 68.62245800176835,
      "grad_norm": 0.5481375455856323,
      "learning_rate": 4.738479230567736e-06,
      "loss": 1.0257,
      "step": 7342
    },
    {
      "epoch": 68.63188918361332,
      "grad_norm": 0.5353044271469116,
      "learning_rate": 4.7359054466552515e-06,
      "loss": 1.0838,
      "step": 7343
    },
    {
      "epoch": 68.6413203654583,
      "grad_norm": 0.5342121124267578,
      "learning_rate": 4.733332145037367e-06,
      "loss": 1.0421,
      "step": 7344
    },
    {
      "epoch": 68.65075154730327,
      "grad_norm": 0.5289754867553711,
      "learning_rate": 4.730759325949844e-06,
      "loss": 1.0226,
      "step": 7345
    },
    {
      "epoch": 68.66018272914825,
      "grad_norm": 0.5396222472190857,
      "learning_rate": 4.72818698962841e-06,
      "loss": 1.0594,
      "step": 7346
    },
    {
      "epoch": 68.66961391099322,
      "grad_norm": 0.5521662831306458,
      "learning_rate": 4.72561513630874e-06,
      "loss": 1.0684,
      "step": 7347
    },
    {
      "epoch": 68.6790450928382,
      "grad_norm": 0.5301652550697327,
      "learning_rate": 4.723043766226465e-06,
      "loss": 1.1112,
      "step": 7348
    },
    {
      "epoch": 68.68847627468317,
      "grad_norm": 0.625407874584198,
      "learning_rate": 4.720472879617173e-06,
      "loss": 1.0577,
      "step": 7349
    },
    {
      "epoch": 68.69790745652814,
      "grad_norm": 0.5557131171226501,
      "learning_rate": 4.717902476716407e-06,
      "loss": 1.0696,
      "step": 7350
    },
    {
      "epoch": 68.70733863837312,
      "grad_norm": 0.5475738048553467,
      "learning_rate": 4.715332557759669e-06,
      "loss": 1.0295,
      "step": 7351
    },
    {
      "epoch": 68.71676982021809,
      "grad_norm": 0.5294497013092041,
      "learning_rate": 4.7127631229824115e-06,
      "loss": 1.0432,
      "step": 7352
    },
    {
      "epoch": 68.72620100206306,
      "grad_norm": 0.5114355683326721,
      "learning_rate": 4.710194172620043e-06,
      "loss": 1.0694,
      "step": 7353
    },
    {
      "epoch": 68.73563218390805,
      "grad_norm": 0.5492090582847595,
      "learning_rate": 4.707625706907937e-06,
      "loss": 1.0617,
      "step": 7354
    },
    {
      "epoch": 68.74506336575303,
      "grad_norm": 0.5421711206436157,
      "learning_rate": 4.70505772608141e-06,
      "loss": 1.0275,
      "step": 7355
    },
    {
      "epoch": 68.754494547598,
      "grad_norm": 0.5281604528427124,
      "learning_rate": 4.702490230375744e-06,
      "loss": 1.0426,
      "step": 7356
    },
    {
      "epoch": 68.76392572944297,
      "grad_norm": 0.5306251049041748,
      "learning_rate": 4.699923220026167e-06,
      "loss": 1.0504,
      "step": 7357
    },
    {
      "epoch": 68.77335691128795,
      "grad_norm": 0.5337616205215454,
      "learning_rate": 4.697356695267868e-06,
      "loss": 1.0376,
      "step": 7358
    },
    {
      "epoch": 68.78278809313292,
      "grad_norm": 0.5390304327011108,
      "learning_rate": 4.694790656335995e-06,
      "loss": 1.0512,
      "step": 7359
    },
    {
      "epoch": 68.7922192749779,
      "grad_norm": 0.5575780868530273,
      "learning_rate": 4.692225103465646e-06,
      "loss": 1.0313,
      "step": 7360
    },
    {
      "epoch": 68.80165045682287,
      "grad_norm": 0.5512848496437073,
      "learning_rate": 4.689660036891876e-06,
      "loss": 1.0463,
      "step": 7361
    },
    {
      "epoch": 68.81108163866784,
      "grad_norm": 0.5327221751213074,
      "learning_rate": 4.687095456849698e-06,
      "loss": 1.0591,
      "step": 7362
    },
    {
      "epoch": 68.82051282051282,
      "grad_norm": 0.514362633228302,
      "learning_rate": 4.684531363574068e-06,
      "loss": 1.0769,
      "step": 7363
    },
    {
      "epoch": 68.82994400235779,
      "grad_norm": 0.5123759508132935,
      "learning_rate": 4.68196775729992e-06,
      "loss": 1.0542,
      "step": 7364
    },
    {
      "epoch": 68.83937518420277,
      "grad_norm": 0.5618204474449158,
      "learning_rate": 4.679404638262126e-06,
      "loss": 1.0419,
      "step": 7365
    },
    {
      "epoch": 68.84880636604774,
      "grad_norm": 0.5661354660987854,
      "learning_rate": 4.6768420066955165e-06,
      "loss": 1.055,
      "step": 7366
    },
    {
      "epoch": 68.85823754789271,
      "grad_norm": 0.5785258412361145,
      "learning_rate": 4.674279862834881e-06,
      "loss": 1.0132,
      "step": 7367
    },
    {
      "epoch": 68.8676687297377,
      "grad_norm": 0.576779842376709,
      "learning_rate": 4.671718206914957e-06,
      "loss": 1.019,
      "step": 7368
    },
    {
      "epoch": 68.87709991158268,
      "grad_norm": 0.5776259303092957,
      "learning_rate": 4.669157039170451e-06,
      "loss": 1.0535,
      "step": 7369
    },
    {
      "epoch": 68.88653109342765,
      "grad_norm": 0.5635417103767395,
      "learning_rate": 4.6665963598360134e-06,
      "loss": 1.062,
      "step": 7370
    },
    {
      "epoch": 68.89596227527262,
      "grad_norm": 0.5133734345436096,
      "learning_rate": 4.6640361691462495e-06,
      "loss": 1.0909,
      "step": 7371
    },
    {
      "epoch": 68.9053934571176,
      "grad_norm": 0.5587379932403564,
      "learning_rate": 4.661476467335726e-06,
      "loss": 1.0105,
      "step": 7372
    },
    {
      "epoch": 68.91482463896257,
      "grad_norm": 0.5344615578651428,
      "learning_rate": 4.6589172546389615e-06,
      "loss": 1.0372,
      "step": 7373
    },
    {
      "epoch": 68.92425582080755,
      "grad_norm": 0.5290577411651611,
      "learning_rate": 4.65635853129043e-06,
      "loss": 1.0549,
      "step": 7374
    },
    {
      "epoch": 68.93368700265252,
      "grad_norm": 0.5620654821395874,
      "learning_rate": 4.6538002975245564e-06,
      "loss": 1.0179,
      "step": 7375
    },
    {
      "epoch": 68.9431181844975,
      "grad_norm": 0.5643692016601562,
      "learning_rate": 4.651242553575734e-06,
      "loss": 1.1244,
      "step": 7376
    },
    {
      "epoch": 68.95254936634247,
      "grad_norm": 0.5410785675048828,
      "learning_rate": 4.648685299678298e-06,
      "loss": 1.0765,
      "step": 7377
    },
    {
      "epoch": 68.96198054818744,
      "grad_norm": 0.5255137085914612,
      "learning_rate": 4.646128536066544e-06,
      "loss": 1.0764,
      "step": 7378
    },
    {
      "epoch": 68.97141173003241,
      "grad_norm": 0.5466615557670593,
      "learning_rate": 4.64357226297472e-06,
      "loss": 1.0383,
      "step": 7379
    },
    {
      "epoch": 68.98084291187739,
      "grad_norm": 0.6079467535018921,
      "learning_rate": 4.641016480637033e-06,
      "loss": 1.035,
      "step": 7380
    },
    {
      "epoch": 68.99027409372236,
      "grad_norm": 0.5528621673583984,
      "learning_rate": 4.638461189287641e-06,
      "loss": 1.0393,
      "step": 7381
    },
    {
      "epoch": 68.99970527556735,
      "grad_norm": 0.5431455373764038,
      "learning_rate": 4.635906389160656e-06,
      "loss": 1.0202,
      "step": 7382
    },
    {
      "epoch": 69.0,
      "grad_norm": 2.8161163330078125,
      "learning_rate": 4.633352080490157e-06,
      "loss": 0.7519,
      "step": 7383
    },
    {
      "epoch": 69.00943118184497,
      "grad_norm": 0.5191739797592163,
      "learning_rate": 4.630798263510162e-06,
      "loss": 1.0625,
      "step": 7384
    },
    {
      "epoch": 69.01886236368995,
      "grad_norm": 0.5247000455856323,
      "learning_rate": 4.628244938454654e-06,
      "loss": 1.0602,
      "step": 7385
    },
    {
      "epoch": 69.02829354553492,
      "grad_norm": 0.5573982000350952,
      "learning_rate": 4.625692105557565e-06,
      "loss": 1.0943,
      "step": 7386
    },
    {
      "epoch": 69.0377247273799,
      "grad_norm": 0.53983074426651,
      "learning_rate": 4.623139765052786e-06,
      "loss": 1.064,
      "step": 7387
    },
    {
      "epoch": 69.04715590922487,
      "grad_norm": 0.5380741357803345,
      "learning_rate": 4.620587917174163e-06,
      "loss": 1.0615,
      "step": 7388
    },
    {
      "epoch": 69.05658709106984,
      "grad_norm": 0.5999172329902649,
      "learning_rate": 4.618036562155493e-06,
      "loss": 1.032,
      "step": 7389
    },
    {
      "epoch": 69.06601827291482,
      "grad_norm": 0.5313202738761902,
      "learning_rate": 4.615485700230529e-06,
      "loss": 1.0737,
      "step": 7390
    },
    {
      "epoch": 69.0754494547598,
      "grad_norm": 0.5803247094154358,
      "learning_rate": 4.612935331632985e-06,
      "loss": 1.0593,
      "step": 7391
    },
    {
      "epoch": 69.08488063660478,
      "grad_norm": 0.5622990131378174,
      "learning_rate": 4.6103854565965235e-06,
      "loss": 1.0288,
      "step": 7392
    },
    {
      "epoch": 69.09431181844975,
      "grad_norm": 0.5803561210632324,
      "learning_rate": 4.607836075354761e-06,
      "loss": 1.0368,
      "step": 7393
    },
    {
      "epoch": 69.10374300029473,
      "grad_norm": 0.5450895428657532,
      "learning_rate": 4.605287188141273e-06,
      "loss": 1.0673,
      "step": 7394
    },
    {
      "epoch": 69.1131741821397,
      "grad_norm": 0.5236964225769043,
      "learning_rate": 4.6027387951895865e-06,
      "loss": 1.0394,
      "step": 7395
    },
    {
      "epoch": 69.12260536398468,
      "grad_norm": 0.5838234424591064,
      "learning_rate": 4.600190896733185e-06,
      "loss": 1.0447,
      "step": 7396
    },
    {
      "epoch": 69.13203654582965,
      "grad_norm": 0.5201124548912048,
      "learning_rate": 4.597643493005502e-06,
      "loss": 1.0248,
      "step": 7397
    },
    {
      "epoch": 69.14146772767462,
      "grad_norm": 0.5747622847557068,
      "learning_rate": 4.595096584239938e-06,
      "loss": 1.0135,
      "step": 7398
    },
    {
      "epoch": 69.1508989095196,
      "grad_norm": 0.5766681432723999,
      "learning_rate": 4.592550170669835e-06,
      "loss": 1.0822,
      "step": 7399
    },
    {
      "epoch": 69.16033009136457,
      "grad_norm": 0.5323470234870911,
      "learning_rate": 4.590004252528496e-06,
      "loss": 1.0435,
      "step": 7400
    },
    {
      "epoch": 69.16976127320955,
      "grad_norm": 0.5130468606948853,
      "learning_rate": 4.587458830049175e-06,
      "loss": 1.0717,
      "step": 7401
    },
    {
      "epoch": 69.17919245505452,
      "grad_norm": 0.5516079664230347,
      "learning_rate": 4.584913903465085e-06,
      "loss": 1.0497,
      "step": 7402
    },
    {
      "epoch": 69.1886236368995,
      "grad_norm": 0.5626412034034729,
      "learning_rate": 4.58236947300939e-06,
      "loss": 1.0505,
      "step": 7403
    },
    {
      "epoch": 69.19805481874447,
      "grad_norm": 0.5328512787818909,
      "learning_rate": 4.579825538915211e-06,
      "loss": 1.0543,
      "step": 7404
    },
    {
      "epoch": 69.20748600058945,
      "grad_norm": 0.5131741166114807,
      "learning_rate": 4.5772821014156174e-06,
      "loss": 1.0322,
      "step": 7405
    },
    {
      "epoch": 69.21691718243443,
      "grad_norm": 0.5024369955062866,
      "learning_rate": 4.574739160743647e-06,
      "loss": 1.0502,
      "step": 7406
    },
    {
      "epoch": 69.2263483642794,
      "grad_norm": 0.5217913389205933,
      "learning_rate": 4.572196717132278e-06,
      "loss": 1.1155,
      "step": 7407
    },
    {
      "epoch": 69.23577954612438,
      "grad_norm": 0.5340247750282288,
      "learning_rate": 4.569654770814449e-06,
      "loss": 1.0333,
      "step": 7408
    },
    {
      "epoch": 69.24521072796935,
      "grad_norm": 0.5514954328536987,
      "learning_rate": 4.5671133220230525e-06,
      "loss": 1.0671,
      "step": 7409
    },
    {
      "epoch": 69.25464190981432,
      "grad_norm": 0.4970270097255707,
      "learning_rate": 4.564572370990934e-06,
      "loss": 1.0584,
      "step": 7410
    },
    {
      "epoch": 69.2640730916593,
      "grad_norm": 0.5589770674705505,
      "learning_rate": 4.5620319179508954e-06,
      "loss": 1.0002,
      "step": 7411
    },
    {
      "epoch": 69.27350427350427,
      "grad_norm": 0.5394487380981445,
      "learning_rate": 4.5594919631356895e-06,
      "loss": 1.0828,
      "step": 7412
    },
    {
      "epoch": 69.28293545534925,
      "grad_norm": 0.5261629819869995,
      "learning_rate": 4.556952506778032e-06,
      "loss": 1.0598,
      "step": 7413
    },
    {
      "epoch": 69.29236663719422,
      "grad_norm": 0.561426043510437,
      "learning_rate": 4.554413549110583e-06,
      "loss": 1.0083,
      "step": 7414
    },
    {
      "epoch": 69.3017978190392,
      "grad_norm": 0.5173333883285522,
      "learning_rate": 4.551875090365964e-06,
      "loss": 1.0862,
      "step": 7415
    },
    {
      "epoch": 69.31122900088417,
      "grad_norm": 0.5290001034736633,
      "learning_rate": 4.549337130776743e-06,
      "loss": 1.0343,
      "step": 7416
    },
    {
      "epoch": 69.32066018272914,
      "grad_norm": 0.5201765298843384,
      "learning_rate": 4.546799670575451e-06,
      "loss": 1.0284,
      "step": 7417
    },
    {
      "epoch": 69.33009136457412,
      "grad_norm": 0.5453928112983704,
      "learning_rate": 4.544262709994568e-06,
      "loss": 1.0688,
      "step": 7418
    },
    {
      "epoch": 69.3395225464191,
      "grad_norm": 0.5482903718948364,
      "learning_rate": 4.541726249266525e-06,
      "loss": 1.0534,
      "step": 7419
    },
    {
      "epoch": 69.34895372826408,
      "grad_norm": 0.6155462265014648,
      "learning_rate": 4.53919028862372e-06,
      "loss": 1.0689,
      "step": 7420
    },
    {
      "epoch": 69.35838491010905,
      "grad_norm": 0.5287713408470154,
      "learning_rate": 4.536654828298494e-06,
      "loss": 1.0821,
      "step": 7421
    },
    {
      "epoch": 69.36781609195403,
      "grad_norm": 0.5448930263519287,
      "learning_rate": 4.534119868523143e-06,
      "loss": 1.0888,
      "step": 7422
    },
    {
      "epoch": 69.377247273799,
      "grad_norm": 0.5325525999069214,
      "learning_rate": 4.53158540952992e-06,
      "loss": 1.0309,
      "step": 7423
    },
    {
      "epoch": 69.38667845564397,
      "grad_norm": 0.5023916959762573,
      "learning_rate": 4.529051451551028e-06,
      "loss": 1.0408,
      "step": 7424
    },
    {
      "epoch": 69.39610963748895,
      "grad_norm": 0.5400541424751282,
      "learning_rate": 4.526517994818633e-06,
      "loss": 1.0348,
      "step": 7425
    },
    {
      "epoch": 69.40554081933392,
      "grad_norm": 0.5485610961914062,
      "learning_rate": 4.523985039564849e-06,
      "loss": 1.0549,
      "step": 7426
    },
    {
      "epoch": 69.4149720011789,
      "grad_norm": 0.5407615900039673,
      "learning_rate": 4.521452586021743e-06,
      "loss": 1.0047,
      "step": 7427
    },
    {
      "epoch": 69.42440318302387,
      "grad_norm": 0.5584126710891724,
      "learning_rate": 4.518920634421336e-06,
      "loss": 1.0505,
      "step": 7428
    },
    {
      "epoch": 69.43383436486884,
      "grad_norm": 0.5678775906562805,
      "learning_rate": 4.516389184995604e-06,
      "loss": 1.0618,
      "step": 7429
    },
    {
      "epoch": 69.44326554671382,
      "grad_norm": 0.5705377459526062,
      "learning_rate": 4.513858237976482e-06,
      "loss": 1.0321,
      "step": 7430
    },
    {
      "epoch": 69.45269672855879,
      "grad_norm": 0.5774381756782532,
      "learning_rate": 4.511327793595852e-06,
      "loss": 1.0058,
      "step": 7431
    },
    {
      "epoch": 69.46212791040377,
      "grad_norm": 0.5138769745826721,
      "learning_rate": 4.508797852085552e-06,
      "loss": 1.0824,
      "step": 7432
    },
    {
      "epoch": 69.47155909224875,
      "grad_norm": 0.5354479551315308,
      "learning_rate": 4.506268413677376e-06,
      "loss": 1.0686,
      "step": 7433
    },
    {
      "epoch": 69.48099027409373,
      "grad_norm": 0.5455822944641113,
      "learning_rate": 4.503739478603069e-06,
      "loss": 1.0468,
      "step": 7434
    },
    {
      "epoch": 69.4904214559387,
      "grad_norm": 0.5560364723205566,
      "learning_rate": 4.5012110470943274e-06,
      "loss": 1.0317,
      "step": 7435
    },
    {
      "epoch": 69.49985263778368,
      "grad_norm": 0.5322274565696716,
      "learning_rate": 4.498683119382813e-06,
      "loss": 1.077,
      "step": 7436
    },
    {
      "epoch": 69.50928381962865,
      "grad_norm": 0.5601726174354553,
      "learning_rate": 4.49615569570013e-06,
      "loss": 1.0578,
      "step": 7437
    },
    {
      "epoch": 69.51871500147362,
      "grad_norm": 0.514635443687439,
      "learning_rate": 4.493628776277839e-06,
      "loss": 1.0584,
      "step": 7438
    },
    {
      "epoch": 69.5281461833186,
      "grad_norm": 0.5526337623596191,
      "learning_rate": 4.491102361347458e-06,
      "loss": 1.0423,
      "step": 7439
    },
    {
      "epoch": 69.53757736516357,
      "grad_norm": 0.5934162735939026,
      "learning_rate": 4.488576451140455e-06,
      "loss": 1.0343,
      "step": 7440
    },
    {
      "epoch": 69.54700854700855,
      "grad_norm": 0.5334559679031372,
      "learning_rate": 4.4860510458882524e-06,
      "loss": 0.9895,
      "step": 7441
    },
    {
      "epoch": 69.55643972885352,
      "grad_norm": 0.5334284901618958,
      "learning_rate": 4.483526145822223e-06,
      "loss": 1.0542,
      "step": 7442
    },
    {
      "epoch": 69.5658709106985,
      "grad_norm": 0.5573791265487671,
      "learning_rate": 4.481001751173707e-06,
      "loss": 1.0755,
      "step": 7443
    },
    {
      "epoch": 69.57530209254347,
      "grad_norm": 0.5602477788925171,
      "learning_rate": 4.478477862173983e-06,
      "loss": 1.0281,
      "step": 7444
    },
    {
      "epoch": 69.58473327438844,
      "grad_norm": 0.5715140700340271,
      "learning_rate": 4.4759544790542895e-06,
      "loss": 1.0245,
      "step": 7445
    },
    {
      "epoch": 69.59416445623341,
      "grad_norm": 0.527698814868927,
      "learning_rate": 4.473431602045817e-06,
      "loss": 1.0458,
      "step": 7446
    },
    {
      "epoch": 69.6035956380784,
      "grad_norm": 0.5179546475410461,
      "learning_rate": 4.470909231379712e-06,
      "loss": 1.0815,
      "step": 7447
    },
    {
      "epoch": 69.61302681992338,
      "grad_norm": 0.5188034176826477,
      "learning_rate": 4.468387367287074e-06,
      "loss": 1.0515,
      "step": 7448
    },
    {
      "epoch": 69.62245800176835,
      "grad_norm": 0.5628976225852966,
      "learning_rate": 4.465866009998952e-06,
      "loss": 1.0507,
      "step": 7449
    },
    {
      "epoch": 69.63188918361332,
      "grad_norm": 0.5530592799186707,
      "learning_rate": 4.463345159746351e-06,
      "loss": 1.0603,
      "step": 7450
    },
    {
      "epoch": 69.6413203654583,
      "grad_norm": 0.5345408916473389,
      "learning_rate": 4.460824816760238e-06,
      "loss": 1.0527,
      "step": 7451
    },
    {
      "epoch": 69.65075154730327,
      "grad_norm": 0.5693296194076538,
      "learning_rate": 4.458304981271522e-06,
      "loss": 1.0313,
      "step": 7452
    },
    {
      "epoch": 69.66018272914825,
      "grad_norm": 0.5040904879570007,
      "learning_rate": 4.455785653511068e-06,
      "loss": 1.0564,
      "step": 7453
    },
    {
      "epoch": 69.66961391099322,
      "grad_norm": 0.5636966228485107,
      "learning_rate": 4.453266833709696e-06,
      "loss": 1.0823,
      "step": 7454
    },
    {
      "epoch": 69.6790450928382,
      "grad_norm": 0.5098719000816345,
      "learning_rate": 4.450748522098181e-06,
      "loss": 1.0574,
      "step": 7455
    },
    {
      "epoch": 69.68847627468317,
      "grad_norm": 0.5328410267829895,
      "learning_rate": 4.44823071890725e-06,
      "loss": 1.0578,
      "step": 7456
    },
    {
      "epoch": 69.69790745652814,
      "grad_norm": 0.5646579265594482,
      "learning_rate": 4.445713424367579e-06,
      "loss": 1.0568,
      "step": 7457
    },
    {
      "epoch": 69.70733863837312,
      "grad_norm": 0.5228796601295471,
      "learning_rate": 4.443196638709809e-06,
      "loss": 1.0908,
      "step": 7458
    },
    {
      "epoch": 69.71676982021809,
      "grad_norm": 0.5891976356506348,
      "learning_rate": 4.440680362164522e-06,
      "loss": 1.0314,
      "step": 7459
    },
    {
      "epoch": 69.72620100206306,
      "grad_norm": 0.5386804938316345,
      "learning_rate": 4.438164594962261e-06,
      "loss": 1.0953,
      "step": 7460
    },
    {
      "epoch": 69.73563218390805,
      "grad_norm": 0.5614840388298035,
      "learning_rate": 4.4356493373335185e-06,
      "loss": 1.0608,
      "step": 7461
    },
    {
      "epoch": 69.74506336575303,
      "grad_norm": 0.537645697593689,
      "learning_rate": 4.4331345895087415e-06,
      "loss": 1.0489,
      "step": 7462
    },
    {
      "epoch": 69.754494547598,
      "grad_norm": 0.570058286190033,
      "learning_rate": 4.43062035171833e-06,
      "loss": 1.0589,
      "step": 7463
    },
    {
      "epoch": 69.76392572944297,
      "grad_norm": 0.5149854421615601,
      "learning_rate": 4.4281066241926385e-06,
      "loss": 1.072,
      "step": 7464
    },
    {
      "epoch": 69.77335691128795,
      "grad_norm": 0.5699158310890198,
      "learning_rate": 4.42559340716197e-06,
      "loss": 1.0785,
      "step": 7465
    },
    {
      "epoch": 69.78278809313292,
      "grad_norm": 0.5772532820701599,
      "learning_rate": 4.423080700856592e-06,
      "loss": 1.0428,
      "step": 7466
    },
    {
      "epoch": 69.7922192749779,
      "grad_norm": 0.544700026512146,
      "learning_rate": 4.420568505506713e-06,
      "loss": 1.0402,
      "step": 7467
    },
    {
      "epoch": 69.80165045682287,
      "grad_norm": 0.5900189876556396,
      "learning_rate": 4.4180568213425e-06,
      "loss": 1.0381,
      "step": 7468
    },
    {
      "epoch": 69.81108163866784,
      "grad_norm": 0.5268340706825256,
      "learning_rate": 4.415545648594074e-06,
      "loss": 1.0758,
      "step": 7469
    },
    {
      "epoch": 69.82051282051282,
      "grad_norm": 0.6161313652992249,
      "learning_rate": 4.413034987491507e-06,
      "loss": 1.0152,
      "step": 7470
    },
    {
      "epoch": 69.82994400235779,
      "grad_norm": 0.5852562785148621,
      "learning_rate": 4.410524838264825e-06,
      "loss": 1.0139,
      "step": 7471
    },
    {
      "epoch": 69.83937518420277,
      "grad_norm": 0.5745068192481995,
      "learning_rate": 4.408015201144003e-06,
      "loss": 1.0563,
      "step": 7472
    },
    {
      "epoch": 69.84880636604774,
      "grad_norm": 0.555483877658844,
      "learning_rate": 4.405506076358979e-06,
      "loss": 1.0802,
      "step": 7473
    },
    {
      "epoch": 69.85823754789271,
      "grad_norm": 0.5523455142974854,
      "learning_rate": 4.402997464139638e-06,
      "loss": 1.0506,
      "step": 7474
    },
    {
      "epoch": 69.8676687297377,
      "grad_norm": 0.5172565579414368,
      "learning_rate": 4.400489364715816e-06,
      "loss": 1.0518,
      "step": 7475
    },
    {
      "epoch": 69.87709991158268,
      "grad_norm": 0.5210754871368408,
      "learning_rate": 4.397981778317305e-06,
      "loss": 1.0418,
      "step": 7476
    },
    {
      "epoch": 69.88653109342765,
      "grad_norm": 0.5423353314399719,
      "learning_rate": 4.395474705173849e-06,
      "loss": 1.011,
      "step": 7477
    },
    {
      "epoch": 69.89596227527262,
      "grad_norm": 0.5408910512924194,
      "learning_rate": 4.392968145515144e-06,
      "loss": 1.0384,
      "step": 7478
    },
    {
      "epoch": 69.9053934571176,
      "grad_norm": 0.5309842824935913,
      "learning_rate": 4.390462099570842e-06,
      "loss": 1.0304,
      "step": 7479
    },
    {
      "epoch": 69.91482463896257,
      "grad_norm": 0.5484552383422852,
      "learning_rate": 4.387956567570542e-06,
      "loss": 1.0719,
      "step": 7480
    },
    {
      "epoch": 69.92425582080755,
      "grad_norm": 0.5384601354598999,
      "learning_rate": 4.385451549743807e-06,
      "loss": 1.0793,
      "step": 7481
    },
    {
      "epoch": 69.93368700265252,
      "grad_norm": 0.5312583446502686,
      "learning_rate": 4.382947046320142e-06,
      "loss": 1.0651,
      "step": 7482
    },
    {
      "epoch": 69.9431181844975,
      "grad_norm": 0.5695362687110901,
      "learning_rate": 4.380443057529008e-06,
      "loss": 1.0323,
      "step": 7483
    },
    {
      "epoch": 69.95254936634247,
      "grad_norm": 0.5236114263534546,
      "learning_rate": 4.3779395835998215e-06,
      "loss": 1.0564,
      "step": 7484
    },
    {
      "epoch": 69.96198054818744,
      "grad_norm": 0.5506778955459595,
      "learning_rate": 4.37543662476195e-06,
      "loss": 1.058,
      "step": 7485
    },
    {
      "epoch": 69.97141173003241,
      "grad_norm": 0.5510220527648926,
      "learning_rate": 4.372934181244709e-06,
      "loss": 1.0499,
      "step": 7486
    },
    {
      "epoch": 69.98084291187739,
      "grad_norm": 0.5106691718101501,
      "learning_rate": 4.3704322532773776e-06,
      "loss": 1.0086,
      "step": 7487
    },
    {
      "epoch": 69.99027409372236,
      "grad_norm": 0.518578827381134,
      "learning_rate": 4.36793084108918e-06,
      "loss": 1.069,
      "step": 7488
    },
    {
      "epoch": 69.99970527556735,
      "grad_norm": 0.5749258399009705,
      "learning_rate": 4.365429944909295e-06,
      "loss": 1.0358,
      "step": 7489
    },
    {
      "epoch": 70.0,
      "grad_norm": 2.3804829120635986,
      "learning_rate": 4.362929564966854e-06,
      "loss": 0.5484,
      "step": 7490
    },
    {
      "epoch": 70.00943118184497,
      "grad_norm": 0.5215336084365845,
      "learning_rate": 4.360429701490935e-06,
      "loss": 1.0389,
      "step": 7491
    },
    {
      "epoch": 70.01886236368995,
      "grad_norm": 0.5139678716659546,
      "learning_rate": 4.357930354710584e-06,
      "loss": 1.0575,
      "step": 7492
    },
    {
      "epoch": 70.02829354553492,
      "grad_norm": 0.5299134254455566,
      "learning_rate": 4.355431524854785e-06,
      "loss": 1.0651,
      "step": 7493
    },
    {
      "epoch": 70.0377247273799,
      "grad_norm": 0.5333324670791626,
      "learning_rate": 4.352933212152481e-06,
      "loss": 1.0386,
      "step": 7494
    },
    {
      "epoch": 70.04715590922487,
      "grad_norm": 0.5573613047599792,
      "learning_rate": 4.350435416832568e-06,
      "loss": 1.0566,
      "step": 7495
    },
    {
      "epoch": 70.05658709106984,
      "grad_norm": 0.5461257696151733,
      "learning_rate": 4.347938139123887e-06,
      "loss": 1.1086,
      "step": 7496
    },
    {
      "epoch": 70.06601827291482,
      "grad_norm": 0.5375727415084839,
      "learning_rate": 4.345441379255246e-06,
      "loss": 1.0452,
      "step": 7497
    },
    {
      "epoch": 70.0754494547598,
      "grad_norm": 0.5520890951156616,
      "learning_rate": 4.342945137455395e-06,
      "loss": 1.0266,
      "step": 7498
    },
    {
      "epoch": 70.08488063660478,
      "grad_norm": 0.5113041996955872,
      "learning_rate": 4.340449413953037e-06,
      "loss": 1.0841,
      "step": 7499
    },
    {
      "epoch": 70.09431181844975,
      "grad_norm": 0.5473449230194092,
      "learning_rate": 4.33795420897683e-06,
      "loss": 1.0469,
      "step": 7500
    },
    {
      "epoch": 70.10374300029473,
      "grad_norm": 0.5658267736434937,
      "learning_rate": 4.3354595227553846e-06,
      "loss": 1.0389,
      "step": 7501
    },
    {
      "epoch": 70.1131741821397,
      "grad_norm": 0.5916860699653625,
      "learning_rate": 4.332965355517258e-06,
      "loss": 1.0426,
      "step": 7502
    },
    {
      "epoch": 70.12260536398468,
      "grad_norm": 0.5549426674842834,
      "learning_rate": 4.330471707490973e-06,
      "loss": 1.0396,
      "step": 7503
    },
    {
      "epoch": 70.13203654582965,
      "grad_norm": 0.5281518697738647,
      "learning_rate": 4.327978578904994e-06,
      "loss": 1.0249,
      "step": 7504
    },
    {
      "epoch": 70.14146772767462,
      "grad_norm": 0.5218923687934875,
      "learning_rate": 4.325485969987739e-06,
      "loss": 1.0402,
      "step": 7505
    },
    {
      "epoch": 70.1508989095196,
      "grad_norm": 0.6041162610054016,
      "learning_rate": 4.322993880967581e-06,
      "loss": 1.0637,
      "step": 7506
    },
    {
      "epoch": 70.16033009136457,
      "grad_norm": 0.5248508453369141,
      "learning_rate": 4.320502312072845e-06,
      "loss": 1.0681,
      "step": 7507
    },
    {
      "epoch": 70.16976127320955,
      "grad_norm": 0.5564585328102112,
      "learning_rate": 4.318011263531806e-06,
      "loss": 1.0316,
      "step": 7508
    },
    {
      "epoch": 70.17919245505452,
      "grad_norm": 0.548964262008667,
      "learning_rate": 4.31552073557269e-06,
      "loss": 1.0367,
      "step": 7509
    },
    {
      "epoch": 70.1886236368995,
      "grad_norm": 0.5614336729049683,
      "learning_rate": 4.313030728423687e-06,
      "loss": 1.0708,
      "step": 7510
    },
    {
      "epoch": 70.19805481874447,
      "grad_norm": 0.4963088929653168,
      "learning_rate": 4.310541242312925e-06,
      "loss": 1.0527,
      "step": 7511
    },
    {
      "epoch": 70.20748600058945,
      "grad_norm": 0.4780980050563812,
      "learning_rate": 4.308052277468492e-06,
      "loss": 1.0485,
      "step": 7512
    },
    {
      "epoch": 70.21691718243443,
      "grad_norm": 0.5688390731811523,
      "learning_rate": 4.305563834118424e-06,
      "loss": 1.0435,
      "step": 7513
    },
    {
      "epoch": 70.2263483642794,
      "grad_norm": 0.5621531009674072,
      "learning_rate": 4.303075912490714e-06,
      "loss": 1.0588,
      "step": 7514
    },
    {
      "epoch": 70.23577954612438,
      "grad_norm": 0.5450005531311035,
      "learning_rate": 4.3005885128133005e-06,
      "loss": 1.0616,
      "step": 7515
    },
    {
      "epoch": 70.24521072796935,
      "grad_norm": 0.5220147371292114,
      "learning_rate": 4.298101635314083e-06,
      "loss": 1.0711,
      "step": 7516
    },
    {
      "epoch": 70.25464190981432,
      "grad_norm": 0.5766336917877197,
      "learning_rate": 4.295615280220901e-06,
      "loss": 1.0358,
      "step": 7517
    },
    {
      "epoch": 70.2640730916593,
      "grad_norm": 0.5182135701179504,
      "learning_rate": 4.293129447761562e-06,
      "loss": 1.0721,
      "step": 7518
    },
    {
      "epoch": 70.27350427350427,
      "grad_norm": 0.5204690098762512,
      "learning_rate": 4.290644138163814e-06,
      "loss": 1.0785,
      "step": 7519
    },
    {
      "epoch": 70.28293545534925,
      "grad_norm": 0.518763542175293,
      "learning_rate": 4.28815935165536e-06,
      "loss": 1.0432,
      "step": 7520
    },
    {
      "epoch": 70.29236663719422,
      "grad_norm": 0.5734983086585999,
      "learning_rate": 4.285675088463855e-06,
      "loss": 1.0824,
      "step": 7521
    },
    {
      "epoch": 70.3017978190392,
      "grad_norm": 0.5204513072967529,
      "learning_rate": 4.283191348816906e-06,
      "loss": 1.0687,
      "step": 7522
    },
    {
      "epoch": 70.31122900088417,
      "grad_norm": 0.5309038758277893,
      "learning_rate": 4.280708132942075e-06,
      "loss": 1.0407,
      "step": 7523
    },
    {
      "epoch": 70.32066018272914,
      "grad_norm": 0.5504275560379028,
      "learning_rate": 4.278225441066867e-06,
      "loss": 1.0699,
      "step": 7524
    },
    {
      "epoch": 70.33009136457412,
      "grad_norm": 0.5398542881011963,
      "learning_rate": 4.275743273418752e-06,
      "loss": 1.0261,
      "step": 7525
    },
    {
      "epoch": 70.3395225464191,
      "grad_norm": 0.507179856300354,
      "learning_rate": 4.273261630225145e-06,
      "loss": 1.0466,
      "step": 7526
    },
    {
      "epoch": 70.34895372826408,
      "grad_norm": 0.5606938004493713,
      "learning_rate": 4.27078051171341e-06,
      "loss": 1.0347,
      "step": 7527
    },
    {
      "epoch": 70.35838491010905,
      "grad_norm": 0.5247993469238281,
      "learning_rate": 4.26829991811087e-06,
      "loss": 1.0395,
      "step": 7528
    },
    {
      "epoch": 70.36781609195403,
      "grad_norm": 0.5559723377227783,
      "learning_rate": 4.265819849644792e-06,
      "loss": 1.0451,
      "step": 7529
    },
    {
      "epoch": 70.377247273799,
      "grad_norm": 0.5719350576400757,
      "learning_rate": 4.263340306542402e-06,
      "loss": 1.0515,
      "step": 7530
    },
    {
      "epoch": 70.38667845564397,
      "grad_norm": 0.5091489553451538,
      "learning_rate": 4.260861289030874e-06,
      "loss": 1.0002,
      "step": 7531
    },
    {
      "epoch": 70.39610963748895,
      "grad_norm": 0.535088062286377,
      "learning_rate": 4.258382797337331e-06,
      "loss": 1.0043,
      "step": 7532
    },
    {
      "epoch": 70.40554081933392,
      "grad_norm": 0.5866312980651855,
      "learning_rate": 4.2559048316888585e-06,
      "loss": 0.9974,
      "step": 7533
    },
    {
      "epoch": 70.4149720011789,
      "grad_norm": 0.5742954611778259,
      "learning_rate": 4.253427392312485e-06,
      "loss": 1.0434,
      "step": 7534
    },
    {
      "epoch": 70.42440318302387,
      "grad_norm": 0.5404293537139893,
      "learning_rate": 4.2509504794351895e-06,
      "loss": 1.062,
      "step": 7535
    },
    {
      "epoch": 70.43383436486884,
      "grad_norm": 0.5361789464950562,
      "learning_rate": 4.2484740932839085e-06,
      "loss": 1.0612,
      "step": 7536
    },
    {
      "epoch": 70.44326554671382,
      "grad_norm": 0.5564627051353455,
      "learning_rate": 4.245998234085527e-06,
      "loss": 1.0589,
      "step": 7537
    },
    {
      "epoch": 70.45269672855879,
      "grad_norm": 0.5050662755966187,
      "learning_rate": 4.243522902066883e-06,
      "loss": 1.0419,
      "step": 7538
    },
    {
      "epoch": 70.46212791040377,
      "grad_norm": 0.5348150730133057,
      "learning_rate": 4.241048097454761e-06,
      "loss": 1.0536,
      "step": 7539
    },
    {
      "epoch": 70.47155909224875,
      "grad_norm": 0.5465042591094971,
      "learning_rate": 4.238573820475909e-06,
      "loss": 1.0689,
      "step": 7540
    },
    {
      "epoch": 70.48099027409373,
      "grad_norm": 0.5507010817527771,
      "learning_rate": 4.236100071357017e-06,
      "loss": 1.0818,
      "step": 7541
    },
    {
      "epoch": 70.4904214559387,
      "grad_norm": 0.5220155119895935,
      "learning_rate": 4.233626850324727e-06,
      "loss": 1.0737,
      "step": 7542
    },
    {
      "epoch": 70.49985263778368,
      "grad_norm": 0.5765977501869202,
      "learning_rate": 4.231154157605637e-06,
      "loss": 1.0065,
      "step": 7543
    },
    {
      "epoch": 70.50928381962865,
      "grad_norm": 0.5552588701248169,
      "learning_rate": 4.228681993426293e-06,
      "loss": 1.0546,
      "step": 7544
    },
    {
      "epoch": 70.51871500147362,
      "grad_norm": 0.5500206351280212,
      "learning_rate": 4.226210358013194e-06,
      "loss": 1.0676,
      "step": 7545
    },
    {
      "epoch": 70.5281461833186,
      "grad_norm": 0.5775647759437561,
      "learning_rate": 4.223739251592791e-06,
      "loss": 1.0396,
      "step": 7546
    },
    {
      "epoch": 70.53757736516357,
      "grad_norm": 0.5508424043655396,
      "learning_rate": 4.221268674391481e-06,
      "loss": 1.0382,
      "step": 7547
    },
    {
      "epoch": 70.54700854700855,
      "grad_norm": 0.5390298962593079,
      "learning_rate": 4.218798626635626e-06,
      "loss": 1.0307,
      "step": 7548
    },
    {
      "epoch": 70.55643972885352,
      "grad_norm": 0.5602632164955139,
      "learning_rate": 4.216329108551528e-06,
      "loss": 1.0042,
      "step": 7549
    },
    {
      "epoch": 70.5658709106985,
      "grad_norm": 0.5199458599090576,
      "learning_rate": 4.21386012036544e-06,
      "loss": 1.0486,
      "step": 7550
    },
    {
      "epoch": 70.57530209254347,
      "grad_norm": 0.555828332901001,
      "learning_rate": 4.2113916623035746e-06,
      "loss": 1.0358,
      "step": 7551
    },
    {
      "epoch": 70.58473327438844,
      "grad_norm": 0.5406264662742615,
      "learning_rate": 4.208923734592084e-06,
      "loss": 1.0235,
      "step": 7552
    },
    {
      "epoch": 70.59416445623341,
      "grad_norm": 0.560482919216156,
      "learning_rate": 4.2064563374570876e-06,
      "loss": 1.083,
      "step": 7553
    },
    {
      "epoch": 70.6035956380784,
      "grad_norm": 0.48728233575820923,
      "learning_rate": 4.203989471124643e-06,
      "loss": 1.0703,
      "step": 7554
    },
    {
      "epoch": 70.61302681992338,
      "grad_norm": 0.5575813055038452,
      "learning_rate": 4.201523135820766e-06,
      "loss": 1.0238,
      "step": 7555
    },
    {
      "epoch": 70.62245800176835,
      "grad_norm": 0.5520394444465637,
      "learning_rate": 4.199057331771418e-06,
      "loss": 1.0495,
      "step": 7556
    },
    {
      "epoch": 70.63188918361332,
      "grad_norm": 0.5333064794540405,
      "learning_rate": 4.196592059202514e-06,
      "loss": 1.0467,
      "step": 7557
    },
    {
      "epoch": 70.6413203654583,
      "grad_norm": 0.5628541707992554,
      "learning_rate": 4.194127318339927e-06,
      "loss": 1.0547,
      "step": 7558
    },
    {
      "epoch": 70.65075154730327,
      "grad_norm": 0.5615425109863281,
      "learning_rate": 4.191663109409473e-06,
      "loss": 1.0489,
      "step": 7559
    },
    {
      "epoch": 70.66018272914825,
      "grad_norm": 0.5244160890579224,
      "learning_rate": 4.189199432636922e-06,
      "loss": 1.095,
      "step": 7560
    },
    {
      "epoch": 70.66961391099322,
      "grad_norm": 0.565127432346344,
      "learning_rate": 4.186736288247996e-06,
      "loss": 1.0008,
      "step": 7561
    },
    {
      "epoch": 70.6790450928382,
      "grad_norm": 0.5217722058296204,
      "learning_rate": 4.184273676468363e-06,
      "loss": 1.076,
      "step": 7562
    },
    {
      "epoch": 70.68847627468317,
      "grad_norm": 0.5663543343544006,
      "learning_rate": 4.181811597523654e-06,
      "loss": 1.0647,
      "step": 7563
    },
    {
      "epoch": 70.69790745652814,
      "grad_norm": 0.556233823299408,
      "learning_rate": 4.17935005163944e-06,
      "loss": 1.0498,
      "step": 7564
    },
    {
      "epoch": 70.70733863837312,
      "grad_norm": 0.5256882309913635,
      "learning_rate": 4.1768890390412475e-06,
      "loss": 1.0539,
      "step": 7565
    },
    {
      "epoch": 70.71676982021809,
      "grad_norm": 0.6383643746376038,
      "learning_rate": 4.174428559954553e-06,
      "loss": 1.0502,
      "step": 7566
    },
    {
      "epoch": 70.72620100206306,
      "grad_norm": 0.5237544775009155,
      "learning_rate": 4.171968614604786e-06,
      "loss": 1.0794,
      "step": 7567
    },
    {
      "epoch": 70.73563218390805,
      "grad_norm": 0.523422122001648,
      "learning_rate": 4.169509203217324e-06,
      "loss": 1.0472,
      "step": 7568
    },
    {
      "epoch": 70.74506336575303,
      "grad_norm": 0.5517410039901733,
      "learning_rate": 4.167050326017496e-06,
      "loss": 1.068,
      "step": 7569
    },
    {
      "epoch": 70.754494547598,
      "grad_norm": 0.5734049677848816,
      "learning_rate": 4.1645919832305905e-06,
      "loss": 1.0607,
      "step": 7570
    },
    {
      "epoch": 70.76392572944297,
      "grad_norm": 0.5641721487045288,
      "learning_rate": 4.162134175081835e-06,
      "loss": 1.0383,
      "step": 7571
    },
    {
      "epoch": 70.77335691128795,
      "grad_norm": 0.5453543066978455,
      "learning_rate": 4.159676901796412e-06,
      "loss": 1.059,
      "step": 7572
    },
    {
      "epoch": 70.78278809313292,
      "grad_norm": 0.6292982697486877,
      "learning_rate": 4.157220163599459e-06,
      "loss": 1.0652,
      "step": 7573
    },
    {
      "epoch": 70.7922192749779,
      "grad_norm": 0.5808789134025574,
      "learning_rate": 4.154763960716061e-06,
      "loss": 1.0685,
      "step": 7574
    },
    {
      "epoch": 70.80165045682287,
      "grad_norm": 0.5097841620445251,
      "learning_rate": 4.152308293371253e-06,
      "loss": 1.0599,
      "step": 7575
    },
    {
      "epoch": 70.81108163866784,
      "grad_norm": 0.5745314359664917,
      "learning_rate": 4.149853161790024e-06,
      "loss": 1.0659,
      "step": 7576
    },
    {
      "epoch": 70.82051282051282,
      "grad_norm": 0.5698210000991821,
      "learning_rate": 4.147398566197307e-06,
      "loss": 1.0428,
      "step": 7577
    },
    {
      "epoch": 70.82994400235779,
      "grad_norm": 0.5209977030754089,
      "learning_rate": 4.144944506818e-06,
      "loss": 1.0645,
      "step": 7578
    },
    {
      "epoch": 70.83937518420277,
      "grad_norm": 0.5445173382759094,
      "learning_rate": 4.142490983876939e-06,
      "loss": 1.0773,
      "step": 7579
    },
    {
      "epoch": 70.84880636604774,
      "grad_norm": 0.570210874080658,
      "learning_rate": 4.140037997598915e-06,
      "loss": 1.0339,
      "step": 7580
    },
    {
      "epoch": 70.85823754789271,
      "grad_norm": 0.5272963643074036,
      "learning_rate": 4.137585548208669e-06,
      "loss": 1.0845,
      "step": 7581
    },
    {
      "epoch": 70.8676687297377,
      "grad_norm": 0.5724536776542664,
      "learning_rate": 4.135133635930895e-06,
      "loss": 1.0376,
      "step": 7582
    },
    {
      "epoch": 70.87709991158268,
      "grad_norm": 0.5473410487174988,
      "learning_rate": 4.132682260990235e-06,
      "loss": 1.0725,
      "step": 7583
    },
    {
      "epoch": 70.88653109342765,
      "grad_norm": 0.5063746571540833,
      "learning_rate": 4.130231423611281e-06,
      "loss": 1.058,
      "step": 7584
    },
    {
      "epoch": 70.89596227527262,
      "grad_norm": 0.5303425192832947,
      "learning_rate": 4.1277811240185824e-06,
      "loss": 1.0553,
      "step": 7585
    },
    {
      "epoch": 70.9053934571176,
      "grad_norm": 0.5659580230712891,
      "learning_rate": 4.1253313624366355e-06,
      "loss": 1.0031,
      "step": 7586
    },
    {
      "epoch": 70.91482463896257,
      "grad_norm": 0.5909152626991272,
      "learning_rate": 4.122882139089883e-06,
      "loss": 1.0472,
      "step": 7587
    },
    {
      "epoch": 70.92425582080755,
      "grad_norm": 0.5362896919250488,
      "learning_rate": 4.120433454202725e-06,
      "loss": 1.0641,
      "step": 7588
    },
    {
      "epoch": 70.93368700265252,
      "grad_norm": 0.5738314986228943,
      "learning_rate": 4.117985307999506e-06,
      "loss": 1.0516,
      "step": 7589
    },
    {
      "epoch": 70.9431181844975,
      "grad_norm": 0.5383754372596741,
      "learning_rate": 4.115537700704525e-06,
      "loss": 1.066,
      "step": 7590
    },
    {
      "epoch": 70.95254936634247,
      "grad_norm": 0.5533046126365662,
      "learning_rate": 4.113090632542033e-06,
      "loss": 1.0537,
      "step": 7591
    },
    {
      "epoch": 70.96198054818744,
      "grad_norm": 0.5520166754722595,
      "learning_rate": 4.110644103736225e-06,
      "loss": 1.0455,
      "step": 7592
    },
    {
      "epoch": 70.97141173003241,
      "grad_norm": 0.5577577352523804,
      "learning_rate": 4.108198114511257e-06,
      "loss": 1.0397,
      "step": 7593
    },
    {
      "epoch": 70.98084291187739,
      "grad_norm": 0.5602701902389526,
      "learning_rate": 4.105752665091228e-06,
      "loss": 1.0479,
      "step": 7594
    },
    {
      "epoch": 70.99027409372236,
      "grad_norm": 0.5603504776954651,
      "learning_rate": 4.103307755700188e-06,
      "loss": 1.0341,
      "step": 7595
    },
    {
      "epoch": 70.99970527556735,
      "grad_norm": 0.5423720479011536,
      "learning_rate": 4.10086338656214e-06,
      "loss": 1.0532,
      "step": 7596
    },
    {
      "epoch": 71.0,
      "grad_norm": 6.621179580688477,
      "learning_rate": 4.098419557901036e-06,
      "loss": 0.252,
      "step": 7597
    },
    {
      "epoch": 71.00943118184497,
      "grad_norm": 0.5586271286010742,
      "learning_rate": 4.095976269940777e-06,
      "loss": 1.0281,
      "step": 7598
    },
    {
      "epoch": 71.01886236368995,
      "grad_norm": 0.5868005752563477,
      "learning_rate": 4.093533522905214e-06,
      "loss": 1.0437,
      "step": 7599
    },
    {
      "epoch": 71.02829354553492,
      "grad_norm": 0.5411744713783264,
      "learning_rate": 4.091091317018159e-06,
      "loss": 1.0705,
      "step": 7600
    },
    {
      "epoch": 71.0377247273799,
      "grad_norm": 0.5541170835494995,
      "learning_rate": 4.088649652503361e-06,
      "loss": 1.0198,
      "step": 7601
    },
    {
      "epoch": 71.04715590922487,
      "grad_norm": 0.5047696828842163,
      "learning_rate": 4.086208529584523e-06,
      "loss": 1.0457,
      "step": 7602
    },
    {
      "epoch": 71.05658709106984,
      "grad_norm": 0.5209110975265503,
      "learning_rate": 4.083767948485304e-06,
      "loss": 1.0379,
      "step": 7603
    },
    {
      "epoch": 71.06601827291482,
      "grad_norm": 0.5191223621368408,
      "learning_rate": 4.081327909429306e-06,
      "loss": 1.0724,
      "step": 7604
    },
    {
      "epoch": 71.0754494547598,
      "grad_norm": 0.5635626912117004,
      "learning_rate": 4.078888412640087e-06,
      "loss": 1.0964,
      "step": 7605
    },
    {
      "epoch": 71.08488063660478,
      "grad_norm": 0.5029449462890625,
      "learning_rate": 4.076449458341145e-06,
      "loss": 1.0686,
      "step": 7606
    },
    {
      "epoch": 71.09431181844975,
      "grad_norm": 0.5517661571502686,
      "learning_rate": 4.074011046755948e-06,
      "loss": 1.0053,
      "step": 7607
    },
    {
      "epoch": 71.10374300029473,
      "grad_norm": 0.5665474534034729,
      "learning_rate": 4.071573178107897e-06,
      "loss": 1.0421,
      "step": 7608
    },
    {
      "epoch": 71.1131741821397,
      "grad_norm": 0.5044217705726624,
      "learning_rate": 4.069135852620348e-06,
      "loss": 1.0548,
      "step": 7609
    },
    {
      "epoch": 71.12260536398468,
      "grad_norm": 0.5415668487548828,
      "learning_rate": 4.06669907051661e-06,
      "loss": 1.1007,
      "step": 7610
    },
    {
      "epoch": 71.13203654582965,
      "grad_norm": 0.5427159667015076,
      "learning_rate": 4.064262832019938e-06,
      "loss": 1.0452,
      "step": 7611
    },
    {
      "epoch": 71.14146772767462,
      "grad_norm": 0.5554818511009216,
      "learning_rate": 4.06182713735354e-06,
      "loss": 1.0624,
      "step": 7612
    },
    {
      "epoch": 71.1508989095196,
      "grad_norm": 0.565212070941925,
      "learning_rate": 4.059391986740575e-06,
      "loss": 1.0476,
      "step": 7613
    },
    {
      "epoch": 71.16033009136457,
      "grad_norm": 0.5579720139503479,
      "learning_rate": 4.056957380404145e-06,
      "loss": 1.0422,
      "step": 7614
    },
    {
      "epoch": 71.16976127320955,
      "grad_norm": 0.562746524810791,
      "learning_rate": 4.054523318567316e-06,
      "loss": 1.0545,
      "step": 7615
    },
    {
      "epoch": 71.17919245505452,
      "grad_norm": 0.5741870999336243,
      "learning_rate": 4.052089801453092e-06,
      "loss": 1.0446,
      "step": 7616
    },
    {
      "epoch": 71.1886236368995,
      "grad_norm": 0.5385782122612,
      "learning_rate": 4.0496568292844315e-06,
      "loss": 1.0709,
      "step": 7617
    },
    {
      "epoch": 71.19805481874447,
      "grad_norm": 0.5506933927536011,
      "learning_rate": 4.0472244022842424e-06,
      "loss": 1.0403,
      "step": 7618
    },
    {
      "epoch": 71.20748600058945,
      "grad_norm": 0.5194942355155945,
      "learning_rate": 4.04479252067538e-06,
      "loss": 1.0354,
      "step": 7619
    },
    {
      "epoch": 71.21691718243443,
      "grad_norm": 0.5671399831771851,
      "learning_rate": 4.042361184680657e-06,
      "loss": 1.0411,
      "step": 7620
    },
    {
      "epoch": 71.2263483642794,
      "grad_norm": 0.5292508006095886,
      "learning_rate": 4.039930394522832e-06,
      "loss": 1.0799,
      "step": 7621
    },
    {
      "epoch": 71.23577954612438,
      "grad_norm": 0.5399246215820312,
      "learning_rate": 4.037500150424611e-06,
      "loss": 1.0338,
      "step": 7622
    },
    {
      "epoch": 71.24521072796935,
      "grad_norm": 0.5642996430397034,
      "learning_rate": 4.035070452608648e-06,
      "loss": 1.0831,
      "step": 7623
    },
    {
      "epoch": 71.25464190981432,
      "grad_norm": 0.5095462203025818,
      "learning_rate": 4.032641301297558e-06,
      "loss": 1.0102,
      "step": 7624
    },
    {
      "epoch": 71.2640730916593,
      "grad_norm": 0.5554162859916687,
      "learning_rate": 4.0302126967138974e-06,
      "loss": 1.0757,
      "step": 7625
    },
    {
      "epoch": 71.27350427350427,
      "grad_norm": 0.5497709512710571,
      "learning_rate": 4.027784639080175e-06,
      "loss": 1.0439,
      "step": 7626
    },
    {
      "epoch": 71.28293545534925,
      "grad_norm": 0.560482382774353,
      "learning_rate": 4.025357128618844e-06,
      "loss": 1.0141,
      "step": 7627
    },
    {
      "epoch": 71.29236663719422,
      "grad_norm": 0.51205974817276,
      "learning_rate": 4.0229301655523156e-06,
      "loss": 1.0744,
      "step": 7628
    },
    {
      "epoch": 71.3017978190392,
      "grad_norm": 0.5604465007781982,
      "learning_rate": 4.020503750102944e-06,
      "loss": 1.0548,
      "step": 7629
    },
    {
      "epoch": 71.31122900088417,
      "grad_norm": 0.5279605984687805,
      "learning_rate": 4.018077882493042e-06,
      "loss": 1.0509,
      "step": 7630
    },
    {
      "epoch": 71.32066018272914,
      "grad_norm": 0.5657275915145874,
      "learning_rate": 4.015652562944865e-06,
      "loss": 1.0664,
      "step": 7631
    },
    {
      "epoch": 71.33009136457412,
      "grad_norm": 0.5774166584014893,
      "learning_rate": 4.013227791680619e-06,
      "loss": 1.0197,
      "step": 7632
    },
    {
      "epoch": 71.3395225464191,
      "grad_norm": 0.5241028666496277,
      "learning_rate": 4.01080356892246e-06,
      "loss": 1.0651,
      "step": 7633
    },
    {
      "epoch": 71.34895372826408,
      "grad_norm": 0.5307607650756836,
      "learning_rate": 4.008379894892495e-06,
      "loss": 1.0727,
      "step": 7634
    },
    {
      "epoch": 71.35838491010905,
      "grad_norm": 0.5576642155647278,
      "learning_rate": 4.005956769812781e-06,
      "loss": 1.0821,
      "step": 7635
    },
    {
      "epoch": 71.36781609195403,
      "grad_norm": 0.5469256043434143,
      "learning_rate": 4.0035341939053195e-06,
      "loss": 1.0392,
      "step": 7636
    },
    {
      "epoch": 71.377247273799,
      "grad_norm": 0.5379509329795837,
      "learning_rate": 4.001112167392074e-06,
      "loss": 1.0551,
      "step": 7637
    },
    {
      "epoch": 71.38667845564397,
      "grad_norm": 0.5317189693450928,
      "learning_rate": 3.998690690494945e-06,
      "loss": 1.0447,
      "step": 7638
    },
    {
      "epoch": 71.39610963748895,
      "grad_norm": 0.5295922756195068,
      "learning_rate": 3.99626976343579e-06,
      "loss": 1.06,
      "step": 7639
    },
    {
      "epoch": 71.40554081933392,
      "grad_norm": 0.5439155697822571,
      "learning_rate": 3.993849386436409e-06,
      "loss": 1.0648,
      "step": 7640
    },
    {
      "epoch": 71.4149720011789,
      "grad_norm": 0.5222702026367188,
      "learning_rate": 3.991429559718561e-06,
      "loss": 1.0086,
      "step": 7641
    },
    {
      "epoch": 71.42440318302387,
      "grad_norm": 0.5657865405082703,
      "learning_rate": 3.989010283503947e-06,
      "loss": 1.0436,
      "step": 7642
    },
    {
      "epoch": 71.43383436486884,
      "grad_norm": 0.5679565668106079,
      "learning_rate": 3.98659155801422e-06,
      "loss": 1.0637,
      "step": 7643
    },
    {
      "epoch": 71.44326554671382,
      "grad_norm": 0.5771006345748901,
      "learning_rate": 3.98417338347098e-06,
      "loss": 1.0418,
      "step": 7644
    },
    {
      "epoch": 71.45269672855879,
      "grad_norm": 0.5239815711975098,
      "learning_rate": 3.981755760095788e-06,
      "loss": 1.0538,
      "step": 7645
    },
    {
      "epoch": 71.46212791040377,
      "grad_norm": 0.5660192966461182,
      "learning_rate": 3.9793386881101404e-06,
      "loss": 1.0991,
      "step": 7646
    },
    {
      "epoch": 71.47155909224875,
      "grad_norm": 0.5237753391265869,
      "learning_rate": 3.9769221677354885e-06,
      "loss": 1.0543,
      "step": 7647
    },
    {
      "epoch": 71.48099027409373,
      "grad_norm": 0.5186812877655029,
      "learning_rate": 3.9745061991932344e-06,
      "loss": 1.0352,
      "step": 7648
    },
    {
      "epoch": 71.4904214559387,
      "grad_norm": 0.553507387638092,
      "learning_rate": 3.972090782704726e-06,
      "loss": 1.0406,
      "step": 7649
    },
    {
      "epoch": 71.49985263778368,
      "grad_norm": 0.5454880595207214,
      "learning_rate": 3.969675918491266e-06,
      "loss": 1.0489,
      "step": 7650
    },
    {
      "epoch": 71.50928381962865,
      "grad_norm": 0.5323579907417297,
      "learning_rate": 3.9672616067740974e-06,
      "loss": 1.0533,
      "step": 7651
    },
    {
      "epoch": 71.51871500147362,
      "grad_norm": 0.5338299870491028,
      "learning_rate": 3.964847847774428e-06,
      "loss": 1.0418,
      "step": 7652
    },
    {
      "epoch": 71.5281461833186,
      "grad_norm": 0.5843886733055115,
      "learning_rate": 3.962434641713401e-06,
      "loss": 1.0512,
      "step": 7653
    },
    {
      "epoch": 71.53757736516357,
      "grad_norm": 0.576958954334259,
      "learning_rate": 3.960021988812111e-06,
      "loss": 1.057,
      "step": 7654
    },
    {
      "epoch": 71.54700854700855,
      "grad_norm": 0.5308473110198975,
      "learning_rate": 3.9576098892916095e-06,
      "loss": 1.0627,
      "step": 7655
    },
    {
      "epoch": 71.55643972885352,
      "grad_norm": 0.5477454662322998,
      "learning_rate": 3.955198343372888e-06,
      "loss": 1.0466,
      "step": 7656
    },
    {
      "epoch": 71.5658709106985,
      "grad_norm": 0.6014602184295654,
      "learning_rate": 3.952787351276893e-06,
      "loss": 1.0051,
      "step": 7657
    },
    {
      "epoch": 71.57530209254347,
      "grad_norm": 0.5781441330909729,
      "learning_rate": 3.9503769132245195e-06,
      "loss": 1.0598,
      "step": 7658
    },
    {
      "epoch": 71.58473327438844,
      "grad_norm": 0.5399585962295532,
      "learning_rate": 3.947967029436606e-06,
      "loss": 1.0602,
      "step": 7659
    },
    {
      "epoch": 71.59416445623341,
      "grad_norm": 0.5183917284011841,
      "learning_rate": 3.945557700133955e-06,
      "loss": 1.0704,
      "step": 7660
    },
    {
      "epoch": 71.6035956380784,
      "grad_norm": 0.5391868948936462,
      "learning_rate": 3.9431489255373026e-06,
      "loss": 1.0312,
      "step": 7661
    },
    {
      "epoch": 71.61302681992338,
      "grad_norm": 0.5646269917488098,
      "learning_rate": 3.94074070586734e-06,
      "loss": 1.0727,
      "step": 7662
    },
    {
      "epoch": 71.62245800176835,
      "grad_norm": 0.5544595122337341,
      "learning_rate": 3.938333041344708e-06,
      "loss": 1.0806,
      "step": 7663
    },
    {
      "epoch": 71.63188918361332,
      "grad_norm": 0.5521014928817749,
      "learning_rate": 3.935925932189996e-06,
      "loss": 1.054,
      "step": 7664
    },
    {
      "epoch": 71.6413203654583,
      "grad_norm": 0.5356414914131165,
      "learning_rate": 3.933519378623743e-06,
      "loss": 1.0161,
      "step": 7665
    },
    {
      "epoch": 71.65075154730327,
      "grad_norm": 0.5748133063316345,
      "learning_rate": 3.931113380866431e-06,
      "loss": 1.0413,
      "step": 7666
    },
    {
      "epoch": 71.66018272914825,
      "grad_norm": 0.5242595076560974,
      "learning_rate": 3.928707939138506e-06,
      "loss": 1.038,
      "step": 7667
    },
    {
      "epoch": 71.66961391099322,
      "grad_norm": 0.5444120168685913,
      "learning_rate": 3.926303053660351e-06,
      "loss": 1.0688,
      "step": 7668
    },
    {
      "epoch": 71.6790450928382,
      "grad_norm": 0.5489774942398071,
      "learning_rate": 3.923898724652298e-06,
      "loss": 1.0638,
      "step": 7669
    },
    {
      "epoch": 71.68847627468317,
      "grad_norm": 0.5548710227012634,
      "learning_rate": 3.921494952334634e-06,
      "loss": 1.0543,
      "step": 7670
    },
    {
      "epoch": 71.69790745652814,
      "grad_norm": 0.5613259077072144,
      "learning_rate": 3.919091736927589e-06,
      "loss": 1.0523,
      "step": 7671
    },
    {
      "epoch": 71.70733863837312,
      "grad_norm": 0.5210185050964355,
      "learning_rate": 3.916689078651346e-06,
      "loss": 1.0329,
      "step": 7672
    },
    {
      "epoch": 71.71676982021809,
      "grad_norm": 0.5338994860649109,
      "learning_rate": 3.9142869777260364e-06,
      "loss": 1.0677,
      "step": 7673
    },
    {
      "epoch": 71.72620100206306,
      "grad_norm": 0.5375556349754333,
      "learning_rate": 3.911885434371735e-06,
      "loss": 1.0497,
      "step": 7674
    },
    {
      "epoch": 71.73563218390805,
      "grad_norm": 0.5342122316360474,
      "learning_rate": 3.9094844488084785e-06,
      "loss": 1.0073,
      "step": 7675
    },
    {
      "epoch": 71.74506336575303,
      "grad_norm": 0.5448982119560242,
      "learning_rate": 3.907084021256239e-06,
      "loss": 1.0605,
      "step": 7676
    },
    {
      "epoch": 71.754494547598,
      "grad_norm": 0.5413927435874939,
      "learning_rate": 3.9046841519349455e-06,
      "loss": 1.0528,
      "step": 7677
    },
    {
      "epoch": 71.76392572944297,
      "grad_norm": 0.5745046734809875,
      "learning_rate": 3.902284841064473e-06,
      "loss": 1.0499,
      "step": 7678
    },
    {
      "epoch": 71.77335691128795,
      "grad_norm": 0.5418760180473328,
      "learning_rate": 3.899886088864644e-06,
      "loss": 1.0374,
      "step": 7679
    },
    {
      "epoch": 71.78278809313292,
      "grad_norm": 0.5282325148582458,
      "learning_rate": 3.897487895555228e-06,
      "loss": 1.0704,
      "step": 7680
    },
    {
      "epoch": 71.7922192749779,
      "grad_norm": 0.5293101668357849,
      "learning_rate": 3.895090261355954e-06,
      "loss": 1.0492,
      "step": 7681
    },
    {
      "epoch": 71.80165045682287,
      "grad_norm": 0.5880104899406433,
      "learning_rate": 3.8926931864864905e-06,
      "loss": 1.0475,
      "step": 7682
    },
    {
      "epoch": 71.81108163866784,
      "grad_norm": 0.5868656039237976,
      "learning_rate": 3.890296671166455e-06,
      "loss": 1.0652,
      "step": 7683
    },
    {
      "epoch": 71.82051282051282,
      "grad_norm": 0.556210994720459,
      "learning_rate": 3.887900715615415e-06,
      "loss": 1.0456,
      "step": 7684
    },
    {
      "epoch": 71.82994400235779,
      "grad_norm": 0.5497251749038696,
      "learning_rate": 3.885505320052885e-06,
      "loss": 1.0272,
      "step": 7685
    },
    {
      "epoch": 71.83937518420277,
      "grad_norm": 0.5208966135978699,
      "learning_rate": 3.883110484698337e-06,
      "loss": 1.0601,
      "step": 7686
    },
    {
      "epoch": 71.84880636604774,
      "grad_norm": 0.5332595705986023,
      "learning_rate": 3.88071620977118e-06,
      "loss": 1.0434,
      "step": 7687
    },
    {
      "epoch": 71.85823754789271,
      "grad_norm": 0.5635025501251221,
      "learning_rate": 3.878322495490779e-06,
      "loss": 1.0371,
      "step": 7688
    },
    {
      "epoch": 71.8676687297377,
      "grad_norm": 0.5526645183563232,
      "learning_rate": 3.875929342076445e-06,
      "loss": 1.0627,
      "step": 7689
    },
    {
      "epoch": 71.87709991158268,
      "grad_norm": 0.5442962050437927,
      "learning_rate": 3.873536749747434e-06,
      "loss": 1.0633,
      "step": 7690
    },
    {
      "epoch": 71.88653109342765,
      "grad_norm": 0.5395747423171997,
      "learning_rate": 3.87114471872296e-06,
      "loss": 1.0382,
      "step": 7691
    },
    {
      "epoch": 71.89596227527262,
      "grad_norm": 0.558549165725708,
      "learning_rate": 3.868753249222179e-06,
      "loss": 1.0756,
      "step": 7692
    },
    {
      "epoch": 71.9053934571176,
      "grad_norm": 0.5580211877822876,
      "learning_rate": 3.866362341464195e-06,
      "loss": 1.0464,
      "step": 7693
    },
    {
      "epoch": 71.91482463896257,
      "grad_norm": 0.5185554027557373,
      "learning_rate": 3.8639719956680624e-06,
      "loss": 1.0482,
      "step": 7694
    },
    {
      "epoch": 71.92425582080755,
      "grad_norm": 0.5592374205589294,
      "learning_rate": 3.861582212052784e-06,
      "loss": 1.0132,
      "step": 7695
    },
    {
      "epoch": 71.93368700265252,
      "grad_norm": 0.5500648617744446,
      "learning_rate": 3.859192990837309e-06,
      "loss": 1.0515,
      "step": 7696
    },
    {
      "epoch": 71.9431181844975,
      "grad_norm": 0.5378367900848389,
      "learning_rate": 3.856804332240541e-06,
      "loss": 1.0727,
      "step": 7697
    },
    {
      "epoch": 71.95254936634247,
      "grad_norm": 0.5463414192199707,
      "learning_rate": 3.854416236481327e-06,
      "loss": 1.0277,
      "step": 7698
    },
    {
      "epoch": 71.96198054818744,
      "grad_norm": 0.5807560682296753,
      "learning_rate": 3.852028703778463e-06,
      "loss": 1.0381,
      "step": 7699
    },
    {
      "epoch": 71.97141173003241,
      "grad_norm": 0.5708379149436951,
      "learning_rate": 3.849641734350694e-06,
      "loss": 1.0397,
      "step": 7700
    },
    {
      "epoch": 71.98084291187739,
      "grad_norm": 0.5208612084388733,
      "learning_rate": 3.847255328416712e-06,
      "loss": 1.0482,
      "step": 7701
    },
    {
      "epoch": 71.99027409372236,
      "grad_norm": 0.5579177737236023,
      "learning_rate": 3.844869486195162e-06,
      "loss": 1.0265,
      "step": 7702
    },
    {
      "epoch": 71.99970527556735,
      "grad_norm": 0.53152996301651,
      "learning_rate": 3.842484207904631e-06,
      "loss": 1.0872,
      "step": 7703
    },
    {
      "epoch": 72.0,
      "grad_norm": 2.7150626182556152,
      "learning_rate": 3.840099493763655e-06,
      "loss": 0.867,
      "step": 7704
    },
    {
      "epoch": 72.00943118184497,
      "grad_norm": 0.5388163924217224,
      "learning_rate": 3.837715343990727e-06,
      "loss": 1.0472,
      "step": 7705
    },
    {
      "epoch": 72.01886236368995,
      "grad_norm": 0.554924488067627,
      "learning_rate": 3.8353317588042795e-06,
      "loss": 1.0867,
      "step": 7706
    },
    {
      "epoch": 72.02829354553492,
      "grad_norm": 0.512934684753418,
      "learning_rate": 3.832948738422695e-06,
      "loss": 1.0594,
      "step": 7707
    },
    {
      "epoch": 72.0377247273799,
      "grad_norm": 0.5289496779441833,
      "learning_rate": 3.830566283064307e-06,
      "loss": 1.0342,
      "step": 7708
    },
    {
      "epoch": 72.04715590922487,
      "grad_norm": 0.5421996116638184,
      "learning_rate": 3.828184392947391e-06,
      "loss": 1.1048,
      "step": 7709
    },
    {
      "epoch": 72.05658709106984,
      "grad_norm": 0.574943482875824,
      "learning_rate": 3.82580306829018e-06,
      "loss": 1.0493,
      "step": 7710
    },
    {
      "epoch": 72.06601827291482,
      "grad_norm": 0.5760508179664612,
      "learning_rate": 3.823422309310843e-06,
      "loss": 1.0298,
      "step": 7711
    },
    {
      "epoch": 72.0754494547598,
      "grad_norm": 0.5582602620124817,
      "learning_rate": 3.821042116227513e-06,
      "loss": 1.025,
      "step": 7712
    },
    {
      "epoch": 72.08488063660478,
      "grad_norm": 0.5773264765739441,
      "learning_rate": 3.8186624892582605e-06,
      "loss": 1.0366,
      "step": 7713
    },
    {
      "epoch": 72.09431181844975,
      "grad_norm": 0.5743717551231384,
      "learning_rate": 3.816283428621103e-06,
      "loss": 1.033,
      "step": 7714
    },
    {
      "epoch": 72.10374300029473,
      "grad_norm": 0.5376135110855103,
      "learning_rate": 3.8139049345340097e-06,
      "loss": 1.0403,
      "step": 7715
    },
    {
      "epoch": 72.1131741821397,
      "grad_norm": 0.5176612138748169,
      "learning_rate": 3.811527007214899e-06,
      "loss": 1.0294,
      "step": 7716
    },
    {
      "epoch": 72.12260536398468,
      "grad_norm": 0.5817627310752869,
      "learning_rate": 3.809149646881636e-06,
      "loss": 1.0164,
      "step": 7717
    },
    {
      "epoch": 72.13203654582965,
      "grad_norm": 0.5566990375518799,
      "learning_rate": 3.806772853752028e-06,
      "loss": 1.0879,
      "step": 7718
    },
    {
      "epoch": 72.14146772767462,
      "grad_norm": 0.5706802010536194,
      "learning_rate": 3.804396628043845e-06,
      "loss": 1.0776,
      "step": 7719
    },
    {
      "epoch": 72.1508989095196,
      "grad_norm": 0.5276045203208923,
      "learning_rate": 3.802020969974792e-06,
      "loss": 1.0345,
      "step": 7720
    },
    {
      "epoch": 72.16033009136457,
      "grad_norm": 0.5248873829841614,
      "learning_rate": 3.799645879762526e-06,
      "loss": 1.0684,
      "step": 7721
    },
    {
      "epoch": 72.16976127320955,
      "grad_norm": 0.578755259513855,
      "learning_rate": 3.797271357624651e-06,
      "loss": 1.0607,
      "step": 7722
    },
    {
      "epoch": 72.17919245505452,
      "grad_norm": 0.567238986492157,
      "learning_rate": 3.7948974037787213e-06,
      "loss": 1.0382,
      "step": 7723
    },
    {
      "epoch": 72.1886236368995,
      "grad_norm": 0.5974987149238586,
      "learning_rate": 3.7925240184422374e-06,
      "loss": 1.0033,
      "step": 7724
    },
    {
      "epoch": 72.19805481874447,
      "grad_norm": 0.5450833439826965,
      "learning_rate": 3.790151201832647e-06,
      "loss": 1.0708,
      "step": 7725
    },
    {
      "epoch": 72.20748600058945,
      "grad_norm": 0.5197253823280334,
      "learning_rate": 3.787778954167345e-06,
      "loss": 1.0436,
      "step": 7726
    },
    {
      "epoch": 72.21691718243443,
      "grad_norm": 0.5597102046012878,
      "learning_rate": 3.7854072756636817e-06,
      "loss": 1.0252,
      "step": 7727
    },
    {
      "epoch": 72.2263483642794,
      "grad_norm": 0.5304985046386719,
      "learning_rate": 3.783036166538947e-06,
      "loss": 1.0751,
      "step": 7728
    },
    {
      "epoch": 72.23577954612438,
      "grad_norm": 0.5709590911865234,
      "learning_rate": 3.7806656270103804e-06,
      "loss": 1.0435,
      "step": 7729
    },
    {
      "epoch": 72.24521072796935,
      "grad_norm": 0.5232326984405518,
      "learning_rate": 3.77829565729517e-06,
      "loss": 1.0748,
      "step": 7730
    },
    {
      "epoch": 72.25464190981432,
      "grad_norm": 0.5494133234024048,
      "learning_rate": 3.7759262576104515e-06,
      "loss": 1.0582,
      "step": 7731
    },
    {
      "epoch": 72.2640730916593,
      "grad_norm": 0.5692492723464966,
      "learning_rate": 3.7735574281733088e-06,
      "loss": 1.0212,
      "step": 7732
    },
    {
      "epoch": 72.27350427350427,
      "grad_norm": 0.5726298689842224,
      "learning_rate": 3.7711891692007697e-06,
      "loss": 1.0343,
      "step": 7733
    },
    {
      "epoch": 72.28293545534925,
      "grad_norm": 0.5413699746131897,
      "learning_rate": 3.76882148090982e-06,
      "loss": 1.0291,
      "step": 7734
    },
    {
      "epoch": 72.29236663719422,
      "grad_norm": 0.5462958216667175,
      "learning_rate": 3.7664543635173847e-06,
      "loss": 1.0922,
      "step": 7735
    },
    {
      "epoch": 72.3017978190392,
      "grad_norm": 0.5369194746017456,
      "learning_rate": 3.764087817240336e-06,
      "loss": 1.0546,
      "step": 7736
    },
    {
      "epoch": 72.31122900088417,
      "grad_norm": 0.5961876511573792,
      "learning_rate": 3.761721842295497e-06,
      "loss": 0.9949,
      "step": 7737
    },
    {
      "epoch": 72.32066018272914,
      "grad_norm": 0.5081350207328796,
      "learning_rate": 3.759356438899637e-06,
      "loss": 1.0526,
      "step": 7738
    },
    {
      "epoch": 72.33009136457412,
      "grad_norm": 0.5664035081863403,
      "learning_rate": 3.756991607269476e-06,
      "loss": 1.0353,
      "step": 7739
    },
    {
      "epoch": 72.3395225464191,
      "grad_norm": 0.5643385648727417,
      "learning_rate": 3.7546273476216754e-06,
      "loss": 1.0504,
      "step": 7740
    },
    {
      "epoch": 72.34895372826408,
      "grad_norm": 0.5417006611824036,
      "learning_rate": 3.752263660172846e-06,
      "loss": 1.0232,
      "step": 7741
    },
    {
      "epoch": 72.35838491010905,
      "grad_norm": 0.5309303998947144,
      "learning_rate": 3.7499005451395564e-06,
      "loss": 1.0623,
      "step": 7742
    },
    {
      "epoch": 72.36781609195403,
      "grad_norm": 0.5030819773674011,
      "learning_rate": 3.747538002738309e-06,
      "loss": 1.0716,
      "step": 7743
    },
    {
      "epoch": 72.377247273799,
      "grad_norm": 0.5441704392433167,
      "learning_rate": 3.7451760331855603e-06,
      "loss": 1.0874,
      "step": 7744
    },
    {
      "epoch": 72.38667845564397,
      "grad_norm": 0.5255193710327148,
      "learning_rate": 3.7428146366977113e-06,
      "loss": 1.0726,
      "step": 7745
    },
    {
      "epoch": 72.39610963748895,
      "grad_norm": 0.5508472323417664,
      "learning_rate": 3.740453813491112e-06,
      "loss": 1.0744,
      "step": 7746
    },
    {
      "epoch": 72.40554081933392,
      "grad_norm": 0.5116961598396301,
      "learning_rate": 3.738093563782065e-06,
      "loss": 1.0405,
      "step": 7747
    },
    {
      "epoch": 72.4149720011789,
      "grad_norm": 0.5303345918655396,
      "learning_rate": 3.735733887786812e-06,
      "loss": 1.0614,
      "step": 7748
    },
    {
      "epoch": 72.42440318302387,
      "grad_norm": 0.5596044659614563,
      "learning_rate": 3.7333747857215476e-06,
      "loss": 1.0568,
      "step": 7749
    },
    {
      "epoch": 72.43383436486884,
      "grad_norm": 0.5645508170127869,
      "learning_rate": 3.73101625780241e-06,
      "loss": 1.0529,
      "step": 7750
    },
    {
      "epoch": 72.44326554671382,
      "grad_norm": 0.5639617443084717,
      "learning_rate": 3.7286583042454836e-06,
      "loss": 1.0324,
      "step": 7751
    },
    {
      "epoch": 72.45269672855879,
      "grad_norm": 0.516158401966095,
      "learning_rate": 3.7263009252668112e-06,
      "loss": 1.053,
      "step": 7752
    },
    {
      "epoch": 72.46212791040377,
      "grad_norm": 0.554945707321167,
      "learning_rate": 3.7239441210823713e-06,
      "loss": 1.1018,
      "step": 7753
    },
    {
      "epoch": 72.47155909224875,
      "grad_norm": 0.5201148986816406,
      "learning_rate": 3.721587891908093e-06,
      "loss": 1.0614,
      "step": 7754
    },
    {
      "epoch": 72.48099027409373,
      "grad_norm": 0.5638073086738586,
      "learning_rate": 3.7192322379598533e-06,
      "loss": 1.0429,
      "step": 7755
    },
    {
      "epoch": 72.4904214559387,
      "grad_norm": 0.5305178761482239,
      "learning_rate": 3.7168771594534737e-06,
      "loss": 1.0659,
      "step": 7756
    },
    {
      "epoch": 72.49985263778368,
      "grad_norm": 0.5866076946258545,
      "learning_rate": 3.714522656604732e-06,
      "loss": 1.0215,
      "step": 7757
    },
    {
      "epoch": 72.50928381962865,
      "grad_norm": 0.5378051996231079,
      "learning_rate": 3.712168729629344e-06,
      "loss": 1.0367,
      "step": 7758
    },
    {
      "epoch": 72.51871500147362,
      "grad_norm": 0.5535235404968262,
      "learning_rate": 3.7098153787429747e-06,
      "loss": 1.0598,
      "step": 7759
    },
    {
      "epoch": 72.5281461833186,
      "grad_norm": 0.5483608245849609,
      "learning_rate": 3.707462604161238e-06,
      "loss": 1.0454,
      "step": 7760
    },
    {
      "epoch": 72.53757736516357,
      "grad_norm": 0.5414489507675171,
      "learning_rate": 3.7051104060996947e-06,
      "loss": 1.0353,
      "step": 7761
    },
    {
      "epoch": 72.54700854700855,
      "grad_norm": 0.5290088653564453,
      "learning_rate": 3.702758784773852e-06,
      "loss": 1.0602,
      "step": 7762
    },
    {
      "epoch": 72.55643972885352,
      "grad_norm": 0.5733159780502319,
      "learning_rate": 3.700407740399161e-06,
      "loss": 1.0386,
      "step": 7763
    },
    {
      "epoch": 72.5658709106985,
      "grad_norm": 0.5500713586807251,
      "learning_rate": 3.69805727319103e-06,
      "loss": 1.0143,
      "step": 7764
    },
    {
      "epoch": 72.57530209254347,
      "grad_norm": 0.5328813195228577,
      "learning_rate": 3.695707383364807e-06,
      "loss": 1.0649,
      "step": 7765
    },
    {
      "epoch": 72.58473327438844,
      "grad_norm": 0.521041214466095,
      "learning_rate": 3.6933580711357854e-06,
      "loss": 1.0571,
      "step": 7766
    },
    {
      "epoch": 72.59416445623341,
      "grad_norm": 0.5478102564811707,
      "learning_rate": 3.691009336719209e-06,
      "loss": 1.0927,
      "step": 7767
    },
    {
      "epoch": 72.6035956380784,
      "grad_norm": 0.6014443635940552,
      "learning_rate": 3.6886611803302697e-06,
      "loss": 1.0341,
      "step": 7768
    },
    {
      "epoch": 72.61302681992338,
      "grad_norm": 0.5308398008346558,
      "learning_rate": 3.686313602184103e-06,
      "loss": 1.053,
      "step": 7769
    },
    {
      "epoch": 72.62245800176835,
      "grad_norm": 0.5486584305763245,
      "learning_rate": 3.683966602495794e-06,
      "loss": 1.0595,
      "step": 7770
    },
    {
      "epoch": 72.63188918361332,
      "grad_norm": 0.5232503414154053,
      "learning_rate": 3.681620181480371e-06,
      "loss": 1.0424,
      "step": 7771
    },
    {
      "epoch": 72.6413203654583,
      "grad_norm": 0.5981168746948242,
      "learning_rate": 3.6792743393528174e-06,
      "loss": 1.0258,
      "step": 7772
    },
    {
      "epoch": 72.65075154730327,
      "grad_norm": 0.5365103483200073,
      "learning_rate": 3.6769290763280575e-06,
      "loss": 1.0342,
      "step": 7773
    },
    {
      "epoch": 72.66018272914825,
      "grad_norm": 0.5439714193344116,
      "learning_rate": 3.6745843926209625e-06,
      "loss": 1.0079,
      "step": 7774
    },
    {
      "epoch": 72.66961391099322,
      "grad_norm": 0.5187471508979797,
      "learning_rate": 3.6722402884463506e-06,
      "loss": 1.0194,
      "step": 7775
    },
    {
      "epoch": 72.6790450928382,
      "grad_norm": 0.4896129369735718,
      "learning_rate": 3.6698967640189887e-06,
      "loss": 1.1002,
      "step": 7776
    },
    {
      "epoch": 72.68847627468317,
      "grad_norm": 0.5225029587745667,
      "learning_rate": 3.66755381955359e-06,
      "loss": 1.0698,
      "step": 7777
    },
    {
      "epoch": 72.69790745652814,
      "grad_norm": 0.5441282391548157,
      "learning_rate": 3.66521145526481e-06,
      "loss": 1.0643,
      "step": 7778
    },
    {
      "epoch": 72.70733863837312,
      "grad_norm": 0.555283784866333,
      "learning_rate": 3.662869671367263e-06,
      "loss": 1.0543,
      "step": 7779
    },
    {
      "epoch": 72.71676982021809,
      "grad_norm": 0.5580742359161377,
      "learning_rate": 3.660528468075498e-06,
      "loss": 1.068,
      "step": 7780
    },
    {
      "epoch": 72.72620100206306,
      "grad_norm": 0.5883610248565674,
      "learning_rate": 3.6581878456040177e-06,
      "loss": 1.0328,
      "step": 7781
    },
    {
      "epoch": 72.73563218390805,
      "grad_norm": 0.5372263193130493,
      "learning_rate": 3.655847804167266e-06,
      "loss": 1.0382,
      "step": 7782
    },
    {
      "epoch": 72.74506336575303,
      "grad_norm": 0.5669247508049011,
      "learning_rate": 3.6535083439796395e-06,
      "loss": 1.0225,
      "step": 7783
    },
    {
      "epoch": 72.754494547598,
      "grad_norm": 0.5763365626335144,
      "learning_rate": 3.6511694652554765e-06,
      "loss": 1.062,
      "step": 7784
    },
    {
      "epoch": 72.76392572944297,
      "grad_norm": 0.5170996189117432,
      "learning_rate": 3.6488311682090648e-06,
      "loss": 1.0666,
      "step": 7785
    },
    {
      "epoch": 72.77335691128795,
      "grad_norm": 0.5565894842147827,
      "learning_rate": 3.6464934530546358e-06,
      "loss": 1.0664,
      "step": 7786
    },
    {
      "epoch": 72.78278809313292,
      "grad_norm": 0.5360977053642273,
      "learning_rate": 3.644156320006378e-06,
      "loss": 1.0454,
      "step": 7787
    },
    {
      "epoch": 72.7922192749779,
      "grad_norm": 0.4833981990814209,
      "learning_rate": 3.641819769278413e-06,
      "loss": 1.0657,
      "step": 7788
    },
    {
      "epoch": 72.80165045682287,
      "grad_norm": 0.5818271040916443,
      "learning_rate": 3.6394838010848156e-06,
      "loss": 1.0688,
      "step": 7789
    },
    {
      "epoch": 72.81108163866784,
      "grad_norm": 0.5633574724197388,
      "learning_rate": 3.6371484156396063e-06,
      "loss": 1.0502,
      "step": 7790
    },
    {
      "epoch": 72.82051282051282,
      "grad_norm": 0.5541603565216064,
      "learning_rate": 3.6348136131567537e-06,
      "loss": 1.0142,
      "step": 7791
    },
    {
      "epoch": 72.82994400235779,
      "grad_norm": 0.5573000907897949,
      "learning_rate": 3.63247939385017e-06,
      "loss": 1.0835,
      "step": 7792
    },
    {
      "epoch": 72.83937518420277,
      "grad_norm": 0.5799896121025085,
      "learning_rate": 3.630145757933713e-06,
      "loss": 1.0656,
      "step": 7793
    },
    {
      "epoch": 72.84880636604774,
      "grad_norm": 0.5598301291465759,
      "learning_rate": 3.6278127056211966e-06,
      "loss": 1.0468,
      "step": 7794
    },
    {
      "epoch": 72.85823754789271,
      "grad_norm": 0.519381582736969,
      "learning_rate": 3.6254802371263697e-06,
      "loss": 1.0403,
      "step": 7795
    },
    {
      "epoch": 72.8676687297377,
      "grad_norm": 0.5208426117897034,
      "learning_rate": 3.623148352662934e-06,
      "loss": 1.0542,
      "step": 7796
    },
    {
      "epoch": 72.87709991158268,
      "grad_norm": 0.542084276676178,
      "learning_rate": 3.6208170524445353e-06,
      "loss": 1.0401,
      "step": 7797
    },
    {
      "epoch": 72.88653109342765,
      "grad_norm": 0.5606867671012878,
      "learning_rate": 3.6184863366847657e-06,
      "loss": 1.03,
      "step": 7798
    },
    {
      "epoch": 72.89596227527262,
      "grad_norm": 0.5476840734481812,
      "learning_rate": 3.6161562055971655e-06,
      "loss": 1.0706,
      "step": 7799
    },
    {
      "epoch": 72.9053934571176,
      "grad_norm": 0.5463279485702515,
      "learning_rate": 3.61382665939522e-06,
      "loss": 1.0388,
      "step": 7800
    },
    {
      "epoch": 72.91482463896257,
      "grad_norm": 0.5364047884941101,
      "learning_rate": 3.611497698292359e-06,
      "loss": 1.0686,
      "step": 7801
    },
    {
      "epoch": 72.92425582080755,
      "grad_norm": 0.5708563923835754,
      "learning_rate": 3.6091693225019665e-06,
      "loss": 1.0675,
      "step": 7802
    },
    {
      "epoch": 72.93368700265252,
      "grad_norm": 0.5436078906059265,
      "learning_rate": 3.6068415322373664e-06,
      "loss": 1.0557,
      "step": 7803
    },
    {
      "epoch": 72.9431181844975,
      "grad_norm": 0.5673224925994873,
      "learning_rate": 3.6045143277118265e-06,
      "loss": 1.0275,
      "step": 7804
    },
    {
      "epoch": 72.95254936634247,
      "grad_norm": 0.568467378616333,
      "learning_rate": 3.602187709138568e-06,
      "loss": 1.0489,
      "step": 7805
    },
    {
      "epoch": 72.96198054818744,
      "grad_norm": 0.5230621695518494,
      "learning_rate": 3.599861676730754e-06,
      "loss": 1.0488,
      "step": 7806
    },
    {
      "epoch": 72.97141173003241,
      "grad_norm": 0.5504446625709534,
      "learning_rate": 3.597536230701494e-06,
      "loss": 1.0651,
      "step": 7807
    },
    {
      "epoch": 72.98084291187739,
      "grad_norm": 0.5310227274894714,
      "learning_rate": 3.5952113712638403e-06,
      "loss": 1.0318,
      "step": 7808
    },
    {
      "epoch": 72.99027409372236,
      "grad_norm": 0.5941973924636841,
      "learning_rate": 3.5928870986308053e-06,
      "loss": 1.0387,
      "step": 7809
    },
    {
      "epoch": 72.99970527556735,
      "grad_norm": 0.5184640884399414,
      "learning_rate": 3.5905634130153323e-06,
      "loss": 1.0769,
      "step": 7810
    },
    {
      "epoch": 73.0,
      "grad_norm": 5.788144111633301,
      "learning_rate": 3.588240314630318e-06,
      "loss": 0.5074,
      "step": 7811
    },
    {
      "epoch": 73.00943118184497,
      "grad_norm": 0.528320848941803,
      "learning_rate": 3.585917803688603e-06,
      "loss": 1.0504,
      "step": 7812
    },
    {
      "epoch": 73.01886236368995,
      "grad_norm": 0.5768554210662842,
      "learning_rate": 3.5835958804029723e-06,
      "loss": 1.0192,
      "step": 7813
    },
    {
      "epoch": 73.02829354553492,
      "grad_norm": 0.5965160131454468,
      "learning_rate": 3.5812745449861673e-06,
      "loss": 1.0263,
      "step": 7814
    },
    {
      "epoch": 73.0377247273799,
      "grad_norm": 0.5196680426597595,
      "learning_rate": 3.5789537976508626e-06,
      "loss": 1.0785,
      "step": 7815
    },
    {
      "epoch": 73.04715590922487,
      "grad_norm": 0.5341524481773376,
      "learning_rate": 3.5766336386096846e-06,
      "loss": 1.0404,
      "step": 7816
    },
    {
      "epoch": 73.05658709106984,
      "grad_norm": 0.5640053749084473,
      "learning_rate": 3.5743140680752064e-06,
      "loss": 1.0562,
      "step": 7817
    },
    {
      "epoch": 73.06601827291482,
      "grad_norm": 0.5672932267189026,
      "learning_rate": 3.571995086259943e-06,
      "loss": 1.0614,
      "step": 7818
    },
    {
      "epoch": 73.0754494547598,
      "grad_norm": 0.5375357866287231,
      "learning_rate": 3.569676693376365e-06,
      "loss": 1.0493,
      "step": 7819
    },
    {
      "epoch": 73.08488063660478,
      "grad_norm": 0.5380333065986633,
      "learning_rate": 3.567358889636879e-06,
      "loss": 1.0318,
      "step": 7820
    },
    {
      "epoch": 73.09431181844975,
      "grad_norm": 0.5668618679046631,
      "learning_rate": 3.565041675253843e-06,
      "loss": 0.9775,
      "step": 7821
    },
    {
      "epoch": 73.10374300029473,
      "grad_norm": 0.5774973630905151,
      "learning_rate": 3.5627250504395582e-06,
      "loss": 1.0453,
      "step": 7822
    },
    {
      "epoch": 73.1131741821397,
      "grad_norm": 0.521544337272644,
      "learning_rate": 3.5604090154062696e-06,
      "loss": 1.0689,
      "step": 7823
    },
    {
      "epoch": 73.12260536398468,
      "grad_norm": 0.5432474613189697,
      "learning_rate": 3.558093570366178e-06,
      "loss": 1.0563,
      "step": 7824
    },
    {
      "epoch": 73.13203654582965,
      "grad_norm": 0.5307376384735107,
      "learning_rate": 3.5557787155314216e-06,
      "loss": 1.0676,
      "step": 7825
    },
    {
      "epoch": 73.14146772767462,
      "grad_norm": 0.5417267680168152,
      "learning_rate": 3.5534644511140846e-06,
      "loss": 1.0389,
      "step": 7826
    },
    {
      "epoch": 73.1508989095196,
      "grad_norm": 0.6099950075149536,
      "learning_rate": 3.551150777326202e-06,
      "loss": 1.0922,
      "step": 7827
    },
    {
      "epoch": 73.16033009136457,
      "grad_norm": 0.5268737077713013,
      "learning_rate": 3.54883769437975e-06,
      "loss": 1.0548,
      "step": 7828
    },
    {
      "epoch": 73.16976127320955,
      "grad_norm": 0.5721543431282043,
      "learning_rate": 3.5465252024866513e-06,
      "loss": 1.0799,
      "step": 7829
    },
    {
      "epoch": 73.17919245505452,
      "grad_norm": 0.5379221439361572,
      "learning_rate": 3.5442133018587755e-06,
      "loss": 0.9933,
      "step": 7830
    },
    {
      "epoch": 73.1886236368995,
      "grad_norm": 0.6067437529563904,
      "learning_rate": 3.5419019927079425e-06,
      "loss": 1.0662,
      "step": 7831
    },
    {
      "epoch": 73.19805481874447,
      "grad_norm": 0.593481183052063,
      "learning_rate": 3.5395912752459104e-06,
      "loss": 1.0167,
      "step": 7832
    },
    {
      "epoch": 73.20748600058945,
      "grad_norm": 0.5344891548156738,
      "learning_rate": 3.5372811496843875e-06,
      "loss": 1.0207,
      "step": 7833
    },
    {
      "epoch": 73.21691718243443,
      "grad_norm": 0.5042771697044373,
      "learning_rate": 3.534971616235026e-06,
      "loss": 1.0686,
      "step": 7834
    },
    {
      "epoch": 73.2263483642794,
      "grad_norm": 0.5858035683631897,
      "learning_rate": 3.532662675109424e-06,
      "loss": 1.0598,
      "step": 7835
    },
    {
      "epoch": 73.23577954612438,
      "grad_norm": 0.5344067811965942,
      "learning_rate": 3.5303543265191277e-06,
      "loss": 1.0384,
      "step": 7836
    },
    {
      "epoch": 73.24521072796935,
      "grad_norm": 0.5403496623039246,
      "learning_rate": 3.528046570675626e-06,
      "loss": 1.0088,
      "step": 7837
    },
    {
      "epoch": 73.25464190981432,
      "grad_norm": 0.5499295592308044,
      "learning_rate": 3.5257394077903507e-06,
      "loss": 1.0398,
      "step": 7838
    },
    {
      "epoch": 73.2640730916593,
      "grad_norm": 0.5383797883987427,
      "learning_rate": 3.5234328380746917e-06,
      "loss": 1.0409,
      "step": 7839
    },
    {
      "epoch": 73.27350427350427,
      "grad_norm": 0.5566563010215759,
      "learning_rate": 3.5211268617399718e-06,
      "loss": 1.0701,
      "step": 7840
    },
    {
      "epoch": 73.28293545534925,
      "grad_norm": 0.531796932220459,
      "learning_rate": 3.518821478997464e-06,
      "loss": 1.0585,
      "step": 7841
    },
    {
      "epoch": 73.29236663719422,
      "grad_norm": 0.5553427338600159,
      "learning_rate": 3.5165166900583867e-06,
      "loss": 1.0323,
      "step": 7842
    },
    {
      "epoch": 73.3017978190392,
      "grad_norm": 0.5674642324447632,
      "learning_rate": 3.514212495133903e-06,
      "loss": 1.0961,
      "step": 7843
    },
    {
      "epoch": 73.31122900088417,
      "grad_norm": 0.5547541975975037,
      "learning_rate": 3.5119088944351243e-06,
      "loss": 1.0459,
      "step": 7844
    },
    {
      "epoch": 73.32066018272914,
      "grad_norm": 0.5290971994400024,
      "learning_rate": 3.509605888173101e-06,
      "loss": 1.036,
      "step": 7845
    },
    {
      "epoch": 73.33009136457412,
      "grad_norm": 0.5536751747131348,
      "learning_rate": 3.5073034765588408e-06,
      "loss": 1.0717,
      "step": 7846
    },
    {
      "epoch": 73.3395225464191,
      "grad_norm": 0.5413821339607239,
      "learning_rate": 3.5050016598032876e-06,
      "loss": 1.0546,
      "step": 7847
    },
    {
      "epoch": 73.34895372826408,
      "grad_norm": 0.532137393951416,
      "learning_rate": 3.5027004381173315e-06,
      "loss": 1.0707,
      "step": 7848
    },
    {
      "epoch": 73.35838491010905,
      "grad_norm": 0.5998331904411316,
      "learning_rate": 3.50039981171181e-06,
      "loss": 1.0016,
      "step": 7849
    },
    {
      "epoch": 73.36781609195403,
      "grad_norm": 0.5734037756919861,
      "learning_rate": 3.4980997807975057e-06,
      "loss": 1.1089,
      "step": 7850
    },
    {
      "epoch": 73.377247273799,
      "grad_norm": 0.5382786393165588,
      "learning_rate": 3.4958003455851477e-06,
      "loss": 1.024,
      "step": 7851
    },
    {
      "epoch": 73.38667845564397,
      "grad_norm": 0.5205650329589844,
      "learning_rate": 3.4935015062854084e-06,
      "loss": 1.0548,
      "step": 7852
    },
    {
      "epoch": 73.39610963748895,
      "grad_norm": 0.579163134098053,
      "learning_rate": 3.4912032631089032e-06,
      "loss": 1.0489,
      "step": 7853
    },
    {
      "epoch": 73.40554081933392,
      "grad_norm": 0.5280090570449829,
      "learning_rate": 3.488905616266205e-06,
      "loss": 1.0585,
      "step": 7854
    },
    {
      "epoch": 73.4149720011789,
      "grad_norm": 0.5548548102378845,
      "learning_rate": 3.486608565967817e-06,
      "loss": 1.0442,
      "step": 7855
    },
    {
      "epoch": 73.42440318302387,
      "grad_norm": 0.5308566093444824,
      "learning_rate": 3.4843121124241976e-06,
      "loss": 1.0435,
      "step": 7856
    },
    {
      "epoch": 73.43383436486884,
      "grad_norm": 0.5254812240600586,
      "learning_rate": 3.4820162558457447e-06,
      "loss": 1.0333,
      "step": 7857
    },
    {
      "epoch": 73.44326554671382,
      "grad_norm": 0.5138787031173706,
      "learning_rate": 3.4797209964428045e-06,
      "loss": 1.0518,
      "step": 7858
    },
    {
      "epoch": 73.45269672855879,
      "grad_norm": 0.5343544483184814,
      "learning_rate": 3.4774263344256675e-06,
      "loss": 1.0663,
      "step": 7859
    },
    {
      "epoch": 73.46212791040377,
      "grad_norm": 0.5804070234298706,
      "learning_rate": 3.4751322700045676e-06,
      "loss": 1.0587,
      "step": 7860
    },
    {
      "epoch": 73.47155909224875,
      "grad_norm": 0.5928567051887512,
      "learning_rate": 3.472838803389693e-06,
      "loss": 1.0556,
      "step": 7861
    },
    {
      "epoch": 73.48099027409373,
      "grad_norm": 0.5808968544006348,
      "learning_rate": 3.4705459347911652e-06,
      "loss": 1.0232,
      "step": 7862
    },
    {
      "epoch": 73.4904214559387,
      "grad_norm": 0.5557582974433899,
      "learning_rate": 3.468253664419058e-06,
      "loss": 1.0602,
      "step": 7863
    },
    {
      "epoch": 73.49985263778368,
      "grad_norm": 0.5773324966430664,
      "learning_rate": 3.4659619924833874e-06,
      "loss": 1.062,
      "step": 7864
    },
    {
      "epoch": 73.50928381962865,
      "grad_norm": 0.5235994458198547,
      "learning_rate": 3.4636709191941154e-06,
      "loss": 1.0533,
      "step": 7865
    },
    {
      "epoch": 73.51871500147362,
      "grad_norm": 0.6148281097412109,
      "learning_rate": 3.461380444761151e-06,
      "loss": 1.0228,
      "step": 7866
    },
    {
      "epoch": 73.5281461833186,
      "grad_norm": 0.5909032225608826,
      "learning_rate": 3.4590905693943443e-06,
      "loss": 1.057,
      "step": 7867
    },
    {
      "epoch": 73.53757736516357,
      "grad_norm": 0.5210009813308716,
      "learning_rate": 3.4568012933034923e-06,
      "loss": 1.0619,
      "step": 7868
    },
    {
      "epoch": 73.54700854700855,
      "grad_norm": 0.49765312671661377,
      "learning_rate": 3.4545126166983424e-06,
      "loss": 1.0533,
      "step": 7869
    },
    {
      "epoch": 73.55643972885352,
      "grad_norm": 0.5383503437042236,
      "learning_rate": 3.452224539788579e-06,
      "loss": 1.0565,
      "step": 7870
    },
    {
      "epoch": 73.5658709106985,
      "grad_norm": 0.5044674873352051,
      "learning_rate": 3.4499370627838356e-06,
      "loss": 1.0365,
      "step": 7871
    },
    {
      "epoch": 73.57530209254347,
      "grad_norm": 0.5091300010681152,
      "learning_rate": 3.447650185893692e-06,
      "loss": 1.0525,
      "step": 7872
    },
    {
      "epoch": 73.58473327438844,
      "grad_norm": 0.5152440071105957,
      "learning_rate": 3.445363909327667e-06,
      "loss": 1.0286,
      "step": 7873
    },
    {
      "epoch": 73.59416445623341,
      "grad_norm": 0.5488507151603699,
      "learning_rate": 3.4430782332952285e-06,
      "loss": 1.0432,
      "step": 7874
    },
    {
      "epoch": 73.6035956380784,
      "grad_norm": 0.5565236806869507,
      "learning_rate": 3.4407931580057942e-06,
      "loss": 1.0612,
      "step": 7875
    },
    {
      "epoch": 73.61302681992338,
      "grad_norm": 0.5206196308135986,
      "learning_rate": 3.4385086836687186e-06,
      "loss": 1.0412,
      "step": 7876
    },
    {
      "epoch": 73.62245800176835,
      "grad_norm": 0.5477989315986633,
      "learning_rate": 3.436224810493307e-06,
      "loss": 1.0373,
      "step": 7877
    },
    {
      "epoch": 73.63188918361332,
      "grad_norm": 0.5838768482208252,
      "learning_rate": 3.433941538688804e-06,
      "loss": 1.0788,
      "step": 7878
    },
    {
      "epoch": 73.6413203654583,
      "grad_norm": 0.5459206104278564,
      "learning_rate": 3.4316588684644e-06,
      "loss": 1.0459,
      "step": 7879
    },
    {
      "epoch": 73.65075154730327,
      "grad_norm": 0.567746639251709,
      "learning_rate": 3.4293768000292403e-06,
      "loss": 1.0106,
      "step": 7880
    },
    {
      "epoch": 73.66018272914825,
      "grad_norm": 0.5276201963424683,
      "learning_rate": 3.427095333592403e-06,
      "loss": 1.0333,
      "step": 7881
    },
    {
      "epoch": 73.66961391099322,
      "grad_norm": 0.5251650810241699,
      "learning_rate": 3.424814469362914e-06,
      "loss": 1.0319,
      "step": 7882
    },
    {
      "epoch": 73.6790450928382,
      "grad_norm": 0.5390521287918091,
      "learning_rate": 3.4225342075497468e-06,
      "loss": 1.0702,
      "step": 7883
    },
    {
      "epoch": 73.68847627468317,
      "grad_norm": 0.5622903108596802,
      "learning_rate": 3.4202545483618156e-06,
      "loss": 1.0329,
      "step": 7884
    },
    {
      "epoch": 73.69790745652814,
      "grad_norm": 0.5557287931442261,
      "learning_rate": 3.4179754920079877e-06,
      "loss": 1.0884,
      "step": 7885
    },
    {
      "epoch": 73.70733863837312,
      "grad_norm": 0.5424978137016296,
      "learning_rate": 3.4156970386970656e-06,
      "loss": 1.0243,
      "step": 7886
    },
    {
      "epoch": 73.71676982021809,
      "grad_norm": 0.5549862384796143,
      "learning_rate": 3.413419188637801e-06,
      "loss": 1.0164,
      "step": 7887
    },
    {
      "epoch": 73.72620100206306,
      "grad_norm": 0.5235329866409302,
      "learning_rate": 3.4111419420388904e-06,
      "loss": 1.0878,
      "step": 7888
    },
    {
      "epoch": 73.73563218390805,
      "grad_norm": 0.5093603134155273,
      "learning_rate": 3.4088652991089734e-06,
      "loss": 1.0536,
      "step": 7889
    },
    {
      "epoch": 73.74506336575303,
      "grad_norm": 0.5614303350448608,
      "learning_rate": 3.406589260056632e-06,
      "loss": 1.0691,
      "step": 7890
    },
    {
      "epoch": 73.754494547598,
      "grad_norm": 0.5536660552024841,
      "learning_rate": 3.404313825090403e-06,
      "loss": 1.0782,
      "step": 7891
    },
    {
      "epoch": 73.76392572944297,
      "grad_norm": 0.546936571598053,
      "learning_rate": 3.4020389944187583e-06,
      "loss": 1.0965,
      "step": 7892
    },
    {
      "epoch": 73.77335691128795,
      "grad_norm": 0.5199140906333923,
      "learning_rate": 3.3997647682501167e-06,
      "loss": 1.0293,
      "step": 7893
    },
    {
      "epoch": 73.78278809313292,
      "grad_norm": 0.5643855929374695,
      "learning_rate": 3.397491146792842e-06,
      "loss": 1.0673,
      "step": 7894
    },
    {
      "epoch": 73.7922192749779,
      "grad_norm": 0.5296353101730347,
      "learning_rate": 3.3952181302552424e-06,
      "loss": 1.0399,
      "step": 7895
    },
    {
      "epoch": 73.80165045682287,
      "grad_norm": 0.5827730894088745,
      "learning_rate": 3.3929457188455707e-06,
      "loss": 1.0593,
      "step": 7896
    },
    {
      "epoch": 73.81108163866784,
      "grad_norm": 0.5215872526168823,
      "learning_rate": 3.3906739127720254e-06,
      "loss": 1.074,
      "step": 7897
    },
    {
      "epoch": 73.82051282051282,
      "grad_norm": 0.550588071346283,
      "learning_rate": 3.388402712242744e-06,
      "loss": 1.074,
      "step": 7898
    },
    {
      "epoch": 73.82994400235779,
      "grad_norm": 0.512017011642456,
      "learning_rate": 3.3861321174658224e-06,
      "loss": 1.0163,
      "step": 7899
    },
    {
      "epoch": 73.83937518420277,
      "grad_norm": 0.5199655294418335,
      "learning_rate": 3.3838621286492857e-06,
      "loss": 1.072,
      "step": 7900
    },
    {
      "epoch": 73.84880636604774,
      "grad_norm": 0.5184376239776611,
      "learning_rate": 3.38159274600111e-06,
      "loss": 1.0684,
      "step": 7901
    },
    {
      "epoch": 73.85823754789271,
      "grad_norm": 0.5348972082138062,
      "learning_rate": 3.379323969729217e-06,
      "loss": 1.0349,
      "step": 7902
    },
    {
      "epoch": 73.8676687297377,
      "grad_norm": 0.5383873581886292,
      "learning_rate": 3.3770558000414686e-06,
      "loss": 1.0382,
      "step": 7903
    },
    {
      "epoch": 73.87709991158268,
      "grad_norm": 0.599099338054657,
      "learning_rate": 3.3747882371456764e-06,
      "loss": 1.0266,
      "step": 7904
    },
    {
      "epoch": 73.88653109342765,
      "grad_norm": 0.560120165348053,
      "learning_rate": 3.3725212812495888e-06,
      "loss": 1.0394,
      "step": 7905
    },
    {
      "epoch": 73.89596227527262,
      "grad_norm": 0.5430765151977539,
      "learning_rate": 3.37025493256091e-06,
      "loss": 1.0713,
      "step": 7906
    },
    {
      "epoch": 73.9053934571176,
      "grad_norm": 0.5122547745704651,
      "learning_rate": 3.367989191287281e-06,
      "loss": 1.066,
      "step": 7907
    },
    {
      "epoch": 73.91482463896257,
      "grad_norm": 0.5794170498847961,
      "learning_rate": 3.365724057636285e-06,
      "loss": 1.0235,
      "step": 7908
    },
    {
      "epoch": 73.92425582080755,
      "grad_norm": 0.5830096006393433,
      "learning_rate": 3.363459531815455e-06,
      "loss": 1.0784,
      "step": 7909
    },
    {
      "epoch": 73.93368700265252,
      "grad_norm": 0.5452927947044373,
      "learning_rate": 3.3611956140322665e-06,
      "loss": 1.0429,
      "step": 7910
    },
    {
      "epoch": 73.9431181844975,
      "grad_norm": 0.5355130434036255,
      "learning_rate": 3.358932304494138e-06,
      "loss": 0.9945,
      "step": 7911
    },
    {
      "epoch": 73.95254936634247,
      "grad_norm": 0.5282901525497437,
      "learning_rate": 3.3566696034084324e-06,
      "loss": 1.0957,
      "step": 7912
    },
    {
      "epoch": 73.96198054818744,
      "grad_norm": 0.5427165031433105,
      "learning_rate": 3.354407510982455e-06,
      "loss": 1.082,
      "step": 7913
    },
    {
      "epoch": 73.97141173003241,
      "grad_norm": 0.5633653998374939,
      "learning_rate": 3.352146027423465e-06,
      "loss": 1.0559,
      "step": 7914
    },
    {
      "epoch": 73.98084291187739,
      "grad_norm": 0.5567128658294678,
      "learning_rate": 3.3498851529386555e-06,
      "loss": 1.0483,
      "step": 7915
    },
    {
      "epoch": 73.99027409372236,
      "grad_norm": 0.5617225766181946,
      "learning_rate": 3.347624887735167e-06,
      "loss": 1.0634,
      "step": 7916
    },
    {
      "epoch": 73.99970527556735,
      "grad_norm": 0.5294977426528931,
      "learning_rate": 3.3453652320200826e-06,
      "loss": 1.0734,
      "step": 7917
    },
    {
      "epoch": 74.0,
      "grad_norm": 2.2830066680908203,
      "learning_rate": 3.3431061860004333e-06,
      "loss": 1.0662,
      "step": 7918
    },
    {
      "epoch": 74.00943118184497,
      "grad_norm": 0.5939463376998901,
      "learning_rate": 3.3408477498831917e-06,
      "loss": 1.0311,
      "step": 7919
    },
    {
      "epoch": 74.01886236368995,
      "grad_norm": 0.5356582403182983,
      "learning_rate": 3.338589923875272e-06,
      "loss": 1.0427,
      "step": 7920
    },
    {
      "epoch": 74.02829354553492,
      "grad_norm": 0.5596248507499695,
      "learning_rate": 3.3363327081835395e-06,
      "loss": 1.0734,
      "step": 7921
    },
    {
      "epoch": 74.0377247273799,
      "grad_norm": 0.5473711490631104,
      "learning_rate": 3.3340761030147996e-06,
      "loss": 1.0565,
      "step": 7922
    },
    {
      "epoch": 74.04715590922487,
      "grad_norm": 0.5279357433319092,
      "learning_rate": 3.331820108575801e-06,
      "loss": 1.0394,
      "step": 7923
    },
    {
      "epoch": 74.05658709106984,
      "grad_norm": 0.5471570491790771,
      "learning_rate": 3.3295647250732354e-06,
      "loss": 1.0722,
      "step": 7924
    },
    {
      "epoch": 74.06601827291482,
      "grad_norm": 0.5822686553001404,
      "learning_rate": 3.327309952713741e-06,
      "loss": 1.0586,
      "step": 7925
    },
    {
      "epoch": 74.0754494547598,
      "grad_norm": 0.5604586005210876,
      "learning_rate": 3.3250557917039004e-06,
      "loss": 1.0335,
      "step": 7926
    },
    {
      "epoch": 74.08488063660478,
      "grad_norm": 0.5469382405281067,
      "learning_rate": 3.322802242250238e-06,
      "loss": 1.0626,
      "step": 7927
    },
    {
      "epoch": 74.09431181844975,
      "grad_norm": 0.5133106112480164,
      "learning_rate": 3.320549304559221e-06,
      "loss": 1.048,
      "step": 7928
    },
    {
      "epoch": 74.10374300029473,
      "grad_norm": 0.5947800278663635,
      "learning_rate": 3.3182969788372687e-06,
      "loss": 1.074,
      "step": 7929
    },
    {
      "epoch": 74.1131741821397,
      "grad_norm": 0.5435068607330322,
      "learning_rate": 3.316045265290735e-06,
      "loss": 1.002,
      "step": 7930
    },
    {
      "epoch": 74.12260536398468,
      "grad_norm": 0.5309661626815796,
      "learning_rate": 3.3137941641259207e-06,
      "loss": 1.0463,
      "step": 7931
    },
    {
      "epoch": 74.13203654582965,
      "grad_norm": 0.5593357086181641,
      "learning_rate": 3.3115436755490714e-06,
      "loss": 1.0895,
      "step": 7932
    },
    {
      "epoch": 74.14146772767462,
      "grad_norm": 0.5125184655189514,
      "learning_rate": 3.3092937997663775e-06,
      "loss": 1.0109,
      "step": 7933
    },
    {
      "epoch": 74.1508989095196,
      "grad_norm": 0.5296892523765564,
      "learning_rate": 3.30704453698397e-06,
      "loss": 1.0201,
      "step": 7934
    },
    {
      "epoch": 74.16033009136457,
      "grad_norm": 0.5493915677070618,
      "learning_rate": 3.304795887407923e-06,
      "loss": 1.0019,
      "step": 7935
    },
    {
      "epoch": 74.16976127320955,
      "grad_norm": 0.5379582047462463,
      "learning_rate": 3.3025478512442633e-06,
      "loss": 1.048,
      "step": 7936
    },
    {
      "epoch": 74.17919245505452,
      "grad_norm": 0.5337520241737366,
      "learning_rate": 3.300300428698953e-06,
      "loss": 1.0813,
      "step": 7937
    },
    {
      "epoch": 74.1886236368995,
      "grad_norm": 0.646035373210907,
      "learning_rate": 3.298053619977899e-06,
      "loss": 1.0068,
      "step": 7938
    },
    {
      "epoch": 74.19805481874447,
      "grad_norm": 0.4964582324028015,
      "learning_rate": 3.295807425286953e-06,
      "loss": 1.0457,
      "step": 7939
    },
    {
      "epoch": 74.20748600058945,
      "grad_norm": 0.5235835909843445,
      "learning_rate": 3.2935618448319128e-06,
      "loss": 1.08,
      "step": 7940
    },
    {
      "epoch": 74.21691718243443,
      "grad_norm": 0.5449045896530151,
      "learning_rate": 3.2913168788185114e-06,
      "loss": 1.0556,
      "step": 7941
    },
    {
      "epoch": 74.2263483642794,
      "grad_norm": 0.543057382106781,
      "learning_rate": 3.2890725274524405e-06,
      "loss": 1.078,
      "step": 7942
    },
    {
      "epoch": 74.23577954612438,
      "grad_norm": 0.5701203942298889,
      "learning_rate": 3.2868287909393228e-06,
      "loss": 1.0472,
      "step": 7943
    },
    {
      "epoch": 74.24521072796935,
      "grad_norm": 0.5399880409240723,
      "learning_rate": 3.284585669484729e-06,
      "loss": 1.0656,
      "step": 7944
    },
    {
      "epoch": 74.25464190981432,
      "grad_norm": 0.5556320548057556,
      "learning_rate": 3.2823431632941727e-06,
      "loss": 1.0552,
      "step": 7945
    },
    {
      "epoch": 74.2640730916593,
      "grad_norm": 0.5513309836387634,
      "learning_rate": 3.2801012725731085e-06,
      "loss": 1.0359,
      "step": 7946
    },
    {
      "epoch": 74.27350427350427,
      "grad_norm": 0.5450735688209534,
      "learning_rate": 3.2778599975269443e-06,
      "loss": 1.0353,
      "step": 7947
    },
    {
      "epoch": 74.28293545534925,
      "grad_norm": 0.5564484596252441,
      "learning_rate": 3.2756193383610225e-06,
      "loss": 1.0245,
      "step": 7948
    },
    {
      "epoch": 74.29236663719422,
      "grad_norm": 0.5699350833892822,
      "learning_rate": 3.27337929528063e-06,
      "loss": 1.0237,
      "step": 7949
    },
    {
      "epoch": 74.3017978190392,
      "grad_norm": 0.5627061128616333,
      "learning_rate": 3.271139868491e-06,
      "loss": 1.0874,
      "step": 7950
    },
    {
      "epoch": 74.31122900088417,
      "grad_norm": 0.5458934903144836,
      "learning_rate": 3.2689010581973036e-06,
      "loss": 1.0409,
      "step": 7951
    },
    {
      "epoch": 74.32066018272914,
      "grad_norm": 0.5007389187812805,
      "learning_rate": 3.2666628646046684e-06,
      "loss": 1.0683,
      "step": 7952
    },
    {
      "epoch": 74.33009136457412,
      "grad_norm": 0.5608682632446289,
      "learning_rate": 3.2644252879181515e-06,
      "loss": 1.074,
      "step": 7953
    },
    {
      "epoch": 74.3395225464191,
      "grad_norm": 0.5191761255264282,
      "learning_rate": 3.2621883283427602e-06,
      "loss": 1.0868,
      "step": 7954
    },
    {
      "epoch": 74.34895372826408,
      "grad_norm": 0.603432297706604,
      "learning_rate": 3.2599519860834427e-06,
      "loss": 1.0377,
      "step": 7955
    },
    {
      "epoch": 74.35838491010905,
      "grad_norm": 0.5183596014976501,
      "learning_rate": 3.257716261345094e-06,
      "loss": 1.0611,
      "step": 7956
    },
    {
      "epoch": 74.36781609195403,
      "grad_norm": 0.5396662950515747,
      "learning_rate": 3.255481154332546e-06,
      "loss": 1.0419,
      "step": 7957
    },
    {
      "epoch": 74.377247273799,
      "grad_norm": 0.527819037437439,
      "learning_rate": 3.2532466652505843e-06,
      "loss": 1.0941,
      "step": 7958
    },
    {
      "epoch": 74.38667845564397,
      "grad_norm": 0.5205494165420532,
      "learning_rate": 3.2510127943039306e-06,
      "loss": 1.0566,
      "step": 7959
    },
    {
      "epoch": 74.39610963748895,
      "grad_norm": 0.5121445059776306,
      "learning_rate": 3.24877954169725e-06,
      "loss": 1.0634,
      "step": 7960
    },
    {
      "epoch": 74.40554081933392,
      "grad_norm": 0.543933093547821,
      "learning_rate": 3.246546907635153e-06,
      "loss": 1.0587,
      "step": 7961
    },
    {
      "epoch": 74.4149720011789,
      "grad_norm": 0.5555201768875122,
      "learning_rate": 3.2443148923221925e-06,
      "loss": 1.0277,
      "step": 7962
    },
    {
      "epoch": 74.42440318302387,
      "grad_norm": 0.5119867920875549,
      "learning_rate": 3.2420834959628645e-06,
      "loss": 1.0434,
      "step": 7963
    },
    {
      "epoch": 74.43383436486884,
      "grad_norm": 0.5834303498268127,
      "learning_rate": 3.239852718761609e-06,
      "loss": 1.0495,
      "step": 7964
    },
    {
      "epoch": 74.44326554671382,
      "grad_norm": 0.5058063864707947,
      "learning_rate": 3.2376225609228075e-06,
      "loss": 1.1079,
      "step": 7965
    },
    {
      "epoch": 74.45269672855879,
      "grad_norm": 0.5754209160804749,
      "learning_rate": 3.23539302265079e-06,
      "loss": 1.038,
      "step": 7966
    },
    {
      "epoch": 74.46212791040377,
      "grad_norm": 0.5158410668373108,
      "learning_rate": 3.2331641041498253e-06,
      "loss": 1.056,
      "step": 7967
    },
    {
      "epoch": 74.47155909224875,
      "grad_norm": 0.5209560394287109,
      "learning_rate": 3.230935805624125e-06,
      "loss": 1.0577,
      "step": 7968
    },
    {
      "epoch": 74.48099027409373,
      "grad_norm": 0.5746512413024902,
      "learning_rate": 3.2287081272778455e-06,
      "loss": 1.0472,
      "step": 7969
    },
    {
      "epoch": 74.4904214559387,
      "grad_norm": 0.5492198467254639,
      "learning_rate": 3.226481069315085e-06,
      "loss": 1.0654,
      "step": 7970
    },
    {
      "epoch": 74.49985263778368,
      "grad_norm": 0.5342824459075928,
      "learning_rate": 3.224254631939887e-06,
      "loss": 1.0426,
      "step": 7971
    },
    {
      "epoch": 74.50928381962865,
      "grad_norm": 0.5038155317306519,
      "learning_rate": 3.222028815356233e-06,
      "loss": 1.078,
      "step": 7972
    },
    {
      "epoch": 74.51871500147362,
      "grad_norm": 0.5566906929016113,
      "learning_rate": 3.2198036197680583e-06,
      "loss": 1.0448,
      "step": 7973
    },
    {
      "epoch": 74.5281461833186,
      "grad_norm": 0.5442190170288086,
      "learning_rate": 3.217579045379231e-06,
      "loss": 1.0458,
      "step": 7974
    },
    {
      "epoch": 74.53757736516357,
      "grad_norm": 0.5447760224342346,
      "learning_rate": 3.2153550923935674e-06,
      "loss": 1.0223,
      "step": 7975
    },
    {
      "epoch": 74.54700854700855,
      "grad_norm": 0.5665221214294434,
      "learning_rate": 3.2131317610148228e-06,
      "loss": 1.0521,
      "step": 7976
    },
    {
      "epoch": 74.55643972885352,
      "grad_norm": 0.5420560240745544,
      "learning_rate": 3.2109090514467e-06,
      "loss": 1.01,
      "step": 7977
    },
    {
      "epoch": 74.5658709106985,
      "grad_norm": 0.5807760953903198,
      "learning_rate": 3.208686963892842e-06,
      "loss": 1.0337,
      "step": 7978
    },
    {
      "epoch": 74.57530209254347,
      "grad_norm": 0.5843930244445801,
      "learning_rate": 3.2064654985568366e-06,
      "loss": 1.0259,
      "step": 7979
    },
    {
      "epoch": 74.58473327438844,
      "grad_norm": 0.5630831718444824,
      "learning_rate": 3.204244655642209e-06,
      "loss": 1.0421,
      "step": 7980
    },
    {
      "epoch": 74.59416445623341,
      "grad_norm": 0.5536830425262451,
      "learning_rate": 3.202024435352441e-06,
      "loss": 1.0486,
      "step": 7981
    },
    {
      "epoch": 74.6035956380784,
      "grad_norm": 0.5334997773170471,
      "learning_rate": 3.1998048378909418e-06,
      "loss": 1.0261,
      "step": 7982
    },
    {
      "epoch": 74.61302681992338,
      "grad_norm": 0.5339928269386292,
      "learning_rate": 3.1975858634610737e-06,
      "loss": 1.0438,
      "step": 7983
    },
    {
      "epoch": 74.62245800176835,
      "grad_norm": 0.534858226776123,
      "learning_rate": 3.1953675122661363e-06,
      "loss": 1.026,
      "step": 7984
    },
    {
      "epoch": 74.63188918361332,
      "grad_norm": 0.5604094862937927,
      "learning_rate": 3.1931497845093753e-06,
      "loss": 1.0671,
      "step": 7985
    },
    {
      "epoch": 74.6413203654583,
      "grad_norm": 0.5446494817733765,
      "learning_rate": 3.190932680393977e-06,
      "loss": 1.0358,
      "step": 7986
    },
    {
      "epoch": 74.65075154730327,
      "grad_norm": 0.5519714951515198,
      "learning_rate": 3.1887162001230688e-06,
      "loss": 1.0659,
      "step": 7987
    },
    {
      "epoch": 74.66018272914825,
      "grad_norm": 0.5538680553436279,
      "learning_rate": 3.18650034389973e-06,
      "loss": 1.0344,
      "step": 7988
    },
    {
      "epoch": 74.66961391099322,
      "grad_norm": 0.599970281124115,
      "learning_rate": 3.184285111926976e-06,
      "loss": 1.0868,
      "step": 7989
    },
    {
      "epoch": 74.6790450928382,
      "grad_norm": 0.5465742945671082,
      "learning_rate": 3.1820705044077614e-06,
      "loss": 1.0264,
      "step": 7990
    },
    {
      "epoch": 74.68847627468317,
      "grad_norm": 0.5813354849815369,
      "learning_rate": 3.1798565215449916e-06,
      "loss": 1.0662,
      "step": 7991
    },
    {
      "epoch": 74.69790745652814,
      "grad_norm": 0.5340223908424377,
      "learning_rate": 3.1776431635415093e-06,
      "loss": 1.0672,
      "step": 7992
    },
    {
      "epoch": 74.70733863837312,
      "grad_norm": 0.5667905211448669,
      "learning_rate": 3.1754304306001014e-06,
      "loss": 1.0564,
      "step": 7993
    },
    {
      "epoch": 74.71676982021809,
      "grad_norm": 0.5302618741989136,
      "learning_rate": 3.173218322923498e-06,
      "loss": 1.0737,
      "step": 7994
    },
    {
      "epoch": 74.72620100206306,
      "grad_norm": 0.540131151676178,
      "learning_rate": 3.171006840714369e-06,
      "loss": 1.0165,
      "step": 7995
    },
    {
      "epoch": 74.73563218390805,
      "grad_norm": 0.5375869274139404,
      "learning_rate": 3.1687959841753357e-06,
      "loss": 1.073,
      "step": 7996
    },
    {
      "epoch": 74.74506336575303,
      "grad_norm": 0.5569618344306946,
      "learning_rate": 3.1665857535089526e-06,
      "loss": 1.0232,
      "step": 7997
    },
    {
      "epoch": 74.754494547598,
      "grad_norm": 0.54453444480896,
      "learning_rate": 3.1643761489177204e-06,
      "loss": 1.0692,
      "step": 7998
    },
    {
      "epoch": 74.76392572944297,
      "grad_norm": 0.5815091729164124,
      "learning_rate": 3.1621671706040824e-06,
      "loss": 1.0456,
      "step": 7999
    },
    {
      "epoch": 74.77335691128795,
      "grad_norm": 0.5452747344970703,
      "learning_rate": 3.1599588187704257e-06,
      "loss": 1.0722,
      "step": 8000
    },
    {
      "epoch": 74.78278809313292,
      "grad_norm": 0.5461739897727966,
      "learning_rate": 3.1577510936190756e-06,
      "loss": 1.0219,
      "step": 8001
    },
    {
      "epoch": 74.7922192749779,
      "grad_norm": 0.574843168258667,
      "learning_rate": 3.1555439953523036e-06,
      "loss": 1.0234,
      "step": 8002
    },
    {
      "epoch": 74.80165045682287,
      "grad_norm": 0.5973488688468933,
      "learning_rate": 3.1533375241723273e-06,
      "loss": 1.0586,
      "step": 8003
    },
    {
      "epoch": 74.81108163866784,
      "grad_norm": 0.5216473937034607,
      "learning_rate": 3.151131680281301e-06,
      "loss": 1.0467,
      "step": 8004
    },
    {
      "epoch": 74.82051282051282,
      "grad_norm": 0.5505244731903076,
      "learning_rate": 3.148926463881322e-06,
      "loss": 1.0428,
      "step": 8005
    },
    {
      "epoch": 74.82994400235779,
      "grad_norm": 0.553376317024231,
      "learning_rate": 3.146721875174432e-06,
      "loss": 1.0349,
      "step": 8006
    },
    {
      "epoch": 74.83937518420277,
      "grad_norm": 0.5195844769477844,
      "learning_rate": 3.1445179143626124e-06,
      "loss": 1.0547,
      "step": 8007
    },
    {
      "epoch": 74.84880636604774,
      "grad_norm": 0.5497186779975891,
      "learning_rate": 3.142314581647795e-06,
      "loss": 1.0173,
      "step": 8008
    },
    {
      "epoch": 74.85823754789271,
      "grad_norm": 0.535591721534729,
      "learning_rate": 3.140111877231845e-06,
      "loss": 1.0846,
      "step": 8009
    },
    {
      "epoch": 74.8676687297377,
      "grad_norm": 0.5698122382164001,
      "learning_rate": 3.1379098013165743e-06,
      "loss": 1.0239,
      "step": 8010
    },
    {
      "epoch": 74.87709991158268,
      "grad_norm": 0.5097666382789612,
      "learning_rate": 3.135708354103736e-06,
      "loss": 1.0771,
      "step": 8011
    },
    {
      "epoch": 74.88653109342765,
      "grad_norm": 0.5418546199798584,
      "learning_rate": 3.133507535795023e-06,
      "loss": 1.0361,
      "step": 8012
    },
    {
      "epoch": 74.89596227527262,
      "grad_norm": 0.5294681191444397,
      "learning_rate": 3.1313073465920783e-06,
      "loss": 1.045,
      "step": 8013
    },
    {
      "epoch": 74.9053934571176,
      "grad_norm": 0.5414849519729614,
      "learning_rate": 3.1291077866964812e-06,
      "loss": 1.1035,
      "step": 8014
    },
    {
      "epoch": 74.91482463896257,
      "grad_norm": 0.5448154807090759,
      "learning_rate": 3.1269088563097536e-06,
      "loss": 1.0417,
      "step": 8015
    },
    {
      "epoch": 74.92425582080755,
      "grad_norm": 0.5958359241485596,
      "learning_rate": 3.1247105556333623e-06,
      "loss": 1.0579,
      "step": 8016
    },
    {
      "epoch": 74.93368700265252,
      "grad_norm": 0.58183354139328,
      "learning_rate": 3.1225128848687093e-06,
      "loss": 1.0599,
      "step": 8017
    },
    {
      "epoch": 74.9431181844975,
      "grad_norm": 0.5350399613380432,
      "learning_rate": 3.1203158442171533e-06,
      "loss": 1.0684,
      "step": 8018
    },
    {
      "epoch": 74.95254936634247,
      "grad_norm": 0.531242847442627,
      "learning_rate": 3.118119433879981e-06,
      "loss": 1.0397,
      "step": 8019
    },
    {
      "epoch": 74.96198054818744,
      "grad_norm": 0.5689720511436462,
      "learning_rate": 3.1159236540584293e-06,
      "loss": 1.0651,
      "step": 8020
    },
    {
      "epoch": 74.97141173003241,
      "grad_norm": 0.5123381018638611,
      "learning_rate": 3.1137285049536727e-06,
      "loss": 1.0619,
      "step": 8021
    },
    {
      "epoch": 74.98084291187739,
      "grad_norm": 0.5819805860519409,
      "learning_rate": 3.1115339867668304e-06,
      "loss": 1.0408,
      "step": 8022
    },
    {
      "epoch": 74.99027409372236,
      "grad_norm": 0.5555042028427124,
      "learning_rate": 3.109340099698963e-06,
      "loss": 1.0452,
      "step": 8023
    },
    {
      "epoch": 74.99970527556735,
      "grad_norm": 0.5925151705741882,
      "learning_rate": 3.1071468439510753e-06,
      "loss": 1.0344,
      "step": 8024
    },
    {
      "epoch": 75.0,
      "grad_norm": 6.478511333465576,
      "learning_rate": 3.104954219724109e-06,
      "loss": 0.5015,
      "step": 8025
    },
    {
      "epoch": 75.00943118184497,
      "grad_norm": 0.5488516092300415,
      "learning_rate": 3.1027622272189572e-06,
      "loss": 1.0578,
      "step": 8026
    },
    {
      "epoch": 75.01886236368995,
      "grad_norm": 0.5716227293014526,
      "learning_rate": 3.1005708666364474e-06,
      "loss": 1.0134,
      "step": 8027
    },
    {
      "epoch": 75.02829354553492,
      "grad_norm": 0.559089183807373,
      "learning_rate": 3.0983801381773492e-06,
      "loss": 1.0723,
      "step": 8028
    },
    {
      "epoch": 75.0377247273799,
      "grad_norm": 0.580154538154602,
      "learning_rate": 3.09619004204238e-06,
      "loss": 1.0504,
      "step": 8029
    },
    {
      "epoch": 75.04715590922487,
      "grad_norm": 0.5173194408416748,
      "learning_rate": 3.0940005784321923e-06,
      "loss": 1.0283,
      "step": 8030
    },
    {
      "epoch": 75.05658709106984,
      "grad_norm": 0.5652999877929688,
      "learning_rate": 3.091811747547385e-06,
      "loss": 1.0189,
      "step": 8031
    },
    {
      "epoch": 75.06601827291482,
      "grad_norm": 0.5679088830947876,
      "learning_rate": 3.0896235495884963e-06,
      "loss": 1.0611,
      "step": 8032
    },
    {
      "epoch": 75.0754494547598,
      "grad_norm": 0.558741569519043,
      "learning_rate": 3.087435984756012e-06,
      "loss": 1.0452,
      "step": 8033
    },
    {
      "epoch": 75.08488063660478,
      "grad_norm": 0.5385209918022156,
      "learning_rate": 3.0852490532503555e-06,
      "loss": 1.0333,
      "step": 8034
    },
    {
      "epoch": 75.09431181844975,
      "grad_norm": 0.5711424946784973,
      "learning_rate": 3.0830627552718915e-06,
      "loss": 1.1018,
      "step": 8035
    },
    {
      "epoch": 75.10374300029473,
      "grad_norm": 0.5394284129142761,
      "learning_rate": 3.0808770910209272e-06,
      "loss": 1.0744,
      "step": 8036
    },
    {
      "epoch": 75.1131741821397,
      "grad_norm": 0.5686211585998535,
      "learning_rate": 3.078692060697713e-06,
      "loss": 1.079,
      "step": 8037
    },
    {
      "epoch": 75.12260536398468,
      "grad_norm": 0.5692099332809448,
      "learning_rate": 3.076507664502443e-06,
      "loss": 1.0504,
      "step": 8038
    },
    {
      "epoch": 75.13203654582965,
      "grad_norm": 0.5090308785438538,
      "learning_rate": 3.074323902635247e-06,
      "loss": 1.0259,
      "step": 8039
    },
    {
      "epoch": 75.14146772767462,
      "grad_norm": 0.5360984206199646,
      "learning_rate": 3.0721407752961985e-06,
      "loss": 1.0512,
      "step": 8040
    },
    {
      "epoch": 75.1508989095196,
      "grad_norm": 0.5163670778274536,
      "learning_rate": 3.0699582826853224e-06,
      "loss": 1.0828,
      "step": 8041
    },
    {
      "epoch": 75.16033009136457,
      "grad_norm": 0.5609018206596375,
      "learning_rate": 3.0677764250025754e-06,
      "loss": 1.0542,
      "step": 8042
    },
    {
      "epoch": 75.16976127320955,
      "grad_norm": 0.5736234188079834,
      "learning_rate": 3.0655952024478565e-06,
      "loss": 1.0355,
      "step": 8043
    },
    {
      "epoch": 75.17919245505452,
      "grad_norm": 0.5421058535575867,
      "learning_rate": 3.0634146152210086e-06,
      "loss": 0.9988,
      "step": 8044
    },
    {
      "epoch": 75.1886236368995,
      "grad_norm": 0.5079460740089417,
      "learning_rate": 3.061234663521818e-06,
      "loss": 1.0322,
      "step": 8045
    },
    {
      "epoch": 75.19805481874447,
      "grad_norm": 0.5560025572776794,
      "learning_rate": 3.0590553475500107e-06,
      "loss": 0.9939,
      "step": 8046
    },
    {
      "epoch": 75.20748600058945,
      "grad_norm": 0.5803040266036987,
      "learning_rate": 3.056876667505251e-06,
      "loss": 1.0528,
      "step": 8047
    },
    {
      "epoch": 75.21691718243443,
      "grad_norm": 0.5754943490028381,
      "learning_rate": 3.0546986235871555e-06,
      "loss": 1.0269,
      "step": 8048
    },
    {
      "epoch": 75.2263483642794,
      "grad_norm": 0.5499480962753296,
      "learning_rate": 3.0525212159952734e-06,
      "loss": 1.0247,
      "step": 8049
    },
    {
      "epoch": 75.23577954612438,
      "grad_norm": 0.5273668766021729,
      "learning_rate": 3.0503444449290974e-06,
      "loss": 1.0325,
      "step": 8050
    },
    {
      "epoch": 75.24521072796935,
      "grad_norm": 0.5427751541137695,
      "learning_rate": 3.0481683105880623e-06,
      "loss": 1.0391,
      "step": 8051
    },
    {
      "epoch": 75.25464190981432,
      "grad_norm": 0.5407187342643738,
      "learning_rate": 3.0459928131715456e-06,
      "loss": 1.0454,
      "step": 8052
    },
    {
      "epoch": 75.2640730916593,
      "grad_norm": 0.548609733581543,
      "learning_rate": 3.0438179528788647e-06,
      "loss": 1.0389,
      "step": 8053
    },
    {
      "epoch": 75.27350427350427,
      "grad_norm": 0.5172582864761353,
      "learning_rate": 3.041643729909277e-06,
      "loss": 1.0178,
      "step": 8054
    },
    {
      "epoch": 75.28293545534925,
      "grad_norm": 0.5596988201141357,
      "learning_rate": 3.03947014446199e-06,
      "loss": 1.0502,
      "step": 8055
    },
    {
      "epoch": 75.29236663719422,
      "grad_norm": 0.5156602263450623,
      "learning_rate": 3.037297196736144e-06,
      "loss": 1.0673,
      "step": 8056
    },
    {
      "epoch": 75.3017978190392,
      "grad_norm": 0.531795084476471,
      "learning_rate": 3.0351248869308246e-06,
      "loss": 1.0397,
      "step": 8057
    },
    {
      "epoch": 75.31122900088417,
      "grad_norm": 0.6030668020248413,
      "learning_rate": 3.032953215245057e-06,
      "loss": 1.0774,
      "step": 8058
    },
    {
      "epoch": 75.32066018272914,
      "grad_norm": 0.6170402765274048,
      "learning_rate": 3.0307821818778094e-06,
      "loss": 1.0506,
      "step": 8059
    },
    {
      "epoch": 75.33009136457412,
      "grad_norm": 0.5735088586807251,
      "learning_rate": 3.028611787027991e-06,
      "loss": 1.0995,
      "step": 8060
    },
    {
      "epoch": 75.3395225464191,
      "grad_norm": 0.602332592010498,
      "learning_rate": 3.026442030894452e-06,
      "loss": 1.0461,
      "step": 8061
    },
    {
      "epoch": 75.34895372826408,
      "grad_norm": 0.569087564945221,
      "learning_rate": 3.024272913675983e-06,
      "loss": 1.0516,
      "step": 8062
    },
    {
      "epoch": 75.35838491010905,
      "grad_norm": 0.552389919757843,
      "learning_rate": 3.0221044355713237e-06,
      "loss": 1.0509,
      "step": 8063
    },
    {
      "epoch": 75.36781609195403,
      "grad_norm": 0.574510931968689,
      "learning_rate": 3.019936596779145e-06,
      "loss": 1.0202,
      "step": 8064
    },
    {
      "epoch": 75.377247273799,
      "grad_norm": 0.5391615629196167,
      "learning_rate": 3.0177693974980647e-06,
      "loss": 1.0447,
      "step": 8065
    },
    {
      "epoch": 75.38667845564397,
      "grad_norm": 0.581752598285675,
      "learning_rate": 3.015602837926641e-06,
      "loss": 1.0963,
      "step": 8066
    },
    {
      "epoch": 75.39610963748895,
      "grad_norm": 0.5134830474853516,
      "learning_rate": 3.013436918263374e-06,
      "loss": 1.0444,
      "step": 8067
    },
    {
      "epoch": 75.40554081933392,
      "grad_norm": 0.5362519025802612,
      "learning_rate": 3.011271638706702e-06,
      "loss": 1.0601,
      "step": 8068
    },
    {
      "epoch": 75.4149720011789,
      "grad_norm": 0.5504261255264282,
      "learning_rate": 3.0091069994550072e-06,
      "loss": 1.0679,
      "step": 8069
    },
    {
      "epoch": 75.42440318302387,
      "grad_norm": 0.5683090686798096,
      "learning_rate": 3.006943000706617e-06,
      "loss": 1.037,
      "step": 8070
    },
    {
      "epoch": 75.43383436486884,
      "grad_norm": 0.5285228490829468,
      "learning_rate": 3.004779642659793e-06,
      "loss": 1.0383,
      "step": 8071
    },
    {
      "epoch": 75.44326554671382,
      "grad_norm": 0.5681454539299011,
      "learning_rate": 3.0026169255127435e-06,
      "loss": 1.0539,
      "step": 8072
    },
    {
      "epoch": 75.45269672855879,
      "grad_norm": 0.5216319561004639,
      "learning_rate": 3.0004548494636145e-06,
      "loss": 1.0494,
      "step": 8073
    },
    {
      "epoch": 75.46212791040377,
      "grad_norm": 0.5976723432540894,
      "learning_rate": 2.9982934147104927e-06,
      "loss": 1.0266,
      "step": 8074
    },
    {
      "epoch": 75.47155909224875,
      "grad_norm": 0.5792526602745056,
      "learning_rate": 2.996132621451412e-06,
      "loss": 1.0371,
      "step": 8075
    },
    {
      "epoch": 75.48099027409373,
      "grad_norm": 0.5265405178070068,
      "learning_rate": 2.993972469884342e-06,
      "loss": 1.0782,
      "step": 8076
    },
    {
      "epoch": 75.4904214559387,
      "grad_norm": 0.569194495677948,
      "learning_rate": 2.9918129602071956e-06,
      "loss": 1.0743,
      "step": 8077
    },
    {
      "epoch": 75.49985263778368,
      "grad_norm": 0.5453017950057983,
      "learning_rate": 2.989654092617823e-06,
      "loss": 1.0944,
      "step": 8078
    },
    {
      "epoch": 75.50928381962865,
      "grad_norm": 0.5544405579566956,
      "learning_rate": 2.987495867314023e-06,
      "loss": 1.0612,
      "step": 8079
    },
    {
      "epoch": 75.51871500147362,
      "grad_norm": 0.5099679827690125,
      "learning_rate": 2.9853382844935318e-06,
      "loss": 1.05,
      "step": 8080
    },
    {
      "epoch": 75.5281461833186,
      "grad_norm": 0.5825111865997314,
      "learning_rate": 2.9831813443540235e-06,
      "loss": 1.0713,
      "step": 8081
    },
    {
      "epoch": 75.53757736516357,
      "grad_norm": 0.5041347742080688,
      "learning_rate": 2.981025047093118e-06,
      "loss": 1.061,
      "step": 8082
    },
    {
      "epoch": 75.54700854700855,
      "grad_norm": 0.5168744325637817,
      "learning_rate": 2.9788693929083746e-06,
      "loss": 1.0704,
      "step": 8083
    },
    {
      "epoch": 75.55643972885352,
      "grad_norm": 0.5615163445472717,
      "learning_rate": 2.9767143819972887e-06,
      "loss": 1.0523,
      "step": 8084
    },
    {
      "epoch": 75.5658709106985,
      "grad_norm": 0.5545435547828674,
      "learning_rate": 2.9745600145573107e-06,
      "loss": 1.035,
      "step": 8085
    },
    {
      "epoch": 75.57530209254347,
      "grad_norm": 0.5408052206039429,
      "learning_rate": 2.9724062907858165e-06,
      "loss": 1.0856,
      "step": 8086
    },
    {
      "epoch": 75.58473327438844,
      "grad_norm": 0.535365641117096,
      "learning_rate": 2.970253210880133e-06,
      "loss": 1.0653,
      "step": 8087
    },
    {
      "epoch": 75.59416445623341,
      "grad_norm": 0.5695526599884033,
      "learning_rate": 2.9681007750375223e-06,
      "loss": 1.0733,
      "step": 8088
    },
    {
      "epoch": 75.6035956380784,
      "grad_norm": 0.5250662565231323,
      "learning_rate": 2.96594898345519e-06,
      "loss": 1.0175,
      "step": 8089
    },
    {
      "epoch": 75.61302681992338,
      "grad_norm": 0.5432601571083069,
      "learning_rate": 2.9637978363302845e-06,
      "loss": 1.0541,
      "step": 8090
    },
    {
      "epoch": 75.62245800176835,
      "grad_norm": 0.5865335464477539,
      "learning_rate": 2.9616473338598907e-06,
      "loss": 1.0743,
      "step": 8091
    },
    {
      "epoch": 75.63188918361332,
      "grad_norm": 0.5808231830596924,
      "learning_rate": 2.9594974762410355e-06,
      "loss": 1.0014,
      "step": 8092
    },
    {
      "epoch": 75.6413203654583,
      "grad_norm": 0.5399692058563232,
      "learning_rate": 2.957348263670694e-06,
      "loss": 1.0186,
      "step": 8093
    },
    {
      "epoch": 75.65075154730327,
      "grad_norm": 0.5258709192276001,
      "learning_rate": 2.955199696345773e-06,
      "loss": 1.0288,
      "step": 8094
    },
    {
      "epoch": 75.66018272914825,
      "grad_norm": 0.5668089985847473,
      "learning_rate": 2.9530517744631236e-06,
      "loss": 1.0791,
      "step": 8095
    },
    {
      "epoch": 75.66961391099322,
      "grad_norm": 0.54161536693573,
      "learning_rate": 2.950904498219538e-06,
      "loss": 1.0528,
      "step": 8096
    },
    {
      "epoch": 75.6790450928382,
      "grad_norm": 0.5713207125663757,
      "learning_rate": 2.948757867811748e-06,
      "loss": 1.0665,
      "step": 8097
    },
    {
      "epoch": 75.68847627468317,
      "grad_norm": 0.5144708752632141,
      "learning_rate": 2.9466118834364277e-06,
      "loss": 1.0081,
      "step": 8098
    },
    {
      "epoch": 75.69790745652814,
      "grad_norm": 0.5171853303909302,
      "learning_rate": 2.944466545290189e-06,
      "loss": 1.0342,
      "step": 8099
    },
    {
      "epoch": 75.70733863837312,
      "grad_norm": 0.5281304717063904,
      "learning_rate": 2.9423218535695917e-06,
      "loss": 1.0456,
      "step": 8100
    },
    {
      "epoch": 75.71676982021809,
      "grad_norm": 0.5813530683517456,
      "learning_rate": 2.940177808471131e-06,
      "loss": 1.0971,
      "step": 8101
    },
    {
      "epoch": 75.72620100206306,
      "grad_norm": 0.5684915781021118,
      "learning_rate": 2.9380344101912416e-06,
      "loss": 1.0568,
      "step": 8102
    },
    {
      "epoch": 75.73563218390805,
      "grad_norm": 0.536014974117279,
      "learning_rate": 2.935891658926301e-06,
      "loss": 1.0318,
      "step": 8103
    },
    {
      "epoch": 75.74506336575303,
      "grad_norm": 0.5347630381584167,
      "learning_rate": 2.933749554872629e-06,
      "loss": 1.0329,
      "step": 8104
    },
    {
      "epoch": 75.754494547598,
      "grad_norm": 0.516454815864563,
      "learning_rate": 2.931608098226484e-06,
      "loss": 1.0962,
      "step": 8105
    },
    {
      "epoch": 75.76392572944297,
      "grad_norm": 0.5986361503601074,
      "learning_rate": 2.9294672891840635e-06,
      "loss": 1.0239,
      "step": 8106
    },
    {
      "epoch": 75.77335691128795,
      "grad_norm": 0.528603196144104,
      "learning_rate": 2.9273271279415074e-06,
      "loss": 1.0387,
      "step": 8107
    },
    {
      "epoch": 75.78278809313292,
      "grad_norm": 0.6035559177398682,
      "learning_rate": 2.9251876146949e-06,
      "loss": 1.0427,
      "step": 8108
    },
    {
      "epoch": 75.7922192749779,
      "grad_norm": 0.5562511682510376,
      "learning_rate": 2.923048749640263e-06,
      "loss": 1.0624,
      "step": 8109
    },
    {
      "epoch": 75.80165045682287,
      "grad_norm": 0.550305187702179,
      "learning_rate": 2.9209105329735555e-06,
      "loss": 1.0073,
      "step": 8110
    },
    {
      "epoch": 75.81108163866784,
      "grad_norm": 0.5178248286247253,
      "learning_rate": 2.9187729648906815e-06,
      "loss": 1.0246,
      "step": 8111
    },
    {
      "epoch": 75.82051282051282,
      "grad_norm": 0.5419172644615173,
      "learning_rate": 2.916636045587484e-06,
      "loss": 1.0386,
      "step": 8112
    },
    {
      "epoch": 75.82994400235779,
      "grad_norm": 0.5990443229675293,
      "learning_rate": 2.9144997752597472e-06,
      "loss": 1.0398,
      "step": 8113
    },
    {
      "epoch": 75.83937518420277,
      "grad_norm": 0.5498785376548767,
      "learning_rate": 2.9123641541031924e-06,
      "loss": 1.0532,
      "step": 8114
    },
    {
      "epoch": 75.84880636604774,
      "grad_norm": 0.5885403156280518,
      "learning_rate": 2.9102291823134897e-06,
      "loss": 1.0845,
      "step": 8115
    },
    {
      "epoch": 75.85823754789271,
      "grad_norm": 0.561478853225708,
      "learning_rate": 2.9080948600862433e-06,
      "loss": 1.0615,
      "step": 8116
    },
    {
      "epoch": 75.8676687297377,
      "grad_norm": 0.549260675907135,
      "learning_rate": 2.905961187616998e-06,
      "loss": 1.0591,
      "step": 8117
    },
    {
      "epoch": 75.87709991158268,
      "grad_norm": 0.5566788911819458,
      "learning_rate": 2.903828165101239e-06,
      "loss": 1.0485,
      "step": 8118
    },
    {
      "epoch": 75.88653109342765,
      "grad_norm": 0.5492871999740601,
      "learning_rate": 2.9016957927343946e-06,
      "loss": 1.0261,
      "step": 8119
    },
    {
      "epoch": 75.89596227527262,
      "grad_norm": 0.5226711630821228,
      "learning_rate": 2.899564070711832e-06,
      "loss": 1.0647,
      "step": 8120
    },
    {
      "epoch": 75.9053934571176,
      "grad_norm": 0.5285815000534058,
      "learning_rate": 2.8974329992288598e-06,
      "loss": 1.0867,
      "step": 8121
    },
    {
      "epoch": 75.91482463896257,
      "grad_norm": 0.5398088693618774,
      "learning_rate": 2.89530257848072e-06,
      "loss": 1.067,
      "step": 8122
    },
    {
      "epoch": 75.92425582080755,
      "grad_norm": 0.5272465348243713,
      "learning_rate": 2.89317280866261e-06,
      "loss": 1.0475,
      "step": 8123
    },
    {
      "epoch": 75.93368700265252,
      "grad_norm": 0.6037917137145996,
      "learning_rate": 2.8910436899696536e-06,
      "loss": 1.08,
      "step": 8124
    },
    {
      "epoch": 75.9431181844975,
      "grad_norm": 0.5465781688690186,
      "learning_rate": 2.888915222596922e-06,
      "loss": 1.0348,
      "step": 8125
    },
    {
      "epoch": 75.95254936634247,
      "grad_norm": 0.5437944531440735,
      "learning_rate": 2.8867874067394232e-06,
      "loss": 1.0419,
      "step": 8126
    },
    {
      "epoch": 75.96198054818744,
      "grad_norm": 0.5647554397583008,
      "learning_rate": 2.8846602425921066e-06,
      "loss": 1.0415,
      "step": 8127
    },
    {
      "epoch": 75.97141173003241,
      "grad_norm": 0.5581365823745728,
      "learning_rate": 2.8825337303498637e-06,
      "loss": 1.0736,
      "step": 8128
    },
    {
      "epoch": 75.98084291187739,
      "grad_norm": 0.5322055816650391,
      "learning_rate": 2.88040787020752e-06,
      "loss": 1.0445,
      "step": 8129
    },
    {
      "epoch": 75.99027409372236,
      "grad_norm": 0.5159603357315063,
      "learning_rate": 2.8782826623598548e-06,
      "loss": 1.0407,
      "step": 8130
    },
    {
      "epoch": 75.99970527556735,
      "grad_norm": 0.5554086565971375,
      "learning_rate": 2.8761581070015732e-06,
      "loss": 1.0589,
      "step": 8131
    },
    {
      "epoch": 76.0,
      "grad_norm": 5.153065204620361,
      "learning_rate": 2.874034204327328e-06,
      "loss": 0.4501,
      "step": 8132
    },
    {
      "epoch": 76.00943118184497,
      "grad_norm": 0.5006815195083618,
      "learning_rate": 2.8719109545317102e-06,
      "loss": 1.0577,
      "step": 8133
    },
    {
      "epoch": 76.01886236368995,
      "grad_norm": 0.6220974922180176,
      "learning_rate": 2.86978835780925e-06,
      "loss": 1.0,
      "step": 8134
    },
    {
      "epoch": 76.02829354553492,
      "grad_norm": 0.5905219316482544,
      "learning_rate": 2.8676664143544177e-06,
      "loss": 1.0618,
      "step": 8135
    },
    {
      "epoch": 76.0377247273799,
      "grad_norm": 0.5537222623825073,
      "learning_rate": 2.8655451243616294e-06,
      "loss": 1.0708,
      "step": 8136
    },
    {
      "epoch": 76.04715590922487,
      "grad_norm": 0.5552432537078857,
      "learning_rate": 2.863424488025236e-06,
      "loss": 1.0386,
      "step": 8137
    },
    {
      "epoch": 76.05658709106984,
      "grad_norm": 0.5470541715621948,
      "learning_rate": 2.8613045055395284e-06,
      "loss": 1.045,
      "step": 8138
    },
    {
      "epoch": 76.06601827291482,
      "grad_norm": 0.5733142495155334,
      "learning_rate": 2.8591851770987378e-06,
      "loss": 1.0529,
      "step": 8139
    },
    {
      "epoch": 76.0754494547598,
      "grad_norm": 0.5491257309913635,
      "learning_rate": 2.8570665028970345e-06,
      "loss": 1.0684,
      "step": 8140
    },
    {
      "epoch": 76.08488063660478,
      "grad_norm": 0.5418717265129089,
      "learning_rate": 2.8549484831285366e-06,
      "loss": 1.0423,
      "step": 8141
    },
    {
      "epoch": 76.09431181844975,
      "grad_norm": 0.6058090925216675,
      "learning_rate": 2.8528311179872937e-06,
      "loss": 0.9876,
      "step": 8142
    },
    {
      "epoch": 76.10374300029473,
      "grad_norm": 0.5497945547103882,
      "learning_rate": 2.8507144076672976e-06,
      "loss": 1.0421,
      "step": 8143
    },
    {
      "epoch": 76.1131741821397,
      "grad_norm": 0.5479182004928589,
      "learning_rate": 2.8485983523624806e-06,
      "loss": 1.0286,
      "step": 8144
    },
    {
      "epoch": 76.12260536398468,
      "grad_norm": 0.5796065330505371,
      "learning_rate": 2.8464829522667114e-06,
      "loss": 1.0247,
      "step": 8145
    },
    {
      "epoch": 76.13203654582965,
      "grad_norm": 0.5435441136360168,
      "learning_rate": 2.8443682075738087e-06,
      "loss": 1.0576,
      "step": 8146
    },
    {
      "epoch": 76.14146772767462,
      "grad_norm": 0.5717350840568542,
      "learning_rate": 2.8422541184775234e-06,
      "loss": 1.1035,
      "step": 8147
    },
    {
      "epoch": 76.1508989095196,
      "grad_norm": 0.5369668006896973,
      "learning_rate": 2.840140685171545e-06,
      "loss": 1.0351,
      "step": 8148
    },
    {
      "epoch": 76.16033009136457,
      "grad_norm": 0.5393410921096802,
      "learning_rate": 2.8380279078495066e-06,
      "loss": 1.1084,
      "step": 8149
    },
    {
      "epoch": 76.16976127320955,
      "grad_norm": 0.5625800490379333,
      "learning_rate": 2.8359157867049814e-06,
      "loss": 1.0693,
      "step": 8150
    },
    {
      "epoch": 76.17919245505452,
      "grad_norm": 0.5215740203857422,
      "learning_rate": 2.8338043219314794e-06,
      "loss": 1.0634,
      "step": 8151
    },
    {
      "epoch": 76.1886236368995,
      "grad_norm": 0.5585961937904358,
      "learning_rate": 2.831693513722449e-06,
      "loss": 1.0598,
      "step": 8152
    },
    {
      "epoch": 76.19805481874447,
      "grad_norm": 0.5566022992134094,
      "learning_rate": 2.82958336227129e-06,
      "loss": 1.0419,
      "step": 8153
    },
    {
      "epoch": 76.20748600058945,
      "grad_norm": 0.532245934009552,
      "learning_rate": 2.8274738677713287e-06,
      "loss": 1.0202,
      "step": 8154
    },
    {
      "epoch": 76.21691718243443,
      "grad_norm": 0.5332383513450623,
      "learning_rate": 2.8253650304158374e-06,
      "loss": 1.036,
      "step": 8155
    },
    {
      "epoch": 76.2263483642794,
      "grad_norm": 0.5602819919586182,
      "learning_rate": 2.8232568503980272e-06,
      "loss": 1.0479,
      "step": 8156
    },
    {
      "epoch": 76.23577954612438,
      "grad_norm": 0.5414401888847351,
      "learning_rate": 2.821149327911049e-06,
      "loss": 1.0749,
      "step": 8157
    },
    {
      "epoch": 76.24521072796935,
      "grad_norm": 0.5098785161972046,
      "learning_rate": 2.819042463147992e-06,
      "loss": 1.0245,
      "step": 8158
    },
    {
      "epoch": 76.25464190981432,
      "grad_norm": 0.5899060368537903,
      "learning_rate": 2.816936256301884e-06,
      "loss": 1.0405,
      "step": 8159
    },
    {
      "epoch": 76.2640730916593,
      "grad_norm": 0.5443609952926636,
      "learning_rate": 2.814830707565701e-06,
      "loss": 1.1025,
      "step": 8160
    },
    {
      "epoch": 76.27350427350427,
      "grad_norm": 0.5609893798828125,
      "learning_rate": 2.8127258171323503e-06,
      "loss": 1.0393,
      "step": 8161
    },
    {
      "epoch": 76.28293545534925,
      "grad_norm": 0.5690256357192993,
      "learning_rate": 2.8106215851946817e-06,
      "loss": 1.0247,
      "step": 8162
    },
    {
      "epoch": 76.29236663719422,
      "grad_norm": 0.552532970905304,
      "learning_rate": 2.8085180119454824e-06,
      "loss": 1.0812,
      "step": 8163
    },
    {
      "epoch": 76.3017978190392,
      "grad_norm": 0.5229400396347046,
      "learning_rate": 2.8064150975774816e-06,
      "loss": 0.999,
      "step": 8164
    },
    {
      "epoch": 76.31122900088417,
      "grad_norm": 0.5368872284889221,
      "learning_rate": 2.804312842283349e-06,
      "loss": 1.0438,
      "step": 8165
    },
    {
      "epoch": 76.32066018272914,
      "grad_norm": 0.5728849768638611,
      "learning_rate": 2.8022112462556883e-06,
      "loss": 1.022,
      "step": 8166
    },
    {
      "epoch": 76.33009136457412,
      "grad_norm": 0.5172935128211975,
      "learning_rate": 2.800110309687053e-06,
      "loss": 1.0358,
      "step": 8167
    },
    {
      "epoch": 76.3395225464191,
      "grad_norm": 0.5333718061447144,
      "learning_rate": 2.7980100327699277e-06,
      "loss": 1.0473,
      "step": 8168
    },
    {
      "epoch": 76.34895372826408,
      "grad_norm": 0.5669671893119812,
      "learning_rate": 2.7959104156967388e-06,
      "loss": 1.0144,
      "step": 8169
    },
    {
      "epoch": 76.35838491010905,
      "grad_norm": 0.5245022177696228,
      "learning_rate": 2.793811458659853e-06,
      "loss": 1.0619,
      "step": 8170
    },
    {
      "epoch": 76.36781609195403,
      "grad_norm": 0.4945583641529083,
      "learning_rate": 2.7917131618515757e-06,
      "loss": 1.0748,
      "step": 8171
    },
    {
      "epoch": 76.377247273799,
      "grad_norm": 0.5137313604354858,
      "learning_rate": 2.7896155254641523e-06,
      "loss": 1.0564,
      "step": 8172
    },
    {
      "epoch": 76.38667845564397,
      "grad_norm": 0.5672455430030823,
      "learning_rate": 2.787518549689767e-06,
      "loss": 1.0328,
      "step": 8173
    },
    {
      "epoch": 76.39610963748895,
      "grad_norm": 0.5934741497039795,
      "learning_rate": 2.7854222347205417e-06,
      "loss": 1.0762,
      "step": 8174
    },
    {
      "epoch": 76.40554081933392,
      "grad_norm": 0.535931408405304,
      "learning_rate": 2.7833265807485454e-06,
      "loss": 1.0705,
      "step": 8175
    },
    {
      "epoch": 76.4149720011789,
      "grad_norm": 0.518997311592102,
      "learning_rate": 2.781231587965779e-06,
      "loss": 1.0613,
      "step": 8176
    },
    {
      "epoch": 76.42440318302387,
      "grad_norm": 0.5767991542816162,
      "learning_rate": 2.779137256564185e-06,
      "loss": 1.0169,
      "step": 8177
    },
    {
      "epoch": 76.43383436486884,
      "grad_norm": 0.5706053972244263,
      "learning_rate": 2.777043586735645e-06,
      "loss": 1.0109,
      "step": 8178
    },
    {
      "epoch": 76.44326554671382,
      "grad_norm": 0.5851373672485352,
      "learning_rate": 2.77495057867198e-06,
      "loss": 1.017,
      "step": 8179
    },
    {
      "epoch": 76.45269672855879,
      "grad_norm": 0.5423750281333923,
      "learning_rate": 2.772858232564951e-06,
      "loss": 1.0327,
      "step": 8180
    },
    {
      "epoch": 76.46212791040377,
      "grad_norm": 0.5519300699234009,
      "learning_rate": 2.7707665486062554e-06,
      "loss": 1.0646,
      "step": 8181
    },
    {
      "epoch": 76.47155909224875,
      "grad_norm": 0.5630967020988464,
      "learning_rate": 2.7686755269875377e-06,
      "loss": 1.0535,
      "step": 8182
    },
    {
      "epoch": 76.48099027409373,
      "grad_norm": 0.5296524167060852,
      "learning_rate": 2.7665851679003753e-06,
      "loss": 1.0062,
      "step": 8183
    },
    {
      "epoch": 76.4904214559387,
      "grad_norm": 0.5341771841049194,
      "learning_rate": 2.764495471536284e-06,
      "loss": 1.0308,
      "step": 8184
    },
    {
      "epoch": 76.49985263778368,
      "grad_norm": 0.5503398180007935,
      "learning_rate": 2.7624064380867233e-06,
      "loss": 1.0725,
      "step": 8185
    },
    {
      "epoch": 76.50928381962865,
      "grad_norm": 0.5233337879180908,
      "learning_rate": 2.7603180677430872e-06,
      "loss": 1.0508,
      "step": 8186
    },
    {
      "epoch": 76.51871500147362,
      "grad_norm": 0.5311473608016968,
      "learning_rate": 2.7582303606967144e-06,
      "loss": 1.0613,
      "step": 8187
    },
    {
      "epoch": 76.5281461833186,
      "grad_norm": 0.5429585576057434,
      "learning_rate": 2.7561433171388783e-06,
      "loss": 1.0229,
      "step": 8188
    },
    {
      "epoch": 76.53757736516357,
      "grad_norm": 0.540501594543457,
      "learning_rate": 2.7540569372607896e-06,
      "loss": 1.0626,
      "step": 8189
    },
    {
      "epoch": 76.54700854700855,
      "grad_norm": 0.5386971235275269,
      "learning_rate": 2.75197122125361e-06,
      "loss": 1.069,
      "step": 8190
    },
    {
      "epoch": 76.55643972885352,
      "grad_norm": 0.5642395615577698,
      "learning_rate": 2.7498861693084257e-06,
      "loss": 1.0309,
      "step": 8191
    },
    {
      "epoch": 76.5658709106985,
      "grad_norm": 0.5838149189949036,
      "learning_rate": 2.7478017816162717e-06,
      "loss": 1.0643,
      "step": 8192
    },
    {
      "epoch": 76.57530209254347,
      "grad_norm": 0.597055971622467,
      "learning_rate": 2.7457180583681166e-06,
      "loss": 1.085,
      "step": 8193
    },
    {
      "epoch": 76.58473327438844,
      "grad_norm": 0.5589750409126282,
      "learning_rate": 2.7436349997548727e-06,
      "loss": 1.0338,
      "step": 8194
    },
    {
      "epoch": 76.59416445623341,
      "grad_norm": 0.5647673606872559,
      "learning_rate": 2.741552605967387e-06,
      "loss": 1.0321,
      "step": 8195
    },
    {
      "epoch": 76.6035956380784,
      "grad_norm": 0.536841094493866,
      "learning_rate": 2.739470877196445e-06,
      "loss": 1.063,
      "step": 8196
    },
    {
      "epoch": 76.61302681992338,
      "grad_norm": 0.5489439368247986,
      "learning_rate": 2.7373898136327815e-06,
      "loss": 1.076,
      "step": 8197
    },
    {
      "epoch": 76.62245800176835,
      "grad_norm": 0.5642783045768738,
      "learning_rate": 2.735309415467059e-06,
      "loss": 1.0629,
      "step": 8198
    },
    {
      "epoch": 76.63188918361332,
      "grad_norm": 0.5453411936759949,
      "learning_rate": 2.733229682889883e-06,
      "loss": 1.0914,
      "step": 8199
    },
    {
      "epoch": 76.6413203654583,
      "grad_norm": 0.5364547371864319,
      "learning_rate": 2.7311506160917966e-06,
      "loss": 1.0459,
      "step": 8200
    },
    {
      "epoch": 76.65075154730327,
      "grad_norm": 0.5501879453659058,
      "learning_rate": 2.729072215263282e-06,
      "loss": 1.0686,
      "step": 8201
    },
    {
      "epoch": 76.66018272914825,
      "grad_norm": 0.5559148788452148,
      "learning_rate": 2.726994480594767e-06,
      "loss": 1.0804,
      "step": 8202
    },
    {
      "epoch": 76.66961391099322,
      "grad_norm": 0.6030043363571167,
      "learning_rate": 2.7249174122766096e-06,
      "loss": 1.0625,
      "step": 8203
    },
    {
      "epoch": 76.6790450928382,
      "grad_norm": 0.5187376737594604,
      "learning_rate": 2.722841010499111e-06,
      "loss": 1.0694,
      "step": 8204
    },
    {
      "epoch": 76.68847627468317,
      "grad_norm": 0.5427172183990479,
      "learning_rate": 2.7207652754525093e-06,
      "loss": 1.0444,
      "step": 8205
    },
    {
      "epoch": 76.69790745652814,
      "grad_norm": 0.5370727181434631,
      "learning_rate": 2.7186902073269806e-06,
      "loss": 1.0564,
      "step": 8206
    },
    {
      "epoch": 76.70733863837312,
      "grad_norm": 0.5853226184844971,
      "learning_rate": 2.716615806312648e-06,
      "loss": 1.0137,
      "step": 8207
    },
    {
      "epoch": 76.71676982021809,
      "grad_norm": 0.6013625264167786,
      "learning_rate": 2.714542072599563e-06,
      "loss": 1.0368,
      "step": 8208
    },
    {
      "epoch": 76.72620100206306,
      "grad_norm": 0.513812780380249,
      "learning_rate": 2.7124690063777215e-06,
      "loss": 1.0074,
      "step": 8209
    },
    {
      "epoch": 76.73563218390805,
      "grad_norm": 0.5620431303977966,
      "learning_rate": 2.710396607837058e-06,
      "loss": 1.0229,
      "step": 8210
    },
    {
      "epoch": 76.74506336575303,
      "grad_norm": 0.5214462280273438,
      "learning_rate": 2.708324877167441e-06,
      "loss": 1.0356,
      "step": 8211
    },
    {
      "epoch": 76.754494547598,
      "grad_norm": 0.5246919989585876,
      "learning_rate": 2.706253814558687e-06,
      "loss": 1.0609,
      "step": 8212
    },
    {
      "epoch": 76.76392572944297,
      "grad_norm": 0.5858354568481445,
      "learning_rate": 2.7041834202005434e-06,
      "loss": 1.0619,
      "step": 8213
    },
    {
      "epoch": 76.77335691128795,
      "grad_norm": 0.5492236018180847,
      "learning_rate": 2.702113694282701e-06,
      "loss": 1.0479,
      "step": 8214
    },
    {
      "epoch": 76.78278809313292,
      "grad_norm": 0.5225703120231628,
      "learning_rate": 2.700044636994784e-06,
      "loss": 1.0453,
      "step": 8215
    },
    {
      "epoch": 76.7922192749779,
      "grad_norm": 0.5188139081001282,
      "learning_rate": 2.697976248526363e-06,
      "loss": 1.0285,
      "step": 8216
    },
    {
      "epoch": 76.80165045682287,
      "grad_norm": 0.5624069571495056,
      "learning_rate": 2.6959085290669383e-06,
      "loss": 1.0494,
      "step": 8217
    },
    {
      "epoch": 76.81108163866784,
      "grad_norm": 0.5345027446746826,
      "learning_rate": 2.6938414788059576e-06,
      "loss": 1.0417,
      "step": 8218
    },
    {
      "epoch": 76.82051282051282,
      "grad_norm": 0.5476799607276917,
      "learning_rate": 2.6917750979327984e-06,
      "loss": 1.0947,
      "step": 8219
    },
    {
      "epoch": 76.82994400235779,
      "grad_norm": 0.5295398235321045,
      "learning_rate": 2.6897093866367886e-06,
      "loss": 1.1055,
      "step": 8220
    },
    {
      "epoch": 76.83937518420277,
      "grad_norm": 0.5676711797714233,
      "learning_rate": 2.687644345107184e-06,
      "loss": 1.0161,
      "step": 8221
    },
    {
      "epoch": 76.84880636604774,
      "grad_norm": 0.5110957026481628,
      "learning_rate": 2.6855799735331846e-06,
      "loss": 1.0547,
      "step": 8222
    },
    {
      "epoch": 76.85823754789271,
      "grad_norm": 0.5471176505088806,
      "learning_rate": 2.6835162721039267e-06,
      "loss": 1.0933,
      "step": 8223
    },
    {
      "epoch": 76.8676687297377,
      "grad_norm": 0.5934484601020813,
      "learning_rate": 2.681453241008485e-06,
      "loss": 1.0148,
      "step": 8224
    },
    {
      "epoch": 76.87709991158268,
      "grad_norm": 0.5530202388763428,
      "learning_rate": 2.6793908804358748e-06,
      "loss": 1.0745,
      "step": 8225
    },
    {
      "epoch": 76.88653109342765,
      "grad_norm": 0.5287911295890808,
      "learning_rate": 2.6773291905750456e-06,
      "loss": 1.0776,
      "step": 8226
    },
    {
      "epoch": 76.89596227527262,
      "grad_norm": 0.5817283987998962,
      "learning_rate": 2.675268171614894e-06,
      "loss": 1.0431,
      "step": 8227
    },
    {
      "epoch": 76.9053934571176,
      "grad_norm": 0.5243897438049316,
      "learning_rate": 2.6732078237442482e-06,
      "loss": 1.0266,
      "step": 8228
    },
    {
      "epoch": 76.91482463896257,
      "grad_norm": 0.5522017478942871,
      "learning_rate": 2.6711481471518764e-06,
      "loss": 1.1034,
      "step": 8229
    },
    {
      "epoch": 76.92425582080755,
      "grad_norm": 0.546612024307251,
      "learning_rate": 2.669089142026484e-06,
      "loss": 1.0626,
      "step": 8230
    },
    {
      "epoch": 76.93368700265252,
      "grad_norm": 0.5273033380508423,
      "learning_rate": 2.6670308085567188e-06,
      "loss": 1.0511,
      "step": 8231
    },
    {
      "epoch": 76.9431181844975,
      "grad_norm": 0.5421284437179565,
      "learning_rate": 2.6649731469311625e-06,
      "loss": 1.0784,
      "step": 8232
    },
    {
      "epoch": 76.95254936634247,
      "grad_norm": 0.5518053770065308,
      "learning_rate": 2.662916157338339e-06,
      "loss": 1.0439,
      "step": 8233
    },
    {
      "epoch": 76.96198054818744,
      "grad_norm": 0.589529812335968,
      "learning_rate": 2.6608598399667042e-06,
      "loss": 1.0193,
      "step": 8234
    },
    {
      "epoch": 76.97141173003241,
      "grad_norm": 0.5415707230567932,
      "learning_rate": 2.658804195004664e-06,
      "loss": 1.0718,
      "step": 8235
    },
    {
      "epoch": 76.98084291187739,
      "grad_norm": 0.5803754329681396,
      "learning_rate": 2.6567492226405546e-06,
      "loss": 1.0204,
      "step": 8236
    },
    {
      "epoch": 76.99027409372236,
      "grad_norm": 0.49538707733154297,
      "learning_rate": 2.6546949230626483e-06,
      "loss": 1.0419,
      "step": 8237
    },
    {
      "epoch": 76.99970527556735,
      "grad_norm": 0.5507129430770874,
      "learning_rate": 2.652641296459163e-06,
      "loss": 1.0536,
      "step": 8238
    },
    {
      "epoch": 77.0,
      "grad_norm": 7.1279473304748535,
      "learning_rate": 2.6505883430182487e-06,
      "loss": 0.3021,
      "step": 8239
    },
    {
      "epoch": 77.00943118184497,
      "grad_norm": 0.5580838322639465,
      "learning_rate": 2.648536062927999e-06,
      "loss": 1.0226,
      "step": 8240
    },
    {
      "epoch": 77.01886236368995,
      "grad_norm": 0.5486486554145813,
      "learning_rate": 2.646484456376437e-06,
      "loss": 1.0281,
      "step": 8241
    },
    {
      "epoch": 77.02829354553492,
      "grad_norm": 0.5500059127807617,
      "learning_rate": 2.6444335235515372e-06,
      "loss": 1.0461,
      "step": 8242
    },
    {
      "epoch": 77.0377247273799,
      "grad_norm": 0.5058534741401672,
      "learning_rate": 2.642383264641204e-06,
      "loss": 1.0473,
      "step": 8243
    },
    {
      "epoch": 77.04715590922487,
      "grad_norm": 0.5547966361045837,
      "learning_rate": 2.6403336798332803e-06,
      "loss": 1.0557,
      "step": 8244
    },
    {
      "epoch": 77.05658709106984,
      "grad_norm": 0.5964668989181519,
      "learning_rate": 2.6382847693155476e-06,
      "loss": 1.0813,
      "step": 8245
    },
    {
      "epoch": 77.06601827291482,
      "grad_norm": 0.526759147644043,
      "learning_rate": 2.636236533275728e-06,
      "loss": 1.0331,
      "step": 8246
    },
    {
      "epoch": 77.0754494547598,
      "grad_norm": 0.557682454586029,
      "learning_rate": 2.634188971901479e-06,
      "loss": 1.0837,
      "step": 8247
    },
    {
      "epoch": 77.08488063660478,
      "grad_norm": 0.5409117937088013,
      "learning_rate": 2.632142085380398e-06,
      "loss": 1.0474,
      "step": 8248
    },
    {
      "epoch": 77.09431181844975,
      "grad_norm": 0.5374531149864197,
      "learning_rate": 2.630095873900017e-06,
      "loss": 1.0972,
      "step": 8249
    },
    {
      "epoch": 77.10374300029473,
      "grad_norm": 0.5414910316467285,
      "learning_rate": 2.628050337647815e-06,
      "loss": 1.0736,
      "step": 8250
    },
    {
      "epoch": 77.1131741821397,
      "grad_norm": 0.5797746181488037,
      "learning_rate": 2.626005476811201e-06,
      "loss": 1.0338,
      "step": 8251
    },
    {
      "epoch": 77.12260536398468,
      "grad_norm": 0.5375059247016907,
      "learning_rate": 2.6239612915775224e-06,
      "loss": 1.0717,
      "step": 8252
    },
    {
      "epoch": 77.13203654582965,
      "grad_norm": 0.5687500238418579,
      "learning_rate": 2.6219177821340702e-06,
      "loss": 1.0512,
      "step": 8253
    },
    {
      "epoch": 77.14146772767462,
      "grad_norm": 0.5358357429504395,
      "learning_rate": 2.6198749486680675e-06,
      "loss": 1.0387,
      "step": 8254
    },
    {
      "epoch": 77.1508989095196,
      "grad_norm": 0.5915253162384033,
      "learning_rate": 2.617832791366679e-06,
      "loss": 1.081,
      "step": 8255
    },
    {
      "epoch": 77.16033009136457,
      "grad_norm": 0.5496151447296143,
      "learning_rate": 2.615791310417003e-06,
      "loss": 1.097,
      "step": 8256
    },
    {
      "epoch": 77.16976127320955,
      "grad_norm": 0.5413825511932373,
      "learning_rate": 2.613750506006085e-06,
      "loss": 1.0381,
      "step": 8257
    },
    {
      "epoch": 77.17919245505452,
      "grad_norm": 0.5490641593933105,
      "learning_rate": 2.611710378320902e-06,
      "loss": 1.0585,
      "step": 8258
    },
    {
      "epoch": 77.1886236368995,
      "grad_norm": 0.5066468715667725,
      "learning_rate": 2.609670927548367e-06,
      "loss": 1.0532,
      "step": 8259
    },
    {
      "epoch": 77.19805481874447,
      "grad_norm": 0.5239428281784058,
      "learning_rate": 2.6076321538753347e-06,
      "loss": 1.0537,
      "step": 8260
    },
    {
      "epoch": 77.20748600058945,
      "grad_norm": 0.5479756593704224,
      "learning_rate": 2.6055940574885974e-06,
      "loss": 1.0561,
      "step": 8261
    },
    {
      "epoch": 77.21691718243443,
      "grad_norm": 0.52250075340271,
      "learning_rate": 2.6035566385748855e-06,
      "loss": 1.0671,
      "step": 8262
    },
    {
      "epoch": 77.2263483642794,
      "grad_norm": 0.5459886193275452,
      "learning_rate": 2.601519897320862e-06,
      "loss": 1.0314,
      "step": 8263
    },
    {
      "epoch": 77.23577954612438,
      "grad_norm": 0.5759149789810181,
      "learning_rate": 2.599483833913139e-06,
      "loss": 1.0498,
      "step": 8264
    },
    {
      "epoch": 77.24521072796935,
      "grad_norm": 0.5111475586891174,
      "learning_rate": 2.597448448538258e-06,
      "loss": 1.0867,
      "step": 8265
    },
    {
      "epoch": 77.25464190981432,
      "grad_norm": 0.5599597096443176,
      "learning_rate": 2.5954137413826983e-06,
      "loss": 0.994,
      "step": 8266
    },
    {
      "epoch": 77.2640730916593,
      "grad_norm": 0.557090163230896,
      "learning_rate": 2.5933797126328807e-06,
      "loss": 1.0355,
      "step": 8267
    },
    {
      "epoch": 77.27350427350427,
      "grad_norm": 0.5421702265739441,
      "learning_rate": 2.5913463624751587e-06,
      "loss": 1.0845,
      "step": 8268
    },
    {
      "epoch": 77.28293545534925,
      "grad_norm": 0.5587428212165833,
      "learning_rate": 2.589313691095834e-06,
      "loss": 1.0422,
      "step": 8269
    },
    {
      "epoch": 77.29236663719422,
      "grad_norm": 0.5173035860061646,
      "learning_rate": 2.5872816986811357e-06,
      "loss": 1.0532,
      "step": 8270
    },
    {
      "epoch": 77.3017978190392,
      "grad_norm": 0.5626363754272461,
      "learning_rate": 2.5852503854172327e-06,
      "loss": 1.0142,
      "step": 8271
    },
    {
      "epoch": 77.31122900088417,
      "grad_norm": 0.5661516785621643,
      "learning_rate": 2.583219751490236e-06,
      "loss": 1.0468,
      "step": 8272
    },
    {
      "epoch": 77.32066018272914,
      "grad_norm": 0.5736352205276489,
      "learning_rate": 2.5811897970861866e-06,
      "loss": 1.055,
      "step": 8273
    },
    {
      "epoch": 77.33009136457412,
      "grad_norm": 0.5141695737838745,
      "learning_rate": 2.5791605223910755e-06,
      "loss": 1.0195,
      "step": 8274
    },
    {
      "epoch": 77.3395225464191,
      "grad_norm": 0.5139238834381104,
      "learning_rate": 2.57713192759082e-06,
      "loss": 1.053,
      "step": 8275
    },
    {
      "epoch": 77.34895372826408,
      "grad_norm": 0.5452156662940979,
      "learning_rate": 2.57510401287128e-06,
      "loss": 1.0813,
      "step": 8276
    },
    {
      "epoch": 77.35838491010905,
      "grad_norm": 0.5577101707458496,
      "learning_rate": 2.573076778418252e-06,
      "loss": 1.0107,
      "step": 8277
    },
    {
      "epoch": 77.36781609195403,
      "grad_norm": 0.5478723645210266,
      "learning_rate": 2.5710502244174683e-06,
      "loss": 1.0522,
      "step": 8278
    },
    {
      "epoch": 77.377247273799,
      "grad_norm": 0.5596665143966675,
      "learning_rate": 2.5690243510546076e-06,
      "loss": 1.0334,
      "step": 8279
    },
    {
      "epoch": 77.38667845564397,
      "grad_norm": 0.5377812385559082,
      "learning_rate": 2.566999158515274e-06,
      "loss": 1.0463,
      "step": 8280
    },
    {
      "epoch": 77.39610963748895,
      "grad_norm": 0.5555818676948547,
      "learning_rate": 2.5649746469850177e-06,
      "loss": 1.0309,
      "step": 8281
    },
    {
      "epoch": 77.40554081933392,
      "grad_norm": 0.5341477990150452,
      "learning_rate": 2.562950816649322e-06,
      "loss": 1.0248,
      "step": 8282
    },
    {
      "epoch": 77.4149720011789,
      "grad_norm": 0.5408123731613159,
      "learning_rate": 2.560927667693611e-06,
      "loss": 1.0388,
      "step": 8283
    },
    {
      "epoch": 77.42440318302387,
      "grad_norm": 0.555345892906189,
      "learning_rate": 2.5589052003032454e-06,
      "loss": 0.9781,
      "step": 8284
    },
    {
      "epoch": 77.43383436486884,
      "grad_norm": 0.5431117415428162,
      "learning_rate": 2.55688341466352e-06,
      "loss": 1.0371,
      "step": 8285
    },
    {
      "epoch": 77.44326554671382,
      "grad_norm": 0.5073021650314331,
      "learning_rate": 2.5548623109596703e-06,
      "loss": 1.0535,
      "step": 8286
    },
    {
      "epoch": 77.45269672855879,
      "grad_norm": 0.5516090989112854,
      "learning_rate": 2.5528418893768735e-06,
      "loss": 1.0485,
      "step": 8287
    },
    {
      "epoch": 77.46212791040377,
      "grad_norm": 0.5761049389839172,
      "learning_rate": 2.5508221501002373e-06,
      "loss": 1.0479,
      "step": 8288
    },
    {
      "epoch": 77.47155909224875,
      "grad_norm": 0.5788651704788208,
      "learning_rate": 2.54880309331481e-06,
      "loss": 1.0817,
      "step": 8289
    },
    {
      "epoch": 77.48099027409373,
      "grad_norm": 0.5836065411567688,
      "learning_rate": 2.5467847192055762e-06,
      "loss": 1.0638,
      "step": 8290
    },
    {
      "epoch": 77.4904214559387,
      "grad_norm": 0.5608495473861694,
      "learning_rate": 2.5447670279574586e-06,
      "loss": 1.0359,
      "step": 8291
    },
    {
      "epoch": 77.49985263778368,
      "grad_norm": 0.5335595607757568,
      "learning_rate": 2.542750019755319e-06,
      "loss": 1.0644,
      "step": 8292
    },
    {
      "epoch": 77.50928381962865,
      "grad_norm": 0.5692837238311768,
      "learning_rate": 2.54073369478395e-06,
      "loss": 1.0734,
      "step": 8293
    },
    {
      "epoch": 77.51871500147362,
      "grad_norm": 0.5170053243637085,
      "learning_rate": 2.538718053228093e-06,
      "loss": 1.0599,
      "step": 8294
    },
    {
      "epoch": 77.5281461833186,
      "grad_norm": 0.5845197439193726,
      "learning_rate": 2.536703095272418e-06,
      "loss": 1.0446,
      "step": 8295
    },
    {
      "epoch": 77.53757736516357,
      "grad_norm": 0.5782924890518188,
      "learning_rate": 2.534688821101535e-06,
      "loss": 1.054,
      "step": 8296
    },
    {
      "epoch": 77.54700854700855,
      "grad_norm": 0.5693539381027222,
      "learning_rate": 2.532675230899991e-06,
      "loss": 1.0408,
      "step": 8297
    },
    {
      "epoch": 77.55643972885352,
      "grad_norm": 0.5189324021339417,
      "learning_rate": 2.53066232485227e-06,
      "loss": 1.0407,
      "step": 8298
    },
    {
      "epoch": 77.5658709106985,
      "grad_norm": 0.5544236302375793,
      "learning_rate": 2.5286501031427936e-06,
      "loss": 1.0455,
      "step": 8299
    },
    {
      "epoch": 77.57530209254347,
      "grad_norm": 0.5497087240219116,
      "learning_rate": 2.526638565955922e-06,
      "loss": 1.063,
      "step": 8300
    },
    {
      "epoch": 77.58473327438844,
      "grad_norm": 0.49751517176628113,
      "learning_rate": 2.524627713475948e-06,
      "loss": 1.0581,
      "step": 8301
    },
    {
      "epoch": 77.59416445623341,
      "grad_norm": 0.5881944894790649,
      "learning_rate": 2.5226175458871105e-06,
      "loss": 1.044,
      "step": 8302
    },
    {
      "epoch": 77.6035956380784,
      "grad_norm": 0.5357951521873474,
      "learning_rate": 2.5206080633735773e-06,
      "loss": 1.037,
      "step": 8303
    },
    {
      "epoch": 77.61302681992338,
      "grad_norm": 0.5675437450408936,
      "learning_rate": 2.518599266119458e-06,
      "loss": 1.0393,
      "step": 8304
    },
    {
      "epoch": 77.62245800176835,
      "grad_norm": 0.5325996279716492,
      "learning_rate": 2.5165911543087974e-06,
      "loss": 1.0692,
      "step": 8305
    },
    {
      "epoch": 77.63188918361332,
      "grad_norm": 0.5518708229064941,
      "learning_rate": 2.514583728125578e-06,
      "loss": 1.0576,
      "step": 8306
    },
    {
      "epoch": 77.6413203654583,
      "grad_norm": 0.5789770483970642,
      "learning_rate": 2.5125769877537187e-06,
      "loss": 1.0928,
      "step": 8307
    },
    {
      "epoch": 77.65075154730327,
      "grad_norm": 0.539787769317627,
      "learning_rate": 2.5105709333770746e-06,
      "loss": 1.0179,
      "step": 8308
    },
    {
      "epoch": 77.66018272914825,
      "grad_norm": 0.5511674880981445,
      "learning_rate": 2.508565565179445e-06,
      "loss": 1.0725,
      "step": 8309
    },
    {
      "epoch": 77.66961391099322,
      "grad_norm": 0.5706483721733093,
      "learning_rate": 2.5065608833445577e-06,
      "loss": 1.0335,
      "step": 8310
    },
    {
      "epoch": 77.6790450928382,
      "grad_norm": 0.5969970226287842,
      "learning_rate": 2.504556888056082e-06,
      "loss": 1.0239,
      "step": 8311
    },
    {
      "epoch": 77.68847627468317,
      "grad_norm": 0.5499646663665771,
      "learning_rate": 2.5025535794976232e-06,
      "loss": 1.0387,
      "step": 8312
    },
    {
      "epoch": 77.69790745652814,
      "grad_norm": 0.5715661644935608,
      "learning_rate": 2.500550957852722e-06,
      "loss": 1.07,
      "step": 8313
    },
    {
      "epoch": 77.70733863837312,
      "grad_norm": 0.541414737701416,
      "learning_rate": 2.4985490233048602e-06,
      "loss": 1.071,
      "step": 8314
    },
    {
      "epoch": 77.71676982021809,
      "grad_norm": 0.5625354051589966,
      "learning_rate": 2.4965477760374535e-06,
      "loss": 1.0523,
      "step": 8315
    },
    {
      "epoch": 77.72620100206306,
      "grad_norm": 0.5093523263931274,
      "learning_rate": 2.4945472162338524e-06,
      "loss": 1.0481,
      "step": 8316
    },
    {
      "epoch": 77.73563218390805,
      "grad_norm": 0.5528404116630554,
      "learning_rate": 2.492547344077354e-06,
      "loss": 1.0292,
      "step": 8317
    },
    {
      "epoch": 77.74506336575303,
      "grad_norm": 0.5341137051582336,
      "learning_rate": 2.4905481597511827e-06,
      "loss": 1.0623,
      "step": 8318
    },
    {
      "epoch": 77.754494547598,
      "grad_norm": 0.5307436585426331,
      "learning_rate": 2.488549663438502e-06,
      "loss": 1.0594,
      "step": 8319
    },
    {
      "epoch": 77.76392572944297,
      "grad_norm": 0.5487267374992371,
      "learning_rate": 2.486551855322414e-06,
      "loss": 1.0922,
      "step": 8320
    },
    {
      "epoch": 77.77335691128795,
      "grad_norm": 0.5629333853721619,
      "learning_rate": 2.484554735585959e-06,
      "loss": 1.0497,
      "step": 8321
    },
    {
      "epoch": 77.78278809313292,
      "grad_norm": 0.5483888387680054,
      "learning_rate": 2.4825583044121104e-06,
      "loss": 1.0556,
      "step": 8322
    },
    {
      "epoch": 77.7922192749779,
      "grad_norm": 0.5305334329605103,
      "learning_rate": 2.4805625619837793e-06,
      "loss": 1.0503,
      "step": 8323
    },
    {
      "epoch": 77.80165045682287,
      "grad_norm": 0.520882248878479,
      "learning_rate": 2.4785675084838178e-06,
      "loss": 1.0886,
      "step": 8324
    },
    {
      "epoch": 77.81108163866784,
      "grad_norm": 0.5337820053100586,
      "learning_rate": 2.4765731440950127e-06,
      "loss": 1.0443,
      "step": 8325
    },
    {
      "epoch": 77.82051282051282,
      "grad_norm": 0.5383878350257874,
      "learning_rate": 2.474579469000086e-06,
      "loss": 1.1141,
      "step": 8326
    },
    {
      "epoch": 77.82994400235779,
      "grad_norm": 0.5390275716781616,
      "learning_rate": 2.472586483381696e-06,
      "loss": 1.0541,
      "step": 8327
    },
    {
      "epoch": 77.83937518420277,
      "grad_norm": 0.5216785669326782,
      "learning_rate": 2.47059418742244e-06,
      "loss": 1.0646,
      "step": 8328
    },
    {
      "epoch": 77.84880636604774,
      "grad_norm": 0.5317500233650208,
      "learning_rate": 2.4686025813048496e-06,
      "loss": 1.0742,
      "step": 8329
    },
    {
      "epoch": 77.85823754789271,
      "grad_norm": 0.501886785030365,
      "learning_rate": 2.4666116652114005e-06,
      "loss": 1.0394,
      "step": 8330
    },
    {
      "epoch": 77.8676687297377,
      "grad_norm": 0.5282495617866516,
      "learning_rate": 2.4646214393244962e-06,
      "loss": 1.0679,
      "step": 8331
    },
    {
      "epoch": 77.87709991158268,
      "grad_norm": 0.5229548811912537,
      "learning_rate": 2.4626319038264803e-06,
      "loss": 1.029,
      "step": 8332
    },
    {
      "epoch": 77.88653109342765,
      "grad_norm": 0.5452056527137756,
      "learning_rate": 2.4606430588996353e-06,
      "loss": 1.0202,
      "step": 8333
    },
    {
      "epoch": 77.89596227527262,
      "grad_norm": 0.5585241913795471,
      "learning_rate": 2.4586549047261742e-06,
      "loss": 1.0189,
      "step": 8334
    },
    {
      "epoch": 77.9053934571176,
      "grad_norm": 0.5826184153556824,
      "learning_rate": 2.456667441488256e-06,
      "loss": 1.0083,
      "step": 8335
    },
    {
      "epoch": 77.91482463896257,
      "grad_norm": 0.5503359436988831,
      "learning_rate": 2.4546806693679702e-06,
      "loss": 1.0107,
      "step": 8336
    },
    {
      "epoch": 77.92425582080755,
      "grad_norm": 0.533324122428894,
      "learning_rate": 2.452694588547344e-06,
      "loss": 1.0845,
      "step": 8337
    },
    {
      "epoch": 77.93368700265252,
      "grad_norm": 0.5329372882843018,
      "learning_rate": 2.450709199208341e-06,
      "loss": 1.0028,
      "step": 8338
    },
    {
      "epoch": 77.9431181844975,
      "grad_norm": 0.5522831678390503,
      "learning_rate": 2.4487245015328585e-06,
      "loss": 1.0492,
      "step": 8339
    },
    {
      "epoch": 77.95254936634247,
      "grad_norm": 0.5373007655143738,
      "learning_rate": 2.4467404957027395e-06,
      "loss": 1.0716,
      "step": 8340
    },
    {
      "epoch": 77.96198054818744,
      "grad_norm": 0.5629615783691406,
      "learning_rate": 2.4447571818997572e-06,
      "loss": 1.0294,
      "step": 8341
    },
    {
      "epoch": 77.97141173003241,
      "grad_norm": 0.532496452331543,
      "learning_rate": 2.44277456030562e-06,
      "loss": 1.0306,
      "step": 8342
    },
    {
      "epoch": 77.98084291187739,
      "grad_norm": 0.5593845248222351,
      "learning_rate": 2.440792631101976e-06,
      "loss": 1.0373,
      "step": 8343
    },
    {
      "epoch": 77.99027409372236,
      "grad_norm": 0.5520005822181702,
      "learning_rate": 2.4388113944704085e-06,
      "loss": 1.0452,
      "step": 8344
    },
    {
      "epoch": 77.99970527556735,
      "grad_norm": 0.5733890533447266,
      "learning_rate": 2.4368308505924377e-06,
      "loss": 1.0217,
      "step": 8345
    },
    {
      "epoch": 78.0,
      "grad_norm": 3.346217393875122,
      "learning_rate": 2.434850999649517e-06,
      "loss": 0.7792,
      "step": 8346
    },
    {
      "epoch": 78.00943118184497,
      "grad_norm": 0.5443052053451538,
      "learning_rate": 2.432871841823047e-06,
      "loss": 1.0253,
      "step": 8347
    },
    {
      "epoch": 78.01886236368995,
      "grad_norm": 0.5763054490089417,
      "learning_rate": 2.430893377294353e-06,
      "loss": 1.0694,
      "step": 8348
    },
    {
      "epoch": 78.02829354553492,
      "grad_norm": 0.5590745210647583,
      "learning_rate": 2.428915606244703e-06,
      "loss": 1.0822,
      "step": 8349
    },
    {
      "epoch": 78.0377247273799,
      "grad_norm": 0.5696324110031128,
      "learning_rate": 2.4269385288552983e-06,
      "loss": 0.9825,
      "step": 8350
    },
    {
      "epoch": 78.04715590922487,
      "grad_norm": 0.5357955098152161,
      "learning_rate": 2.424962145307278e-06,
      "loss": 1.0132,
      "step": 8351
    },
    {
      "epoch": 78.05658709106984,
      "grad_norm": 0.5390240550041199,
      "learning_rate": 2.4229864557817183e-06,
      "loss": 1.0569,
      "step": 8352
    },
    {
      "epoch": 78.06601827291482,
      "grad_norm": 0.5322936177253723,
      "learning_rate": 2.421011460459628e-06,
      "loss": 1.0769,
      "step": 8353
    },
    {
      "epoch": 78.0754494547598,
      "grad_norm": 0.5578362345695496,
      "learning_rate": 2.419037159521962e-06,
      "loss": 1.0373,
      "step": 8354
    },
    {
      "epoch": 78.08488063660478,
      "grad_norm": 0.5269730091094971,
      "learning_rate": 2.4170635531496025e-06,
      "loss": 1.0203,
      "step": 8355
    },
    {
      "epoch": 78.09431181844975,
      "grad_norm": 0.5304335951805115,
      "learning_rate": 2.415090641523368e-06,
      "loss": 1.063,
      "step": 8356
    },
    {
      "epoch": 78.10374300029473,
      "grad_norm": 0.6077707409858704,
      "learning_rate": 2.4131184248240192e-06,
      "loss": 1.0837,
      "step": 8357
    },
    {
      "epoch": 78.1131741821397,
      "grad_norm": 0.5415675044059753,
      "learning_rate": 2.4111469032322476e-06,
      "loss": 1.0244,
      "step": 8358
    },
    {
      "epoch": 78.12260536398468,
      "grad_norm": 0.5476002097129822,
      "learning_rate": 2.4091760769286854e-06,
      "loss": 1.0726,
      "step": 8359
    },
    {
      "epoch": 78.13203654582965,
      "grad_norm": 0.5191022157669067,
      "learning_rate": 2.407205946093898e-06,
      "loss": 1.0438,
      "step": 8360
    },
    {
      "epoch": 78.14146772767462,
      "grad_norm": 0.525506854057312,
      "learning_rate": 2.4052365109083843e-06,
      "loss": 1.0229,
      "step": 8361
    },
    {
      "epoch": 78.1508989095196,
      "grad_norm": 0.5388500690460205,
      "learning_rate": 2.4032677715525908e-06,
      "loss": 1.0084,
      "step": 8362
    },
    {
      "epoch": 78.16033009136457,
      "grad_norm": 0.5596141219139099,
      "learning_rate": 2.4012997282068896e-06,
      "loss": 1.0479,
      "step": 8363
    },
    {
      "epoch": 78.16976127320955,
      "grad_norm": 0.5597156882286072,
      "learning_rate": 2.399332381051591e-06,
      "loss": 1.0323,
      "step": 8364
    },
    {
      "epoch": 78.17919245505452,
      "grad_norm": 0.5278707146644592,
      "learning_rate": 2.397365730266943e-06,
      "loss": 1.0646,
      "step": 8365
    },
    {
      "epoch": 78.1886236368995,
      "grad_norm": 0.5166046023368835,
      "learning_rate": 2.3953997760331304e-06,
      "loss": 1.0444,
      "step": 8366
    },
    {
      "epoch": 78.19805481874447,
      "grad_norm": 0.5789602398872375,
      "learning_rate": 2.393434518530273e-06,
      "loss": 1.0733,
      "step": 8367
    },
    {
      "epoch": 78.20748600058945,
      "grad_norm": 0.5406318306922913,
      "learning_rate": 2.3914699579384236e-06,
      "loss": 1.0487,
      "step": 8368
    },
    {
      "epoch": 78.21691718243443,
      "grad_norm": 0.582940399646759,
      "learning_rate": 2.3895060944375815e-06,
      "loss": 1.0333,
      "step": 8369
    },
    {
      "epoch": 78.2263483642794,
      "grad_norm": 0.5636154413223267,
      "learning_rate": 2.38754292820767e-06,
      "loss": 1.0258,
      "step": 8370
    },
    {
      "epoch": 78.23577954612438,
      "grad_norm": 0.5300270318984985,
      "learning_rate": 2.385580459428556e-06,
      "loss": 1.0572,
      "step": 8371
    },
    {
      "epoch": 78.24521072796935,
      "grad_norm": 0.5659538507461548,
      "learning_rate": 2.3836186882800392e-06,
      "loss": 1.0337,
      "step": 8372
    },
    {
      "epoch": 78.25464190981432,
      "grad_norm": 0.5642776489257812,
      "learning_rate": 2.381657614941858e-06,
      "loss": 1.0675,
      "step": 8373
    },
    {
      "epoch": 78.2640730916593,
      "grad_norm": 0.5110804438591003,
      "learning_rate": 2.3796972395936825e-06,
      "loss": 1.0798,
      "step": 8374
    },
    {
      "epoch": 78.27350427350427,
      "grad_norm": 0.5918006300926208,
      "learning_rate": 2.3777375624151243e-06,
      "loss": 1.0424,
      "step": 8375
    },
    {
      "epoch": 78.28293545534925,
      "grad_norm": 0.6046363711357117,
      "learning_rate": 2.375778583585724e-06,
      "loss": 1.0362,
      "step": 8376
    },
    {
      "epoch": 78.29236663719422,
      "grad_norm": 0.5743514895439148,
      "learning_rate": 2.3738203032849682e-06,
      "loss": 1.0356,
      "step": 8377
    },
    {
      "epoch": 78.3017978190392,
      "grad_norm": 0.5477096438407898,
      "learning_rate": 2.371862721692272e-06,
      "loss": 1.0034,
      "step": 8378
    },
    {
      "epoch": 78.31122900088417,
      "grad_norm": 0.5768871307373047,
      "learning_rate": 2.3699058389869867e-06,
      "loss": 1.0502,
      "step": 8379
    },
    {
      "epoch": 78.32066018272914,
      "grad_norm": 0.5609411597251892,
      "learning_rate": 2.3679496553484038e-06,
      "loss": 1.0593,
      "step": 8380
    },
    {
      "epoch": 78.33009136457412,
      "grad_norm": 0.5975347757339478,
      "learning_rate": 2.3659941709557468e-06,
      "loss": 1.0215,
      "step": 8381
    },
    {
      "epoch": 78.3395225464191,
      "grad_norm": 0.5313554406166077,
      "learning_rate": 2.364039385988176e-06,
      "loss": 1.084,
      "step": 8382
    },
    {
      "epoch": 78.34895372826408,
      "grad_norm": 0.5403364300727844,
      "learning_rate": 2.3620853006247857e-06,
      "loss": 1.0206,
      "step": 8383
    },
    {
      "epoch": 78.35838491010905,
      "grad_norm": 0.5693466067314148,
      "learning_rate": 2.3601319150446144e-06,
      "loss": 1.0557,
      "step": 8384
    },
    {
      "epoch": 78.36781609195403,
      "grad_norm": 0.5520793795585632,
      "learning_rate": 2.3581792294266294e-06,
      "loss": 1.053,
      "step": 8385
    },
    {
      "epoch": 78.377247273799,
      "grad_norm": 0.5053099393844604,
      "learning_rate": 2.3562272439497313e-06,
      "loss": 1.0084,
      "step": 8386
    },
    {
      "epoch": 78.38667845564397,
      "grad_norm": 0.5418437123298645,
      "learning_rate": 2.3542759587927643e-06,
      "loss": 1.0613,
      "step": 8387
    },
    {
      "epoch": 78.39610963748895,
      "grad_norm": 0.5750175714492798,
      "learning_rate": 2.352325374134502e-06,
      "loss": 1.0552,
      "step": 8388
    },
    {
      "epoch": 78.40554081933392,
      "grad_norm": 0.6006349325180054,
      "learning_rate": 2.3503754901536567e-06,
      "loss": 1.0622,
      "step": 8389
    },
    {
      "epoch": 78.4149720011789,
      "grad_norm": 0.563897967338562,
      "learning_rate": 2.348426307028875e-06,
      "loss": 0.9991,
      "step": 8390
    },
    {
      "epoch": 78.42440318302387,
      "grad_norm": 0.5537317395210266,
      "learning_rate": 2.3464778249387432e-06,
      "loss": 1.0651,
      "step": 8391
    },
    {
      "epoch": 78.43383436486884,
      "grad_norm": 0.5243322253227234,
      "learning_rate": 2.3445300440617804e-06,
      "loss": 1.0716,
      "step": 8392
    },
    {
      "epoch": 78.44326554671382,
      "grad_norm": 0.5333325862884521,
      "learning_rate": 2.3425829645764397e-06,
      "loss": 1.046,
      "step": 8393
    },
    {
      "epoch": 78.45269672855879,
      "grad_norm": 0.5653961896896362,
      "learning_rate": 2.3406365866611135e-06,
      "loss": 1.0674,
      "step": 8394
    },
    {
      "epoch": 78.46212791040377,
      "grad_norm": 0.5642959475517273,
      "learning_rate": 2.338690910494128e-06,
      "loss": 1.0353,
      "step": 8395
    },
    {
      "epoch": 78.47155909224875,
      "grad_norm": 0.552157998085022,
      "learning_rate": 2.3367459362537416e-06,
      "loss": 1.0595,
      "step": 8396
    },
    {
      "epoch": 78.48099027409373,
      "grad_norm": 0.5729328393936157,
      "learning_rate": 2.334801664118157e-06,
      "loss": 1.0503,
      "step": 8397
    },
    {
      "epoch": 78.4904214559387,
      "grad_norm": 0.5381242036819458,
      "learning_rate": 2.3328580942655075e-06,
      "loss": 1.0865,
      "step": 8398
    },
    {
      "epoch": 78.49985263778368,
      "grad_norm": 0.5110806226730347,
      "learning_rate": 2.3309152268738622e-06,
      "loss": 1.0944,
      "step": 8399
    },
    {
      "epoch": 78.50928381962865,
      "grad_norm": 0.5413131713867188,
      "learning_rate": 2.3289730621212235e-06,
      "loss": 1.0592,
      "step": 8400
    },
    {
      "epoch": 78.51871500147362,
      "grad_norm": 0.5281133055686951,
      "learning_rate": 2.327031600185531e-06,
      "loss": 1.057,
      "step": 8401
    },
    {
      "epoch": 78.5281461833186,
      "grad_norm": 0.560480535030365,
      "learning_rate": 2.325090841244666e-06,
      "loss": 1.0378,
      "step": 8402
    },
    {
      "epoch": 78.53757736516357,
      "grad_norm": 0.5074587464332581,
      "learning_rate": 2.3231507854764367e-06,
      "loss": 1.0281,
      "step": 8403
    },
    {
      "epoch": 78.54700854700855,
      "grad_norm": 0.5626989603042603,
      "learning_rate": 2.321211433058591e-06,
      "loss": 1.0622,
      "step": 8404
    },
    {
      "epoch": 78.55643972885352,
      "grad_norm": 0.5403265357017517,
      "learning_rate": 2.319272784168809e-06,
      "loss": 1.0561,
      "step": 8405
    },
    {
      "epoch": 78.5658709106985,
      "grad_norm": 0.524481475353241,
      "learning_rate": 2.317334838984715e-06,
      "loss": 1.0288,
      "step": 8406
    },
    {
      "epoch": 78.57530209254347,
      "grad_norm": 0.5287274718284607,
      "learning_rate": 2.315397597683859e-06,
      "loss": 1.0585,
      "step": 8407
    },
    {
      "epoch": 78.58473327438844,
      "grad_norm": 0.5294715166091919,
      "learning_rate": 2.3134610604437323e-06,
      "loss": 1.0506,
      "step": 8408
    },
    {
      "epoch": 78.59416445623341,
      "grad_norm": 0.542514979839325,
      "learning_rate": 2.3115252274417577e-06,
      "loss": 1.0042,
      "step": 8409
    },
    {
      "epoch": 78.6035956380784,
      "grad_norm": 0.5325788259506226,
      "learning_rate": 2.3095900988552968e-06,
      "loss": 1.0536,
      "step": 8410
    },
    {
      "epoch": 78.61302681992338,
      "grad_norm": 0.5464385151863098,
      "learning_rate": 2.307655674861644e-06,
      "loss": 1.0394,
      "step": 8411
    },
    {
      "epoch": 78.62245800176835,
      "grad_norm": 0.5407161116600037,
      "learning_rate": 2.305721955638033e-06,
      "loss": 1.0633,
      "step": 8412
    },
    {
      "epoch": 78.63188918361332,
      "grad_norm": 0.5398411154747009,
      "learning_rate": 2.3037889413616253e-06,
      "loss": 1.052,
      "step": 8413
    },
    {
      "epoch": 78.6413203654583,
      "grad_norm": 0.5744861364364624,
      "learning_rate": 2.3018566322095303e-06,
      "loss": 1.0423,
      "step": 8414
    },
    {
      "epoch": 78.65075154730327,
      "grad_norm": 0.6018922328948975,
      "learning_rate": 2.2999250283587828e-06,
      "loss": 1.0341,
      "step": 8415
    },
    {
      "epoch": 78.66018272914825,
      "grad_norm": 0.5618491768836975,
      "learning_rate": 2.297994129986354e-06,
      "loss": 1.0598,
      "step": 8416
    },
    {
      "epoch": 78.66961391099322,
      "grad_norm": 0.5615059733390808,
      "learning_rate": 2.296063937269153e-06,
      "loss": 1.0359,
      "step": 8417
    },
    {
      "epoch": 78.6790450928382,
      "grad_norm": 0.5352426171302795,
      "learning_rate": 2.2941344503840244e-06,
      "loss": 1.0321,
      "step": 8418
    },
    {
      "epoch": 78.68847627468317,
      "grad_norm": 0.5589780807495117,
      "learning_rate": 2.2922056695077454e-06,
      "loss": 1.0713,
      "step": 8419
    },
    {
      "epoch": 78.69790745652814,
      "grad_norm": 0.5299034714698792,
      "learning_rate": 2.2902775948170295e-06,
      "loss": 1.0361,
      "step": 8420
    },
    {
      "epoch": 78.70733863837312,
      "grad_norm": 0.5265281200408936,
      "learning_rate": 2.2883502264885305e-06,
      "loss": 1.0937,
      "step": 8421
    },
    {
      "epoch": 78.71676982021809,
      "grad_norm": 0.5525676012039185,
      "learning_rate": 2.2864235646988307e-06,
      "loss": 1.0924,
      "step": 8422
    },
    {
      "epoch": 78.72620100206306,
      "grad_norm": 0.5393883585929871,
      "learning_rate": 2.2844976096244496e-06,
      "loss": 1.0673,
      "step": 8423
    },
    {
      "epoch": 78.73563218390805,
      "grad_norm": 0.563056468963623,
      "learning_rate": 2.282572361441845e-06,
      "loss": 1.0559,
      "step": 8424
    },
    {
      "epoch": 78.74506336575303,
      "grad_norm": 0.5358297228813171,
      "learning_rate": 2.280647820327404e-06,
      "loss": 1.0207,
      "step": 8425
    },
    {
      "epoch": 78.754494547598,
      "grad_norm": 0.5481724739074707,
      "learning_rate": 2.2787239864574553e-06,
      "loss": 1.0577,
      "step": 8426
    },
    {
      "epoch": 78.76392572944297,
      "grad_norm": 0.5366855263710022,
      "learning_rate": 2.2768008600082583e-06,
      "loss": 1.0874,
      "step": 8427
    },
    {
      "epoch": 78.77335691128795,
      "grad_norm": 0.5492004752159119,
      "learning_rate": 2.2748784411560066e-06,
      "loss": 1.0665,
      "step": 8428
    },
    {
      "epoch": 78.78278809313292,
      "grad_norm": 0.5352981686592102,
      "learning_rate": 2.2729567300768374e-06,
      "loss": 1.0383,
      "step": 8429
    },
    {
      "epoch": 78.7922192749779,
      "grad_norm": 0.5651869177818298,
      "learning_rate": 2.2710357269468153e-06,
      "loss": 1.0306,
      "step": 8430
    },
    {
      "epoch": 78.80165045682287,
      "grad_norm": 0.5371221899986267,
      "learning_rate": 2.2691154319419406e-06,
      "loss": 1.0693,
      "step": 8431
    },
    {
      "epoch": 78.81108163866784,
      "grad_norm": 0.5200662612915039,
      "learning_rate": 2.2671958452381502e-06,
      "loss": 1.0905,
      "step": 8432
    },
    {
      "epoch": 78.82051282051282,
      "grad_norm": 0.6127660274505615,
      "learning_rate": 2.2652769670113173e-06,
      "loss": 1.0185,
      "step": 8433
    },
    {
      "epoch": 78.82994400235779,
      "grad_norm": 0.5681795477867126,
      "learning_rate": 2.2633587974372474e-06,
      "loss": 1.0397,
      "step": 8434
    },
    {
      "epoch": 78.83937518420277,
      "grad_norm": 0.5612042546272278,
      "learning_rate": 2.261441336691681e-06,
      "loss": 1.063,
      "step": 8435
    },
    {
      "epoch": 78.84880636604774,
      "grad_norm": 0.5713716745376587,
      "learning_rate": 2.2595245849503e-06,
      "loss": 1.0772,
      "step": 8436
    },
    {
      "epoch": 78.85823754789271,
      "grad_norm": 0.5435604453086853,
      "learning_rate": 2.257608542388714e-06,
      "loss": 1.057,
      "step": 8437
    },
    {
      "epoch": 78.8676687297377,
      "grad_norm": 0.537713348865509,
      "learning_rate": 2.25569320918247e-06,
      "loss": 1.0595,
      "step": 8438
    },
    {
      "epoch": 78.87709991158268,
      "grad_norm": 0.524662435054779,
      "learning_rate": 2.2537785855070505e-06,
      "loss": 1.0467,
      "step": 8439
    },
    {
      "epoch": 78.88653109342765,
      "grad_norm": 0.5457783341407776,
      "learning_rate": 2.251864671537872e-06,
      "loss": 1.0435,
      "step": 8440
    },
    {
      "epoch": 78.89596227527262,
      "grad_norm": 0.5217299461364746,
      "learning_rate": 2.2499514674502877e-06,
      "loss": 1.0894,
      "step": 8441
    },
    {
      "epoch": 78.9053934571176,
      "grad_norm": 0.527605414390564,
      "learning_rate": 2.248038973419585e-06,
      "loss": 1.0643,
      "step": 8442
    },
    {
      "epoch": 78.91482463896257,
      "grad_norm": 0.5272256731987,
      "learning_rate": 2.2461271896209813e-06,
      "loss": 1.0648,
      "step": 8443
    },
    {
      "epoch": 78.92425582080755,
      "grad_norm": 0.5578391551971436,
      "learning_rate": 2.2442161162296404e-06,
      "loss": 1.0117,
      "step": 8444
    },
    {
      "epoch": 78.93368700265252,
      "grad_norm": 0.520653247833252,
      "learning_rate": 2.2423057534206515e-06,
      "loss": 1.0572,
      "step": 8445
    },
    {
      "epoch": 78.9431181844975,
      "grad_norm": 0.5925711989402771,
      "learning_rate": 2.2403961013690424e-06,
      "loss": 1.0399,
      "step": 8446
    },
    {
      "epoch": 78.95254936634247,
      "grad_norm": 0.5165755152702332,
      "learning_rate": 2.2384871602497716e-06,
      "loss": 1.0464,
      "step": 8447
    },
    {
      "epoch": 78.96198054818744,
      "grad_norm": 0.5833275318145752,
      "learning_rate": 2.2365789302377396e-06,
      "loss": 0.9996,
      "step": 8448
    },
    {
      "epoch": 78.97141173003241,
      "grad_norm": 0.5573645234107971,
      "learning_rate": 2.234671411507774e-06,
      "loss": 1.0305,
      "step": 8449
    },
    {
      "epoch": 78.98084291187739,
      "grad_norm": 0.542565643787384,
      "learning_rate": 2.2327646042346416e-06,
      "loss": 1.0545,
      "step": 8450
    },
    {
      "epoch": 78.99027409372236,
      "grad_norm": 0.5239542722702026,
      "learning_rate": 2.2308585085930467e-06,
      "loss": 1.0215,
      "step": 8451
    },
    {
      "epoch": 78.99970527556735,
      "grad_norm": 0.52766352891922,
      "learning_rate": 2.2289531247576235e-06,
      "loss": 1.0863,
      "step": 8452
    },
    {
      "epoch": 79.0,
      "grad_norm": 4.009427547454834,
      "learning_rate": 2.2270484529029422e-06,
      "loss": 0.6614,
      "step": 8453
    },
    {
      "epoch": 79.00943118184497,
      "grad_norm": 0.5429513454437256,
      "learning_rate": 2.2251444932035094e-06,
      "loss": 1.0592,
      "step": 8454
    },
    {
      "epoch": 79.01886236368995,
      "grad_norm": 0.5532210469245911,
      "learning_rate": 2.2232412458337627e-06,
      "loss": 1.0659,
      "step": 8455
    },
    {
      "epoch": 79.02829354553492,
      "grad_norm": 0.5465640425682068,
      "learning_rate": 2.2213387109680797e-06,
      "loss": 1.0485,
      "step": 8456
    },
    {
      "epoch": 79.0377247273799,
      "grad_norm": 0.5610669851303101,
      "learning_rate": 2.2194368887807648e-06,
      "loss": 1.0885,
      "step": 8457
    },
    {
      "epoch": 79.04715590922487,
      "grad_norm": 0.5236106514930725,
      "learning_rate": 2.2175357794460694e-06,
      "loss": 1.0244,
      "step": 8458
    },
    {
      "epoch": 79.05658709106984,
      "grad_norm": 0.5241447687149048,
      "learning_rate": 2.2156353831381707e-06,
      "loss": 1.0328,
      "step": 8459
    },
    {
      "epoch": 79.06601827291482,
      "grad_norm": 0.5530127286911011,
      "learning_rate": 2.2137357000311797e-06,
      "loss": 1.0357,
      "step": 8460
    },
    {
      "epoch": 79.0754494547598,
      "grad_norm": 0.5560042858123779,
      "learning_rate": 2.2118367302991472e-06,
      "loss": 1.075,
      "step": 8461
    },
    {
      "epoch": 79.08488063660478,
      "grad_norm": 0.5155770778656006,
      "learning_rate": 2.2099384741160514e-06,
      "loss": 1.0554,
      "step": 8462
    },
    {
      "epoch": 79.09431181844975,
      "grad_norm": 0.5468617081642151,
      "learning_rate": 2.208040931655817e-06,
      "loss": 1.0502,
      "step": 8463
    },
    {
      "epoch": 79.10374300029473,
      "grad_norm": 0.5683478713035583,
      "learning_rate": 2.2061441030922926e-06,
      "loss": 1.0396,
      "step": 8464
    },
    {
      "epoch": 79.1131741821397,
      "grad_norm": 0.5649660229682922,
      "learning_rate": 2.204247988599266e-06,
      "loss": 1.0294,
      "step": 8465
    },
    {
      "epoch": 79.12260536398468,
      "grad_norm": 0.5492601990699768,
      "learning_rate": 2.202352588350457e-06,
      "loss": 1.0592,
      "step": 8466
    },
    {
      "epoch": 79.13203654582965,
      "grad_norm": 0.557654619216919,
      "learning_rate": 2.2004579025195204e-06,
      "loss": 1.0194,
      "step": 8467
    },
    {
      "epoch": 79.14146772767462,
      "grad_norm": 0.5930455327033997,
      "learning_rate": 2.1985639312800513e-06,
      "loss": 1.091,
      "step": 8468
    },
    {
      "epoch": 79.1508989095196,
      "grad_norm": 0.5287896394729614,
      "learning_rate": 2.196670674805571e-06,
      "loss": 1.0491,
      "step": 8469
    },
    {
      "epoch": 79.16033009136457,
      "grad_norm": 0.5601893067359924,
      "learning_rate": 2.1947781332695406e-06,
      "loss": 1.0359,
      "step": 8470
    },
    {
      "epoch": 79.16976127320955,
      "grad_norm": 0.5494599342346191,
      "learning_rate": 2.1928863068453544e-06,
      "loss": 1.0737,
      "step": 8471
    },
    {
      "epoch": 79.17919245505452,
      "grad_norm": 0.5520455241203308,
      "learning_rate": 2.190995195706339e-06,
      "loss": 1.0723,
      "step": 8472
    },
    {
      "epoch": 79.1886236368995,
      "grad_norm": 0.5809375643730164,
      "learning_rate": 2.1891048000257566e-06,
      "loss": 1.0281,
      "step": 8473
    },
    {
      "epoch": 79.19805481874447,
      "grad_norm": 0.5860055685043335,
      "learning_rate": 2.1872151199768097e-06,
      "loss": 1.0437,
      "step": 8474
    },
    {
      "epoch": 79.20748600058945,
      "grad_norm": 0.5094242691993713,
      "learning_rate": 2.1853261557326265e-06,
      "loss": 1.0588,
      "step": 8475
    },
    {
      "epoch": 79.21691718243443,
      "grad_norm": 0.5414355993270874,
      "learning_rate": 2.1834379074662735e-06,
      "loss": 1.034,
      "step": 8476
    },
    {
      "epoch": 79.2263483642794,
      "grad_norm": 0.5628615021705627,
      "learning_rate": 2.1815503753507526e-06,
      "loss": 1.024,
      "step": 8477
    },
    {
      "epoch": 79.23577954612438,
      "grad_norm": 0.5646776556968689,
      "learning_rate": 2.1796635595589966e-06,
      "loss": 1.0708,
      "step": 8478
    },
    {
      "epoch": 79.24521072796935,
      "grad_norm": 0.5923069715499878,
      "learning_rate": 2.177777460263878e-06,
      "loss": 1.057,
      "step": 8479
    },
    {
      "epoch": 79.25464190981432,
      "grad_norm": 0.5180016756057739,
      "learning_rate": 2.175892077638194e-06,
      "loss": 1.0424,
      "step": 8480
    },
    {
      "epoch": 79.2640730916593,
      "grad_norm": 0.5443366169929504,
      "learning_rate": 2.174007411854693e-06,
      "loss": 1.0326,
      "step": 8481
    },
    {
      "epoch": 79.27350427350427,
      "grad_norm": 0.5612993836402893,
      "learning_rate": 2.17212346308604e-06,
      "loss": 1.0717,
      "step": 8482
    },
    {
      "epoch": 79.28293545534925,
      "grad_norm": 0.5453026294708252,
      "learning_rate": 2.1702402315048443e-06,
      "loss": 1.0425,
      "step": 8483
    },
    {
      "epoch": 79.29236663719422,
      "grad_norm": 0.5411946177482605,
      "learning_rate": 2.168357717283647e-06,
      "loss": 1.0701,
      "step": 8484
    },
    {
      "epoch": 79.3017978190392,
      "grad_norm": 0.5953702330589294,
      "learning_rate": 2.1664759205949227e-06,
      "loss": 1.0451,
      "step": 8485
    },
    {
      "epoch": 79.31122900088417,
      "grad_norm": 0.5131298899650574,
      "learning_rate": 2.1645948416110807e-06,
      "loss": 1.0412,
      "step": 8486
    },
    {
      "epoch": 79.32066018272914,
      "grad_norm": 0.5622216463088989,
      "learning_rate": 2.1627144805044662e-06,
      "loss": 1.0519,
      "step": 8487
    },
    {
      "epoch": 79.33009136457412,
      "grad_norm": 0.5179750919342041,
      "learning_rate": 2.160834837447353e-06,
      "loss": 1.0279,
      "step": 8488
    },
    {
      "epoch": 79.3395225464191,
      "grad_norm": 0.5699694752693176,
      "learning_rate": 2.15895591261196e-06,
      "loss": 1.0383,
      "step": 8489
    },
    {
      "epoch": 79.34895372826408,
      "grad_norm": 0.5440118908882141,
      "learning_rate": 2.1570777061704297e-06,
      "loss": 1.0758,
      "step": 8490
    },
    {
      "epoch": 79.35838491010905,
      "grad_norm": 0.5606326460838318,
      "learning_rate": 2.155200218294844e-06,
      "loss": 1.0153,
      "step": 8491
    },
    {
      "epoch": 79.36781609195403,
      "grad_norm": 0.5807044506072998,
      "learning_rate": 2.1533234491572174e-06,
      "loss": 1.0166,
      "step": 8492
    },
    {
      "epoch": 79.377247273799,
      "grad_norm": 0.5159506797790527,
      "learning_rate": 2.151447398929498e-06,
      "loss": 1.0871,
      "step": 8493
    },
    {
      "epoch": 79.38667845564397,
      "grad_norm": 0.5207033157348633,
      "learning_rate": 2.149572067783571e-06,
      "loss": 1.0671,
      "step": 8494
    },
    {
      "epoch": 79.39610963748895,
      "grad_norm": 0.5600839853286743,
      "learning_rate": 2.1476974558912476e-06,
      "loss": 1.065,
      "step": 8495
    },
    {
      "epoch": 79.40554081933392,
      "grad_norm": 0.5234832763671875,
      "learning_rate": 2.1458235634242884e-06,
      "loss": 1.0865,
      "step": 8496
    },
    {
      "epoch": 79.4149720011789,
      "grad_norm": 0.549227774143219,
      "learning_rate": 2.1439503905543724e-06,
      "loss": 1.047,
      "step": 8497
    },
    {
      "epoch": 79.42440318302387,
      "grad_norm": 0.5654042959213257,
      "learning_rate": 2.142077937453122e-06,
      "loss": 1.0259,
      "step": 8498
    },
    {
      "epoch": 79.43383436486884,
      "grad_norm": 0.5159164071083069,
      "learning_rate": 2.140206204292089e-06,
      "loss": 1.0573,
      "step": 8499
    },
    {
      "epoch": 79.44326554671382,
      "grad_norm": 0.5296075344085693,
      "learning_rate": 2.138335191242762e-06,
      "loss": 1.0322,
      "step": 8500
    },
    {
      "epoch": 79.45269672855879,
      "grad_norm": 0.5326284170150757,
      "learning_rate": 2.136464898476561e-06,
      "loss": 1.0981,
      "step": 8501
    },
    {
      "epoch": 79.46212791040377,
      "grad_norm": 0.5781298875808716,
      "learning_rate": 2.134595326164841e-06,
      "loss": 1.0748,
      "step": 8502
    },
    {
      "epoch": 79.47155909224875,
      "grad_norm": 0.5173018574714661,
      "learning_rate": 2.132726474478897e-06,
      "loss": 1.0731,
      "step": 8503
    },
    {
      "epoch": 79.48099027409373,
      "grad_norm": 0.5483432412147522,
      "learning_rate": 2.1308583435899466e-06,
      "loss": 1.0575,
      "step": 8504
    },
    {
      "epoch": 79.4904214559387,
      "grad_norm": 0.5386296510696411,
      "learning_rate": 2.1289909336691517e-06,
      "loss": 1.08,
      "step": 8505
    },
    {
      "epoch": 79.49985263778368,
      "grad_norm": 0.5335248112678528,
      "learning_rate": 2.1271242448876005e-06,
      "loss": 1.052,
      "step": 8506
    },
    {
      "epoch": 79.50928381962865,
      "grad_norm": 0.5580503344535828,
      "learning_rate": 2.12525827741632e-06,
      "loss": 1.0244,
      "step": 8507
    },
    {
      "epoch": 79.51871500147362,
      "grad_norm": 0.519888699054718,
      "learning_rate": 2.1233930314262686e-06,
      "loss": 1.0305,
      "step": 8508
    },
    {
      "epoch": 79.5281461833186,
      "grad_norm": 0.6245010495185852,
      "learning_rate": 2.1215285070883406e-06,
      "loss": 1.0946,
      "step": 8509
    },
    {
      "epoch": 79.53757736516357,
      "grad_norm": 0.529549777507782,
      "learning_rate": 2.1196647045733597e-06,
      "loss": 1.0448,
      "step": 8510
    },
    {
      "epoch": 79.54700854700855,
      "grad_norm": 0.49426019191741943,
      "learning_rate": 2.1178016240520928e-06,
      "loss": 1.0439,
      "step": 8511
    },
    {
      "epoch": 79.55643972885352,
      "grad_norm": 0.5889077186584473,
      "learning_rate": 2.1159392656952306e-06,
      "loss": 1.0462,
      "step": 8512
    },
    {
      "epoch": 79.5658709106985,
      "grad_norm": 0.5400094985961914,
      "learning_rate": 2.1140776296734033e-06,
      "loss": 1.0385,
      "step": 8513
    },
    {
      "epoch": 79.57530209254347,
      "grad_norm": 0.5591973662376404,
      "learning_rate": 2.1122167161571737e-06,
      "loss": 1.0162,
      "step": 8514
    },
    {
      "epoch": 79.58473327438844,
      "grad_norm": 0.5352159142494202,
      "learning_rate": 2.1103565253170367e-06,
      "loss": 1.0353,
      "step": 8515
    },
    {
      "epoch": 79.59416445623341,
      "grad_norm": 0.5845668911933899,
      "learning_rate": 2.108497057323422e-06,
      "loss": 1.054,
      "step": 8516
    },
    {
      "epoch": 79.6035956380784,
      "grad_norm": 0.5722494125366211,
      "learning_rate": 2.1066383123466926e-06,
      "loss": 1.0159,
      "step": 8517
    },
    {
      "epoch": 79.61302681992338,
      "grad_norm": 0.5231354236602783,
      "learning_rate": 2.104780290557151e-06,
      "loss": 1.0565,
      "step": 8518
    },
    {
      "epoch": 79.62245800176835,
      "grad_norm": 0.5916172862052917,
      "learning_rate": 2.102922992125025e-06,
      "loss": 1.072,
      "step": 8519
    },
    {
      "epoch": 79.63188918361332,
      "grad_norm": 0.5563530921936035,
      "learning_rate": 2.10106641722048e-06,
      "loss": 1.0512,
      "step": 8520
    },
    {
      "epoch": 79.6413203654583,
      "grad_norm": 0.5433448553085327,
      "learning_rate": 2.099210566013614e-06,
      "loss": 1.0739,
      "step": 8521
    },
    {
      "epoch": 79.65075154730327,
      "grad_norm": 0.583004355430603,
      "learning_rate": 2.0973554386744622e-06,
      "loss": 1.0231,
      "step": 8522
    },
    {
      "epoch": 79.66018272914825,
      "grad_norm": 0.5508294105529785,
      "learning_rate": 2.0955010353729877e-06,
      "loss": 1.0426,
      "step": 8523
    },
    {
      "epoch": 79.66961391099322,
      "grad_norm": 0.5218020677566528,
      "learning_rate": 2.093647356279088e-06,
      "loss": 1.0589,
      "step": 8524
    },
    {
      "epoch": 79.6790450928382,
      "grad_norm": 0.5791343450546265,
      "learning_rate": 2.0917944015626047e-06,
      "loss": 1.0006,
      "step": 8525
    },
    {
      "epoch": 79.68847627468317,
      "grad_norm": 0.552790641784668,
      "learning_rate": 2.089942171393299e-06,
      "loss": 1.0457,
      "step": 8526
    },
    {
      "epoch": 79.69790745652814,
      "grad_norm": 0.5781399011611938,
      "learning_rate": 2.0880906659408726e-06,
      "loss": 1.0493,
      "step": 8527
    },
    {
      "epoch": 79.70733863837312,
      "grad_norm": 0.5285917520523071,
      "learning_rate": 2.086239885374961e-06,
      "loss": 1.0514,
      "step": 8528
    },
    {
      "epoch": 79.71676982021809,
      "grad_norm": 0.5279314517974854,
      "learning_rate": 2.084389829865128e-06,
      "loss": 1.0264,
      "step": 8529
    },
    {
      "epoch": 79.72620100206306,
      "grad_norm": 0.5739324688911438,
      "learning_rate": 2.082540499580881e-06,
      "loss": 1.0664,
      "step": 8530
    },
    {
      "epoch": 79.73563218390805,
      "grad_norm": 0.5993980765342712,
      "learning_rate": 2.0806918946916522e-06,
      "loss": 1.0627,
      "step": 8531
    },
    {
      "epoch": 79.74506336575303,
      "grad_norm": 0.5383269786834717,
      "learning_rate": 2.078844015366811e-06,
      "loss": 1.0621,
      "step": 8532
    },
    {
      "epoch": 79.754494547598,
      "grad_norm": 0.5398639440536499,
      "learning_rate": 2.0769968617756553e-06,
      "loss": 1.0399,
      "step": 8533
    },
    {
      "epoch": 79.76392572944297,
      "grad_norm": 0.5509414076805115,
      "learning_rate": 2.0751504340874274e-06,
      "loss": 1.0361,
      "step": 8534
    },
    {
      "epoch": 79.77335691128795,
      "grad_norm": 0.6161754131317139,
      "learning_rate": 2.073304732471292e-06,
      "loss": 1.0216,
      "step": 8535
    },
    {
      "epoch": 79.78278809313292,
      "grad_norm": 0.5760955810546875,
      "learning_rate": 2.0714597570963534e-06,
      "loss": 1.0408,
      "step": 8536
    },
    {
      "epoch": 79.7922192749779,
      "grad_norm": 0.5530055165290833,
      "learning_rate": 2.0696155081316484e-06,
      "loss": 1.0387,
      "step": 8537
    },
    {
      "epoch": 79.80165045682287,
      "grad_norm": 0.5635296106338501,
      "learning_rate": 2.067771985746143e-06,
      "loss": 1.0279,
      "step": 8538
    },
    {
      "epoch": 79.81108163866784,
      "grad_norm": 0.5467023253440857,
      "learning_rate": 2.065929190108744e-06,
      "loss": 1.027,
      "step": 8539
    },
    {
      "epoch": 79.82051282051282,
      "grad_norm": 0.5539665818214417,
      "learning_rate": 2.0640871213882816e-06,
      "loss": 1.0579,
      "step": 8540
    },
    {
      "epoch": 79.82994400235779,
      "grad_norm": 0.5202780365943909,
      "learning_rate": 2.0622457797535346e-06,
      "loss": 1.0174,
      "step": 8541
    },
    {
      "epoch": 79.83937518420277,
      "grad_norm": 0.5505480170249939,
      "learning_rate": 2.0604051653731995e-06,
      "loss": 1.0499,
      "step": 8542
    },
    {
      "epoch": 79.84880636604774,
      "grad_norm": 0.5724672079086304,
      "learning_rate": 2.058565278415916e-06,
      "loss": 1.0634,
      "step": 8543
    },
    {
      "epoch": 79.85823754789271,
      "grad_norm": 0.5568140745162964,
      "learning_rate": 2.056726119050252e-06,
      "loss": 1.0168,
      "step": 8544
    },
    {
      "epoch": 79.8676687297377,
      "grad_norm": 0.5852230191230774,
      "learning_rate": 2.054887687444711e-06,
      "loss": 1.0203,
      "step": 8545
    },
    {
      "epoch": 79.87709991158268,
      "grad_norm": 0.5112519860267639,
      "learning_rate": 2.053049983767731e-06,
      "loss": 1.0943,
      "step": 8546
    },
    {
      "epoch": 79.88653109342765,
      "grad_norm": 0.6032317280769348,
      "learning_rate": 2.0512130081876768e-06,
      "loss": 1.0721,
      "step": 8547
    },
    {
      "epoch": 79.89596227527262,
      "grad_norm": 0.5879530310630798,
      "learning_rate": 2.0493767608728588e-06,
      "loss": 1.0376,
      "step": 8548
    },
    {
      "epoch": 79.9053934571176,
      "grad_norm": 0.5828002691268921,
      "learning_rate": 2.0475412419915087e-06,
      "loss": 1.0467,
      "step": 8549
    },
    {
      "epoch": 79.91482463896257,
      "grad_norm": 0.5304528474807739,
      "learning_rate": 2.0457064517117986e-06,
      "loss": 1.0552,
      "step": 8550
    },
    {
      "epoch": 79.92425582080755,
      "grad_norm": 0.560880184173584,
      "learning_rate": 2.0438723902018285e-06,
      "loss": 1.0444,
      "step": 8551
    },
    {
      "epoch": 79.93368700265252,
      "grad_norm": 0.5135582685470581,
      "learning_rate": 2.042039057629638e-06,
      "loss": 1.103,
      "step": 8552
    },
    {
      "epoch": 79.9431181844975,
      "grad_norm": 0.5376614332199097,
      "learning_rate": 2.0402064541631926e-06,
      "loss": 1.0496,
      "step": 8553
    },
    {
      "epoch": 79.95254936634247,
      "grad_norm": 0.49142757058143616,
      "learning_rate": 2.038374579970398e-06,
      "loss": 1.0666,
      "step": 8554
    },
    {
      "epoch": 79.96198054818744,
      "grad_norm": 0.5739914178848267,
      "learning_rate": 2.036543435219086e-06,
      "loss": 1.0215,
      "step": 8555
    },
    {
      "epoch": 79.97141173003241,
      "grad_norm": 0.5319231748580933,
      "learning_rate": 2.0347130200770304e-06,
      "loss": 1.013,
      "step": 8556
    },
    {
      "epoch": 79.98084291187739,
      "grad_norm": 0.5327998995780945,
      "learning_rate": 2.0328833347119303e-06,
      "loss": 1.0581,
      "step": 8557
    },
    {
      "epoch": 79.99027409372236,
      "grad_norm": 0.5397865772247314,
      "learning_rate": 2.031054379291422e-06,
      "loss": 1.0115,
      "step": 8558
    },
    {
      "epoch": 79.99970527556735,
      "grad_norm": 0.5683572292327881,
      "learning_rate": 2.029226153983074e-06,
      "loss": 1.0587,
      "step": 8559
    },
    {
      "epoch": 80.0,
      "grad_norm": 2.822707414627075,
      "learning_rate": 2.0273986589543858e-06,
      "loss": 0.9152,
      "step": 8560
    },
    {
      "epoch": 80.00943118184497,
      "grad_norm": 0.5100048184394836,
      "learning_rate": 2.025571894372794e-06,
      "loss": 1.0921,
      "step": 8561
    },
    {
      "epoch": 80.01886236368995,
      "grad_norm": 0.5414084196090698,
      "learning_rate": 2.0237458604056624e-06,
      "loss": 1.0567,
      "step": 8562
    },
    {
      "epoch": 80.02829354553492,
      "grad_norm": 0.5541417598724365,
      "learning_rate": 2.021920557220297e-06,
      "loss": 1.0533,
      "step": 8563
    },
    {
      "epoch": 80.0377247273799,
      "grad_norm": 0.520145833492279,
      "learning_rate": 2.0200959849839285e-06,
      "loss": 1.0549,
      "step": 8564
    },
    {
      "epoch": 80.04715590922487,
      "grad_norm": 0.5761206150054932,
      "learning_rate": 2.0182721438637242e-06,
      "loss": 1.0269,
      "step": 8565
    },
    {
      "epoch": 80.05658709106984,
      "grad_norm": 0.5532537698745728,
      "learning_rate": 2.016449034026784e-06,
      "loss": 1.0644,
      "step": 8566
    },
    {
      "epoch": 80.06601827291482,
      "grad_norm": 0.5626537799835205,
      "learning_rate": 2.0146266556401405e-06,
      "loss": 1.0644,
      "step": 8567
    },
    {
      "epoch": 80.0754494547598,
      "grad_norm": 0.609338104724884,
      "learning_rate": 2.0128050088707583e-06,
      "loss": 1.0285,
      "step": 8568
    },
    {
      "epoch": 80.08488063660478,
      "grad_norm": 0.5820717811584473,
      "learning_rate": 2.0109840938855383e-06,
      "loss": 1.0607,
      "step": 8569
    },
    {
      "epoch": 80.09431181844975,
      "grad_norm": 0.5984396934509277,
      "learning_rate": 2.0091639108513074e-06,
      "loss": 1.0301,
      "step": 8570
    },
    {
      "epoch": 80.10374300029473,
      "grad_norm": 0.5406872630119324,
      "learning_rate": 2.007344459934836e-06,
      "loss": 1.0373,
      "step": 8571
    },
    {
      "epoch": 80.1131741821397,
      "grad_norm": 0.5623232126235962,
      "learning_rate": 2.0055257413028196e-06,
      "loss": 1.0808,
      "step": 8572
    },
    {
      "epoch": 80.12260536398468,
      "grad_norm": 0.5545302033424377,
      "learning_rate": 2.0037077551218875e-06,
      "loss": 1.0342,
      "step": 8573
    },
    {
      "epoch": 80.13203654582965,
      "grad_norm": 0.5815625786781311,
      "learning_rate": 2.0018905015586043e-06,
      "loss": 1.0652,
      "step": 8574
    },
    {
      "epoch": 80.14146772767462,
      "grad_norm": 0.5384097099304199,
      "learning_rate": 2.0000739807794657e-06,
      "loss": 1.0653,
      "step": 8575
    },
    {
      "epoch": 80.1508989095196,
      "grad_norm": 0.5630418062210083,
      "learning_rate": 1.9982581929509003e-06,
      "loss": 1.0108,
      "step": 8576
    },
    {
      "epoch": 80.16033009136457,
      "grad_norm": 0.5221061706542969,
      "learning_rate": 1.996443138239267e-06,
      "loss": 1.0338,
      "step": 8577
    },
    {
      "epoch": 80.16976127320955,
      "grad_norm": 0.520598828792572,
      "learning_rate": 1.9946288168108675e-06,
      "loss": 1.0026,
      "step": 8578
    },
    {
      "epoch": 80.17919245505452,
      "grad_norm": 0.5475488305091858,
      "learning_rate": 1.992815228831925e-06,
      "loss": 1.0747,
      "step": 8579
    },
    {
      "epoch": 80.1886236368995,
      "grad_norm": 0.5329920053482056,
      "learning_rate": 1.9910023744686002e-06,
      "loss": 1.0293,
      "step": 8580
    },
    {
      "epoch": 80.19805481874447,
      "grad_norm": 0.5388688445091248,
      "learning_rate": 1.9891902538869854e-06,
      "loss": 1.1059,
      "step": 8581
    },
    {
      "epoch": 80.20748600058945,
      "grad_norm": 0.5203839540481567,
      "learning_rate": 1.987378867253109e-06,
      "loss": 1.061,
      "step": 8582
    },
    {
      "epoch": 80.21691718243443,
      "grad_norm": 0.5130612254142761,
      "learning_rate": 1.985568214732928e-06,
      "loss": 1.0283,
      "step": 8583
    },
    {
      "epoch": 80.2263483642794,
      "grad_norm": 0.5652316212654114,
      "learning_rate": 1.983758296492334e-06,
      "loss": 0.9812,
      "step": 8584
    },
    {
      "epoch": 80.23577954612438,
      "grad_norm": 0.5912480354309082,
      "learning_rate": 1.981949112697148e-06,
      "loss": 1.0265,
      "step": 8585
    },
    {
      "epoch": 80.24521072796935,
      "grad_norm": 0.5653443336486816,
      "learning_rate": 1.9801406635131327e-06,
      "loss": 1.0354,
      "step": 8586
    },
    {
      "epoch": 80.25464190981432,
      "grad_norm": 0.549750566482544,
      "learning_rate": 1.9783329491059746e-06,
      "loss": 1.0619,
      "step": 8587
    },
    {
      "epoch": 80.2640730916593,
      "grad_norm": 0.5719213485717773,
      "learning_rate": 1.9765259696412964e-06,
      "loss": 1.0324,
      "step": 8588
    },
    {
      "epoch": 80.27350427350427,
      "grad_norm": 0.5870859622955322,
      "learning_rate": 1.9747197252846516e-06,
      "loss": 1.0488,
      "step": 8589
    },
    {
      "epoch": 80.28293545534925,
      "grad_norm": 0.6022012829780579,
      "learning_rate": 1.972914216201527e-06,
      "loss": 1.0347,
      "step": 8590
    },
    {
      "epoch": 80.29236663719422,
      "grad_norm": 0.5251972079277039,
      "learning_rate": 1.9711094425573475e-06,
      "loss": 1.0699,
      "step": 8591
    },
    {
      "epoch": 80.3017978190392,
      "grad_norm": 0.5383216738700867,
      "learning_rate": 1.969305404517462e-06,
      "loss": 1.0665,
      "step": 8592
    },
    {
      "epoch": 80.31122900088417,
      "grad_norm": 0.5126550793647766,
      "learning_rate": 1.9675021022471574e-06,
      "loss": 1.0878,
      "step": 8593
    },
    {
      "epoch": 80.32066018272914,
      "grad_norm": 0.5286092758178711,
      "learning_rate": 1.9656995359116503e-06,
      "loss": 1.0427,
      "step": 8594
    },
    {
      "epoch": 80.33009136457412,
      "grad_norm": 0.592248260974884,
      "learning_rate": 1.96389770567609e-06,
      "loss": 1.0095,
      "step": 8595
    },
    {
      "epoch": 80.3395225464191,
      "grad_norm": 0.5626522898674011,
      "learning_rate": 1.962096611705564e-06,
      "loss": 1.0113,
      "step": 8596
    },
    {
      "epoch": 80.34895372826408,
      "grad_norm": 0.5293847918510437,
      "learning_rate": 1.960296254165085e-06,
      "loss": 1.0117,
      "step": 8597
    },
    {
      "epoch": 80.35838491010905,
      "grad_norm": 0.5253871083259583,
      "learning_rate": 1.958496633219602e-06,
      "loss": 1.0448,
      "step": 8598
    },
    {
      "epoch": 80.36781609195403,
      "grad_norm": 0.5315000414848328,
      "learning_rate": 1.9566977490339944e-06,
      "loss": 1.0424,
      "step": 8599
    },
    {
      "epoch": 80.377247273799,
      "grad_norm": 0.5412352085113525,
      "learning_rate": 1.954899601773075e-06,
      "loss": 1.0862,
      "step": 8600
    },
    {
      "epoch": 80.38667845564397,
      "grad_norm": 0.5515048503875732,
      "learning_rate": 1.9531021916015924e-06,
      "loss": 1.0137,
      "step": 8601
    },
    {
      "epoch": 80.39610963748895,
      "grad_norm": 0.5436417460441589,
      "learning_rate": 1.9513055186842232e-06,
      "loss": 1.0337,
      "step": 8602
    },
    {
      "epoch": 80.40554081933392,
      "grad_norm": 0.5791096687316895,
      "learning_rate": 1.949509583185578e-06,
      "loss": 1.0554,
      "step": 8603
    },
    {
      "epoch": 80.4149720011789,
      "grad_norm": 0.5660507082939148,
      "learning_rate": 1.9477143852701997e-06,
      "loss": 1.0236,
      "step": 8604
    },
    {
      "epoch": 80.42440318302387,
      "grad_norm": 0.5405275225639343,
      "learning_rate": 1.945919925102563e-06,
      "loss": 1.0709,
      "step": 8605
    },
    {
      "epoch": 80.43383436486884,
      "grad_norm": 0.5532678365707397,
      "learning_rate": 1.9441262028470763e-06,
      "loss": 1.0314,
      "step": 8606
    },
    {
      "epoch": 80.44326554671382,
      "grad_norm": 0.5783084034919739,
      "learning_rate": 1.9423332186680778e-06,
      "loss": 1.0355,
      "step": 8607
    },
    {
      "epoch": 80.45269672855879,
      "grad_norm": 0.5291631817817688,
      "learning_rate": 1.9405409727298454e-06,
      "loss": 1.0605,
      "step": 8608
    },
    {
      "epoch": 80.46212791040377,
      "grad_norm": 0.5706737041473389,
      "learning_rate": 1.9387494651965798e-06,
      "loss": 1.0413,
      "step": 8609
    },
    {
      "epoch": 80.47155909224875,
      "grad_norm": 0.6071237325668335,
      "learning_rate": 1.9369586962324207e-06,
      "loss": 1.0733,
      "step": 8610
    },
    {
      "epoch": 80.48099027409373,
      "grad_norm": 0.5272541642189026,
      "learning_rate": 1.935168666001436e-06,
      "loss": 1.0287,
      "step": 8611
    },
    {
      "epoch": 80.4904214559387,
      "grad_norm": 0.5123494267463684,
      "learning_rate": 1.933379374667628e-06,
      "loss": 1.052,
      "step": 8612
    },
    {
      "epoch": 80.49985263778368,
      "grad_norm": 0.5543227195739746,
      "learning_rate": 1.931590822394931e-06,
      "loss": 1.0661,
      "step": 8613
    },
    {
      "epoch": 80.50928381962865,
      "grad_norm": 0.5028876066207886,
      "learning_rate": 1.9298030093472098e-06,
      "loss": 1.0666,
      "step": 8614
    },
    {
      "epoch": 80.51871500147362,
      "grad_norm": 0.5630298256874084,
      "learning_rate": 1.9280159356882667e-06,
      "loss": 1.0911,
      "step": 8615
    },
    {
      "epoch": 80.5281461833186,
      "grad_norm": 0.6140034198760986,
      "learning_rate": 1.926229601581833e-06,
      "loss": 1.0419,
      "step": 8616
    },
    {
      "epoch": 80.53757736516357,
      "grad_norm": 0.5485947132110596,
      "learning_rate": 1.924444007191568e-06,
      "loss": 1.029,
      "step": 8617
    },
    {
      "epoch": 80.54700854700855,
      "grad_norm": 0.5465092658996582,
      "learning_rate": 1.922659152681071e-06,
      "loss": 1.0504,
      "step": 8618
    },
    {
      "epoch": 80.55643972885352,
      "grad_norm": 0.5399539470672607,
      "learning_rate": 1.9208750382138686e-06,
      "loss": 1.0814,
      "step": 8619
    },
    {
      "epoch": 80.5658709106985,
      "grad_norm": 0.526434063911438,
      "learning_rate": 1.9190916639534206e-06,
      "loss": 1.0465,
      "step": 8620
    },
    {
      "epoch": 80.57530209254347,
      "grad_norm": 0.5326357483863831,
      "learning_rate": 1.9173090300631194e-06,
      "loss": 1.0331,
      "step": 8621
    },
    {
      "epoch": 80.58473327438844,
      "grad_norm": 0.534913182258606,
      "learning_rate": 1.9155271367062856e-06,
      "loss": 1.0469,
      "step": 8622
    },
    {
      "epoch": 80.59416445623341,
      "grad_norm": 0.5483395457267761,
      "learning_rate": 1.9137459840461814e-06,
      "loss": 1.0413,
      "step": 8623
    },
    {
      "epoch": 80.6035956380784,
      "grad_norm": 0.5873321890830994,
      "learning_rate": 1.911965572245994e-06,
      "loss": 1.029,
      "step": 8624
    },
    {
      "epoch": 80.61302681992338,
      "grad_norm": 0.5877894759178162,
      "learning_rate": 1.910185901468843e-06,
      "loss": 1.0903,
      "step": 8625
    },
    {
      "epoch": 80.62245800176835,
      "grad_norm": 0.5690626502037048,
      "learning_rate": 1.908406971877781e-06,
      "loss": 1.0546,
      "step": 8626
    },
    {
      "epoch": 80.63188918361332,
      "grad_norm": 0.5294542908668518,
      "learning_rate": 1.9066287836357933e-06,
      "loss": 1.0772,
      "step": 8627
    },
    {
      "epoch": 80.6413203654583,
      "grad_norm": 0.5723636746406555,
      "learning_rate": 1.9048513369057987e-06,
      "loss": 1.0136,
      "step": 8628
    },
    {
      "epoch": 80.65075154730327,
      "grad_norm": 0.5468010902404785,
      "learning_rate": 1.9030746318506398e-06,
      "loss": 1.0457,
      "step": 8629
    },
    {
      "epoch": 80.66018272914825,
      "grad_norm": 0.5446361899375916,
      "learning_rate": 1.9012986686331069e-06,
      "loss": 1.0553,
      "step": 8630
    },
    {
      "epoch": 80.66961391099322,
      "grad_norm": 0.5229180455207825,
      "learning_rate": 1.8995234474159087e-06,
      "loss": 1.0333,
      "step": 8631
    },
    {
      "epoch": 80.6790450928382,
      "grad_norm": 0.5530166625976562,
      "learning_rate": 1.89774896836169e-06,
      "loss": 1.0587,
      "step": 8632
    },
    {
      "epoch": 80.68847627468317,
      "grad_norm": 0.5655696392059326,
      "learning_rate": 1.8959752316330281e-06,
      "loss": 1.0224,
      "step": 8633
    },
    {
      "epoch": 80.69790745652814,
      "grad_norm": 0.5532182455062866,
      "learning_rate": 1.8942022373924329e-06,
      "loss": 1.0311,
      "step": 8634
    },
    {
      "epoch": 80.70733863837312,
      "grad_norm": 0.5660986304283142,
      "learning_rate": 1.8924299858023466e-06,
      "loss": 1.0548,
      "step": 8635
    },
    {
      "epoch": 80.71676982021809,
      "grad_norm": 0.5350309014320374,
      "learning_rate": 1.890658477025139e-06,
      "loss": 1.0773,
      "step": 8636
    },
    {
      "epoch": 80.72620100206306,
      "grad_norm": 0.5645886063575745,
      "learning_rate": 1.8888877112231153e-06,
      "loss": 1.0357,
      "step": 8637
    },
    {
      "epoch": 80.73563218390805,
      "grad_norm": 0.558821439743042,
      "learning_rate": 1.8871176885585173e-06,
      "loss": 1.0878,
      "step": 8638
    },
    {
      "epoch": 80.74506336575303,
      "grad_norm": 0.5285439491271973,
      "learning_rate": 1.8853484091935103e-06,
      "loss": 1.0187,
      "step": 8639
    },
    {
      "epoch": 80.754494547598,
      "grad_norm": 0.5782182216644287,
      "learning_rate": 1.8835798732901956e-06,
      "loss": 1.0439,
      "step": 8640
    },
    {
      "epoch": 80.76392572944297,
      "grad_norm": 0.5577489137649536,
      "learning_rate": 1.8818120810106055e-06,
      "loss": 1.0331,
      "step": 8641
    },
    {
      "epoch": 80.77335691128795,
      "grad_norm": 0.562006950378418,
      "learning_rate": 1.8800450325167063e-06,
      "loss": 1.0404,
      "step": 8642
    },
    {
      "epoch": 80.78278809313292,
      "grad_norm": 0.5719953775405884,
      "learning_rate": 1.8782787279703918e-06,
      "loss": 1.0703,
      "step": 8643
    },
    {
      "epoch": 80.7922192749779,
      "grad_norm": 0.6006784439086914,
      "learning_rate": 1.876513167533489e-06,
      "loss": 1.0526,
      "step": 8644
    },
    {
      "epoch": 80.80165045682287,
      "grad_norm": 0.5285396575927734,
      "learning_rate": 1.8747483513677633e-06,
      "loss": 1.0694,
      "step": 8645
    },
    {
      "epoch": 80.81108163866784,
      "grad_norm": 0.5284510254859924,
      "learning_rate": 1.872984279634904e-06,
      "loss": 1.0257,
      "step": 8646
    },
    {
      "epoch": 80.82051282051282,
      "grad_norm": 0.5099877119064331,
      "learning_rate": 1.871220952496534e-06,
      "loss": 1.0217,
      "step": 8647
    },
    {
      "epoch": 80.82994400235779,
      "grad_norm": 0.582868218421936,
      "learning_rate": 1.86945837011421e-06,
      "loss": 1.0438,
      "step": 8648
    },
    {
      "epoch": 80.83937518420277,
      "grad_norm": 0.5496417880058289,
      "learning_rate": 1.8676965326494179e-06,
      "loss": 1.0346,
      "step": 8649
    },
    {
      "epoch": 80.84880636604774,
      "grad_norm": 0.5736984610557556,
      "learning_rate": 1.8659354402635776e-06,
      "loss": 1.0464,
      "step": 8650
    },
    {
      "epoch": 80.85823754789271,
      "grad_norm": 0.5978407859802246,
      "learning_rate": 1.8641750931180392e-06,
      "loss": 1.0743,
      "step": 8651
    },
    {
      "epoch": 80.8676687297377,
      "grad_norm": 0.5373215675354004,
      "learning_rate": 1.862415491374082e-06,
      "loss": 1.0224,
      "step": 8652
    },
    {
      "epoch": 80.87709991158268,
      "grad_norm": 0.5768288373947144,
      "learning_rate": 1.8606566351929267e-06,
      "loss": 1.0796,
      "step": 8653
    },
    {
      "epoch": 80.88653109342765,
      "grad_norm": 0.5377041101455688,
      "learning_rate": 1.8588985247357161e-06,
      "loss": 1.0517,
      "step": 8654
    },
    {
      "epoch": 80.89596227527262,
      "grad_norm": 0.6133590936660767,
      "learning_rate": 1.8571411601635282e-06,
      "loss": 1.0703,
      "step": 8655
    },
    {
      "epoch": 80.9053934571176,
      "grad_norm": 0.5723485350608826,
      "learning_rate": 1.8553845416373673e-06,
      "loss": 1.0326,
      "step": 8656
    },
    {
      "epoch": 80.91482463896257,
      "grad_norm": 0.5325681567192078,
      "learning_rate": 1.8536286693181816e-06,
      "loss": 1.0812,
      "step": 8657
    },
    {
      "epoch": 80.92425582080755,
      "grad_norm": 0.5355737805366516,
      "learning_rate": 1.85187354336684e-06,
      "loss": 1.056,
      "step": 8658
    },
    {
      "epoch": 80.93368700265252,
      "grad_norm": 0.5574623942375183,
      "learning_rate": 1.8501191639441474e-06,
      "loss": 1.0638,
      "step": 8659
    },
    {
      "epoch": 80.9431181844975,
      "grad_norm": 0.5255457162857056,
      "learning_rate": 1.8483655312108396e-06,
      "loss": 1.045,
      "step": 8660
    },
    {
      "epoch": 80.95254936634247,
      "grad_norm": 0.5479012131690979,
      "learning_rate": 1.846612645327579e-06,
      "loss": 1.0663,
      "step": 8661
    },
    {
      "epoch": 80.96198054818744,
      "grad_norm": 0.5559223890304565,
      "learning_rate": 1.8448605064549719e-06,
      "loss": 1.056,
      "step": 8662
    },
    {
      "epoch": 80.97141173003241,
      "grad_norm": 0.5045602917671204,
      "learning_rate": 1.843109114753545e-06,
      "loss": 1.0674,
      "step": 8663
    },
    {
      "epoch": 80.98084291187739,
      "grad_norm": 0.5052102208137512,
      "learning_rate": 1.8413584703837618e-06,
      "loss": 1.0902,
      "step": 8664
    },
    {
      "epoch": 80.99027409372236,
      "grad_norm": 0.5328744053840637,
      "learning_rate": 1.8396085735060133e-06,
      "loss": 1.0379,
      "step": 8665
    },
    {
      "epoch": 80.99970527556735,
      "grad_norm": 0.5361360907554626,
      "learning_rate": 1.8378594242806269e-06,
      "loss": 1.0401,
      "step": 8666
    },
    {
      "epoch": 81.0,
      "grad_norm": 3.3794736862182617,
      "learning_rate": 1.8361110228678548e-06,
      "loss": 0.7328,
      "step": 8667
    },
    {
      "epoch": 81.00943118184497,
      "grad_norm": 0.5610269904136658,
      "learning_rate": 1.8343633694278895e-06,
      "loss": 1.0548,
      "step": 8668
    },
    {
      "epoch": 81.01886236368995,
      "grad_norm": 0.5806581377983093,
      "learning_rate": 1.8326164641208498e-06,
      "loss": 1.0138,
      "step": 8669
    },
    {
      "epoch": 81.02829354553492,
      "grad_norm": 0.521295428276062,
      "learning_rate": 1.8308703071067868e-06,
      "loss": 1.0737,
      "step": 8670
    },
    {
      "epoch": 81.0377247273799,
      "grad_norm": 0.49933069944381714,
      "learning_rate": 1.8291248985456812e-06,
      "loss": 1.0383,
      "step": 8671
    },
    {
      "epoch": 81.04715590922487,
      "grad_norm": 0.5174444317817688,
      "learning_rate": 1.827380238597447e-06,
      "loss": 1.0759,
      "step": 8672
    },
    {
      "epoch": 81.05658709106984,
      "grad_norm": 0.5327219367027283,
      "learning_rate": 1.825636327421929e-06,
      "loss": 1.0948,
      "step": 8673
    },
    {
      "epoch": 81.06601827291482,
      "grad_norm": 0.5537602305412292,
      "learning_rate": 1.8238931651789038e-06,
      "loss": 1.0601,
      "step": 8674
    },
    {
      "epoch": 81.0754494547598,
      "grad_norm": 0.5104470252990723,
      "learning_rate": 1.8221507520280812e-06,
      "loss": 1.0803,
      "step": 8675
    },
    {
      "epoch": 81.08488063660478,
      "grad_norm": 0.5375407934188843,
      "learning_rate": 1.8204090881291004e-06,
      "loss": 1.0484,
      "step": 8676
    },
    {
      "epoch": 81.09431181844975,
      "grad_norm": 0.578274667263031,
      "learning_rate": 1.8186681736415302e-06,
      "loss": 1.0796,
      "step": 8677
    },
    {
      "epoch": 81.10374300029473,
      "grad_norm": 0.60211580991745,
      "learning_rate": 1.816928008724873e-06,
      "loss": 1.0297,
      "step": 8678
    },
    {
      "epoch": 81.1131741821397,
      "grad_norm": 0.53859543800354,
      "learning_rate": 1.815188593538564e-06,
      "loss": 1.0503,
      "step": 8679
    },
    {
      "epoch": 81.12260536398468,
      "grad_norm": 0.530839741230011,
      "learning_rate": 1.8134499282419648e-06,
      "loss": 1.0458,
      "step": 8680
    },
    {
      "epoch": 81.13203654582965,
      "grad_norm": 0.5966233015060425,
      "learning_rate": 1.8117120129943744e-06,
      "loss": 1.0275,
      "step": 8681
    },
    {
      "epoch": 81.14146772767462,
      "grad_norm": 0.5570128560066223,
      "learning_rate": 1.809974847955015e-06,
      "loss": 1.0508,
      "step": 8682
    },
    {
      "epoch": 81.1508989095196,
      "grad_norm": 0.513192355632782,
      "learning_rate": 1.8082384332830516e-06,
      "loss": 1.057,
      "step": 8683
    },
    {
      "epoch": 81.16033009136457,
      "grad_norm": 0.5493103861808777,
      "learning_rate": 1.8065027691375715e-06,
      "loss": 1.0549,
      "step": 8684
    },
    {
      "epoch": 81.16976127320955,
      "grad_norm": 0.5655338168144226,
      "learning_rate": 1.8047678556775948e-06,
      "loss": 1.0522,
      "step": 8685
    },
    {
      "epoch": 81.17919245505452,
      "grad_norm": 0.5289748311042786,
      "learning_rate": 1.803033693062074e-06,
      "loss": 1.0241,
      "step": 8686
    },
    {
      "epoch": 81.1886236368995,
      "grad_norm": 0.5483255386352539,
      "learning_rate": 1.8013002814498925e-06,
      "loss": 1.064,
      "step": 8687
    },
    {
      "epoch": 81.19805481874447,
      "grad_norm": 0.5388056635856628,
      "learning_rate": 1.7995676209998648e-06,
      "loss": 1.0359,
      "step": 8688
    },
    {
      "epoch": 81.20748600058945,
      "grad_norm": 0.5894757509231567,
      "learning_rate": 1.7978357118707346e-06,
      "loss": 1.045,
      "step": 8689
    },
    {
      "epoch": 81.21691718243443,
      "grad_norm": 0.5195713043212891,
      "learning_rate": 1.796104554221184e-06,
      "loss": 1.066,
      "step": 8690
    },
    {
      "epoch": 81.2263483642794,
      "grad_norm": 0.5402346849441528,
      "learning_rate": 1.7943741482098176e-06,
      "loss": 1.0954,
      "step": 8691
    },
    {
      "epoch": 81.23577954612438,
      "grad_norm": 0.5576263070106506,
      "learning_rate": 1.792644493995176e-06,
      "loss": 1.0097,
      "step": 8692
    },
    {
      "epoch": 81.24521072796935,
      "grad_norm": 0.534085750579834,
      "learning_rate": 1.790915591735728e-06,
      "loss": 1.0299,
      "step": 8693
    },
    {
      "epoch": 81.25464190981432,
      "grad_norm": 0.5515660047531128,
      "learning_rate": 1.7891874415898757e-06,
      "loss": 1.0231,
      "step": 8694
    },
    {
      "epoch": 81.2640730916593,
      "grad_norm": 0.5506267547607422,
      "learning_rate": 1.7874600437159517e-06,
      "loss": 1.0553,
      "step": 8695
    },
    {
      "epoch": 81.27350427350427,
      "grad_norm": 0.5349407196044922,
      "learning_rate": 1.7857333982722202e-06,
      "loss": 1.0248,
      "step": 8696
    },
    {
      "epoch": 81.28293545534925,
      "grad_norm": 0.5374359488487244,
      "learning_rate": 1.7840075054168726e-06,
      "loss": 1.0112,
      "step": 8697
    },
    {
      "epoch": 81.29236663719422,
      "grad_norm": 0.5315234661102295,
      "learning_rate": 1.7822823653080402e-06,
      "loss": 1.046,
      "step": 8698
    },
    {
      "epoch": 81.3017978190392,
      "grad_norm": 0.5411333441734314,
      "learning_rate": 1.7805579781037763e-06,
      "loss": 1.0638,
      "step": 8699
    },
    {
      "epoch": 81.31122900088417,
      "grad_norm": 0.6201714277267456,
      "learning_rate": 1.7788343439620704e-06,
      "loss": 1.0242,
      "step": 8700
    },
    {
      "epoch": 81.32066018272914,
      "grad_norm": 0.5199558734893799,
      "learning_rate": 1.7771114630408393e-06,
      "loss": 1.0684,
      "step": 8701
    },
    {
      "epoch": 81.33009136457412,
      "grad_norm": 0.5432899594306946,
      "learning_rate": 1.7753893354979335e-06,
      "loss": 1.0411,
      "step": 8702
    },
    {
      "epoch": 81.3395225464191,
      "grad_norm": 0.5555132627487183,
      "learning_rate": 1.7736679614911345e-06,
      "loss": 1.0711,
      "step": 8703
    },
    {
      "epoch": 81.34895372826408,
      "grad_norm": 0.5617049932479858,
      "learning_rate": 1.7719473411781507e-06,
      "loss": 1.0668,
      "step": 8704
    },
    {
      "epoch": 81.35838491010905,
      "grad_norm": 0.5526766180992126,
      "learning_rate": 1.7702274747166305e-06,
      "loss": 1.0641,
      "step": 8705
    },
    {
      "epoch": 81.36781609195403,
      "grad_norm": 0.5550597310066223,
      "learning_rate": 1.7685083622641442e-06,
      "loss": 1.0586,
      "step": 8706
    },
    {
      "epoch": 81.377247273799,
      "grad_norm": 0.5504812598228455,
      "learning_rate": 1.766790003978197e-06,
      "loss": 1.0635,
      "step": 8707
    },
    {
      "epoch": 81.38667845564397,
      "grad_norm": 0.5339758992195129,
      "learning_rate": 1.7650724000162233e-06,
      "loss": 1.0472,
      "step": 8708
    },
    {
      "epoch": 81.39610963748895,
      "grad_norm": 0.5278438925743103,
      "learning_rate": 1.7633555505355915e-06,
      "loss": 1.0267,
      "step": 8709
    },
    {
      "epoch": 81.40554081933392,
      "grad_norm": 0.5803185105323792,
      "learning_rate": 1.7616394556935955e-06,
      "loss": 1.0114,
      "step": 8710
    },
    {
      "epoch": 81.4149720011789,
      "grad_norm": 0.5442630052566528,
      "learning_rate": 1.7599241156474667e-06,
      "loss": 1.0258,
      "step": 8711
    },
    {
      "epoch": 81.42440318302387,
      "grad_norm": 0.5692495703697205,
      "learning_rate": 1.758209530554359e-06,
      "loss": 1.0283,
      "step": 8712
    },
    {
      "epoch": 81.43383436486884,
      "grad_norm": 0.5400941967964172,
      "learning_rate": 1.756495700571369e-06,
      "loss": 0.9895,
      "step": 8713
    },
    {
      "epoch": 81.44326554671382,
      "grad_norm": 0.5501150488853455,
      "learning_rate": 1.7547826258555133e-06,
      "loss": 1.0322,
      "step": 8714
    },
    {
      "epoch": 81.45269672855879,
      "grad_norm": 0.5425782203674316,
      "learning_rate": 1.7530703065637433e-06,
      "loss": 1.0611,
      "step": 8715
    },
    {
      "epoch": 81.46212791040377,
      "grad_norm": 0.5418592691421509,
      "learning_rate": 1.7513587428529422e-06,
      "loss": 1.0421,
      "step": 8716
    },
    {
      "epoch": 81.47155909224875,
      "grad_norm": 0.5241654515266418,
      "learning_rate": 1.7496479348799221e-06,
      "loss": 1.0497,
      "step": 8717
    },
    {
      "epoch": 81.48099027409373,
      "grad_norm": 0.5294216275215149,
      "learning_rate": 1.747937882801425e-06,
      "loss": 1.0142,
      "step": 8718
    },
    {
      "epoch": 81.4904214559387,
      "grad_norm": 0.5562115907669067,
      "learning_rate": 1.746228586774129e-06,
      "loss": 1.012,
      "step": 8719
    },
    {
      "epoch": 81.49985263778368,
      "grad_norm": 0.5828069448471069,
      "learning_rate": 1.7445200469546375e-06,
      "loss": 1.0391,
      "step": 8720
    },
    {
      "epoch": 81.50928381962865,
      "grad_norm": 0.5437248349189758,
      "learning_rate": 1.742812263499486e-06,
      "loss": 1.0904,
      "step": 8721
    },
    {
      "epoch": 81.51871500147362,
      "grad_norm": 0.5127459168434143,
      "learning_rate": 1.7411052365651415e-06,
      "loss": 1.0418,
      "step": 8722
    },
    {
      "epoch": 81.5281461833186,
      "grad_norm": 0.5554586052894592,
      "learning_rate": 1.739398966307999e-06,
      "loss": 1.0574,
      "step": 8723
    },
    {
      "epoch": 81.53757736516357,
      "grad_norm": 0.535932719707489,
      "learning_rate": 1.7376934528843903e-06,
      "loss": 1.0805,
      "step": 8724
    },
    {
      "epoch": 81.54700854700855,
      "grad_norm": 0.5658379793167114,
      "learning_rate": 1.7359886964505722e-06,
      "loss": 1.0436,
      "step": 8725
    },
    {
      "epoch": 81.55643972885352,
      "grad_norm": 0.5367518067359924,
      "learning_rate": 1.7342846971627347e-06,
      "loss": 1.0499,
      "step": 8726
    },
    {
      "epoch": 81.5658709106985,
      "grad_norm": 0.5455859303474426,
      "learning_rate": 1.732581455176997e-06,
      "loss": 1.029,
      "step": 8727
    },
    {
      "epoch": 81.57530209254347,
      "grad_norm": 0.532630205154419,
      "learning_rate": 1.7308789706494055e-06,
      "loss": 1.0753,
      "step": 8728
    },
    {
      "epoch": 81.58473327438844,
      "grad_norm": 0.5943999290466309,
      "learning_rate": 1.7291772437359488e-06,
      "loss": 1.0799,
      "step": 8729
    },
    {
      "epoch": 81.59416445623341,
      "grad_norm": 0.5428958535194397,
      "learning_rate": 1.7274762745925344e-06,
      "loss": 1.0296,
      "step": 8730
    },
    {
      "epoch": 81.6035956380784,
      "grad_norm": 0.56717449426651,
      "learning_rate": 1.7257760633750054e-06,
      "loss": 1.0616,
      "step": 8731
    },
    {
      "epoch": 81.61302681992338,
      "grad_norm": 0.570256769657135,
      "learning_rate": 1.7240766102391337e-06,
      "loss": 1.0483,
      "step": 8732
    },
    {
      "epoch": 81.62245800176835,
      "grad_norm": 0.5358783006668091,
      "learning_rate": 1.722377915340623e-06,
      "loss": 1.0783,
      "step": 8733
    },
    {
      "epoch": 81.63188918361332,
      "grad_norm": 0.5566782355308533,
      "learning_rate": 1.7206799788351058e-06,
      "loss": 1.0523,
      "step": 8734
    },
    {
      "epoch": 81.6413203654583,
      "grad_norm": 0.6407710313796997,
      "learning_rate": 1.7189828008781496e-06,
      "loss": 1.0005,
      "step": 8735
    },
    {
      "epoch": 81.65075154730327,
      "grad_norm": 0.5347768068313599,
      "learning_rate": 1.7172863816252483e-06,
      "loss": 1.0558,
      "step": 8736
    },
    {
      "epoch": 81.66018272914825,
      "grad_norm": 0.5744040608406067,
      "learning_rate": 1.7155907212318258e-06,
      "loss": 1.073,
      "step": 8737
    },
    {
      "epoch": 81.66961391099322,
      "grad_norm": 0.5923137068748474,
      "learning_rate": 1.7138958198532396e-06,
      "loss": 1.0261,
      "step": 8738
    },
    {
      "epoch": 81.6790450928382,
      "grad_norm": 0.5474561452865601,
      "learning_rate": 1.7122016776447748e-06,
      "loss": 1.0557,
      "step": 8739
    },
    {
      "epoch": 81.68847627468317,
      "grad_norm": 0.558473527431488,
      "learning_rate": 1.710508294761649e-06,
      "loss": 1.0177,
      "step": 8740
    },
    {
      "epoch": 81.69790745652814,
      "grad_norm": 0.5953349471092224,
      "learning_rate": 1.7088156713590065e-06,
      "loss": 1.0452,
      "step": 8741
    },
    {
      "epoch": 81.70733863837312,
      "grad_norm": 0.5437284111976624,
      "learning_rate": 1.7071238075919295e-06,
      "loss": 1.003,
      "step": 8742
    },
    {
      "epoch": 81.71676982021809,
      "grad_norm": 0.5568480491638184,
      "learning_rate": 1.7054327036154238e-06,
      "loss": 1.0452,
      "step": 8743
    },
    {
      "epoch": 81.72620100206306,
      "grad_norm": 0.5651222467422485,
      "learning_rate": 1.7037423595844283e-06,
      "loss": 1.0434,
      "step": 8744
    },
    {
      "epoch": 81.73563218390805,
      "grad_norm": 0.5456162095069885,
      "learning_rate": 1.7020527756538108e-06,
      "loss": 1.042,
      "step": 8745
    },
    {
      "epoch": 81.74506336575303,
      "grad_norm": 0.5420081615447998,
      "learning_rate": 1.700363951978371e-06,
      "loss": 1.0209,
      "step": 8746
    },
    {
      "epoch": 81.754494547598,
      "grad_norm": 0.5300523638725281,
      "learning_rate": 1.6986758887128385e-06,
      "loss": 1.0529,
      "step": 8747
    },
    {
      "epoch": 81.76392572944297,
      "grad_norm": 0.535244345664978,
      "learning_rate": 1.6969885860118717e-06,
      "loss": 1.0635,
      "step": 8748
    },
    {
      "epoch": 81.77335691128795,
      "grad_norm": 0.5506467819213867,
      "learning_rate": 1.6953020440300605e-06,
      "loss": 1.059,
      "step": 8749
    },
    {
      "epoch": 81.78278809313292,
      "grad_norm": 0.5344988703727722,
      "learning_rate": 1.69361626292193e-06,
      "loss": 1.0246,
      "step": 8750
    },
    {
      "epoch": 81.7922192749779,
      "grad_norm": 0.5471201539039612,
      "learning_rate": 1.6919312428419254e-06,
      "loss": 1.0344,
      "step": 8751
    },
    {
      "epoch": 81.80165045682287,
      "grad_norm": 0.5729320645332336,
      "learning_rate": 1.6902469839444314e-06,
      "loss": 1.0575,
      "step": 8752
    },
    {
      "epoch": 81.81108163866784,
      "grad_norm": 0.5486124753952026,
      "learning_rate": 1.6885634863837575e-06,
      "loss": 1.0927,
      "step": 8753
    },
    {
      "epoch": 81.82051282051282,
      "grad_norm": 0.5354857444763184,
      "learning_rate": 1.686880750314146e-06,
      "loss": 1.0546,
      "step": 8754
    },
    {
      "epoch": 81.82994400235779,
      "grad_norm": 0.5143479704856873,
      "learning_rate": 1.685198775889767e-06,
      "loss": 1.0376,
      "step": 8755
    },
    {
      "epoch": 81.83937518420277,
      "grad_norm": 0.522103488445282,
      "learning_rate": 1.6835175632647216e-06,
      "loss": 1.0736,
      "step": 8756
    },
    {
      "epoch": 81.84880636604774,
      "grad_norm": 0.5573784708976746,
      "learning_rate": 1.6818371125930455e-06,
      "loss": 1.0514,
      "step": 8757
    },
    {
      "epoch": 81.85823754789271,
      "grad_norm": 0.5535646080970764,
      "learning_rate": 1.6801574240287e-06,
      "loss": 1.0792,
      "step": 8758
    },
    {
      "epoch": 81.8676687297377,
      "grad_norm": 0.5480827689170837,
      "learning_rate": 1.6784784977255775e-06,
      "loss": 1.0536,
      "step": 8759
    },
    {
      "epoch": 81.87709991158268,
      "grad_norm": 0.5582969784736633,
      "learning_rate": 1.6768003338374994e-06,
      "loss": 1.0076,
      "step": 8760
    },
    {
      "epoch": 81.88653109342765,
      "grad_norm": 0.5702380537986755,
      "learning_rate": 1.6751229325182194e-06,
      "loss": 1.0658,
      "step": 8761
    },
    {
      "epoch": 81.89596227527262,
      "grad_norm": 0.5426482558250427,
      "learning_rate": 1.6734462939214203e-06,
      "loss": 1.0658,
      "step": 8762
    },
    {
      "epoch": 81.9053934571176,
      "grad_norm": 0.5387306809425354,
      "learning_rate": 1.6717704182007145e-06,
      "loss": 1.0439,
      "step": 8763
    },
    {
      "epoch": 81.91482463896257,
      "grad_norm": 0.5820152759552002,
      "learning_rate": 1.6700953055096448e-06,
      "loss": 1.0441,
      "step": 8764
    },
    {
      "epoch": 81.92425582080755,
      "grad_norm": 0.5984434485435486,
      "learning_rate": 1.6684209560016873e-06,
      "loss": 1.0578,
      "step": 8765
    },
    {
      "epoch": 81.93368700265252,
      "grad_norm": 0.5767509341239929,
      "learning_rate": 1.6667473698302427e-06,
      "loss": 1.0344,
      "step": 8766
    },
    {
      "epoch": 81.9431181844975,
      "grad_norm": 0.5466111898422241,
      "learning_rate": 1.6650745471486463e-06,
      "loss": 1.0927,
      "step": 8767
    },
    {
      "epoch": 81.95254936634247,
      "grad_norm": 0.5992300510406494,
      "learning_rate": 1.6634024881101606e-06,
      "loss": 1.0174,
      "step": 8768
    },
    {
      "epoch": 81.96198054818744,
      "grad_norm": 0.5556737780570984,
      "learning_rate": 1.6617311928679792e-06,
      "loss": 1.0718,
      "step": 8769
    },
    {
      "epoch": 81.97141173003241,
      "grad_norm": 0.5366440415382385,
      "learning_rate": 1.6600606615752246e-06,
      "loss": 1.0777,
      "step": 8770
    },
    {
      "epoch": 81.98084291187739,
      "grad_norm": 0.542034387588501,
      "learning_rate": 1.6583908943849492e-06,
      "loss": 1.0581,
      "step": 8771
    },
    {
      "epoch": 81.99027409372236,
      "grad_norm": 0.5532682538032532,
      "learning_rate": 1.6567218914501415e-06,
      "loss": 1.039,
      "step": 8772
    },
    {
      "epoch": 81.99970527556735,
      "grad_norm": 0.6014267206192017,
      "learning_rate": 1.6550536529237115e-06,
      "loss": 1.0573,
      "step": 8773
    },
    {
      "epoch": 82.0,
      "grad_norm": 2.7906177043914795,
      "learning_rate": 1.6533861789585027e-06,
      "loss": 0.6952,
      "step": 8774
    },
    {
      "epoch": 82.00943118184497,
      "grad_norm": 0.6024234890937805,
      "learning_rate": 1.6517194697072903e-06,
      "loss": 1.0532,
      "step": 8775
    },
    {
      "epoch": 82.01886236368995,
      "grad_norm": 0.545007050037384,
      "learning_rate": 1.6500535253227756e-06,
      "loss": 1.0589,
      "step": 8776
    },
    {
      "epoch": 82.02829354553492,
      "grad_norm": 0.5518550872802734,
      "learning_rate": 1.6483883459575923e-06,
      "loss": 1.0494,
      "step": 8777
    },
    {
      "epoch": 82.0377247273799,
      "grad_norm": 0.565558910369873,
      "learning_rate": 1.6467239317643036e-06,
      "loss": 1.0681,
      "step": 8778
    },
    {
      "epoch": 82.04715590922487,
      "grad_norm": 0.5890279412269592,
      "learning_rate": 1.645060282895401e-06,
      "loss": 1.0336,
      "step": 8779
    },
    {
      "epoch": 82.05658709106984,
      "grad_norm": 0.5711560249328613,
      "learning_rate": 1.6433973995033114e-06,
      "loss": 1.0475,
      "step": 8780
    },
    {
      "epoch": 82.06601827291482,
      "grad_norm": 0.5785685181617737,
      "learning_rate": 1.6417352817403865e-06,
      "loss": 1.0285,
      "step": 8781
    },
    {
      "epoch": 82.0754494547598,
      "grad_norm": 0.581200361251831,
      "learning_rate": 1.6400739297589074e-06,
      "loss": 1.0605,
      "step": 8782
    },
    {
      "epoch": 82.08488063660478,
      "grad_norm": 0.5226556062698364,
      "learning_rate": 1.6384133437110872e-06,
      "loss": 1.0375,
      "step": 8783
    },
    {
      "epoch": 82.09431181844975,
      "grad_norm": 0.533196747303009,
      "learning_rate": 1.6367535237490651e-06,
      "loss": 1.0628,
      "step": 8784
    },
    {
      "epoch": 82.10374300029473,
      "grad_norm": 0.5138500332832336,
      "learning_rate": 1.6350944700249194e-06,
      "loss": 1.0364,
      "step": 8785
    },
    {
      "epoch": 82.1131741821397,
      "grad_norm": 0.586696445941925,
      "learning_rate": 1.6334361826906498e-06,
      "loss": 1.0426,
      "step": 8786
    },
    {
      "epoch": 82.12260536398468,
      "grad_norm": 0.5298725366592407,
      "learning_rate": 1.6317786618981867e-06,
      "loss": 1.0325,
      "step": 8787
    },
    {
      "epoch": 82.13203654582965,
      "grad_norm": 0.5567424893379211,
      "learning_rate": 1.6301219077993924e-06,
      "loss": 1.0318,
      "step": 8788
    },
    {
      "epoch": 82.14146772767462,
      "grad_norm": 0.5335325598716736,
      "learning_rate": 1.628465920546055e-06,
      "loss": 1.0508,
      "step": 8789
    },
    {
      "epoch": 82.1508989095196,
      "grad_norm": 0.5930030941963196,
      "learning_rate": 1.6268107002899014e-06,
      "loss": 1.0111,
      "step": 8790
    },
    {
      "epoch": 82.16033009136457,
      "grad_norm": 0.5158705711364746,
      "learning_rate": 1.62515624718258e-06,
      "loss": 1.0774,
      "step": 8791
    },
    {
      "epoch": 82.16976127320955,
      "grad_norm": 0.559051513671875,
      "learning_rate": 1.6235025613756706e-06,
      "loss": 1.0098,
      "step": 8792
    },
    {
      "epoch": 82.17919245505452,
      "grad_norm": 0.5600495338439941,
      "learning_rate": 1.621849643020682e-06,
      "loss": 1.0444,
      "step": 8793
    },
    {
      "epoch": 82.1886236368995,
      "grad_norm": 0.5850816369056702,
      "learning_rate": 1.6201974922690545e-06,
      "loss": 1.0314,
      "step": 8794
    },
    {
      "epoch": 82.19805481874447,
      "grad_norm": 0.5378851890563965,
      "learning_rate": 1.6185461092721611e-06,
      "loss": 1.0497,
      "step": 8795
    },
    {
      "epoch": 82.20748600058945,
      "grad_norm": 0.5441277027130127,
      "learning_rate": 1.6168954941812975e-06,
      "loss": 1.0813,
      "step": 8796
    },
    {
      "epoch": 82.21691718243443,
      "grad_norm": 0.542057991027832,
      "learning_rate": 1.615245647147694e-06,
      "loss": 1.0842,
      "step": 8797
    },
    {
      "epoch": 82.2263483642794,
      "grad_norm": 0.5136166214942932,
      "learning_rate": 1.6135965683225075e-06,
      "loss": 1.033,
      "step": 8798
    },
    {
      "epoch": 82.23577954612438,
      "grad_norm": 0.5353705883026123,
      "learning_rate": 1.611948257856828e-06,
      "loss": 1.1065,
      "step": 8799
    },
    {
      "epoch": 82.24521072796935,
      "grad_norm": 0.5315673351287842,
      "learning_rate": 1.6103007159016725e-06,
      "loss": 1.0473,
      "step": 8800
    },
    {
      "epoch": 82.25464190981432,
      "grad_norm": 0.6060959100723267,
      "learning_rate": 1.608653942607985e-06,
      "loss": 1.0634,
      "step": 8801
    },
    {
      "epoch": 82.2640730916593,
      "grad_norm": 0.5317062139511108,
      "learning_rate": 1.6070079381266467e-06,
      "loss": 1.0399,
      "step": 8802
    },
    {
      "epoch": 82.27350427350427,
      "grad_norm": 0.5582558512687683,
      "learning_rate": 1.605362702608464e-06,
      "loss": 1.0461,
      "step": 8803
    },
    {
      "epoch": 82.28293545534925,
      "grad_norm": 0.5869553685188293,
      "learning_rate": 1.6037182362041714e-06,
      "loss": 1.0516,
      "step": 8804
    },
    {
      "epoch": 82.29236663719422,
      "grad_norm": 0.5515316128730774,
      "learning_rate": 1.6020745390644333e-06,
      "loss": 0.9918,
      "step": 8805
    },
    {
      "epoch": 82.3017978190392,
      "grad_norm": 0.5397045016288757,
      "learning_rate": 1.6004316113398465e-06,
      "loss": 1.0497,
      "step": 8806
    },
    {
      "epoch": 82.31122900088417,
      "grad_norm": 0.5893183946609497,
      "learning_rate": 1.5987894531809345e-06,
      "loss": 1.0249,
      "step": 8807
    },
    {
      "epoch": 82.32066018272914,
      "grad_norm": 0.5250419974327087,
      "learning_rate": 1.5971480647381522e-06,
      "loss": 1.0437,
      "step": 8808
    },
    {
      "epoch": 82.33009136457412,
      "grad_norm": 0.5376355051994324,
      "learning_rate": 1.5955074461618791e-06,
      "loss": 1.0284,
      "step": 8809
    },
    {
      "epoch": 82.3395225464191,
      "grad_norm": 0.5341960787773132,
      "learning_rate": 1.5938675976024331e-06,
      "loss": 1.0663,
      "step": 8810
    },
    {
      "epoch": 82.34895372826408,
      "grad_norm": 0.5655556917190552,
      "learning_rate": 1.5922285192100561e-06,
      "loss": 1.0606,
      "step": 8811
    },
    {
      "epoch": 82.35838491010905,
      "grad_norm": 0.5981547236442566,
      "learning_rate": 1.5905902111349192e-06,
      "loss": 1.0542,
      "step": 8812
    },
    {
      "epoch": 82.36781609195403,
      "grad_norm": 0.5976911187171936,
      "learning_rate": 1.588952673527122e-06,
      "loss": 1.0562,
      "step": 8813
    },
    {
      "epoch": 82.377247273799,
      "grad_norm": 0.5404314398765564,
      "learning_rate": 1.5873159065366971e-06,
      "loss": 1.0641,
      "step": 8814
    },
    {
      "epoch": 82.38667845564397,
      "grad_norm": 0.5600014328956604,
      "learning_rate": 1.5856799103136035e-06,
      "loss": 1.0347,
      "step": 8815
    },
    {
      "epoch": 82.39610963748895,
      "grad_norm": 0.5944271683692932,
      "learning_rate": 1.5840446850077285e-06,
      "loss": 1.0176,
      "step": 8816
    },
    {
      "epoch": 82.40554081933392,
      "grad_norm": 0.5458071231842041,
      "learning_rate": 1.582410230768895e-06,
      "loss": 1.0264,
      "step": 8817
    },
    {
      "epoch": 82.4149720011789,
      "grad_norm": 0.589745283126831,
      "learning_rate": 1.5807765477468496e-06,
      "loss": 1.0457,
      "step": 8818
    },
    {
      "epoch": 82.42440318302387,
      "grad_norm": 0.531886100769043,
      "learning_rate": 1.5791436360912705e-06,
      "loss": 1.0373,
      "step": 8819
    },
    {
      "epoch": 82.43383436486884,
      "grad_norm": 0.5205106139183044,
      "learning_rate": 1.5775114959517635e-06,
      "loss": 1.029,
      "step": 8820
    },
    {
      "epoch": 82.44326554671382,
      "grad_norm": 0.554115355014801,
      "learning_rate": 1.5758801274778645e-06,
      "loss": 1.0318,
      "step": 8821
    },
    {
      "epoch": 82.45269672855879,
      "grad_norm": 0.5826672911643982,
      "learning_rate": 1.5742495308190398e-06,
      "loss": 1.0729,
      "step": 8822
    },
    {
      "epoch": 82.46212791040377,
      "grad_norm": 0.543736457824707,
      "learning_rate": 1.572619706124683e-06,
      "loss": 1.0396,
      "step": 8823
    },
    {
      "epoch": 82.47155909224875,
      "grad_norm": 0.595568060874939,
      "learning_rate": 1.5709906535441165e-06,
      "loss": 1.0507,
      "step": 8824
    },
    {
      "epoch": 82.48099027409373,
      "grad_norm": 0.619164764881134,
      "learning_rate": 1.5693623732265995e-06,
      "loss": 1.0509,
      "step": 8825
    },
    {
      "epoch": 82.4904214559387,
      "grad_norm": 0.5872036814689636,
      "learning_rate": 1.5677348653213099e-06,
      "loss": 1.0247,
      "step": 8826
    },
    {
      "epoch": 82.49985263778368,
      "grad_norm": 0.5251708626747131,
      "learning_rate": 1.5661081299773594e-06,
      "loss": 1.0415,
      "step": 8827
    },
    {
      "epoch": 82.50928381962865,
      "grad_norm": 0.5703328251838684,
      "learning_rate": 1.5644821673437916e-06,
      "loss": 1.0753,
      "step": 8828
    },
    {
      "epoch": 82.51871500147362,
      "grad_norm": 0.5567609667778015,
      "learning_rate": 1.5628569775695746e-06,
      "loss": 1.0153,
      "step": 8829
    },
    {
      "epoch": 82.5281461833186,
      "grad_norm": 0.5674499273300171,
      "learning_rate": 1.5612325608036072e-06,
      "loss": 1.0356,
      "step": 8830
    },
    {
      "epoch": 82.53757736516357,
      "grad_norm": 0.5339727401733398,
      "learning_rate": 1.5596089171947171e-06,
      "loss": 1.0585,
      "step": 8831
    },
    {
      "epoch": 82.54700854700855,
      "grad_norm": 0.5695894360542297,
      "learning_rate": 1.5579860468916662e-06,
      "loss": 1.0966,
      "step": 8832
    },
    {
      "epoch": 82.55643972885352,
      "grad_norm": 0.5268008708953857,
      "learning_rate": 1.5563639500431393e-06,
      "loss": 1.09,
      "step": 8833
    },
    {
      "epoch": 82.5658709106985,
      "grad_norm": 0.5818716287612915,
      "learning_rate": 1.5547426267977506e-06,
      "loss": 1.0397,
      "step": 8834
    },
    {
      "epoch": 82.57530209254347,
      "grad_norm": 0.5416366457939148,
      "learning_rate": 1.553122077304048e-06,
      "loss": 1.0409,
      "step": 8835
    },
    {
      "epoch": 82.58473327438844,
      "grad_norm": 0.551210880279541,
      "learning_rate": 1.5515023017105036e-06,
      "loss": 1.0783,
      "step": 8836
    },
    {
      "epoch": 82.59416445623341,
      "grad_norm": 0.5565056204795837,
      "learning_rate": 1.5498833001655212e-06,
      "loss": 1.0806,
      "step": 8837
    },
    {
      "epoch": 82.6035956380784,
      "grad_norm": 0.6069784760475159,
      "learning_rate": 1.5482650728174342e-06,
      "loss": 1.0349,
      "step": 8838
    },
    {
      "epoch": 82.61302681992338,
      "grad_norm": 0.5761025547981262,
      "learning_rate": 1.5466476198144997e-06,
      "loss": 1.0474,
      "step": 8839
    },
    {
      "epoch": 82.62245800176835,
      "grad_norm": 0.5435826778411865,
      "learning_rate": 1.5450309413049146e-06,
      "loss": 1.0361,
      "step": 8840
    },
    {
      "epoch": 82.63188918361332,
      "grad_norm": 0.5454633235931396,
      "learning_rate": 1.5434150374367962e-06,
      "loss": 1.0317,
      "step": 8841
    },
    {
      "epoch": 82.6413203654583,
      "grad_norm": 0.5588281154632568,
      "learning_rate": 1.5417999083581925e-06,
      "loss": 1.0608,
      "step": 8842
    },
    {
      "epoch": 82.65075154730327,
      "grad_norm": 0.5663726925849915,
      "learning_rate": 1.5401855542170808e-06,
      "loss": 1.046,
      "step": 8843
    },
    {
      "epoch": 82.66018272914825,
      "grad_norm": 0.5340692400932312,
      "learning_rate": 1.538571975161367e-06,
      "loss": 1.0426,
      "step": 8844
    },
    {
      "epoch": 82.66961391099322,
      "grad_norm": 0.5521204471588135,
      "learning_rate": 1.5369591713388886e-06,
      "loss": 1.0527,
      "step": 8845
    },
    {
      "epoch": 82.6790450928382,
      "grad_norm": 0.5853227972984314,
      "learning_rate": 1.5353471428974065e-06,
      "loss": 1.0412,
      "step": 8846
    },
    {
      "epoch": 82.68847627468317,
      "grad_norm": 0.4818466007709503,
      "learning_rate": 1.533735889984619e-06,
      "loss": 1.063,
      "step": 8847
    },
    {
      "epoch": 82.69790745652814,
      "grad_norm": 0.5293087363243103,
      "learning_rate": 1.5321254127481467e-06,
      "loss": 1.06,
      "step": 8848
    },
    {
      "epoch": 82.70733863837312,
      "grad_norm": 0.5520266890525818,
      "learning_rate": 1.5305157113355406e-06,
      "loss": 1.0331,
      "step": 8849
    },
    {
      "epoch": 82.71676982021809,
      "grad_norm": 0.5490082502365112,
      "learning_rate": 1.528906785894282e-06,
      "loss": 1.0756,
      "step": 8850
    },
    {
      "epoch": 82.72620100206306,
      "grad_norm": 0.5430033802986145,
      "learning_rate": 1.5272986365717757e-06,
      "loss": 1.0629,
      "step": 8851
    },
    {
      "epoch": 82.73563218390805,
      "grad_norm": 0.5553225874900818,
      "learning_rate": 1.5256912635153664e-06,
      "loss": 1.0391,
      "step": 8852
    },
    {
      "epoch": 82.74506336575303,
      "grad_norm": 0.5564853549003601,
      "learning_rate": 1.5240846668723175e-06,
      "loss": 1.0215,
      "step": 8853
    },
    {
      "epoch": 82.754494547598,
      "grad_norm": 0.5398053526878357,
      "learning_rate": 1.5224788467898256e-06,
      "loss": 1.0968,
      "step": 8854
    },
    {
      "epoch": 82.76392572944297,
      "grad_norm": 0.5753728151321411,
      "learning_rate": 1.520873803415015e-06,
      "loss": 1.0689,
      "step": 8855
    },
    {
      "epoch": 82.77335691128795,
      "grad_norm": 0.563252866268158,
      "learning_rate": 1.5192695368949384e-06,
      "loss": 1.074,
      "step": 8856
    },
    {
      "epoch": 82.78278809313292,
      "grad_norm": 0.5423652529716492,
      "learning_rate": 1.5176660473765803e-06,
      "loss": 1.0052,
      "step": 8857
    },
    {
      "epoch": 82.7922192749779,
      "grad_norm": 0.5201429724693298,
      "learning_rate": 1.516063335006851e-06,
      "loss": 1.0592,
      "step": 8858
    },
    {
      "epoch": 82.80165045682287,
      "grad_norm": 0.537847638130188,
      "learning_rate": 1.5144613999325908e-06,
      "loss": 1.0429,
      "step": 8859
    },
    {
      "epoch": 82.81108163866784,
      "grad_norm": 0.5278856158256531,
      "learning_rate": 1.5128602423005668e-06,
      "loss": 1.0656,
      "step": 8860
    },
    {
      "epoch": 82.82051282051282,
      "grad_norm": 0.6086709499359131,
      "learning_rate": 1.511259862257477e-06,
      "loss": 1.0464,
      "step": 8861
    },
    {
      "epoch": 82.82994400235779,
      "grad_norm": 0.5446614027023315,
      "learning_rate": 1.5096602599499488e-06,
      "loss": 1.0562,
      "step": 8862
    },
    {
      "epoch": 82.83937518420277,
      "grad_norm": 0.578742265701294,
      "learning_rate": 1.5080614355245383e-06,
      "loss": 1.0304,
      "step": 8863
    },
    {
      "epoch": 82.84880636604774,
      "grad_norm": 0.5306295156478882,
      "learning_rate": 1.506463389127727e-06,
      "loss": 1.0419,
      "step": 8864
    },
    {
      "epoch": 82.85823754789271,
      "grad_norm": 0.5201399922370911,
      "learning_rate": 1.504866120905929e-06,
      "loss": 1.0527,
      "step": 8865
    },
    {
      "epoch": 82.8676687297377,
      "grad_norm": 0.5387895703315735,
      "learning_rate": 1.503269631005484e-06,
      "loss": 1.0968,
      "step": 8866
    },
    {
      "epoch": 82.87709991158268,
      "grad_norm": 0.5506770610809326,
      "learning_rate": 1.5016739195726626e-06,
      "loss": 1.0413,
      "step": 8867
    },
    {
      "epoch": 82.88653109342765,
      "grad_norm": 0.562502920627594,
      "learning_rate": 1.5000789867536603e-06,
      "loss": 1.0368,
      "step": 8868
    },
    {
      "epoch": 82.89596227527262,
      "grad_norm": 0.5564250946044922,
      "learning_rate": 1.49848483269461e-06,
      "loss": 1.0726,
      "step": 8869
    },
    {
      "epoch": 82.9053934571176,
      "grad_norm": 0.5569106340408325,
      "learning_rate": 1.496891457541565e-06,
      "loss": 1.0011,
      "step": 8870
    },
    {
      "epoch": 82.91482463896257,
      "grad_norm": 0.5305922627449036,
      "learning_rate": 1.4952988614405095e-06,
      "loss": 1.0631,
      "step": 8871
    },
    {
      "epoch": 82.92425582080755,
      "grad_norm": 0.5387310981750488,
      "learning_rate": 1.493707044537357e-06,
      "loss": 1.0687,
      "step": 8872
    },
    {
      "epoch": 82.93368700265252,
      "grad_norm": 0.5484930276870728,
      "learning_rate": 1.4921160069779494e-06,
      "loss": 1.0451,
      "step": 8873
    },
    {
      "epoch": 82.9431181844975,
      "grad_norm": 0.546074628829956,
      "learning_rate": 1.490525748908056e-06,
      "loss": 1.0692,
      "step": 8874
    },
    {
      "epoch": 82.95254936634247,
      "grad_norm": 0.5623008608818054,
      "learning_rate": 1.4889362704733768e-06,
      "loss": 1.0416,
      "step": 8875
    },
    {
      "epoch": 82.96198054818744,
      "grad_norm": 0.5477352738380432,
      "learning_rate": 1.4873475718195362e-06,
      "loss": 1.028,
      "step": 8876
    },
    {
      "epoch": 82.97141173003241,
      "grad_norm": 0.5156565308570862,
      "learning_rate": 1.4857596530920948e-06,
      "loss": 1.083,
      "step": 8877
    },
    {
      "epoch": 82.98084291187739,
      "grad_norm": 0.5546860694885254,
      "learning_rate": 1.484172514436536e-06,
      "loss": 1.0387,
      "step": 8878
    },
    {
      "epoch": 82.99027409372236,
      "grad_norm": 0.5266367793083191,
      "learning_rate": 1.4825861559982724e-06,
      "loss": 1.067,
      "step": 8879
    },
    {
      "epoch": 82.99970527556735,
      "grad_norm": 0.5458305478096008,
      "learning_rate": 1.4810005779226444e-06,
      "loss": 1.0419,
      "step": 8880
    },
    {
      "epoch": 83.0,
      "grad_norm": 3.081566333770752,
      "learning_rate": 1.4794157803549236e-06,
      "loss": 0.7557,
      "step": 8881
    },
    {
      "epoch": 83.00943118184497,
      "grad_norm": 0.5261567234992981,
      "learning_rate": 1.4778317634403082e-06,
      "loss": 1.0467,
      "step": 8882
    },
    {
      "epoch": 83.01886236368995,
      "grad_norm": 0.5566474795341492,
      "learning_rate": 1.4762485273239235e-06,
      "loss": 1.0584,
      "step": 8883
    },
    {
      "epoch": 83.02829354553492,
      "grad_norm": 0.5096016526222229,
      "learning_rate": 1.4746660721508288e-06,
      "loss": 1.0572,
      "step": 8884
    },
    {
      "epoch": 83.0377247273799,
      "grad_norm": 0.5163310766220093,
      "learning_rate": 1.473084398066007e-06,
      "loss": 1.0791,
      "step": 8885
    },
    {
      "epoch": 83.04715590922487,
      "grad_norm": 0.5792574286460876,
      "learning_rate": 1.471503505214369e-06,
      "loss": 1.0279,
      "step": 8886
    },
    {
      "epoch": 83.05658709106984,
      "grad_norm": 0.5844318270683289,
      "learning_rate": 1.4699233937407565e-06,
      "loss": 1.0284,
      "step": 8887
    },
    {
      "epoch": 83.06601827291482,
      "grad_norm": 0.5429049134254456,
      "learning_rate": 1.4683440637899381e-06,
      "loss": 1.0394,
      "step": 8888
    },
    {
      "epoch": 83.0754494547598,
      "grad_norm": 0.5193619132041931,
      "learning_rate": 1.466765515506613e-06,
      "loss": 1.0516,
      "step": 8889
    },
    {
      "epoch": 83.08488063660478,
      "grad_norm": 0.5629513263702393,
      "learning_rate": 1.465187749035406e-06,
      "loss": 1.0575,
      "step": 8890
    },
    {
      "epoch": 83.09431181844975,
      "grad_norm": 0.5882910490036011,
      "learning_rate": 1.4636107645208697e-06,
      "loss": 1.0324,
      "step": 8891
    },
    {
      "epoch": 83.10374300029473,
      "grad_norm": 0.5669862031936646,
      "learning_rate": 1.4620345621074916e-06,
      "loss": 1.0601,
      "step": 8892
    },
    {
      "epoch": 83.1131741821397,
      "grad_norm": 0.5523405075073242,
      "learning_rate": 1.4604591419396797e-06,
      "loss": 1.0368,
      "step": 8893
    },
    {
      "epoch": 83.12260536398468,
      "grad_norm": 0.5379165410995483,
      "learning_rate": 1.458884504161775e-06,
      "loss": 1.0823,
      "step": 8894
    },
    {
      "epoch": 83.13203654582965,
      "grad_norm": 0.5524871945381165,
      "learning_rate": 1.4573106489180432e-06,
      "loss": 1.0408,
      "step": 8895
    },
    {
      "epoch": 83.14146772767462,
      "grad_norm": 0.57261723279953,
      "learning_rate": 1.4557375763526827e-06,
      "loss": 1.056,
      "step": 8896
    },
    {
      "epoch": 83.1508989095196,
      "grad_norm": 0.5816574096679688,
      "learning_rate": 1.4541652866098166e-06,
      "loss": 1.0315,
      "step": 8897
    },
    {
      "epoch": 83.16033009136457,
      "grad_norm": 0.5721120834350586,
      "learning_rate": 1.4525937798334955e-06,
      "loss": 1.0625,
      "step": 8898
    },
    {
      "epoch": 83.16976127320955,
      "grad_norm": 0.5247390270233154,
      "learning_rate": 1.4510230561677053e-06,
      "loss": 1.0247,
      "step": 8899
    },
    {
      "epoch": 83.17919245505452,
      "grad_norm": 0.5464910864830017,
      "learning_rate": 1.4494531157563518e-06,
      "loss": 1.0858,
      "step": 8900
    },
    {
      "epoch": 83.1886236368995,
      "grad_norm": 0.5109317302703857,
      "learning_rate": 1.4478839587432735e-06,
      "loss": 1.0437,
      "step": 8901
    },
    {
      "epoch": 83.19805481874447,
      "grad_norm": 0.5288130640983582,
      "learning_rate": 1.4463155852722355e-06,
      "loss": 1.031,
      "step": 8902
    },
    {
      "epoch": 83.20748600058945,
      "grad_norm": 0.5860071182250977,
      "learning_rate": 1.4447479954869315e-06,
      "loss": 1.0516,
      "step": 8903
    },
    {
      "epoch": 83.21691718243443,
      "grad_norm": 0.5442538857460022,
      "learning_rate": 1.4431811895309845e-06,
      "loss": 1.0526,
      "step": 8904
    },
    {
      "epoch": 83.2263483642794,
      "grad_norm": 0.5433501601219177,
      "learning_rate": 1.4416151675479439e-06,
      "loss": 1.0397,
      "step": 8905
    },
    {
      "epoch": 83.23577954612438,
      "grad_norm": 0.5451092720031738,
      "learning_rate": 1.4400499296812865e-06,
      "loss": 1.0812,
      "step": 8906
    },
    {
      "epoch": 83.24521072796935,
      "grad_norm": 0.5236925482749939,
      "learning_rate": 1.4384854760744215e-06,
      "loss": 1.0632,
      "step": 8907
    },
    {
      "epoch": 83.25464190981432,
      "grad_norm": 0.5330523252487183,
      "learning_rate": 1.4369218068706847e-06,
      "loss": 1.0828,
      "step": 8908
    },
    {
      "epoch": 83.2640730916593,
      "grad_norm": 0.5694047212600708,
      "learning_rate": 1.4353589222133357e-06,
      "loss": 1.0706,
      "step": 8909
    },
    {
      "epoch": 83.27350427350427,
      "grad_norm": 0.529358446598053,
      "learning_rate": 1.4337968222455678e-06,
      "loss": 1.0555,
      "step": 8910
    },
    {
      "epoch": 83.28293545534925,
      "grad_norm": 0.5190234780311584,
      "learning_rate": 1.4322355071104988e-06,
      "loss": 1.0545,
      "step": 8911
    },
    {
      "epoch": 83.29236663719422,
      "grad_norm": 0.5384552478790283,
      "learning_rate": 1.4306749769511729e-06,
      "loss": 1.038,
      "step": 8912
    },
    {
      "epoch": 83.3017978190392,
      "grad_norm": 0.5730909705162048,
      "learning_rate": 1.4291152319105728e-06,
      "loss": 1.0679,
      "step": 8913
    },
    {
      "epoch": 83.31122900088417,
      "grad_norm": 0.4944327175617218,
      "learning_rate": 1.4275562721315962e-06,
      "loss": 1.0635,
      "step": 8914
    },
    {
      "epoch": 83.32066018272914,
      "grad_norm": 0.5719870924949646,
      "learning_rate": 1.4259980977570765e-06,
      "loss": 1.046,
      "step": 8915
    },
    {
      "epoch": 83.33009136457412,
      "grad_norm": 0.5863929986953735,
      "learning_rate": 1.424440708929773e-06,
      "loss": 1.0483,
      "step": 8916
    },
    {
      "epoch": 83.3395225464191,
      "grad_norm": 0.5500187277793884,
      "learning_rate": 1.4228841057923703e-06,
      "loss": 1.0394,
      "step": 8917
    },
    {
      "epoch": 83.34895372826408,
      "grad_norm": 0.5510733127593994,
      "learning_rate": 1.421328288487488e-06,
      "loss": 1.0248,
      "step": 8918
    },
    {
      "epoch": 83.35838491010905,
      "grad_norm": 0.5433348417282104,
      "learning_rate": 1.4197732571576684e-06,
      "loss": 1.0873,
      "step": 8919
    },
    {
      "epoch": 83.36781609195403,
      "grad_norm": 0.5526894330978394,
      "learning_rate": 1.418219011945382e-06,
      "loss": 1.0277,
      "step": 8920
    },
    {
      "epoch": 83.377247273799,
      "grad_norm": 0.5677133202552795,
      "learning_rate": 1.4166655529930296e-06,
      "loss": 1.0939,
      "step": 8921
    },
    {
      "epoch": 83.38667845564397,
      "grad_norm": 0.5632799863815308,
      "learning_rate": 1.4151128804429349e-06,
      "loss": 1.0545,
      "step": 8922
    },
    {
      "epoch": 83.39610963748895,
      "grad_norm": 0.5494099259376526,
      "learning_rate": 1.4135609944373585e-06,
      "loss": 1.0699,
      "step": 8923
    },
    {
      "epoch": 83.40554081933392,
      "grad_norm": 0.5250673890113831,
      "learning_rate": 1.4120098951184802e-06,
      "loss": 1.0485,
      "step": 8924
    },
    {
      "epoch": 83.4149720011789,
      "grad_norm": 0.5795271396636963,
      "learning_rate": 1.410459582628413e-06,
      "loss": 1.0502,
      "step": 8925
    },
    {
      "epoch": 83.42440318302387,
      "grad_norm": 0.5562559962272644,
      "learning_rate": 1.4089100571091952e-06,
      "loss": 1.078,
      "step": 8926
    },
    {
      "epoch": 83.43383436486884,
      "grad_norm": 0.5631839632987976,
      "learning_rate": 1.4073613187027935e-06,
      "loss": 1.0513,
      "step": 8927
    },
    {
      "epoch": 83.44326554671382,
      "grad_norm": 0.5471469759941101,
      "learning_rate": 1.405813367551101e-06,
      "loss": 1.0573,
      "step": 8928
    },
    {
      "epoch": 83.45269672855879,
      "grad_norm": 0.5471915006637573,
      "learning_rate": 1.4042662037959454e-06,
      "loss": 1.0344,
      "step": 8929
    },
    {
      "epoch": 83.46212791040377,
      "grad_norm": 0.564814031124115,
      "learning_rate": 1.4027198275790732e-06,
      "loss": 1.0863,
      "step": 8930
    },
    {
      "epoch": 83.47155909224875,
      "grad_norm": 0.5602453947067261,
      "learning_rate": 1.4011742390421656e-06,
      "loss": 1.0594,
      "step": 8931
    },
    {
      "epoch": 83.48099027409373,
      "grad_norm": 0.5617002844810486,
      "learning_rate": 1.3996294383268272e-06,
      "loss": 1.0782,
      "step": 8932
    },
    {
      "epoch": 83.4904214559387,
      "grad_norm": 0.5396111607551575,
      "learning_rate": 1.3980854255745924e-06,
      "loss": 1.0457,
      "step": 8933
    },
    {
      "epoch": 83.49985263778368,
      "grad_norm": 0.5388100147247314,
      "learning_rate": 1.3965422009269224e-06,
      "loss": 1.0431,
      "step": 8934
    },
    {
      "epoch": 83.50928381962865,
      "grad_norm": 0.545208752155304,
      "learning_rate": 1.394999764525209e-06,
      "loss": 1.0375,
      "step": 8935
    },
    {
      "epoch": 83.51871500147362,
      "grad_norm": 0.5502707362174988,
      "learning_rate": 1.3934581165107653e-06,
      "loss": 1.0211,
      "step": 8936
    },
    {
      "epoch": 83.5281461833186,
      "grad_norm": 0.5499414801597595,
      "learning_rate": 1.3919172570248418e-06,
      "loss": 1.031,
      "step": 8937
    },
    {
      "epoch": 83.53757736516357,
      "grad_norm": 0.4986182451248169,
      "learning_rate": 1.3903771862086101e-06,
      "loss": 1.0843,
      "step": 8938
    },
    {
      "epoch": 83.54700854700855,
      "grad_norm": 0.5418546199798584,
      "learning_rate": 1.3888379042031698e-06,
      "loss": 1.0804,
      "step": 8939
    },
    {
      "epoch": 83.55643972885352,
      "grad_norm": 0.5305761098861694,
      "learning_rate": 1.3872994111495497e-06,
      "loss": 1.0115,
      "step": 8940
    },
    {
      "epoch": 83.5658709106985,
      "grad_norm": 0.5418623685836792,
      "learning_rate": 1.3857617071887075e-06,
      "loss": 1.016,
      "step": 8941
    },
    {
      "epoch": 83.57530209254347,
      "grad_norm": 0.6021710634231567,
      "learning_rate": 1.384224792461525e-06,
      "loss": 1.0788,
      "step": 8942
    },
    {
      "epoch": 83.58473327438844,
      "grad_norm": 0.5139515399932861,
      "learning_rate": 1.3826886671088124e-06,
      "loss": 1.0174,
      "step": 8943
    },
    {
      "epoch": 83.59416445623341,
      "grad_norm": 0.5754347443580627,
      "learning_rate": 1.3811533312713131e-06,
      "loss": 1.015,
      "step": 8944
    },
    {
      "epoch": 83.6035956380784,
      "grad_norm": 0.586254358291626,
      "learning_rate": 1.3796187850896935e-06,
      "loss": 1.0719,
      "step": 8945
    },
    {
      "epoch": 83.61302681992338,
      "grad_norm": 0.5818382501602173,
      "learning_rate": 1.3780850287045456e-06,
      "loss": 1.0683,
      "step": 8946
    },
    {
      "epoch": 83.62245800176835,
      "grad_norm": 0.590300977230072,
      "learning_rate": 1.3765520622563944e-06,
      "loss": 1.0372,
      "step": 8947
    },
    {
      "epoch": 83.63188918361332,
      "grad_norm": 0.5550846457481384,
      "learning_rate": 1.3750198858856877e-06,
      "loss": 1.0686,
      "step": 8948
    },
    {
      "epoch": 83.6413203654583,
      "grad_norm": 0.5326719284057617,
      "learning_rate": 1.3734884997328035e-06,
      "loss": 1.0565,
      "step": 8949
    },
    {
      "epoch": 83.65075154730327,
      "grad_norm": 0.5658915042877197,
      "learning_rate": 1.371957903938047e-06,
      "loss": 1.0646,
      "step": 8950
    },
    {
      "epoch": 83.66018272914825,
      "grad_norm": 0.5701557397842407,
      "learning_rate": 1.3704280986416496e-06,
      "loss": 1.0222,
      "step": 8951
    },
    {
      "epoch": 83.66961391099322,
      "grad_norm": 0.5846172571182251,
      "learning_rate": 1.3688990839837747e-06,
      "loss": 1.0156,
      "step": 8952
    },
    {
      "epoch": 83.6790450928382,
      "grad_norm": 0.5407212376594543,
      "learning_rate": 1.3673708601045078e-06,
      "loss": 1.0665,
      "step": 8953
    },
    {
      "epoch": 83.68847627468317,
      "grad_norm": 0.5574654936790466,
      "learning_rate": 1.3658434271438647e-06,
      "loss": 1.0289,
      "step": 8954
    },
    {
      "epoch": 83.69790745652814,
      "grad_norm": 0.5490780472755432,
      "learning_rate": 1.3643167852417894e-06,
      "loss": 0.9973,
      "step": 8955
    },
    {
      "epoch": 83.70733863837312,
      "grad_norm": 0.5526649355888367,
      "learning_rate": 1.3627909345381508e-06,
      "loss": 1.0519,
      "step": 8956
    },
    {
      "epoch": 83.71676982021809,
      "grad_norm": 0.5724825263023376,
      "learning_rate": 1.3612658751727481e-06,
      "loss": 1.0712,
      "step": 8957
    },
    {
      "epoch": 83.72620100206306,
      "grad_norm": 0.5435836911201477,
      "learning_rate": 1.359741607285303e-06,
      "loss": 1.009,
      "step": 8958
    },
    {
      "epoch": 83.73563218390805,
      "grad_norm": 0.5298847556114197,
      "learning_rate": 1.358218131015473e-06,
      "loss": 1.0538,
      "step": 8959
    },
    {
      "epoch": 83.74506336575303,
      "grad_norm": 0.5858350992202759,
      "learning_rate": 1.3566954465028382e-06,
      "loss": 1.0577,
      "step": 8960
    },
    {
      "epoch": 83.754494547598,
      "grad_norm": 0.5236084461212158,
      "learning_rate": 1.355173553886905e-06,
      "loss": 1.0358,
      "step": 8961
    },
    {
      "epoch": 83.76392572944297,
      "grad_norm": 0.5334238409996033,
      "learning_rate": 1.3536524533071082e-06,
      "loss": 1.032,
      "step": 8962
    },
    {
      "epoch": 83.77335691128795,
      "grad_norm": 0.5453446507453918,
      "learning_rate": 1.3521321449028114e-06,
      "loss": 1.0335,
      "step": 8963
    },
    {
      "epoch": 83.78278809313292,
      "grad_norm": 0.6097311973571777,
      "learning_rate": 1.3506126288133048e-06,
      "loss": 1.0304,
      "step": 8964
    },
    {
      "epoch": 83.7922192749779,
      "grad_norm": 0.538429856300354,
      "learning_rate": 1.349093905177803e-06,
      "loss": 1.0519,
      "step": 8965
    },
    {
      "epoch": 83.80165045682287,
      "grad_norm": 0.5439620018005371,
      "learning_rate": 1.3475759741354566e-06,
      "loss": 1.0647,
      "step": 8966
    },
    {
      "epoch": 83.81108163866784,
      "grad_norm": 0.5761321783065796,
      "learning_rate": 1.3460588358253345e-06,
      "loss": 1.0546,
      "step": 8967
    },
    {
      "epoch": 83.82051282051282,
      "grad_norm": 0.5442655682563782,
      "learning_rate": 1.3445424903864367e-06,
      "loss": 1.0279,
      "step": 8968
    },
    {
      "epoch": 83.82994400235779,
      "grad_norm": 0.6037700772285461,
      "learning_rate": 1.3430269379576911e-06,
      "loss": 1.0133,
      "step": 8969
    },
    {
      "epoch": 83.83937518420277,
      "grad_norm": 0.5888000726699829,
      "learning_rate": 1.3415121786779507e-06,
      "loss": 1.0409,
      "step": 8970
    },
    {
      "epoch": 83.84880636604774,
      "grad_norm": 0.5929722785949707,
      "learning_rate": 1.339998212685999e-06,
      "loss": 1.015,
      "step": 8971
    },
    {
      "epoch": 83.85823754789271,
      "grad_norm": 0.5149104595184326,
      "learning_rate": 1.3384850401205429e-06,
      "loss": 1.0577,
      "step": 8972
    },
    {
      "epoch": 83.8676687297377,
      "grad_norm": 0.5132752060890198,
      "learning_rate": 1.3369726611202195e-06,
      "loss": 1.0412,
      "step": 8973
    },
    {
      "epoch": 83.87709991158268,
      "grad_norm": 0.5710268616676331,
      "learning_rate": 1.3354610758235931e-06,
      "loss": 1.0431,
      "step": 8974
    },
    {
      "epoch": 83.88653109342765,
      "grad_norm": 0.5359346270561218,
      "learning_rate": 1.3339502843691555e-06,
      "loss": 1.045,
      "step": 8975
    },
    {
      "epoch": 83.89596227527262,
      "grad_norm": 0.5210318565368652,
      "learning_rate": 1.3324402868953246e-06,
      "loss": 1.0428,
      "step": 8976
    },
    {
      "epoch": 83.9053934571176,
      "grad_norm": 0.5648148655891418,
      "learning_rate": 1.3309310835404443e-06,
      "loss": 1.0439,
      "step": 8977
    },
    {
      "epoch": 83.91482463896257,
      "grad_norm": 0.5418735146522522,
      "learning_rate": 1.3294226744427884e-06,
      "loss": 1.0408,
      "step": 8978
    },
    {
      "epoch": 83.92425582080755,
      "grad_norm": 0.5925694704055786,
      "learning_rate": 1.3279150597405543e-06,
      "loss": 1.0516,
      "step": 8979
    },
    {
      "epoch": 83.93368700265252,
      "grad_norm": 0.5145518779754639,
      "learning_rate": 1.3264082395718735e-06,
      "loss": 1.0508,
      "step": 8980
    },
    {
      "epoch": 83.9431181844975,
      "grad_norm": 0.5074345469474792,
      "learning_rate": 1.324902214074798e-06,
      "loss": 1.0491,
      "step": 8981
    },
    {
      "epoch": 83.95254936634247,
      "grad_norm": 0.551655650138855,
      "learning_rate": 1.3233969833873105e-06,
      "loss": 1.0132,
      "step": 8982
    },
    {
      "epoch": 83.96198054818744,
      "grad_norm": 0.5826814770698547,
      "learning_rate": 1.3218925476473188e-06,
      "loss": 1.0666,
      "step": 8983
    },
    {
      "epoch": 83.97141173003241,
      "grad_norm": 0.5423779487609863,
      "learning_rate": 1.3203889069926568e-06,
      "loss": 1.059,
      "step": 8984
    },
    {
      "epoch": 83.98084291187739,
      "grad_norm": 0.5525407195091248,
      "learning_rate": 1.318886061561091e-06,
      "loss": 1.0476,
      "step": 8985
    },
    {
      "epoch": 83.99027409372236,
      "grad_norm": 0.5399683713912964,
      "learning_rate": 1.3173840114903114e-06,
      "loss": 1.0402,
      "step": 8986
    },
    {
      "epoch": 83.99970527556735,
      "grad_norm": 0.5539408922195435,
      "learning_rate": 1.3158827569179333e-06,
      "loss": 1.0617,
      "step": 8987
    },
    {
      "epoch": 84.0,
      "grad_norm": 2.4648983478546143,
      "learning_rate": 1.3143822979815012e-06,
      "loss": 0.7593,
      "step": 8988
    },
    {
      "epoch": 84.00943118184497,
      "grad_norm": 0.547435998916626,
      "learning_rate": 1.3128826348184886e-06,
      "loss": 1.039,
      "step": 8989
    },
    {
      "epoch": 84.01886236368995,
      "grad_norm": 0.5491089820861816,
      "learning_rate": 1.3113837675662933e-06,
      "loss": 1.0472,
      "step": 8990
    },
    {
      "epoch": 84.02829354553492,
      "grad_norm": 0.5294445157051086,
      "learning_rate": 1.3098856963622408e-06,
      "loss": 1.0546,
      "step": 8991
    },
    {
      "epoch": 84.0377247273799,
      "grad_norm": 0.5691353678703308,
      "learning_rate": 1.3083884213435837e-06,
      "loss": 1.0614,
      "step": 8992
    },
    {
      "epoch": 84.04715590922487,
      "grad_norm": 0.5492202043533325,
      "learning_rate": 1.3068919426475024e-06,
      "loss": 1.0562,
      "step": 8993
    },
    {
      "epoch": 84.05658709106984,
      "grad_norm": 0.563933253288269,
      "learning_rate": 1.3053962604111036e-06,
      "loss": 1.0833,
      "step": 8994
    },
    {
      "epoch": 84.06601827291482,
      "grad_norm": 0.5830007195472717,
      "learning_rate": 1.303901374771418e-06,
      "loss": 1.054,
      "step": 8995
    },
    {
      "epoch": 84.0754494547598,
      "grad_norm": 0.5205578804016113,
      "learning_rate": 1.3024072858654124e-06,
      "loss": 1.0635,
      "step": 8996
    },
    {
      "epoch": 84.08488063660478,
      "grad_norm": 0.5464113354682922,
      "learning_rate": 1.300913993829972e-06,
      "loss": 1.0148,
      "step": 8997
    },
    {
      "epoch": 84.09431181844975,
      "grad_norm": 0.5280104875564575,
      "learning_rate": 1.2994214988019105e-06,
      "loss": 1.0614,
      "step": 8998
    },
    {
      "epoch": 84.10374300029473,
      "grad_norm": 0.5704532861709595,
      "learning_rate": 1.297929800917972e-06,
      "loss": 1.051,
      "step": 8999
    },
    {
      "epoch": 84.1131741821397,
      "grad_norm": 0.5045729875564575,
      "learning_rate": 1.2964389003148236e-06,
      "loss": 1.0667,
      "step": 9000
    },
    {
      "epoch": 84.12260536398468,
      "grad_norm": 0.5277742147445679,
      "learning_rate": 1.294948797129062e-06,
      "loss": 1.0703,
      "step": 9001
    },
    {
      "epoch": 84.13203654582965,
      "grad_norm": 0.5627158284187317,
      "learning_rate": 1.29345949149721e-06,
      "loss": 1.033,
      "step": 9002
    },
    {
      "epoch": 84.14146772767462,
      "grad_norm": 0.5603859424591064,
      "learning_rate": 1.2919709835557148e-06,
      "loss": 1.0496,
      "step": 9003
    },
    {
      "epoch": 84.1508989095196,
      "grad_norm": 0.5701845288276672,
      "learning_rate": 1.290483273440958e-06,
      "loss": 1.0523,
      "step": 9004
    },
    {
      "epoch": 84.16033009136457,
      "grad_norm": 0.5337625741958618,
      "learning_rate": 1.2889963612892386e-06,
      "loss": 1.0839,
      "step": 9005
    },
    {
      "epoch": 84.16976127320955,
      "grad_norm": 0.6119555234909058,
      "learning_rate": 1.2875102472367895e-06,
      "loss": 1.0275,
      "step": 9006
    },
    {
      "epoch": 84.17919245505452,
      "grad_norm": 0.5524178147315979,
      "learning_rate": 1.286024931419768e-06,
      "loss": 1.0892,
      "step": 9007
    },
    {
      "epoch": 84.1886236368995,
      "grad_norm": 0.5377416014671326,
      "learning_rate": 1.2845404139742568e-06,
      "loss": 1.0618,
      "step": 9008
    },
    {
      "epoch": 84.19805481874447,
      "grad_norm": 0.5529379844665527,
      "learning_rate": 1.2830566950362678e-06,
      "loss": 1.0773,
      "step": 9009
    },
    {
      "epoch": 84.20748600058945,
      "grad_norm": 0.5272524356842041,
      "learning_rate": 1.2815737747417367e-06,
      "loss": 1.0321,
      "step": 9010
    },
    {
      "epoch": 84.21691718243443,
      "grad_norm": 0.6050757765769958,
      "learning_rate": 1.2800916532265317e-06,
      "loss": 1.0159,
      "step": 9011
    },
    {
      "epoch": 84.2263483642794,
      "grad_norm": 0.5297239422798157,
      "learning_rate": 1.278610330626442e-06,
      "loss": 1.0268,
      "step": 9012
    },
    {
      "epoch": 84.23577954612438,
      "grad_norm": 0.5705504417419434,
      "learning_rate": 1.2771298070771876e-06,
      "loss": 1.086,
      "step": 9013
    },
    {
      "epoch": 84.24521072796935,
      "grad_norm": 0.579659104347229,
      "learning_rate": 1.2756500827144125e-06,
      "loss": 1.1164,
      "step": 9014
    },
    {
      "epoch": 84.25464190981432,
      "grad_norm": 0.5252907276153564,
      "learning_rate": 1.2741711576736882e-06,
      "loss": 1.0434,
      "step": 9015
    },
    {
      "epoch": 84.2640730916593,
      "grad_norm": 0.5285199284553528,
      "learning_rate": 1.272693032090514e-06,
      "loss": 1.0752,
      "step": 9016
    },
    {
      "epoch": 84.27350427350427,
      "grad_norm": 0.5392519235610962,
      "learning_rate": 1.271215706100314e-06,
      "loss": 1.0406,
      "step": 9017
    },
    {
      "epoch": 84.28293545534925,
      "grad_norm": 0.5346189141273499,
      "learning_rate": 1.2697391798384396e-06,
      "loss": 1.0711,
      "step": 9018
    },
    {
      "epoch": 84.29236663719422,
      "grad_norm": 0.5009042620658875,
      "learning_rate": 1.2682634534401739e-06,
      "loss": 1.0629,
      "step": 9019
    },
    {
      "epoch": 84.3017978190392,
      "grad_norm": 0.597406268119812,
      "learning_rate": 1.26678852704072e-06,
      "loss": 1.0188,
      "step": 9020
    },
    {
      "epoch": 84.31122900088417,
      "grad_norm": 0.5220375061035156,
      "learning_rate": 1.2653144007752093e-06,
      "loss": 1.0404,
      "step": 9021
    },
    {
      "epoch": 84.32066018272914,
      "grad_norm": 0.5473673939704895,
      "learning_rate": 1.2638410747787022e-06,
      "loss": 1.0372,
      "step": 9022
    },
    {
      "epoch": 84.33009136457412,
      "grad_norm": 0.5211384892463684,
      "learning_rate": 1.2623685491861837e-06,
      "loss": 1.0332,
      "step": 9023
    },
    {
      "epoch": 84.3395225464191,
      "grad_norm": 0.613126277923584,
      "learning_rate": 1.260896824132566e-06,
      "loss": 1.0617,
      "step": 9024
    },
    {
      "epoch": 84.34895372826408,
      "grad_norm": 0.5555307269096375,
      "learning_rate": 1.2594258997526853e-06,
      "loss": 1.032,
      "step": 9025
    },
    {
      "epoch": 84.35838491010905,
      "grad_norm": 0.564628541469574,
      "learning_rate": 1.2579557761813133e-06,
      "loss": 0.9921,
      "step": 9026
    },
    {
      "epoch": 84.36781609195403,
      "grad_norm": 0.5649999976158142,
      "learning_rate": 1.2564864535531384e-06,
      "loss": 1.05,
      "step": 9027
    },
    {
      "epoch": 84.377247273799,
      "grad_norm": 0.5611706376075745,
      "learning_rate": 1.25501793200278e-06,
      "loss": 1.0769,
      "step": 9028
    },
    {
      "epoch": 84.38667845564397,
      "grad_norm": 0.5681462287902832,
      "learning_rate": 1.2535502116647835e-06,
      "loss": 1.0301,
      "step": 9029
    },
    {
      "epoch": 84.39610963748895,
      "grad_norm": 0.5396756529808044,
      "learning_rate": 1.2520832926736203e-06,
      "loss": 1.0796,
      "step": 9030
    },
    {
      "epoch": 84.40554081933392,
      "grad_norm": 0.5403894186019897,
      "learning_rate": 1.250617175163691e-06,
      "loss": 1.0543,
      "step": 9031
    },
    {
      "epoch": 84.4149720011789,
      "grad_norm": 0.5423005223274231,
      "learning_rate": 1.2491518592693186e-06,
      "loss": 1.0109,
      "step": 9032
    },
    {
      "epoch": 84.42440318302387,
      "grad_norm": 0.5313827991485596,
      "learning_rate": 1.2476873451247551e-06,
      "loss": 1.0494,
      "step": 9033
    },
    {
      "epoch": 84.43383436486884,
      "grad_norm": 0.5492511987686157,
      "learning_rate": 1.2462236328641807e-06,
      "loss": 1.0485,
      "step": 9034
    },
    {
      "epoch": 84.44326554671382,
      "grad_norm": 0.5903289914131165,
      "learning_rate": 1.2447607226216984e-06,
      "loss": 1.0525,
      "step": 9035
    },
    {
      "epoch": 84.45269672855879,
      "grad_norm": 0.5560922622680664,
      "learning_rate": 1.2432986145313409e-06,
      "loss": 1.0113,
      "step": 9036
    },
    {
      "epoch": 84.46212791040377,
      "grad_norm": 0.5671987533569336,
      "learning_rate": 1.2418373087270653e-06,
      "loss": 1.0234,
      "step": 9037
    },
    {
      "epoch": 84.47155909224875,
      "grad_norm": 0.5376473665237427,
      "learning_rate": 1.2403768053427568e-06,
      "loss": 1.0648,
      "step": 9038
    },
    {
      "epoch": 84.48099027409373,
      "grad_norm": 0.5671886205673218,
      "learning_rate": 1.238917104512225e-06,
      "loss": 0.9975,
      "step": 9039
    },
    {
      "epoch": 84.4904214559387,
      "grad_norm": 0.5319216847419739,
      "learning_rate": 1.2374582063692054e-06,
      "loss": 1.0751,
      "step": 9040
    },
    {
      "epoch": 84.49985263778368,
      "grad_norm": 0.5943416357040405,
      "learning_rate": 1.2360001110473652e-06,
      "loss": 1.0494,
      "step": 9041
    },
    {
      "epoch": 84.50928381962865,
      "grad_norm": 0.5741986632347107,
      "learning_rate": 1.2345428186802944e-06,
      "loss": 1.0207,
      "step": 9042
    },
    {
      "epoch": 84.51871500147362,
      "grad_norm": 0.5353195071220398,
      "learning_rate": 1.2330863294015084e-06,
      "loss": 1.0386,
      "step": 9043
    },
    {
      "epoch": 84.5281461833186,
      "grad_norm": 0.5421979427337646,
      "learning_rate": 1.2316306433444513e-06,
      "loss": 1.0223,
      "step": 9044
    },
    {
      "epoch": 84.53757736516357,
      "grad_norm": 0.5301855802536011,
      "learning_rate": 1.2301757606424891e-06,
      "loss": 1.0402,
      "step": 9045
    },
    {
      "epoch": 84.54700854700855,
      "grad_norm": 0.5579711198806763,
      "learning_rate": 1.2287216814289226e-06,
      "loss": 1.0445,
      "step": 9046
    },
    {
      "epoch": 84.55643972885352,
      "grad_norm": 0.5318895578384399,
      "learning_rate": 1.2272684058369722e-06,
      "loss": 1.054,
      "step": 9047
    },
    {
      "epoch": 84.5658709106985,
      "grad_norm": 0.530527651309967,
      "learning_rate": 1.2258159339997855e-06,
      "loss": 1.0669,
      "step": 9048
    },
    {
      "epoch": 84.57530209254347,
      "grad_norm": 0.594467282295227,
      "learning_rate": 1.2243642660504385e-06,
      "loss": 1.0022,
      "step": 9049
    },
    {
      "epoch": 84.58473327438844,
      "grad_norm": 0.5522540211677551,
      "learning_rate": 1.2229134021219303e-06,
      "loss": 1.0551,
      "step": 9050
    },
    {
      "epoch": 84.59416445623341,
      "grad_norm": 0.5429742932319641,
      "learning_rate": 1.2214633423471921e-06,
      "loss": 1.0058,
      "step": 9051
    },
    {
      "epoch": 84.6035956380784,
      "grad_norm": 0.5271663069725037,
      "learning_rate": 1.2200140868590759e-06,
      "loss": 1.0742,
      "step": 9052
    },
    {
      "epoch": 84.61302681992338,
      "grad_norm": 0.5514959096908569,
      "learning_rate": 1.218565635790363e-06,
      "loss": 1.0719,
      "step": 9053
    },
    {
      "epoch": 84.62245800176835,
      "grad_norm": 0.5494327545166016,
      "learning_rate": 1.2171179892737595e-06,
      "loss": 1.0761,
      "step": 9054
    },
    {
      "epoch": 84.63188918361332,
      "grad_norm": 0.5628781914710999,
      "learning_rate": 1.215671147441896e-06,
      "loss": 1.0447,
      "step": 9055
    },
    {
      "epoch": 84.6413203654583,
      "grad_norm": 0.5448617935180664,
      "learning_rate": 1.2142251104273362e-06,
      "loss": 1.0403,
      "step": 9056
    },
    {
      "epoch": 84.65075154730327,
      "grad_norm": 0.5019327402114868,
      "learning_rate": 1.2127798783625633e-06,
      "loss": 1.0731,
      "step": 9057
    },
    {
      "epoch": 84.66018272914825,
      "grad_norm": 0.5544483065605164,
      "learning_rate": 1.211335451379988e-06,
      "loss": 1.0586,
      "step": 9058
    },
    {
      "epoch": 84.66961391099322,
      "grad_norm": 0.5260233283042908,
      "learning_rate": 1.20989182961195e-06,
      "loss": 1.0425,
      "step": 9059
    },
    {
      "epoch": 84.6790450928382,
      "grad_norm": 0.5112469792366028,
      "learning_rate": 1.2084490131907122e-06,
      "loss": 1.0545,
      "step": 9060
    },
    {
      "epoch": 84.68847627468317,
      "grad_norm": 0.5336104035377502,
      "learning_rate": 1.2070070022484649e-06,
      "loss": 1.0201,
      "step": 9061
    },
    {
      "epoch": 84.69790745652814,
      "grad_norm": 0.5246385335922241,
      "learning_rate": 1.2055657969173252e-06,
      "loss": 1.0619,
      "step": 9062
    },
    {
      "epoch": 84.70733863837312,
      "grad_norm": 0.5410210490226746,
      "learning_rate": 1.2041253973293344e-06,
      "loss": 1.0409,
      "step": 9063
    },
    {
      "epoch": 84.71676982021809,
      "grad_norm": 0.5218647122383118,
      "learning_rate": 1.2026858036164634e-06,
      "loss": 1.0537,
      "step": 9064
    },
    {
      "epoch": 84.72620100206306,
      "grad_norm": 0.551834225654602,
      "learning_rate": 1.2012470159106083e-06,
      "loss": 1.0633,
      "step": 9065
    },
    {
      "epoch": 84.73563218390805,
      "grad_norm": 0.5499507188796997,
      "learning_rate": 1.1998090343435876e-06,
      "loss": 1.0104,
      "step": 9066
    },
    {
      "epoch": 84.74506336575303,
      "grad_norm": 0.5522611737251282,
      "learning_rate": 1.1983718590471504e-06,
      "loss": 1.0312,
      "step": 9067
    },
    {
      "epoch": 84.754494547598,
      "grad_norm": 0.5746338963508606,
      "learning_rate": 1.1969354901529694e-06,
      "loss": 1.0544,
      "step": 9068
    },
    {
      "epoch": 84.76392572944297,
      "grad_norm": 0.5418747663497925,
      "learning_rate": 1.1954999277926448e-06,
      "loss": 1.0402,
      "step": 9069
    },
    {
      "epoch": 84.77335691128795,
      "grad_norm": 0.5335525274276733,
      "learning_rate": 1.1940651720976993e-06,
      "loss": 1.0562,
      "step": 9070
    },
    {
      "epoch": 84.78278809313292,
      "grad_norm": 0.5939798951148987,
      "learning_rate": 1.1926312231995897e-06,
      "loss": 1.0147,
      "step": 9071
    },
    {
      "epoch": 84.7922192749779,
      "grad_norm": 0.5581476092338562,
      "learning_rate": 1.1911980812296919e-06,
      "loss": 1.0619,
      "step": 9072
    },
    {
      "epoch": 84.80165045682287,
      "grad_norm": 0.5517576932907104,
      "learning_rate": 1.18976574631931e-06,
      "loss": 1.0635,
      "step": 9073
    },
    {
      "epoch": 84.81108163866784,
      "grad_norm": 0.5343186259269714,
      "learning_rate": 1.1883342185996726e-06,
      "loss": 1.0652,
      "step": 9074
    },
    {
      "epoch": 84.82051282051282,
      "grad_norm": 0.5708319544792175,
      "learning_rate": 1.1869034982019377e-06,
      "loss": 1.069,
      "step": 9075
    },
    {
      "epoch": 84.82994400235779,
      "grad_norm": 0.5713857412338257,
      "learning_rate": 1.1854735852571875e-06,
      "loss": 1.0328,
      "step": 9076
    },
    {
      "epoch": 84.83937518420277,
      "grad_norm": 0.5772535800933838,
      "learning_rate": 1.1840444798964257e-06,
      "loss": 1.066,
      "step": 9077
    },
    {
      "epoch": 84.84880636604774,
      "grad_norm": 0.550412118434906,
      "learning_rate": 1.1826161822505932e-06,
      "loss": 1.0363,
      "step": 9078
    },
    {
      "epoch": 84.85823754789271,
      "grad_norm": 0.5447761416435242,
      "learning_rate": 1.1811886924505466e-06,
      "loss": 1.0136,
      "step": 9079
    },
    {
      "epoch": 84.8676687297377,
      "grad_norm": 0.5370776057243347,
      "learning_rate": 1.1797620106270724e-06,
      "loss": 1.0285,
      "step": 9080
    },
    {
      "epoch": 84.87709991158268,
      "grad_norm": 0.5667445063591003,
      "learning_rate": 1.1783361369108825e-06,
      "loss": 1.039,
      "step": 9081
    },
    {
      "epoch": 84.88653109342765,
      "grad_norm": 0.5570525527000427,
      "learning_rate": 1.1769110714326159e-06,
      "loss": 1.0438,
      "step": 9082
    },
    {
      "epoch": 84.89596227527262,
      "grad_norm": 0.5828521847724915,
      "learning_rate": 1.175486814322836e-06,
      "loss": 1.0611,
      "step": 9083
    },
    {
      "epoch": 84.9053934571176,
      "grad_norm": 0.5462924838066101,
      "learning_rate": 1.1740633657120325e-06,
      "loss": 1.0511,
      "step": 9084
    },
    {
      "epoch": 84.91482463896257,
      "grad_norm": 0.5694030523300171,
      "learning_rate": 1.1726407257306193e-06,
      "loss": 1.0558,
      "step": 9085
    },
    {
      "epoch": 84.92425582080755,
      "grad_norm": 0.5453187823295593,
      "learning_rate": 1.1712188945089432e-06,
      "loss": 1.0657,
      "step": 9086
    },
    {
      "epoch": 84.93368700265252,
      "grad_norm": 0.5444726943969727,
      "learning_rate": 1.1697978721772684e-06,
      "loss": 1.0941,
      "step": 9087
    },
    {
      "epoch": 84.9431181844975,
      "grad_norm": 0.5918197631835938,
      "learning_rate": 1.1683776588657892e-06,
      "loss": 1.0398,
      "step": 9088
    },
    {
      "epoch": 84.95254936634247,
      "grad_norm": 0.5272331237792969,
      "learning_rate": 1.1669582547046255e-06,
      "loss": 1.1006,
      "step": 9089
    },
    {
      "epoch": 84.96198054818744,
      "grad_norm": 0.5339945554733276,
      "learning_rate": 1.1655396598238222e-06,
      "loss": 1.0489,
      "step": 9090
    },
    {
      "epoch": 84.97141173003241,
      "grad_norm": 0.5567151308059692,
      "learning_rate": 1.1641218743533512e-06,
      "loss": 0.9992,
      "step": 9091
    },
    {
      "epoch": 84.98084291187739,
      "grad_norm": 0.5456095933914185,
      "learning_rate": 1.162704898423106e-06,
      "loss": 1.0513,
      "step": 9092
    },
    {
      "epoch": 84.99027409372236,
      "grad_norm": 0.5768104791641235,
      "learning_rate": 1.161288732162914e-06,
      "loss": 1.0299,
      "step": 9093
    },
    {
      "epoch": 84.99970527556735,
      "grad_norm": 0.528207540512085,
      "learning_rate": 1.1598733757025228e-06,
      "loss": 1.0253,
      "step": 9094
    },
    {
      "epoch": 85.0,
      "grad_norm": 3.160207748413086,
      "learning_rate": 1.158458829171607e-06,
      "loss": 0.7345,
      "step": 9095
    },
    {
      "epoch": 85.00943118184497,
      "grad_norm": 0.568175196647644,
      "learning_rate": 1.1570450926997657e-06,
      "loss": 1.0548,
      "step": 9096
    },
    {
      "epoch": 85.01886236368995,
      "grad_norm": 0.563820481300354,
      "learning_rate": 1.1556321664165249e-06,
      "loss": 1.0604,
      "step": 9097
    },
    {
      "epoch": 85.02829354553492,
      "grad_norm": 0.5465068221092224,
      "learning_rate": 1.154220050451338e-06,
      "loss": 1.0741,
      "step": 9098
    },
    {
      "epoch": 85.0377247273799,
      "grad_norm": 0.5437268018722534,
      "learning_rate": 1.1528087449335802e-06,
      "loss": 1.036,
      "step": 9099
    },
    {
      "epoch": 85.04715590922487,
      "grad_norm": 0.5759798884391785,
      "learning_rate": 1.1513982499925557e-06,
      "loss": 1.0798,
      "step": 9100
    },
    {
      "epoch": 85.05658709106984,
      "grad_norm": 0.5165538787841797,
      "learning_rate": 1.149988565757495e-06,
      "loss": 1.0414,
      "step": 9101
    },
    {
      "epoch": 85.06601827291482,
      "grad_norm": 0.5078194737434387,
      "learning_rate": 1.1485796923575533e-06,
      "loss": 1.0817,
      "step": 9102
    },
    {
      "epoch": 85.0754494547598,
      "grad_norm": 0.5842039585113525,
      "learning_rate": 1.1471716299218093e-06,
      "loss": 1.0785,
      "step": 9103
    },
    {
      "epoch": 85.08488063660478,
      "grad_norm": 0.5749130249023438,
      "learning_rate": 1.1457643785792694e-06,
      "loss": 1.033,
      "step": 9104
    },
    {
      "epoch": 85.09431181844975,
      "grad_norm": 0.5852944850921631,
      "learning_rate": 1.1443579384588666e-06,
      "loss": 1.0421,
      "step": 9105
    },
    {
      "epoch": 85.10374300029473,
      "grad_norm": 0.5321861505508423,
      "learning_rate": 1.1429523096894569e-06,
      "loss": 1.0421,
      "step": 9106
    },
    {
      "epoch": 85.1131741821397,
      "grad_norm": 0.5507037043571472,
      "learning_rate": 1.1415474923998227e-06,
      "loss": 1.0248,
      "step": 9107
    },
    {
      "epoch": 85.12260536398468,
      "grad_norm": 0.5534699559211731,
      "learning_rate": 1.1401434867186766e-06,
      "loss": 1.0615,
      "step": 9108
    },
    {
      "epoch": 85.13203654582965,
      "grad_norm": 0.5120121240615845,
      "learning_rate": 1.1387402927746515e-06,
      "loss": 1.0453,
      "step": 9109
    },
    {
      "epoch": 85.14146772767462,
      "grad_norm": 0.5535051226615906,
      "learning_rate": 1.1373379106963068e-06,
      "loss": 1.0175,
      "step": 9110
    },
    {
      "epoch": 85.1508989095196,
      "grad_norm": 0.562244713306427,
      "learning_rate": 1.135936340612127e-06,
      "loss": 1.0368,
      "step": 9111
    },
    {
      "epoch": 85.16033009136457,
      "grad_norm": 0.5369480848312378,
      "learning_rate": 1.1345355826505266e-06,
      "loss": 1.0287,
      "step": 9112
    },
    {
      "epoch": 85.16976127320955,
      "grad_norm": 0.5478096008300781,
      "learning_rate": 1.1331356369398416e-06,
      "loss": 1.0628,
      "step": 9113
    },
    {
      "epoch": 85.17919245505452,
      "grad_norm": 0.5386186242103577,
      "learning_rate": 1.1317365036083338e-06,
      "loss": 1.0394,
      "step": 9114
    },
    {
      "epoch": 85.1886236368995,
      "grad_norm": 0.5626429915428162,
      "learning_rate": 1.130338182784192e-06,
      "loss": 1.0436,
      "step": 9115
    },
    {
      "epoch": 85.19805481874447,
      "grad_norm": 0.5597734451293945,
      "learning_rate": 1.1289406745955267e-06,
      "loss": 1.0611,
      "step": 9116
    },
    {
      "epoch": 85.20748600058945,
      "grad_norm": 0.5717460513114929,
      "learning_rate": 1.1275439791703834e-06,
      "loss": 0.9762,
      "step": 9117
    },
    {
      "epoch": 85.21691718243443,
      "grad_norm": 0.5413662791252136,
      "learning_rate": 1.1261480966367222e-06,
      "loss": 1.0568,
      "step": 9118
    },
    {
      "epoch": 85.2263483642794,
      "grad_norm": 0.5620641112327576,
      "learning_rate": 1.124753027122435e-06,
      "loss": 1.0573,
      "step": 9119
    },
    {
      "epoch": 85.23577954612438,
      "grad_norm": 0.5271479487419128,
      "learning_rate": 1.1233587707553372e-06,
      "loss": 1.0767,
      "step": 9120
    },
    {
      "epoch": 85.24521072796935,
      "grad_norm": 0.6251580119132996,
      "learning_rate": 1.1219653276631704e-06,
      "loss": 1.0854,
      "step": 9121
    },
    {
      "epoch": 85.25464190981432,
      "grad_norm": 0.5918985605239868,
      "learning_rate": 1.1205726979735999e-06,
      "loss": 1.0501,
      "step": 9122
    },
    {
      "epoch": 85.2640730916593,
      "grad_norm": 0.5354490280151367,
      "learning_rate": 1.1191808818142202e-06,
      "loss": 1.069,
      "step": 9123
    },
    {
      "epoch": 85.27350427350427,
      "grad_norm": 0.5637355446815491,
      "learning_rate": 1.1177898793125475e-06,
      "loss": 1.0436,
      "step": 9124
    },
    {
      "epoch": 85.28293545534925,
      "grad_norm": 0.6201227903366089,
      "learning_rate": 1.1163996905960261e-06,
      "loss": 1.0693,
      "step": 9125
    },
    {
      "epoch": 85.29236663719422,
      "grad_norm": 0.5229934453964233,
      "learning_rate": 1.1150103157920245e-06,
      "loss": 1.0563,
      "step": 9126
    },
    {
      "epoch": 85.3017978190392,
      "grad_norm": 0.5409087538719177,
      "learning_rate": 1.113621755027836e-06,
      "loss": 1.0697,
      "step": 9127
    },
    {
      "epoch": 85.31122900088417,
      "grad_norm": 0.5865916013717651,
      "learning_rate": 1.1122340084306805e-06,
      "loss": 1.0151,
      "step": 9128
    },
    {
      "epoch": 85.32066018272914,
      "grad_norm": 0.5342528223991394,
      "learning_rate": 1.1108470761277024e-06,
      "loss": 1.0336,
      "step": 9129
    },
    {
      "epoch": 85.33009136457412,
      "grad_norm": 0.5496852993965149,
      "learning_rate": 1.1094609582459713e-06,
      "loss": 1.0658,
      "step": 9130
    },
    {
      "epoch": 85.3395225464191,
      "grad_norm": 0.571377694606781,
      "learning_rate": 1.108075654912485e-06,
      "loss": 1.0665,
      "step": 9131
    },
    {
      "epoch": 85.34895372826408,
      "grad_norm": 0.5514720678329468,
      "learning_rate": 1.1066911662541635e-06,
      "loss": 1.0739,
      "step": 9132
    },
    {
      "epoch": 85.35838491010905,
      "grad_norm": 0.5498279333114624,
      "learning_rate": 1.1053074923978536e-06,
      "loss": 1.04,
      "step": 9133
    },
    {
      "epoch": 85.36781609195403,
      "grad_norm": 0.5660474896430969,
      "learning_rate": 1.103924633470327e-06,
      "loss": 1.0374,
      "step": 9134
    },
    {
      "epoch": 85.377247273799,
      "grad_norm": 0.5509003400802612,
      "learning_rate": 1.102542589598279e-06,
      "loss": 1.0712,
      "step": 9135
    },
    {
      "epoch": 85.38667845564397,
      "grad_norm": 0.559363842010498,
      "learning_rate": 1.101161360908335e-06,
      "loss": 1.018,
      "step": 9136
    },
    {
      "epoch": 85.39610963748895,
      "grad_norm": 0.5204188227653503,
      "learning_rate": 1.0997809475270382e-06,
      "loss": 1.0475,
      "step": 9137
    },
    {
      "epoch": 85.40554081933392,
      "grad_norm": 0.5896538496017456,
      "learning_rate": 1.0984013495808665e-06,
      "loss": 1.0421,
      "step": 9138
    },
    {
      "epoch": 85.4149720011789,
      "grad_norm": 0.5880432724952698,
      "learning_rate": 1.0970225671962155e-06,
      "loss": 1.0033,
      "step": 9139
    },
    {
      "epoch": 85.42440318302387,
      "grad_norm": 0.5656795501708984,
      "learning_rate": 1.0956446004994104e-06,
      "loss": 1.033,
      "step": 9140
    },
    {
      "epoch": 85.43383436486884,
      "grad_norm": 0.5574269890785217,
      "learning_rate": 1.0942674496166993e-06,
      "loss": 1.0595,
      "step": 9141
    },
    {
      "epoch": 85.44326554671382,
      "grad_norm": 0.5398555397987366,
      "learning_rate": 1.092891114674256e-06,
      "loss": 1.0401,
      "step": 9142
    },
    {
      "epoch": 85.45269672855879,
      "grad_norm": 0.5600171089172363,
      "learning_rate": 1.0915155957981792e-06,
      "loss": 1.0198,
      "step": 9143
    },
    {
      "epoch": 85.46212791040377,
      "grad_norm": 0.5565510392189026,
      "learning_rate": 1.0901408931144953e-06,
      "loss": 1.0671,
      "step": 9144
    },
    {
      "epoch": 85.47155909224875,
      "grad_norm": 0.5108773708343506,
      "learning_rate": 1.0887670067491507e-06,
      "loss": 1.0725,
      "step": 9145
    },
    {
      "epoch": 85.48099027409373,
      "grad_norm": 0.5828506946563721,
      "learning_rate": 1.087393936828025e-06,
      "loss": 1.0769,
      "step": 9146
    },
    {
      "epoch": 85.4904214559387,
      "grad_norm": 0.5385055541992188,
      "learning_rate": 1.0860216834769167e-06,
      "loss": 1.0617,
      "step": 9147
    },
    {
      "epoch": 85.49985263778368,
      "grad_norm": 0.4995688199996948,
      "learning_rate": 1.0846502468215503e-06,
      "loss": 1.0509,
      "step": 9148
    },
    {
      "epoch": 85.50928381962865,
      "grad_norm": 0.5508440136909485,
      "learning_rate": 1.0832796269875757e-06,
      "loss": 1.0309,
      "step": 9149
    },
    {
      "epoch": 85.51871500147362,
      "grad_norm": 0.5530233383178711,
      "learning_rate": 1.081909824100571e-06,
      "loss": 1.032,
      "step": 9150
    },
    {
      "epoch": 85.5281461833186,
      "grad_norm": 0.502646267414093,
      "learning_rate": 1.080540838286036e-06,
      "loss": 1.0259,
      "step": 9151
    },
    {
      "epoch": 85.53757736516357,
      "grad_norm": 0.5727666616439819,
      "learning_rate": 1.0791726696693939e-06,
      "loss": 1.0387,
      "step": 9152
    },
    {
      "epoch": 85.54700854700855,
      "grad_norm": 0.5413209795951843,
      "learning_rate": 1.0778053183760006e-06,
      "loss": 1.0415,
      "step": 9153
    },
    {
      "epoch": 85.55643972885352,
      "grad_norm": 0.5488163828849792,
      "learning_rate": 1.0764387845311297e-06,
      "loss": 1.0003,
      "step": 9154
    },
    {
      "epoch": 85.5658709106985,
      "grad_norm": 0.565277636051178,
      "learning_rate": 1.075073068259984e-06,
      "loss": 1.0185,
      "step": 9155
    },
    {
      "epoch": 85.57530209254347,
      "grad_norm": 0.5972318649291992,
      "learning_rate": 1.0737081696876894e-06,
      "loss": 1.0237,
      "step": 9156
    },
    {
      "epoch": 85.58473327438844,
      "grad_norm": 0.5792096853256226,
      "learning_rate": 1.0723440889392966e-06,
      "loss": 1.0111,
      "step": 9157
    },
    {
      "epoch": 85.59416445623341,
      "grad_norm": 0.5646761655807495,
      "learning_rate": 1.0709808261397826e-06,
      "loss": 1.0376,
      "step": 9158
    },
    {
      "epoch": 85.6035956380784,
      "grad_norm": 0.5124265551567078,
      "learning_rate": 1.0696183814140494e-06,
      "loss": 1.0853,
      "step": 9159
    },
    {
      "epoch": 85.61302681992338,
      "grad_norm": 0.568834125995636,
      "learning_rate": 1.0682567548869217e-06,
      "loss": 1.0619,
      "step": 9160
    },
    {
      "epoch": 85.62245800176835,
      "grad_norm": 0.5664188265800476,
      "learning_rate": 1.0668959466831552e-06,
      "loss": 1.0322,
      "step": 9161
    },
    {
      "epoch": 85.63188918361332,
      "grad_norm": 0.5588632225990295,
      "learning_rate": 1.0655359569274248e-06,
      "loss": 1.0358,
      "step": 9162
    },
    {
      "epoch": 85.6413203654583,
      "grad_norm": 0.5565969347953796,
      "learning_rate": 1.064176785744333e-06,
      "loss": 1.067,
      "step": 9163
    },
    {
      "epoch": 85.65075154730327,
      "grad_norm": 0.547196626663208,
      "learning_rate": 1.0628184332584046e-06,
      "loss": 1.0715,
      "step": 9164
    },
    {
      "epoch": 85.66018272914825,
      "grad_norm": 0.5609121322631836,
      "learning_rate": 1.0614608995940933e-06,
      "loss": 1.0565,
      "step": 9165
    },
    {
      "epoch": 85.66961391099322,
      "grad_norm": 0.5195631384849548,
      "learning_rate": 1.0601041848757742e-06,
      "loss": 1.0806,
      "step": 9166
    },
    {
      "epoch": 85.6790450928382,
      "grad_norm": 0.5213801860809326,
      "learning_rate": 1.058748289227749e-06,
      "loss": 1.0525,
      "step": 9167
    },
    {
      "epoch": 85.68847627468317,
      "grad_norm": 0.539429783821106,
      "learning_rate": 1.057393212774248e-06,
      "loss": 1.0338,
      "step": 9168
    },
    {
      "epoch": 85.69790745652814,
      "grad_norm": 0.5782341957092285,
      "learning_rate": 1.0560389556394191e-06,
      "loss": 1.0387,
      "step": 9169
    },
    {
      "epoch": 85.70733863837312,
      "grad_norm": 0.5394437313079834,
      "learning_rate": 1.0546855179473415e-06,
      "loss": 1.0591,
      "step": 9170
    },
    {
      "epoch": 85.71676982021809,
      "grad_norm": 0.5344722867012024,
      "learning_rate": 1.0533328998220139e-06,
      "loss": 1.0488,
      "step": 9171
    },
    {
      "epoch": 85.72620100206306,
      "grad_norm": 0.4965056777000427,
      "learning_rate": 1.0519811013873649e-06,
      "loss": 1.0583,
      "step": 9172
    },
    {
      "epoch": 85.73563218390805,
      "grad_norm": 0.5748836398124695,
      "learning_rate": 1.0506301227672432e-06,
      "loss": 1.0662,
      "step": 9173
    },
    {
      "epoch": 85.74506336575303,
      "grad_norm": 0.5737093687057495,
      "learning_rate": 1.049279964085428e-06,
      "loss": 1.0681,
      "step": 9174
    },
    {
      "epoch": 85.754494547598,
      "grad_norm": 0.5637398362159729,
      "learning_rate": 1.0479306254656208e-06,
      "loss": 1.0746,
      "step": 9175
    },
    {
      "epoch": 85.76392572944297,
      "grad_norm": 0.5554900169372559,
      "learning_rate": 1.046582107031444e-06,
      "loss": 1.0535,
      "step": 9176
    },
    {
      "epoch": 85.77335691128795,
      "grad_norm": 0.5474358797073364,
      "learning_rate": 1.0452344089064526e-06,
      "loss": 1.0778,
      "step": 9177
    },
    {
      "epoch": 85.78278809313292,
      "grad_norm": 0.5618798732757568,
      "learning_rate": 1.0438875312141173e-06,
      "loss": 1.043,
      "step": 9178
    },
    {
      "epoch": 85.7922192749779,
      "grad_norm": 0.5236779451370239,
      "learning_rate": 1.0425414740778428e-06,
      "loss": 1.0818,
      "step": 9179
    },
    {
      "epoch": 85.80165045682287,
      "grad_norm": 0.5886282324790955,
      "learning_rate": 1.0411962376209538e-06,
      "loss": 1.0217,
      "step": 9180
    },
    {
      "epoch": 85.81108163866784,
      "grad_norm": 0.5003389120101929,
      "learning_rate": 1.0398518219666986e-06,
      "loss": 1.0658,
      "step": 9181
    },
    {
      "epoch": 85.82051282051282,
      "grad_norm": 0.5390351414680481,
      "learning_rate": 1.038508227238254e-06,
      "loss": 1.0444,
      "step": 9182
    },
    {
      "epoch": 85.82994400235779,
      "grad_norm": 0.5187357068061829,
      "learning_rate": 1.0371654535587173e-06,
      "loss": 1.0425,
      "step": 9183
    },
    {
      "epoch": 85.83937518420277,
      "grad_norm": 0.5948041081428528,
      "learning_rate": 1.0358235010511154e-06,
      "loss": 1.0664,
      "step": 9184
    },
    {
      "epoch": 85.84880636604774,
      "grad_norm": 0.5707967281341553,
      "learning_rate": 1.0344823698383978e-06,
      "loss": 0.986,
      "step": 9185
    },
    {
      "epoch": 85.85823754789271,
      "grad_norm": 0.537630558013916,
      "learning_rate": 1.033142060043436e-06,
      "loss": 1.0493,
      "step": 9186
    },
    {
      "epoch": 85.8676687297377,
      "grad_norm": 0.5671553611755371,
      "learning_rate": 1.0318025717890312e-06,
      "loss": 1.0265,
      "step": 9187
    },
    {
      "epoch": 85.87709991158268,
      "grad_norm": 0.5581782460212708,
      "learning_rate": 1.030463905197906e-06,
      "loss": 1.0588,
      "step": 9188
    },
    {
      "epoch": 85.88653109342765,
      "grad_norm": 0.5456379055976868,
      "learning_rate": 1.0291260603927056e-06,
      "loss": 1.0771,
      "step": 9189
    },
    {
      "epoch": 85.89596227527262,
      "grad_norm": 0.5535997152328491,
      "learning_rate": 1.0277890374960075e-06,
      "loss": 1.0574,
      "step": 9190
    },
    {
      "epoch": 85.9053934571176,
      "grad_norm": 0.6098183989524841,
      "learning_rate": 1.0264528366303084e-06,
      "loss": 1.0129,
      "step": 9191
    },
    {
      "epoch": 85.91482463896257,
      "grad_norm": 0.5528039932250977,
      "learning_rate": 1.025117457918029e-06,
      "loss": 1.0345,
      "step": 9192
    },
    {
      "epoch": 85.92425582080755,
      "grad_norm": 0.5735383629798889,
      "learning_rate": 1.023782901481517e-06,
      "loss": 1.0831,
      "step": 9193
    },
    {
      "epoch": 85.93368700265252,
      "grad_norm": 0.5503515601158142,
      "learning_rate": 1.0224491674430436e-06,
      "loss": 1.0081,
      "step": 9194
    },
    {
      "epoch": 85.9431181844975,
      "grad_norm": 0.585439920425415,
      "learning_rate": 1.0211162559248067e-06,
      "loss": 1.0558,
      "step": 9195
    },
    {
      "epoch": 85.95254936634247,
      "grad_norm": 0.5619348287582397,
      "learning_rate": 1.019784167048925e-06,
      "loss": 1.0227,
      "step": 9196
    },
    {
      "epoch": 85.96198054818744,
      "grad_norm": 0.5522433519363403,
      "learning_rate": 1.0184529009374422e-06,
      "loss": 1.0364,
      "step": 9197
    },
    {
      "epoch": 85.97141173003241,
      "grad_norm": 0.5269004106521606,
      "learning_rate": 1.017122457712334e-06,
      "loss": 1.0371,
      "step": 9198
    },
    {
      "epoch": 85.98084291187739,
      "grad_norm": 0.5848250985145569,
      "learning_rate": 1.015792837495493e-06,
      "loss": 1.0553,
      "step": 9199
    },
    {
      "epoch": 85.99027409372236,
      "grad_norm": 0.5484124422073364,
      "learning_rate": 1.014464040408737e-06,
      "loss": 1.0218,
      "step": 9200
    },
    {
      "epoch": 85.99970527556735,
      "grad_norm": 0.5654149651527405,
      "learning_rate": 1.013136066573811e-06,
      "loss": 1.0581,
      "step": 9201
    },
    {
      "epoch": 86.0,
      "grad_norm": 2.3784685134887695,
      "learning_rate": 1.0118089161123834e-06,
      "loss": 0.7387,
      "step": 9202
    },
    {
      "epoch": 86.00943118184497,
      "grad_norm": 0.54213947057724,
      "learning_rate": 1.010482589146048e-06,
      "loss": 1.0556,
      "step": 9203
    },
    {
      "epoch": 86.01886236368995,
      "grad_norm": 0.5635005831718445,
      "learning_rate": 1.0091570857963195e-06,
      "loss": 1.0474,
      "step": 9204
    },
    {
      "epoch": 86.02829354553492,
      "grad_norm": 0.5194849967956543,
      "learning_rate": 1.0078324061846434e-06,
      "loss": 1.0275,
      "step": 9205
    },
    {
      "epoch": 86.0377247273799,
      "grad_norm": 0.5675579309463501,
      "learning_rate": 1.0065085504323857e-06,
      "loss": 1.0306,
      "step": 9206
    },
    {
      "epoch": 86.04715590922487,
      "grad_norm": 0.5404160618782043,
      "learning_rate": 1.0051855186608383e-06,
      "loss": 1.0767,
      "step": 9207
    },
    {
      "epoch": 86.05658709106984,
      "grad_norm": 0.5555957555770874,
      "learning_rate": 1.0038633109912144e-06,
      "loss": 1.082,
      "step": 9208
    },
    {
      "epoch": 86.06601827291482,
      "grad_norm": 0.5017712116241455,
      "learning_rate": 1.0025419275446569e-06,
      "loss": 1.0568,
      "step": 9209
    },
    {
      "epoch": 86.0754494547598,
      "grad_norm": 0.5670874118804932,
      "learning_rate": 1.001221368442229e-06,
      "loss": 1.0329,
      "step": 9210
    },
    {
      "epoch": 86.08488063660478,
      "grad_norm": 0.5705111622810364,
      "learning_rate": 9.999016338049194e-07,
      "loss": 1.0108,
      "step": 9211
    },
    {
      "epoch": 86.09431181844975,
      "grad_norm": 0.5661201477050781,
      "learning_rate": 9.985827237536416e-07,
      "loss": 1.0298,
      "step": 9212
    },
    {
      "epoch": 86.10374300029473,
      "grad_norm": 0.5030854344367981,
      "learning_rate": 9.972646384092366e-07,
      "loss": 1.0662,
      "step": 9213
    },
    {
      "epoch": 86.1131741821397,
      "grad_norm": 0.5441567301750183,
      "learning_rate": 9.959473778924645e-07,
      "loss": 1.0566,
      "step": 9214
    },
    {
      "epoch": 86.12260536398468,
      "grad_norm": 0.5464516282081604,
      "learning_rate": 9.94630942324013e-07,
      "loss": 1.0516,
      "step": 9215
    },
    {
      "epoch": 86.13203654582965,
      "grad_norm": 0.6048507690429688,
      "learning_rate": 9.933153318244937e-07,
      "loss": 1.038,
      "step": 9216
    },
    {
      "epoch": 86.14146772767462,
      "grad_norm": 0.5242289304733276,
      "learning_rate": 9.92000546514441e-07,
      "loss": 1.0544,
      "step": 9217
    },
    {
      "epoch": 86.1508989095196,
      "grad_norm": 0.547931969165802,
      "learning_rate": 9.906865865143166e-07,
      "loss": 1.0533,
      "step": 9218
    },
    {
      "epoch": 86.16033009136457,
      "grad_norm": 0.5623027086257935,
      "learning_rate": 9.89373451944503e-07,
      "loss": 1.0217,
      "step": 9219
    },
    {
      "epoch": 86.16976127320955,
      "grad_norm": 0.5546460151672363,
      "learning_rate": 9.880611429253117e-07,
      "loss": 1.0449,
      "step": 9220
    },
    {
      "epoch": 86.17919245505452,
      "grad_norm": 0.528369665145874,
      "learning_rate": 9.867496595769754e-07,
      "loss": 1.0619,
      "step": 9221
    },
    {
      "epoch": 86.1886236368995,
      "grad_norm": 0.5183104872703552,
      "learning_rate": 9.854390020196513e-07,
      "loss": 1.0523,
      "step": 9222
    },
    {
      "epoch": 86.19805481874447,
      "grad_norm": 0.5793153047561646,
      "learning_rate": 9.841291703734223e-07,
      "loss": 1.0184,
      "step": 9223
    },
    {
      "epoch": 86.20748600058945,
      "grad_norm": 0.5243260264396667,
      "learning_rate": 9.828201647582936e-07,
      "loss": 1.0241,
      "step": 9224
    },
    {
      "epoch": 86.21691718243443,
      "grad_norm": 0.5597368478775024,
      "learning_rate": 9.815119852941957e-07,
      "loss": 1.0568,
      "step": 9225
    },
    {
      "epoch": 86.2263483642794,
      "grad_norm": 0.5676289200782776,
      "learning_rate": 9.80204632100984e-07,
      "loss": 1.0117,
      "step": 9226
    },
    {
      "epoch": 86.23577954612438,
      "grad_norm": 0.5361323952674866,
      "learning_rate": 9.788981052984369e-07,
      "loss": 1.0587,
      "step": 9227
    },
    {
      "epoch": 86.24521072796935,
      "grad_norm": 0.5480718612670898,
      "learning_rate": 9.775924050062603e-07,
      "loss": 1.0682,
      "step": 9228
    },
    {
      "epoch": 86.25464190981432,
      "grad_norm": 0.5508305430412292,
      "learning_rate": 9.7628753134408e-07,
      "loss": 1.0812,
      "step": 9229
    },
    {
      "epoch": 86.2640730916593,
      "grad_norm": 0.5350164175033569,
      "learning_rate": 9.749834844314487e-07,
      "loss": 1.0158,
      "step": 9230
    },
    {
      "epoch": 86.27350427350427,
      "grad_norm": 0.5621775388717651,
      "learning_rate": 9.73680264387844e-07,
      "loss": 1.06,
      "step": 9231
    },
    {
      "epoch": 86.28293545534925,
      "grad_norm": 0.543404221534729,
      "learning_rate": 9.723778713326649e-07,
      "loss": 1.0929,
      "step": 9232
    },
    {
      "epoch": 86.29236663719422,
      "grad_norm": 0.5546790957450867,
      "learning_rate": 9.710763053852357e-07,
      "loss": 1.0469,
      "step": 9233
    },
    {
      "epoch": 86.3017978190392,
      "grad_norm": 0.5584787726402283,
      "learning_rate": 9.697755666648045e-07,
      "loss": 1.0609,
      "step": 9234
    },
    {
      "epoch": 86.31122900088417,
      "grad_norm": 0.5397957563400269,
      "learning_rate": 9.684756552905495e-07,
      "loss": 1.0182,
      "step": 9235
    },
    {
      "epoch": 86.32066018272914,
      "grad_norm": 0.5725719928741455,
      "learning_rate": 9.67176571381564e-07,
      "loss": 1.0843,
      "step": 9236
    },
    {
      "epoch": 86.33009136457412,
      "grad_norm": 0.5163249969482422,
      "learning_rate": 9.658783150568708e-07,
      "loss": 1.0609,
      "step": 9237
    },
    {
      "epoch": 86.3395225464191,
      "grad_norm": 0.5425519943237305,
      "learning_rate": 9.645808864354155e-07,
      "loss": 1.0224,
      "step": 9238
    },
    {
      "epoch": 86.34895372826408,
      "grad_norm": 0.5924480557441711,
      "learning_rate": 9.632842856360659e-07,
      "loss": 1.0351,
      "step": 9239
    },
    {
      "epoch": 86.35838491010905,
      "grad_norm": 0.6069400310516357,
      "learning_rate": 9.619885127776208e-07,
      "loss": 1.053,
      "step": 9240
    },
    {
      "epoch": 86.36781609195403,
      "grad_norm": 0.5601242184638977,
      "learning_rate": 9.606935679787966e-07,
      "loss": 1.0435,
      "step": 9241
    },
    {
      "epoch": 86.377247273799,
      "grad_norm": 0.5386582016944885,
      "learning_rate": 9.593994513582338e-07,
      "loss": 1.0909,
      "step": 9242
    },
    {
      "epoch": 86.38667845564397,
      "grad_norm": 0.5734553337097168,
      "learning_rate": 9.581061630345011e-07,
      "loss": 1.0962,
      "step": 9243
    },
    {
      "epoch": 86.39610963748895,
      "grad_norm": 0.5489639043807983,
      "learning_rate": 9.56813703126086e-07,
      "loss": 1.0394,
      "step": 9244
    },
    {
      "epoch": 86.40554081933392,
      "grad_norm": 0.5485984086990356,
      "learning_rate": 9.55522071751408e-07,
      "loss": 1.0244,
      "step": 9245
    },
    {
      "epoch": 86.4149720011789,
      "grad_norm": 0.5686647295951843,
      "learning_rate": 9.542312690288035e-07,
      "loss": 1.0248,
      "step": 9246
    },
    {
      "epoch": 86.42440318302387,
      "grad_norm": 0.6002725958824158,
      "learning_rate": 9.52941295076536e-07,
      "loss": 1.0791,
      "step": 9247
    },
    {
      "epoch": 86.43383436486884,
      "grad_norm": 0.5896556973457336,
      "learning_rate": 9.516521500127906e-07,
      "loss": 1.0265,
      "step": 9248
    },
    {
      "epoch": 86.44326554671382,
      "grad_norm": 0.5086743831634521,
      "learning_rate": 9.503638339556786e-07,
      "loss": 1.0476,
      "step": 9249
    },
    {
      "epoch": 86.45269672855879,
      "grad_norm": 0.5232548713684082,
      "learning_rate": 9.490763470232389e-07,
      "loss": 1.0389,
      "step": 9250
    },
    {
      "epoch": 86.46212791040377,
      "grad_norm": 0.5171222686767578,
      "learning_rate": 9.47789689333427e-07,
      "loss": 1.0565,
      "step": 9251
    },
    {
      "epoch": 86.47155909224875,
      "grad_norm": 0.5192493200302124,
      "learning_rate": 9.465038610041266e-07,
      "loss": 1.0698,
      "step": 9252
    },
    {
      "epoch": 86.48099027409373,
      "grad_norm": 0.5437193512916565,
      "learning_rate": 9.452188621531456e-07,
      "loss": 1.0394,
      "step": 9253
    },
    {
      "epoch": 86.4904214559387,
      "grad_norm": 0.5856334567070007,
      "learning_rate": 9.439346928982152e-07,
      "loss": 1.0397,
      "step": 9254
    },
    {
      "epoch": 86.49985263778368,
      "grad_norm": 0.524567723274231,
      "learning_rate": 9.426513533569903e-07,
      "loss": 1.0439,
      "step": 9255
    },
    {
      "epoch": 86.50928381962865,
      "grad_norm": 0.5402222871780396,
      "learning_rate": 9.413688436470492e-07,
      "loss": 1.0736,
      "step": 9256
    },
    {
      "epoch": 86.51871500147362,
      "grad_norm": 0.5354769229888916,
      "learning_rate": 9.400871638858933e-07,
      "loss": 1.055,
      "step": 9257
    },
    {
      "epoch": 86.5281461833186,
      "grad_norm": 0.561080276966095,
      "learning_rate": 9.388063141909554e-07,
      "loss": 1.0699,
      "step": 9258
    },
    {
      "epoch": 86.53757736516357,
      "grad_norm": 0.5362893342971802,
      "learning_rate": 9.375262946795827e-07,
      "loss": 1.0356,
      "step": 9259
    },
    {
      "epoch": 86.54700854700855,
      "grad_norm": 0.5501064658164978,
      "learning_rate": 9.362471054690503e-07,
      "loss": 1.0222,
      "step": 9260
    },
    {
      "epoch": 86.55643972885352,
      "grad_norm": 0.5108745694160461,
      "learning_rate": 9.349687466765578e-07,
      "loss": 1.0453,
      "step": 9261
    },
    {
      "epoch": 86.5658709106985,
      "grad_norm": 0.5408401489257812,
      "learning_rate": 9.336912184192271e-07,
      "loss": 1.0933,
      "step": 9262
    },
    {
      "epoch": 86.57530209254347,
      "grad_norm": 0.5150827765464783,
      "learning_rate": 9.324145208141067e-07,
      "loss": 1.0157,
      "step": 9263
    },
    {
      "epoch": 86.58473327438844,
      "grad_norm": 0.5515671372413635,
      "learning_rate": 9.311386539781631e-07,
      "loss": 1.0347,
      "step": 9264
    },
    {
      "epoch": 86.59416445623341,
      "grad_norm": 0.5884655117988586,
      "learning_rate": 9.29863618028296e-07,
      "loss": 1.0439,
      "step": 9265
    },
    {
      "epoch": 86.6035956380784,
      "grad_norm": 0.5040629506111145,
      "learning_rate": 9.285894130813211e-07,
      "loss": 1.0586,
      "step": 9266
    },
    {
      "epoch": 86.61302681992338,
      "grad_norm": 0.5614699721336365,
      "learning_rate": 9.273160392539815e-07,
      "loss": 1.0538,
      "step": 9267
    },
    {
      "epoch": 86.62245800176835,
      "grad_norm": 0.5295596718788147,
      "learning_rate": 9.260434966629428e-07,
      "loss": 1.0328,
      "step": 9268
    },
    {
      "epoch": 86.63188918361332,
      "grad_norm": 0.5349279046058655,
      "learning_rate": 9.24771785424795e-07,
      "loss": 1.0297,
      "step": 9269
    },
    {
      "epoch": 86.6413203654583,
      "grad_norm": 0.5586472749710083,
      "learning_rate": 9.235009056560517e-07,
      "loss": 1.0504,
      "step": 9270
    },
    {
      "epoch": 86.65075154730327,
      "grad_norm": 0.5535928606987,
      "learning_rate": 9.222308574731498e-07,
      "loss": 1.0542,
      "step": 9271
    },
    {
      "epoch": 86.66018272914825,
      "grad_norm": 0.5463404059410095,
      "learning_rate": 9.209616409924504e-07,
      "loss": 1.0311,
      "step": 9272
    },
    {
      "epoch": 86.66961391099322,
      "grad_norm": 0.5627765655517578,
      "learning_rate": 9.196932563302419e-07,
      "loss": 1.0137,
      "step": 9273
    },
    {
      "epoch": 86.6790450928382,
      "grad_norm": 0.5546202659606934,
      "learning_rate": 9.184257036027311e-07,
      "loss": 1.0787,
      "step": 9274
    },
    {
      "epoch": 86.68847627468317,
      "grad_norm": 0.5846660137176514,
      "learning_rate": 9.171589829260496e-07,
      "loss": 1.0559,
      "step": 9275
    },
    {
      "epoch": 86.69790745652814,
      "grad_norm": 0.5404379367828369,
      "learning_rate": 9.158930944162558e-07,
      "loss": 1.0701,
      "step": 9276
    },
    {
      "epoch": 86.70733863837312,
      "grad_norm": 0.572198212146759,
      "learning_rate": 9.146280381893302e-07,
      "loss": 1.0345,
      "step": 9277
    },
    {
      "epoch": 86.71676982021809,
      "grad_norm": 0.564662516117096,
      "learning_rate": 9.133638143611756e-07,
      "loss": 1.0401,
      "step": 9278
    },
    {
      "epoch": 86.72620100206306,
      "grad_norm": 0.5299076437950134,
      "learning_rate": 9.121004230476172e-07,
      "loss": 1.036,
      "step": 9279
    },
    {
      "epoch": 86.73563218390805,
      "grad_norm": 0.5289648175239563,
      "learning_rate": 9.108378643644123e-07,
      "loss": 1.0807,
      "step": 9280
    },
    {
      "epoch": 86.74506336575303,
      "grad_norm": 0.5974381566047668,
      "learning_rate": 9.095761384272328e-07,
      "loss": 1.0499,
      "step": 9281
    },
    {
      "epoch": 86.754494547598,
      "grad_norm": 0.522436797618866,
      "learning_rate": 9.083152453516786e-07,
      "loss": 1.0549,
      "step": 9282
    },
    {
      "epoch": 86.76392572944297,
      "grad_norm": 0.5687416791915894,
      "learning_rate": 9.070551852532717e-07,
      "loss": 1.061,
      "step": 9283
    },
    {
      "epoch": 86.77335691128795,
      "grad_norm": 0.5213449597358704,
      "learning_rate": 9.057959582474585e-07,
      "loss": 1.0433,
      "step": 9284
    },
    {
      "epoch": 86.78278809313292,
      "grad_norm": 0.5790725350379944,
      "learning_rate": 9.045375644496079e-07,
      "loss": 1.0649,
      "step": 9285
    },
    {
      "epoch": 86.7922192749779,
      "grad_norm": 0.524541974067688,
      "learning_rate": 9.032800039750156e-07,
      "loss": 1.0874,
      "step": 9286
    },
    {
      "epoch": 86.80165045682287,
      "grad_norm": 0.5738111138343811,
      "learning_rate": 9.020232769388948e-07,
      "loss": 1.0599,
      "step": 9287
    },
    {
      "epoch": 86.81108163866784,
      "grad_norm": 0.556551992893219,
      "learning_rate": 9.007673834563923e-07,
      "loss": 1.0317,
      "step": 9288
    },
    {
      "epoch": 86.82051282051282,
      "grad_norm": 0.567757248878479,
      "learning_rate": 8.995123236425696e-07,
      "loss": 1.037,
      "step": 9289
    },
    {
      "epoch": 86.82994400235779,
      "grad_norm": 0.5257688164710999,
      "learning_rate": 8.982580976124144e-07,
      "loss": 1.0211,
      "step": 9290
    },
    {
      "epoch": 86.83937518420277,
      "grad_norm": 0.591424286365509,
      "learning_rate": 8.970047054808407e-07,
      "loss": 1.0007,
      "step": 9291
    },
    {
      "epoch": 86.84880636604774,
      "grad_norm": 0.5324262976646423,
      "learning_rate": 8.957521473626807e-07,
      "loss": 1.0859,
      "step": 9292
    },
    {
      "epoch": 86.85823754789271,
      "grad_norm": 0.5845750570297241,
      "learning_rate": 8.94500423372695e-07,
      "loss": 1.0577,
      "step": 9293
    },
    {
      "epoch": 86.8676687297377,
      "grad_norm": 0.5442858338356018,
      "learning_rate": 8.932495336255654e-07,
      "loss": 1.0372,
      "step": 9294
    },
    {
      "epoch": 86.87709991158268,
      "grad_norm": 0.5417474508285522,
      "learning_rate": 8.919994782358987e-07,
      "loss": 1.035,
      "step": 9295
    },
    {
      "epoch": 86.88653109342765,
      "grad_norm": 0.5555881261825562,
      "learning_rate": 8.907502573182258e-07,
      "loss": 1.0366,
      "step": 9296
    },
    {
      "epoch": 86.89596227527262,
      "grad_norm": 0.6037666201591492,
      "learning_rate": 8.895018709869985e-07,
      "loss": 1.0763,
      "step": 9297
    },
    {
      "epoch": 86.9053934571176,
      "grad_norm": 0.5444835424423218,
      "learning_rate": 8.88254319356594e-07,
      "loss": 1.0917,
      "step": 9298
    },
    {
      "epoch": 86.91482463896257,
      "grad_norm": 0.5185996294021606,
      "learning_rate": 8.870076025413122e-07,
      "loss": 1.0309,
      "step": 9299
    },
    {
      "epoch": 86.92425582080755,
      "grad_norm": 0.5849537253379822,
      "learning_rate": 8.857617206553759e-07,
      "loss": 1.0455,
      "step": 9300
    },
    {
      "epoch": 86.93368700265252,
      "grad_norm": 0.5873512625694275,
      "learning_rate": 8.84516673812933e-07,
      "loss": 1.0792,
      "step": 9301
    },
    {
      "epoch": 86.9431181844975,
      "grad_norm": 0.5256026983261108,
      "learning_rate": 8.832724621280553e-07,
      "loss": 1.0738,
      "step": 9302
    },
    {
      "epoch": 86.95254936634247,
      "grad_norm": 0.5858666896820068,
      "learning_rate": 8.820290857147373e-07,
      "loss": 1.055,
      "step": 9303
    },
    {
      "epoch": 86.96198054818744,
      "grad_norm": 0.57076495885849,
      "learning_rate": 8.807865446868946e-07,
      "loss": 1.05,
      "step": 9304
    },
    {
      "epoch": 86.97141173003241,
      "grad_norm": 0.5700256824493408,
      "learning_rate": 8.795448391583716e-07,
      "loss": 1.0295,
      "step": 9305
    },
    {
      "epoch": 86.98084291187739,
      "grad_norm": 0.5782557129859924,
      "learning_rate": 8.783039692429274e-07,
      "loss": 1.0167,
      "step": 9306
    },
    {
      "epoch": 86.99027409372236,
      "grad_norm": 0.5451021194458008,
      "learning_rate": 8.770639350542565e-07,
      "loss": 1.0528,
      "step": 9307
    },
    {
      "epoch": 86.99970527556735,
      "grad_norm": 0.5679216980934143,
      "learning_rate": 8.75824736705968e-07,
      "loss": 1.0151,
      "step": 9308
    },
    {
      "epoch": 87.0,
      "grad_norm": 2.310415029525757,
      "learning_rate": 8.745863743115957e-07,
      "loss": 0.9985,
      "step": 9309
    },
    {
      "epoch": 87.00943118184497,
      "grad_norm": 0.5721668004989624,
      "learning_rate": 8.733488479845997e-07,
      "loss": 1.0515,
      "step": 9310
    },
    {
      "epoch": 87.01886236368995,
      "grad_norm": 0.5268223881721497,
      "learning_rate": 8.721121578383584e-07,
      "loss": 1.0503,
      "step": 9311
    },
    {
      "epoch": 87.02829354553492,
      "grad_norm": 0.5674489140510559,
      "learning_rate": 8.708763039861812e-07,
      "loss": 1.077,
      "step": 9312
    },
    {
      "epoch": 87.0377247273799,
      "grad_norm": 0.5459137558937073,
      "learning_rate": 8.696412865412951e-07,
      "loss": 1.1022,
      "step": 9313
    },
    {
      "epoch": 87.04715590922487,
      "grad_norm": 0.5760408043861389,
      "learning_rate": 8.684071056168509e-07,
      "loss": 1.0744,
      "step": 9314
    },
    {
      "epoch": 87.05658709106984,
      "grad_norm": 0.537059485912323,
      "learning_rate": 8.671737613259246e-07,
      "loss": 1.0262,
      "step": 9315
    },
    {
      "epoch": 87.06601827291482,
      "grad_norm": 0.5481799244880676,
      "learning_rate": 8.659412537815138e-07,
      "loss": 1.0204,
      "step": 9316
    },
    {
      "epoch": 87.0754494547598,
      "grad_norm": 0.5595428943634033,
      "learning_rate": 8.647095830965424e-07,
      "loss": 1.0283,
      "step": 9317
    },
    {
      "epoch": 87.08488063660478,
      "grad_norm": 0.5141050815582275,
      "learning_rate": 8.634787493838559e-07,
      "loss": 1.0349,
      "step": 9318
    },
    {
      "epoch": 87.09431181844975,
      "grad_norm": 0.5562843680381775,
      "learning_rate": 8.622487527562206e-07,
      "loss": 1.0877,
      "step": 9319
    },
    {
      "epoch": 87.10374300029473,
      "grad_norm": 0.5721486210823059,
      "learning_rate": 8.610195933263298e-07,
      "loss": 1.0006,
      "step": 9320
    },
    {
      "epoch": 87.1131741821397,
      "grad_norm": 0.5148696303367615,
      "learning_rate": 8.597912712067979e-07,
      "loss": 1.0481,
      "step": 9321
    },
    {
      "epoch": 87.12260536398468,
      "grad_norm": 0.5444616079330444,
      "learning_rate": 8.585637865101648e-07,
      "loss": 1.0509,
      "step": 9322
    },
    {
      "epoch": 87.13203654582965,
      "grad_norm": 0.5331891775131226,
      "learning_rate": 8.573371393488905e-07,
      "loss": 1.0478,
      "step": 9323
    },
    {
      "epoch": 87.14146772767462,
      "grad_norm": 0.5640196204185486,
      "learning_rate": 8.561113298353585e-07,
      "loss": 1.0627,
      "step": 9324
    },
    {
      "epoch": 87.1508989095196,
      "grad_norm": 0.5637186169624329,
      "learning_rate": 8.548863580818823e-07,
      "loss": 1.0454,
      "step": 9325
    },
    {
      "epoch": 87.16033009136457,
      "grad_norm": 0.5801538228988647,
      "learning_rate": 8.536622242006898e-07,
      "loss": 1.0306,
      "step": 9326
    },
    {
      "epoch": 87.16976127320955,
      "grad_norm": 0.5711338520050049,
      "learning_rate": 8.524389283039358e-07,
      "loss": 1.0367,
      "step": 9327
    },
    {
      "epoch": 87.17919245505452,
      "grad_norm": 0.5648853778839111,
      "learning_rate": 8.512164705036996e-07,
      "loss": 1.036,
      "step": 9328
    },
    {
      "epoch": 87.1886236368995,
      "grad_norm": 0.587924063205719,
      "learning_rate": 8.499948509119815e-07,
      "loss": 1.0166,
      "step": 9329
    },
    {
      "epoch": 87.19805481874447,
      "grad_norm": 0.5426425933837891,
      "learning_rate": 8.487740696407065e-07,
      "loss": 1.0946,
      "step": 9330
    },
    {
      "epoch": 87.20748600058945,
      "grad_norm": 0.5234459638595581,
      "learning_rate": 8.475541268017185e-07,
      "loss": 1.0586,
      "step": 9331
    },
    {
      "epoch": 87.21691718243443,
      "grad_norm": 0.5433292984962463,
      "learning_rate": 8.463350225067946e-07,
      "loss": 1.0574,
      "step": 9332
    },
    {
      "epoch": 87.2263483642794,
      "grad_norm": 0.560603141784668,
      "learning_rate": 8.451167568676244e-07,
      "loss": 0.9914,
      "step": 9333
    },
    {
      "epoch": 87.23577954612438,
      "grad_norm": 0.5366122722625732,
      "learning_rate": 8.438993299958264e-07,
      "loss": 1.0228,
      "step": 9334
    },
    {
      "epoch": 87.24521072796935,
      "grad_norm": 0.5597259402275085,
      "learning_rate": 8.426827420029415e-07,
      "loss": 1.0565,
      "step": 9335
    },
    {
      "epoch": 87.25464190981432,
      "grad_norm": 0.5584732890129089,
      "learning_rate": 8.414669930004316e-07,
      "loss": 1.0618,
      "step": 9336
    },
    {
      "epoch": 87.2640730916593,
      "grad_norm": 0.5364394187927246,
      "learning_rate": 8.40252083099683e-07,
      "loss": 1.0521,
      "step": 9337
    },
    {
      "epoch": 87.27350427350427,
      "grad_norm": 0.5642387270927429,
      "learning_rate": 8.390380124120079e-07,
      "loss": 1.0484,
      "step": 9338
    },
    {
      "epoch": 87.28293545534925,
      "grad_norm": 0.604718029499054,
      "learning_rate": 8.378247810486339e-07,
      "loss": 1.0172,
      "step": 9339
    },
    {
      "epoch": 87.29236663719422,
      "grad_norm": 0.567687451839447,
      "learning_rate": 8.366123891207223e-07,
      "loss": 1.0352,
      "step": 9340
    },
    {
      "epoch": 87.3017978190392,
      "grad_norm": 0.5643686056137085,
      "learning_rate": 8.354008367393506e-07,
      "loss": 1.0831,
      "step": 9341
    },
    {
      "epoch": 87.31122900088417,
      "grad_norm": 0.5428371429443359,
      "learning_rate": 8.341901240155204e-07,
      "loss": 1.0779,
      "step": 9342
    },
    {
      "epoch": 87.32066018272914,
      "grad_norm": 0.5390613079071045,
      "learning_rate": 8.329802510601559e-07,
      "loss": 1.0533,
      "step": 9343
    },
    {
      "epoch": 87.33009136457412,
      "grad_norm": 0.5640432834625244,
      "learning_rate": 8.317712179841064e-07,
      "loss": 1.0149,
      "step": 9344
    },
    {
      "epoch": 87.3395225464191,
      "grad_norm": 0.606303334236145,
      "learning_rate": 8.305630248981422e-07,
      "loss": 1.0346,
      "step": 9345
    },
    {
      "epoch": 87.34895372826408,
      "grad_norm": 0.5552353858947754,
      "learning_rate": 8.29355671912957e-07,
      "loss": 1.051,
      "step": 9346
    },
    {
      "epoch": 87.35838491010905,
      "grad_norm": 0.5435505509376526,
      "learning_rate": 8.281491591391711e-07,
      "loss": 1.0475,
      "step": 9347
    },
    {
      "epoch": 87.36781609195403,
      "grad_norm": 0.52791827917099,
      "learning_rate": 8.269434866873216e-07,
      "loss": 1.067,
      "step": 9348
    },
    {
      "epoch": 87.377247273799,
      "grad_norm": 0.5379424691200256,
      "learning_rate": 8.257386546678736e-07,
      "loss": 1.0461,
      "step": 9349
    },
    {
      "epoch": 87.38667845564397,
      "grad_norm": 0.5781530141830444,
      "learning_rate": 8.245346631912132e-07,
      "loss": 1.097,
      "step": 9350
    },
    {
      "epoch": 87.39610963748895,
      "grad_norm": 0.5377870202064514,
      "learning_rate": 8.233315123676488e-07,
      "loss": 1.0757,
      "step": 9351
    },
    {
      "epoch": 87.40554081933392,
      "grad_norm": 0.5600622892379761,
      "learning_rate": 8.221292023074146e-07,
      "loss": 1.0393,
      "step": 9352
    },
    {
      "epoch": 87.4149720011789,
      "grad_norm": 0.586056113243103,
      "learning_rate": 8.209277331206633e-07,
      "loss": 1.0561,
      "step": 9353
    },
    {
      "epoch": 87.42440318302387,
      "grad_norm": 0.5179356336593628,
      "learning_rate": 8.197271049174727e-07,
      "loss": 1.0486,
      "step": 9354
    },
    {
      "epoch": 87.43383436486884,
      "grad_norm": 0.5318006873130798,
      "learning_rate": 8.185273178078467e-07,
      "loss": 1.0258,
      "step": 9355
    },
    {
      "epoch": 87.44326554671382,
      "grad_norm": 0.5307632088661194,
      "learning_rate": 8.173283719017089e-07,
      "loss": 1.0857,
      "step": 9356
    },
    {
      "epoch": 87.45269672855879,
      "grad_norm": 0.6082928776741028,
      "learning_rate": 8.161302673089055e-07,
      "loss": 1.0247,
      "step": 9357
    },
    {
      "epoch": 87.46212791040377,
      "grad_norm": 0.5447684526443481,
      "learning_rate": 8.149330041392067e-07,
      "loss": 1.037,
      "step": 9358
    },
    {
      "epoch": 87.47155909224875,
      "grad_norm": 0.5683355331420898,
      "learning_rate": 8.137365825023058e-07,
      "loss": 1.0199,
      "step": 9359
    },
    {
      "epoch": 87.48099027409373,
      "grad_norm": 0.5186225771903992,
      "learning_rate": 8.125410025078174e-07,
      "loss": 1.0051,
      "step": 9360
    },
    {
      "epoch": 87.4904214559387,
      "grad_norm": 0.5493205189704895,
      "learning_rate": 8.113462642652791e-07,
      "loss": 1.0337,
      "step": 9361
    },
    {
      "epoch": 87.49985263778368,
      "grad_norm": 0.5567707419395447,
      "learning_rate": 8.101523678841561e-07,
      "loss": 1.086,
      "step": 9362
    },
    {
      "epoch": 87.50928381962865,
      "grad_norm": 0.5379347205162048,
      "learning_rate": 8.089593134738305e-07,
      "loss": 1.0584,
      "step": 9363
    },
    {
      "epoch": 87.51871500147362,
      "grad_norm": 0.5460227131843567,
      "learning_rate": 8.077671011436105e-07,
      "loss": 1.07,
      "step": 9364
    },
    {
      "epoch": 87.5281461833186,
      "grad_norm": 0.5624248385429382,
      "learning_rate": 8.065757310027245e-07,
      "loss": 1.0584,
      "step": 9365
    },
    {
      "epoch": 87.53757736516357,
      "grad_norm": 0.5517854690551758,
      "learning_rate": 8.053852031603271e-07,
      "loss": 1.0722,
      "step": 9366
    },
    {
      "epoch": 87.54700854700855,
      "grad_norm": 0.5571144223213196,
      "learning_rate": 8.041955177254923e-07,
      "loss": 1.0668,
      "step": 9367
    },
    {
      "epoch": 87.55643972885352,
      "grad_norm": 0.568011999130249,
      "learning_rate": 8.030066748072208e-07,
      "loss": 1.0566,
      "step": 9368
    },
    {
      "epoch": 87.5658709106985,
      "grad_norm": 0.541343629360199,
      "learning_rate": 8.018186745144329e-07,
      "loss": 1.0252,
      "step": 9369
    },
    {
      "epoch": 87.57530209254347,
      "grad_norm": 0.5348959565162659,
      "learning_rate": 8.006315169559731e-07,
      "loss": 1.0531,
      "step": 9370
    },
    {
      "epoch": 87.58473327438844,
      "grad_norm": 0.5315749645233154,
      "learning_rate": 7.994452022406085e-07,
      "loss": 1.0667,
      "step": 9371
    },
    {
      "epoch": 87.59416445623341,
      "grad_norm": 0.5320276021957397,
      "learning_rate": 7.982597304770256e-07,
      "loss": 1.0344,
      "step": 9372
    },
    {
      "epoch": 87.6035956380784,
      "grad_norm": 0.5712064504623413,
      "learning_rate": 7.970751017738432e-07,
      "loss": 1.0472,
      "step": 9373
    },
    {
      "epoch": 87.61302681992338,
      "grad_norm": 0.5801008939743042,
      "learning_rate": 7.958913162395921e-07,
      "loss": 1.0234,
      "step": 9374
    },
    {
      "epoch": 87.62245800176835,
      "grad_norm": 0.6085193753242493,
      "learning_rate": 7.947083739827322e-07,
      "loss": 1.0013,
      "step": 9375
    },
    {
      "epoch": 87.63188918361332,
      "grad_norm": 0.5382421016693115,
      "learning_rate": 7.935262751116435e-07,
      "loss": 1.0488,
      "step": 9376
    },
    {
      "epoch": 87.6413203654583,
      "grad_norm": 0.5231956839561462,
      "learning_rate": 7.923450197346283e-07,
      "loss": 1.093,
      "step": 9377
    },
    {
      "epoch": 87.65075154730327,
      "grad_norm": 0.5406705737113953,
      "learning_rate": 7.911646079599155e-07,
      "loss": 1.0791,
      "step": 9378
    },
    {
      "epoch": 87.66018272914825,
      "grad_norm": 0.5259362459182739,
      "learning_rate": 7.899850398956521e-07,
      "loss": 1.0415,
      "step": 9379
    },
    {
      "epoch": 87.66961391099322,
      "grad_norm": 0.5575132966041565,
      "learning_rate": 7.888063156499116e-07,
      "loss": 0.9976,
      "step": 9380
    },
    {
      "epoch": 87.6790450928382,
      "grad_norm": 0.5419566631317139,
      "learning_rate": 7.876284353306852e-07,
      "loss": 1.0609,
      "step": 9381
    },
    {
      "epoch": 87.68847627468317,
      "grad_norm": 0.5308980345726013,
      "learning_rate": 7.864513990458933e-07,
      "loss": 1.1022,
      "step": 9382
    },
    {
      "epoch": 87.69790745652814,
      "grad_norm": 0.5411603450775146,
      "learning_rate": 7.852752069033731e-07,
      "loss": 1.0452,
      "step": 9383
    },
    {
      "epoch": 87.70733863837312,
      "grad_norm": 0.5475922226905823,
      "learning_rate": 7.840998590108861e-07,
      "loss": 1.0942,
      "step": 9384
    },
    {
      "epoch": 87.71676982021809,
      "grad_norm": 0.5480539798736572,
      "learning_rate": 7.829253554761218e-07,
      "loss": 1.0392,
      "step": 9385
    },
    {
      "epoch": 87.72620100206306,
      "grad_norm": 0.5677438378334045,
      "learning_rate": 7.81751696406684e-07,
      "loss": 1.0467,
      "step": 9386
    },
    {
      "epoch": 87.73563218390805,
      "grad_norm": 0.5923635959625244,
      "learning_rate": 7.805788819101045e-07,
      "loss": 1.0272,
      "step": 9387
    },
    {
      "epoch": 87.74506336575303,
      "grad_norm": 0.5371894240379333,
      "learning_rate": 7.794069120938352e-07,
      "loss": 1.1185,
      "step": 9388
    },
    {
      "epoch": 87.754494547598,
      "grad_norm": 0.572319507598877,
      "learning_rate": 7.782357870652524e-07,
      "loss": 1.0486,
      "step": 9389
    },
    {
      "epoch": 87.76392572944297,
      "grad_norm": 0.6173297762870789,
      "learning_rate": 7.770655069316546e-07,
      "loss": 1.0587,
      "step": 9390
    },
    {
      "epoch": 87.77335691128795,
      "grad_norm": 0.5733912587165833,
      "learning_rate": 7.758960718002595e-07,
      "loss": 1.0493,
      "step": 9391
    },
    {
      "epoch": 87.78278809313292,
      "grad_norm": 0.5585283637046814,
      "learning_rate": 7.747274817782136e-07,
      "loss": 1.0769,
      "step": 9392
    },
    {
      "epoch": 87.7922192749779,
      "grad_norm": 0.5961583256721497,
      "learning_rate": 7.735597369725822e-07,
      "loss": 1.034,
      "step": 9393
    },
    {
      "epoch": 87.80165045682287,
      "grad_norm": 0.5363353490829468,
      "learning_rate": 7.723928374903534e-07,
      "loss": 1.0647,
      "step": 9394
    },
    {
      "epoch": 87.81108163866784,
      "grad_norm": 0.5233204960823059,
      "learning_rate": 7.712267834384379e-07,
      "loss": 1.0421,
      "step": 9395
    },
    {
      "epoch": 87.82051282051282,
      "grad_norm": 0.5082474946975708,
      "learning_rate": 7.700615749236695e-07,
      "loss": 1.0233,
      "step": 9396
    },
    {
      "epoch": 87.82994400235779,
      "grad_norm": 0.5236945748329163,
      "learning_rate": 7.688972120528027e-07,
      "loss": 1.0643,
      "step": 9397
    },
    {
      "epoch": 87.83937518420277,
      "grad_norm": 0.5407593846321106,
      "learning_rate": 7.677336949325187e-07,
      "loss": 1.0551,
      "step": 9398
    },
    {
      "epoch": 87.84880636604774,
      "grad_norm": 0.58568274974823,
      "learning_rate": 7.665710236694146e-07,
      "loss": 1.0045,
      "step": 9399
    },
    {
      "epoch": 87.85823754789271,
      "grad_norm": 0.5405881404876709,
      "learning_rate": 7.654091983700196e-07,
      "loss": 1.0336,
      "step": 9400
    },
    {
      "epoch": 87.8676687297377,
      "grad_norm": 0.5673733353614807,
      "learning_rate": 7.642482191407752e-07,
      "loss": 1.031,
      "step": 9401
    },
    {
      "epoch": 87.87709991158268,
      "grad_norm": 0.5568118691444397,
      "learning_rate": 7.630880860880519e-07,
      "loss": 1.0689,
      "step": 9402
    },
    {
      "epoch": 87.88653109342765,
      "grad_norm": 0.5392287373542786,
      "learning_rate": 7.619287993181401e-07,
      "loss": 1.0445,
      "step": 9403
    },
    {
      "epoch": 87.89596227527262,
      "grad_norm": 0.5637938976287842,
      "learning_rate": 7.60770358937254e-07,
      "loss": 1.0588,
      "step": 9404
    },
    {
      "epoch": 87.9053934571176,
      "grad_norm": 0.5159433484077454,
      "learning_rate": 7.596127650515283e-07,
      "loss": 1.0289,
      "step": 9405
    },
    {
      "epoch": 87.91482463896257,
      "grad_norm": 0.5989230275154114,
      "learning_rate": 7.584560177670197e-07,
      "loss": 1.0364,
      "step": 9406
    },
    {
      "epoch": 87.92425582080755,
      "grad_norm": 0.5566849708557129,
      "learning_rate": 7.573001171897143e-07,
      "loss": 1.0396,
      "step": 9407
    },
    {
      "epoch": 87.93368700265252,
      "grad_norm": 0.5720009803771973,
      "learning_rate": 7.561450634255107e-07,
      "loss": 1.0352,
      "step": 9408
    },
    {
      "epoch": 87.9431181844975,
      "grad_norm": 0.546245276927948,
      "learning_rate": 7.549908565802377e-07,
      "loss": 1.0549,
      "step": 9409
    },
    {
      "epoch": 87.95254936634247,
      "grad_norm": 0.5441160798072815,
      "learning_rate": 7.538374967596407e-07,
      "loss": 1.0513,
      "step": 9410
    },
    {
      "epoch": 87.96198054818744,
      "grad_norm": 0.5108046531677246,
      "learning_rate": 7.526849840693918e-07,
      "loss": 1.08,
      "step": 9411
    },
    {
      "epoch": 87.97141173003241,
      "grad_norm": 0.5321522355079651,
      "learning_rate": 7.51533318615083e-07,
      "loss": 1.0488,
      "step": 9412
    },
    {
      "epoch": 87.98084291187739,
      "grad_norm": 0.618969738483429,
      "learning_rate": 7.503825005022291e-07,
      "loss": 0.9814,
      "step": 9413
    },
    {
      "epoch": 87.99027409372236,
      "grad_norm": 0.619365394115448,
      "learning_rate": 7.492325298362691e-07,
      "loss": 0.9928,
      "step": 9414
    },
    {
      "epoch": 87.99970527556735,
      "grad_norm": 0.5739262104034424,
      "learning_rate": 7.480834067225617e-07,
      "loss": 1.0772,
      "step": 9415
    },
    {
      "epoch": 88.0,
      "grad_norm": 3.3280203342437744,
      "learning_rate": 7.469351312663909e-07,
      "loss": 0.4449,
      "step": 9416
    },
    {
      "epoch": 88.00943118184497,
      "grad_norm": 0.5634798407554626,
      "learning_rate": 7.457877035729588e-07,
      "loss": 1.0587,
      "step": 9417
    },
    {
      "epoch": 88.01886236368995,
      "grad_norm": 0.5599663853645325,
      "learning_rate": 7.446411237473949e-07,
      "loss": 1.0318,
      "step": 9418
    },
    {
      "epoch": 88.02829354553492,
      "grad_norm": 0.5252606272697449,
      "learning_rate": 7.434953918947463e-07,
      "loss": 1.0759,
      "step": 9419
    },
    {
      "epoch": 88.0377247273799,
      "grad_norm": 0.5959861278533936,
      "learning_rate": 7.423505081199855e-07,
      "loss": 1.0634,
      "step": 9420
    },
    {
      "epoch": 88.04715590922487,
      "grad_norm": 0.5363302230834961,
      "learning_rate": 7.412064725280044e-07,
      "loss": 1.0141,
      "step": 9421
    },
    {
      "epoch": 88.05658709106984,
      "grad_norm": 0.5138832330703735,
      "learning_rate": 7.400632852236234e-07,
      "loss": 1.0455,
      "step": 9422
    },
    {
      "epoch": 88.06601827291482,
      "grad_norm": 0.5424020290374756,
      "learning_rate": 7.389209463115787e-07,
      "loss": 1.0674,
      "step": 9423
    },
    {
      "epoch": 88.0754494547598,
      "grad_norm": 0.572108805179596,
      "learning_rate": 7.377794558965312e-07,
      "loss": 1.0342,
      "step": 9424
    },
    {
      "epoch": 88.08488063660478,
      "grad_norm": 0.548389196395874,
      "learning_rate": 7.366388140830627e-07,
      "loss": 1.0712,
      "step": 9425
    },
    {
      "epoch": 88.09431181844975,
      "grad_norm": 0.5489925742149353,
      "learning_rate": 7.354990209756796e-07,
      "loss": 1.0633,
      "step": 9426
    },
    {
      "epoch": 88.10374300029473,
      "grad_norm": 0.5581616759300232,
      "learning_rate": 7.343600766788095e-07,
      "loss": 0.9973,
      "step": 9427
    },
    {
      "epoch": 88.1131741821397,
      "grad_norm": 0.5797712802886963,
      "learning_rate": 7.332219812968e-07,
      "loss": 1.0577,
      "step": 9428
    },
    {
      "epoch": 88.12260536398468,
      "grad_norm": 0.5361011624336243,
      "learning_rate": 7.320847349339266e-07,
      "loss": 1.0322,
      "step": 9429
    },
    {
      "epoch": 88.13203654582965,
      "grad_norm": 0.5501800179481506,
      "learning_rate": 7.309483376943804e-07,
      "loss": 1.0346,
      "step": 9430
    },
    {
      "epoch": 88.14146772767462,
      "grad_norm": 0.600495457649231,
      "learning_rate": 7.298127896822804e-07,
      "loss": 0.9985,
      "step": 9431
    },
    {
      "epoch": 88.1508989095196,
      "grad_norm": 0.5324424505233765,
      "learning_rate": 7.286780910016633e-07,
      "loss": 1.0909,
      "step": 9432
    },
    {
      "epoch": 88.16033009136457,
      "grad_norm": 0.5167352557182312,
      "learning_rate": 7.275442417564892e-07,
      "loss": 1.06,
      "step": 9433
    },
    {
      "epoch": 88.16976127320955,
      "grad_norm": 0.5619844198226929,
      "learning_rate": 7.264112420506409e-07,
      "loss": 1.0849,
      "step": 9434
    },
    {
      "epoch": 88.17919245505452,
      "grad_norm": 0.5756262540817261,
      "learning_rate": 7.25279091987926e-07,
      "loss": 1.0506,
      "step": 9435
    },
    {
      "epoch": 88.1886236368995,
      "grad_norm": 0.5356447100639343,
      "learning_rate": 7.241477916720707e-07,
      "loss": 1.0602,
      "step": 9436
    },
    {
      "epoch": 88.19805481874447,
      "grad_norm": 0.5980531573295593,
      "learning_rate": 7.230173412067243e-07,
      "loss": 1.039,
      "step": 9437
    },
    {
      "epoch": 88.20748600058945,
      "grad_norm": 0.539996325969696,
      "learning_rate": 7.218877406954572e-07,
      "loss": 1.0545,
      "step": 9438
    },
    {
      "epoch": 88.21691718243443,
      "grad_norm": 0.5598974227905273,
      "learning_rate": 7.207589902417622e-07,
      "loss": 1.0461,
      "step": 9439
    },
    {
      "epoch": 88.2263483642794,
      "grad_norm": 0.5555369257926941,
      "learning_rate": 7.196310899490577e-07,
      "loss": 1.1106,
      "step": 9440
    },
    {
      "epoch": 88.23577954612438,
      "grad_norm": 0.5459839701652527,
      "learning_rate": 7.185040399206811e-07,
      "loss": 1.0375,
      "step": 9441
    },
    {
      "epoch": 88.24521072796935,
      "grad_norm": 0.5509994029998779,
      "learning_rate": 7.173778402598908e-07,
      "loss": 1.041,
      "step": 9442
    },
    {
      "epoch": 88.25464190981432,
      "grad_norm": 0.5301065444946289,
      "learning_rate": 7.162524910698687e-07,
      "loss": 1.0385,
      "step": 9443
    },
    {
      "epoch": 88.2640730916593,
      "grad_norm": 0.579748272895813,
      "learning_rate": 7.151279924537213e-07,
      "loss": 1.0354,
      "step": 9444
    },
    {
      "epoch": 88.27350427350427,
      "grad_norm": 0.5292320251464844,
      "learning_rate": 7.14004344514474e-07,
      "loss": 1.0369,
      "step": 9445
    },
    {
      "epoch": 88.28293545534925,
      "grad_norm": 0.5393222570419312,
      "learning_rate": 7.128815473550743e-07,
      "loss": 1.0103,
      "step": 9446
    },
    {
      "epoch": 88.29236663719422,
      "grad_norm": 0.5468756556510925,
      "learning_rate": 7.117596010783923e-07,
      "loss": 1.0171,
      "step": 9447
    },
    {
      "epoch": 88.3017978190392,
      "grad_norm": 0.546099066734314,
      "learning_rate": 7.106385057872211e-07,
      "loss": 1.0252,
      "step": 9448
    },
    {
      "epoch": 88.31122900088417,
      "grad_norm": 0.5476324558258057,
      "learning_rate": 7.095182615842744e-07,
      "loss": 1.0581,
      "step": 9449
    },
    {
      "epoch": 88.32066018272914,
      "grad_norm": 0.5134066939353943,
      "learning_rate": 7.083988685721887e-07,
      "loss": 1.0891,
      "step": 9450
    },
    {
      "epoch": 88.33009136457412,
      "grad_norm": 0.5676494836807251,
      "learning_rate": 7.072803268535211e-07,
      "loss": 1.0311,
      "step": 9451
    },
    {
      "epoch": 88.3395225464191,
      "grad_norm": 0.5686855912208557,
      "learning_rate": 7.06162636530755e-07,
      "loss": 1.1005,
      "step": 9452
    },
    {
      "epoch": 88.34895372826408,
      "grad_norm": 0.5532041192054749,
      "learning_rate": 7.050457977062919e-07,
      "loss": 1.0507,
      "step": 9453
    },
    {
      "epoch": 88.35838491010905,
      "grad_norm": 0.5346788763999939,
      "learning_rate": 7.039298104824554e-07,
      "loss": 1.037,
      "step": 9454
    },
    {
      "epoch": 88.36781609195403,
      "grad_norm": 0.5856918096542358,
      "learning_rate": 7.028146749614917e-07,
      "loss": 1.0266,
      "step": 9455
    },
    {
      "epoch": 88.377247273799,
      "grad_norm": 0.5532606244087219,
      "learning_rate": 7.0170039124557e-07,
      "loss": 1.0472,
      "step": 9456
    },
    {
      "epoch": 88.38667845564397,
      "grad_norm": 0.5789226293563843,
      "learning_rate": 7.005869594367809e-07,
      "loss": 1.0503,
      "step": 9457
    },
    {
      "epoch": 88.39610963748895,
      "grad_norm": 0.5514774918556213,
      "learning_rate": 6.994743796371328e-07,
      "loss": 1.0292,
      "step": 9458
    },
    {
      "epoch": 88.40554081933392,
      "grad_norm": 0.5764191746711731,
      "learning_rate": 6.983626519485653e-07,
      "loss": 1.041,
      "step": 9459
    },
    {
      "epoch": 88.4149720011789,
      "grad_norm": 0.5289760231971741,
      "learning_rate": 6.972517764729325e-07,
      "loss": 1.0824,
      "step": 9460
    },
    {
      "epoch": 88.42440318302387,
      "grad_norm": 0.5591837167739868,
      "learning_rate": 6.96141753312013e-07,
      "loss": 1.0306,
      "step": 9461
    },
    {
      "epoch": 88.43383436486884,
      "grad_norm": 0.5280089974403381,
      "learning_rate": 6.950325825675042e-07,
      "loss": 1.0796,
      "step": 9462
    },
    {
      "epoch": 88.44326554671382,
      "grad_norm": 0.5581774115562439,
      "learning_rate": 6.939242643410305e-07,
      "loss": 1.0535,
      "step": 9463
    },
    {
      "epoch": 88.45269672855879,
      "grad_norm": 0.5614362359046936,
      "learning_rate": 6.92816798734135e-07,
      "loss": 1.0832,
      "step": 9464
    },
    {
      "epoch": 88.46212791040377,
      "grad_norm": 0.5116167068481445,
      "learning_rate": 6.917101858482834e-07,
      "loss": 1.0499,
      "step": 9465
    },
    {
      "epoch": 88.47155909224875,
      "grad_norm": 0.6007693409919739,
      "learning_rate": 6.90604425784861e-07,
      "loss": 1.0089,
      "step": 9466
    },
    {
      "epoch": 88.48099027409373,
      "grad_norm": 0.5578448176383972,
      "learning_rate": 6.894995186451814e-07,
      "loss": 1.1093,
      "step": 9467
    },
    {
      "epoch": 88.4904214559387,
      "grad_norm": 0.5953989028930664,
      "learning_rate": 6.883954645304725e-07,
      "loss": 1.0087,
      "step": 9468
    },
    {
      "epoch": 88.49985263778368,
      "grad_norm": 0.562852680683136,
      "learning_rate": 6.872922635418889e-07,
      "loss": 1.0557,
      "step": 9469
    },
    {
      "epoch": 88.50928381962865,
      "grad_norm": 0.5749002695083618,
      "learning_rate": 6.861899157805052e-07,
      "loss": 1.0547,
      "step": 9470
    },
    {
      "epoch": 88.51871500147362,
      "grad_norm": 0.5576450824737549,
      "learning_rate": 6.850884213473186e-07,
      "loss": 1.0363,
      "step": 9471
    },
    {
      "epoch": 88.5281461833186,
      "grad_norm": 0.5203351378440857,
      "learning_rate": 6.83987780343246e-07,
      "loss": 1.027,
      "step": 9472
    },
    {
      "epoch": 88.53757736516357,
      "grad_norm": 0.5110044479370117,
      "learning_rate": 6.828879928691279e-07,
      "loss": 1.0675,
      "step": 9473
    },
    {
      "epoch": 88.54700854700855,
      "grad_norm": 0.5584429502487183,
      "learning_rate": 6.817890590257281e-07,
      "loss": 1.052,
      "step": 9474
    },
    {
      "epoch": 88.55643972885352,
      "grad_norm": 0.5431926846504211,
      "learning_rate": 6.806909789137306e-07,
      "loss": 1.0444,
      "step": 9475
    },
    {
      "epoch": 88.5658709106985,
      "grad_norm": 0.580528736114502,
      "learning_rate": 6.795937526337404e-07,
      "loss": 1.1154,
      "step": 9476
    },
    {
      "epoch": 88.57530209254347,
      "grad_norm": 0.5120782852172852,
      "learning_rate": 6.784973802862848e-07,
      "loss": 1.0461,
      "step": 9477
    },
    {
      "epoch": 88.58473327438844,
      "grad_norm": 0.5327943563461304,
      "learning_rate": 6.774018619718137e-07,
      "loss": 0.9821,
      "step": 9478
    },
    {
      "epoch": 88.59416445623341,
      "grad_norm": 0.5399804711341858,
      "learning_rate": 6.763071977906977e-07,
      "loss": 1.0405,
      "step": 9479
    },
    {
      "epoch": 88.6035956380784,
      "grad_norm": 0.5234817862510681,
      "learning_rate": 6.752133878432288e-07,
      "loss": 1.0539,
      "step": 9480
    },
    {
      "epoch": 88.61302681992338,
      "grad_norm": 0.5355409383773804,
      "learning_rate": 6.741204322296214e-07,
      "loss": 1.0596,
      "step": 9481
    },
    {
      "epoch": 88.62245800176835,
      "grad_norm": 0.6168104410171509,
      "learning_rate": 6.73028331050014e-07,
      "loss": 1.0782,
      "step": 9482
    },
    {
      "epoch": 88.63188918361332,
      "grad_norm": 0.5309684872627258,
      "learning_rate": 6.719370844044626e-07,
      "loss": 1.0532,
      "step": 9483
    },
    {
      "epoch": 88.6413203654583,
      "grad_norm": 0.5272821187973022,
      "learning_rate": 6.70846692392948e-07,
      "loss": 1.0629,
      "step": 9484
    },
    {
      "epoch": 88.65075154730327,
      "grad_norm": 0.552783191204071,
      "learning_rate": 6.697571551153714e-07,
      "loss": 1.0672,
      "step": 9485
    },
    {
      "epoch": 88.66018272914825,
      "grad_norm": 0.5886169672012329,
      "learning_rate": 6.686684726715542e-07,
      "loss": 1.0709,
      "step": 9486
    },
    {
      "epoch": 88.66961391099322,
      "grad_norm": 0.6057724952697754,
      "learning_rate": 6.675806451612432e-07,
      "loss": 1.0327,
      "step": 9487
    },
    {
      "epoch": 88.6790450928382,
      "grad_norm": 0.5390561819076538,
      "learning_rate": 6.66493672684102e-07,
      "loss": 1.0514,
      "step": 9488
    },
    {
      "epoch": 88.68847627468317,
      "grad_norm": 0.5791996121406555,
      "learning_rate": 6.654075553397221e-07,
      "loss": 1.0867,
      "step": 9489
    },
    {
      "epoch": 88.69790745652814,
      "grad_norm": 0.5536787509918213,
      "learning_rate": 6.643222932276128e-07,
      "loss": 1.066,
      "step": 9490
    },
    {
      "epoch": 88.70733863837312,
      "grad_norm": 0.5608729124069214,
      "learning_rate": 6.632378864472033e-07,
      "loss": 1.003,
      "step": 9491
    },
    {
      "epoch": 88.71676982021809,
      "grad_norm": 0.5377650856971741,
      "learning_rate": 6.621543350978477e-07,
      "loss": 1.0529,
      "step": 9492
    },
    {
      "epoch": 88.72620100206306,
      "grad_norm": 0.5438446998596191,
      "learning_rate": 6.610716392788207e-07,
      "loss": 1.0578,
      "step": 9493
    },
    {
      "epoch": 88.73563218390805,
      "grad_norm": 0.5468243956565857,
      "learning_rate": 6.599897990893178e-07,
      "loss": 1.0476,
      "step": 9494
    },
    {
      "epoch": 88.74506336575303,
      "grad_norm": 0.5583523511886597,
      "learning_rate": 6.589088146284561e-07,
      "loss": 1.051,
      "step": 9495
    },
    {
      "epoch": 88.754494547598,
      "grad_norm": 0.5212861895561218,
      "learning_rate": 6.578286859952787e-07,
      "loss": 1.0295,
      "step": 9496
    },
    {
      "epoch": 88.76392572944297,
      "grad_norm": 0.5751747488975525,
      "learning_rate": 6.567494132887442e-07,
      "loss": 1.0901,
      "step": 9497
    },
    {
      "epoch": 88.77335691128795,
      "grad_norm": 0.6024865508079529,
      "learning_rate": 6.556709966077346e-07,
      "loss": 1.0746,
      "step": 9498
    },
    {
      "epoch": 88.78278809313292,
      "grad_norm": 0.5619089603424072,
      "learning_rate": 6.545934360510553e-07,
      "loss": 1.052,
      "step": 9499
    },
    {
      "epoch": 88.7922192749779,
      "grad_norm": 0.5453718304634094,
      "learning_rate": 6.535167317174296e-07,
      "loss": 1.0367,
      "step": 9500
    },
    {
      "epoch": 88.80165045682287,
      "grad_norm": 0.5711239576339722,
      "learning_rate": 6.524408837055085e-07,
      "loss": 1.0314,
      "step": 9501
    },
    {
      "epoch": 88.81108163866784,
      "grad_norm": 0.5286865830421448,
      "learning_rate": 6.513658921138588e-07,
      "loss": 1.0596,
      "step": 9502
    },
    {
      "epoch": 88.82051282051282,
      "grad_norm": 0.5958307385444641,
      "learning_rate": 6.502917570409717e-07,
      "loss": 1.0678,
      "step": 9503
    },
    {
      "epoch": 88.82994400235779,
      "grad_norm": 0.5398178696632385,
      "learning_rate": 6.492184785852573e-07,
      "loss": 1.0361,
      "step": 9504
    },
    {
      "epoch": 88.83937518420277,
      "grad_norm": 0.5521999001502991,
      "learning_rate": 6.481460568450503e-07,
      "loss": 1.0171,
      "step": 9505
    },
    {
      "epoch": 88.84880636604774,
      "grad_norm": 0.5324958562850952,
      "learning_rate": 6.470744919186056e-07,
      "loss": 1.0625,
      "step": 9506
    },
    {
      "epoch": 88.85823754789271,
      "grad_norm": 0.5526000261306763,
      "learning_rate": 6.460037839041012e-07,
      "loss": 1.0025,
      "step": 9507
    },
    {
      "epoch": 88.8676687297377,
      "grad_norm": 0.5622791051864624,
      "learning_rate": 6.449339328996318e-07,
      "loss": 1.0652,
      "step": 9508
    },
    {
      "epoch": 88.87709991158268,
      "grad_norm": 0.5771692395210266,
      "learning_rate": 6.438649390032192e-07,
      "loss": 1.0499,
      "step": 9509
    },
    {
      "epoch": 88.88653109342765,
      "grad_norm": 0.5515434741973877,
      "learning_rate": 6.427968023128039e-07,
      "loss": 0.9774,
      "step": 9510
    },
    {
      "epoch": 88.89596227527262,
      "grad_norm": 0.52565598487854,
      "learning_rate": 6.417295229262454e-07,
      "loss": 1.024,
      "step": 9511
    },
    {
      "epoch": 88.9053934571176,
      "grad_norm": 0.5757937431335449,
      "learning_rate": 6.406631009413322e-07,
      "loss": 1.0411,
      "step": 9512
    },
    {
      "epoch": 88.91482463896257,
      "grad_norm": 0.5533618927001953,
      "learning_rate": 6.395975364557672e-07,
      "loss": 1.0643,
      "step": 9513
    },
    {
      "epoch": 88.92425582080755,
      "grad_norm": 0.5766711831092834,
      "learning_rate": 6.385328295671766e-07,
      "loss": 1.028,
      "step": 9514
    },
    {
      "epoch": 88.93368700265252,
      "grad_norm": 0.5223440527915955,
      "learning_rate": 6.374689803731093e-07,
      "loss": 1.051,
      "step": 9515
    },
    {
      "epoch": 88.9431181844975,
      "grad_norm": 0.6061409115791321,
      "learning_rate": 6.364059889710339e-07,
      "loss": 1.0708,
      "step": 9516
    },
    {
      "epoch": 88.95254936634247,
      "grad_norm": 0.593025267124176,
      "learning_rate": 6.353438554583425e-07,
      "loss": 1.0324,
      "step": 9517
    },
    {
      "epoch": 88.96198054818744,
      "grad_norm": 0.5446853041648865,
      "learning_rate": 6.342825799323449e-07,
      "loss": 1.0446,
      "step": 9518
    },
    {
      "epoch": 88.97141173003241,
      "grad_norm": 0.5701221227645874,
      "learning_rate": 6.33222162490279e-07,
      "loss": 1.0208,
      "step": 9519
    },
    {
      "epoch": 88.98084291187739,
      "grad_norm": 0.5466902852058411,
      "learning_rate": 6.321626032292971e-07,
      "loss": 1.0427,
      "step": 9520
    },
    {
      "epoch": 88.99027409372236,
      "grad_norm": 0.5448103547096252,
      "learning_rate": 6.311039022464759e-07,
      "loss": 1.0443,
      "step": 9521
    },
    {
      "epoch": 88.99970527556735,
      "grad_norm": 0.5143229365348816,
      "learning_rate": 6.300460596388136e-07,
      "loss": 1.0222,
      "step": 9522
    },
    {
      "epoch": 89.0,
      "grad_norm": 3.204596996307373,
      "learning_rate": 6.289890755032302e-07,
      "loss": 0.6532,
      "step": 9523
    },
    {
      "epoch": 89.00943118184497,
      "grad_norm": 0.5323242545127869,
      "learning_rate": 6.279329499365649e-07,
      "loss": 1.0247,
      "step": 9524
    },
    {
      "epoch": 89.01886236368995,
      "grad_norm": 0.603533923625946,
      "learning_rate": 6.268776830355783e-07,
      "loss": 1.0614,
      "step": 9525
    },
    {
      "epoch": 89.02829354553492,
      "grad_norm": 0.5135391354560852,
      "learning_rate": 6.258232748969562e-07,
      "loss": 1.0662,
      "step": 9526
    },
    {
      "epoch": 89.0377247273799,
      "grad_norm": 0.5459518432617188,
      "learning_rate": 6.247697256173035e-07,
      "loss": 1.0492,
      "step": 9527
    },
    {
      "epoch": 89.04715590922487,
      "grad_norm": 0.5771081447601318,
      "learning_rate": 6.237170352931444e-07,
      "loss": 1.0941,
      "step": 9528
    },
    {
      "epoch": 89.05658709106984,
      "grad_norm": 0.5933289527893066,
      "learning_rate": 6.226652040209258e-07,
      "loss": 1.044,
      "step": 9529
    },
    {
      "epoch": 89.06601827291482,
      "grad_norm": 0.5373309850692749,
      "learning_rate": 6.216142318970175e-07,
      "loss": 1.0263,
      "step": 9530
    },
    {
      "epoch": 89.0754494547598,
      "grad_norm": 0.574828565120697,
      "learning_rate": 6.205641190177081e-07,
      "loss": 1.016,
      "step": 9531
    },
    {
      "epoch": 89.08488063660478,
      "grad_norm": 0.5775297284126282,
      "learning_rate": 6.195148654792094e-07,
      "loss": 1.0265,
      "step": 9532
    },
    {
      "epoch": 89.09431181844975,
      "grad_norm": 0.5532773733139038,
      "learning_rate": 6.184664713776511e-07,
      "loss": 1.0641,
      "step": 9533
    },
    {
      "epoch": 89.10374300029473,
      "grad_norm": 0.5442829728126526,
      "learning_rate": 6.17418936809091e-07,
      "loss": 1.0855,
      "step": 9534
    },
    {
      "epoch": 89.1131741821397,
      "grad_norm": 0.5260369777679443,
      "learning_rate": 6.163722618695011e-07,
      "loss": 1.0765,
      "step": 9535
    },
    {
      "epoch": 89.12260536398468,
      "grad_norm": 0.5738421082496643,
      "learning_rate": 6.153264466547793e-07,
      "loss": 1.0821,
      "step": 9536
    },
    {
      "epoch": 89.13203654582965,
      "grad_norm": 0.536236047744751,
      "learning_rate": 6.142814912607409e-07,
      "loss": 1.0507,
      "step": 9537
    },
    {
      "epoch": 89.14146772767462,
      "grad_norm": 0.49421426653862,
      "learning_rate": 6.13237395783125e-07,
      "loss": 1.0562,
      "step": 9538
    },
    {
      "epoch": 89.1508989095196,
      "grad_norm": 0.5461061596870422,
      "learning_rate": 6.121941603175907e-07,
      "loss": 1.0327,
      "step": 9539
    },
    {
      "epoch": 89.16033009136457,
      "grad_norm": 0.5936620831489563,
      "learning_rate": 6.111517849597192e-07,
      "loss": 1.0648,
      "step": 9540
    },
    {
      "epoch": 89.16976127320955,
      "grad_norm": 0.5741592049598694,
      "learning_rate": 6.101102698050132e-07,
      "loss": 1.0198,
      "step": 9541
    },
    {
      "epoch": 89.17919245505452,
      "grad_norm": 0.5541824698448181,
      "learning_rate": 6.090696149488962e-07,
      "loss": 1.0526,
      "step": 9542
    },
    {
      "epoch": 89.1886236368995,
      "grad_norm": 0.5712629556655884,
      "learning_rate": 6.08029820486713e-07,
      "loss": 1.0565,
      "step": 9543
    },
    {
      "epoch": 89.19805481874447,
      "grad_norm": 0.5431699752807617,
      "learning_rate": 6.069908865137275e-07,
      "loss": 1.0526,
      "step": 9544
    },
    {
      "epoch": 89.20748600058945,
      "grad_norm": 0.5321373343467712,
      "learning_rate": 6.059528131251269e-07,
      "loss": 1.0379,
      "step": 9545
    },
    {
      "epoch": 89.21691718243443,
      "grad_norm": 0.5756981372833252,
      "learning_rate": 6.049156004160206e-07,
      "loss": 1.0688,
      "step": 9546
    },
    {
      "epoch": 89.2263483642794,
      "grad_norm": 0.5653454661369324,
      "learning_rate": 6.03879248481436e-07,
      "loss": 1.0734,
      "step": 9547
    },
    {
      "epoch": 89.23577954612438,
      "grad_norm": 0.513496458530426,
      "learning_rate": 6.028437574163226e-07,
      "loss": 1.0113,
      "step": 9548
    },
    {
      "epoch": 89.24521072796935,
      "grad_norm": 0.5344411134719849,
      "learning_rate": 6.018091273155547e-07,
      "loss": 1.0726,
      "step": 9549
    },
    {
      "epoch": 89.25464190981432,
      "grad_norm": 0.5654467344284058,
      "learning_rate": 6.00775358273924e-07,
      "loss": 1.0345,
      "step": 9550
    },
    {
      "epoch": 89.2640730916593,
      "grad_norm": 0.514487087726593,
      "learning_rate": 5.997424503861426e-07,
      "loss": 1.0092,
      "step": 9551
    },
    {
      "epoch": 89.27350427350427,
      "grad_norm": 0.5186523795127869,
      "learning_rate": 5.987104037468461e-07,
      "loss": 1.0475,
      "step": 9552
    },
    {
      "epoch": 89.28293545534925,
      "grad_norm": 0.5619106888771057,
      "learning_rate": 5.976792184505898e-07,
      "loss": 1.0395,
      "step": 9553
    },
    {
      "epoch": 89.29236663719422,
      "grad_norm": 0.5186133980751038,
      "learning_rate": 5.966488945918502e-07,
      "loss": 1.0625,
      "step": 9554
    },
    {
      "epoch": 89.3017978190392,
      "grad_norm": 0.5929124355316162,
      "learning_rate": 5.956194322650255e-07,
      "loss": 1.0885,
      "step": 9555
    },
    {
      "epoch": 89.31122900088417,
      "grad_norm": 0.5629518628120422,
      "learning_rate": 5.945908315644355e-07,
      "loss": 1.0196,
      "step": 9556
    },
    {
      "epoch": 89.32066018272914,
      "grad_norm": 0.5541303753852844,
      "learning_rate": 5.935630925843206e-07,
      "loss": 1.0269,
      "step": 9557
    },
    {
      "epoch": 89.33009136457412,
      "grad_norm": 0.5406723618507385,
      "learning_rate": 5.925362154188408e-07,
      "loss": 1.0433,
      "step": 9558
    },
    {
      "epoch": 89.3395225464191,
      "grad_norm": 0.53862065076828,
      "learning_rate": 5.915102001620787e-07,
      "loss": 1.0507,
      "step": 9559
    },
    {
      "epoch": 89.34895372826408,
      "grad_norm": 0.5345583558082581,
      "learning_rate": 5.90485046908037e-07,
      "loss": 1.0438,
      "step": 9560
    },
    {
      "epoch": 89.35838491010905,
      "grad_norm": 0.5911220908164978,
      "learning_rate": 5.894607557506405e-07,
      "loss": 1.0424,
      "step": 9561
    },
    {
      "epoch": 89.36781609195403,
      "grad_norm": 0.5627778768539429,
      "learning_rate": 5.884373267837318e-07,
      "loss": 1.0277,
      "step": 9562
    },
    {
      "epoch": 89.377247273799,
      "grad_norm": 0.5283834338188171,
      "learning_rate": 5.874147601010816e-07,
      "loss": 1.0521,
      "step": 9563
    },
    {
      "epoch": 89.38667845564397,
      "grad_norm": 0.5446931719779968,
      "learning_rate": 5.863930557963749e-07,
      "loss": 1.0556,
      "step": 9564
    },
    {
      "epoch": 89.39610963748895,
      "grad_norm": 0.5193334221839905,
      "learning_rate": 5.853722139632201e-07,
      "loss": 1.0707,
      "step": 9565
    },
    {
      "epoch": 89.40554081933392,
      "grad_norm": 0.5900582075119019,
      "learning_rate": 5.843522346951447e-07,
      "loss": 1.0501,
      "step": 9566
    },
    {
      "epoch": 89.4149720011789,
      "grad_norm": 0.549400269985199,
      "learning_rate": 5.833331180856028e-07,
      "loss": 1.0476,
      "step": 9567
    },
    {
      "epoch": 89.42440318302387,
      "grad_norm": 0.5530518889427185,
      "learning_rate": 5.823148642279619e-07,
      "loss": 1.0275,
      "step": 9568
    },
    {
      "epoch": 89.43383436486884,
      "grad_norm": 0.5581668615341187,
      "learning_rate": 5.812974732155152e-07,
      "loss": 1.0592,
      "step": 9569
    },
    {
      "epoch": 89.44326554671382,
      "grad_norm": 0.5340785384178162,
      "learning_rate": 5.80280945141477e-07,
      "loss": 1.0533,
      "step": 9570
    },
    {
      "epoch": 89.45269672855879,
      "grad_norm": 0.5395413637161255,
      "learning_rate": 5.792652800989784e-07,
      "loss": 1.0383,
      "step": 9571
    },
    {
      "epoch": 89.46212791040377,
      "grad_norm": 0.5513609051704407,
      "learning_rate": 5.782504781810772e-07,
      "loss": 1.078,
      "step": 9572
    },
    {
      "epoch": 89.47155909224875,
      "grad_norm": 0.6080868244171143,
      "learning_rate": 5.772365394807478e-07,
      "loss": 1.0465,
      "step": 9573
    },
    {
      "epoch": 89.48099027409373,
      "grad_norm": 0.5594469308853149,
      "learning_rate": 5.762234640908881e-07,
      "loss": 1.055,
      "step": 9574
    },
    {
      "epoch": 89.4904214559387,
      "grad_norm": 0.5663884282112122,
      "learning_rate": 5.752112521043141e-07,
      "loss": 1.0452,
      "step": 9575
    },
    {
      "epoch": 89.49985263778368,
      "grad_norm": 0.5642284154891968,
      "learning_rate": 5.741999036137646e-07,
      "loss": 0.9961,
      "step": 9576
    },
    {
      "epoch": 89.50928381962865,
      "grad_norm": 0.54432612657547,
      "learning_rate": 5.731894187119003e-07,
      "loss": 1.0346,
      "step": 9577
    },
    {
      "epoch": 89.51871500147362,
      "grad_norm": 0.5760055184364319,
      "learning_rate": 5.72179797491299e-07,
      "loss": 1.0309,
      "step": 9578
    },
    {
      "epoch": 89.5281461833186,
      "grad_norm": 0.5687901973724365,
      "learning_rate": 5.711710400444648e-07,
      "loss": 1.0627,
      "step": 9579
    },
    {
      "epoch": 89.53757736516357,
      "grad_norm": 0.5538475513458252,
      "learning_rate": 5.70163146463818e-07,
      "loss": 1.063,
      "step": 9580
    },
    {
      "epoch": 89.54700854700855,
      "grad_norm": 0.5690980553627014,
      "learning_rate": 5.691561168417026e-07,
      "loss": 1.0414,
      "step": 9581
    },
    {
      "epoch": 89.55643972885352,
      "grad_norm": 0.5401155948638916,
      "learning_rate": 5.681499512703814e-07,
      "loss": 1.0338,
      "step": 9582
    },
    {
      "epoch": 89.5658709106985,
      "grad_norm": 0.541536808013916,
      "learning_rate": 5.671446498420397e-07,
      "loss": 1.0495,
      "step": 9583
    },
    {
      "epoch": 89.57530209254347,
      "grad_norm": 0.5576865077018738,
      "learning_rate": 5.661402126487814e-07,
      "loss": 0.9734,
      "step": 9584
    },
    {
      "epoch": 89.58473327438844,
      "grad_norm": 0.593814492225647,
      "learning_rate": 5.65136639782633e-07,
      "loss": 1.0221,
      "step": 9585
    },
    {
      "epoch": 89.59416445623341,
      "grad_norm": 0.5465313792228699,
      "learning_rate": 5.641339313355432e-07,
      "loss": 1.0512,
      "step": 9586
    },
    {
      "epoch": 89.6035956380784,
      "grad_norm": 0.53810715675354,
      "learning_rate": 5.631320873993785e-07,
      "loss": 1.0404,
      "step": 9587
    },
    {
      "epoch": 89.61302681992338,
      "grad_norm": 0.5828071236610413,
      "learning_rate": 5.621311080659287e-07,
      "loss": 1.0802,
      "step": 9588
    },
    {
      "epoch": 89.62245800176835,
      "grad_norm": 0.5187506079673767,
      "learning_rate": 5.611309934269015e-07,
      "loss": 1.0428,
      "step": 9589
    },
    {
      "epoch": 89.63188918361332,
      "grad_norm": 0.5817573666572571,
      "learning_rate": 5.601317435739285e-07,
      "loss": 1.0285,
      "step": 9590
    },
    {
      "epoch": 89.6413203654583,
      "grad_norm": 0.5433310270309448,
      "learning_rate": 5.591333585985592e-07,
      "loss": 1.0454,
      "step": 9591
    },
    {
      "epoch": 89.65075154730327,
      "grad_norm": 0.5439082384109497,
      "learning_rate": 5.581358385922664e-07,
      "loss": 1.044,
      "step": 9592
    },
    {
      "epoch": 89.66018272914825,
      "grad_norm": 0.5525468587875366,
      "learning_rate": 5.571391836464412e-07,
      "loss": 1.0422,
      "step": 9593
    },
    {
      "epoch": 89.66961391099322,
      "grad_norm": 0.5451417565345764,
      "learning_rate": 5.561433938523997e-07,
      "loss": 1.0711,
      "step": 9594
    },
    {
      "epoch": 89.6790450928382,
      "grad_norm": 0.5261164903640747,
      "learning_rate": 5.551484693013731e-07,
      "loss": 1.0671,
      "step": 9595
    },
    {
      "epoch": 89.68847627468317,
      "grad_norm": 0.5341547727584839,
      "learning_rate": 5.541544100845175e-07,
      "loss": 1.0807,
      "step": 9596
    },
    {
      "epoch": 89.69790745652814,
      "grad_norm": 0.5667080283164978,
      "learning_rate": 5.531612162929067e-07,
      "loss": 1.0904,
      "step": 9597
    },
    {
      "epoch": 89.70733863837312,
      "grad_norm": 0.5456251502037048,
      "learning_rate": 5.521688880175391e-07,
      "loss": 1.0519,
      "step": 9598
    },
    {
      "epoch": 89.71676982021809,
      "grad_norm": 0.5466822385787964,
      "learning_rate": 5.511774253493296e-07,
      "loss": 1.0463,
      "step": 9599
    },
    {
      "epoch": 89.72620100206306,
      "grad_norm": 0.5621135830879211,
      "learning_rate": 5.501868283791157e-07,
      "loss": 1.0219,
      "step": 9600
    },
    {
      "epoch": 89.73563218390805,
      "grad_norm": 0.5615582466125488,
      "learning_rate": 5.491970971976568e-07,
      "loss": 1.0692,
      "step": 9601
    },
    {
      "epoch": 89.74506336575303,
      "grad_norm": 0.5525213479995728,
      "learning_rate": 5.482082318956305e-07,
      "loss": 1.0077,
      "step": 9602
    },
    {
      "epoch": 89.754494547598,
      "grad_norm": 0.5173554420471191,
      "learning_rate": 5.472202325636378e-07,
      "loss": 1.0841,
      "step": 9603
    },
    {
      "epoch": 89.76392572944297,
      "grad_norm": 0.5162606835365295,
      "learning_rate": 5.462330992921983e-07,
      "loss": 1.0503,
      "step": 9604
    },
    {
      "epoch": 89.77335691128795,
      "grad_norm": 0.5379644632339478,
      "learning_rate": 5.452468321717518e-07,
      "loss": 1.0455,
      "step": 9605
    },
    {
      "epoch": 89.78278809313292,
      "grad_norm": 0.5866762399673462,
      "learning_rate": 5.442614312926609e-07,
      "loss": 1.0478,
      "step": 9606
    },
    {
      "epoch": 89.7922192749779,
      "grad_norm": 0.5440871715545654,
      "learning_rate": 5.432768967452073e-07,
      "loss": 1.0508,
      "step": 9607
    },
    {
      "epoch": 89.80165045682287,
      "grad_norm": 0.562432050704956,
      "learning_rate": 5.422932286195914e-07,
      "loss": 1.0567,
      "step": 9608
    },
    {
      "epoch": 89.81108163866784,
      "grad_norm": 0.5646965503692627,
      "learning_rate": 5.41310427005941e-07,
      "loss": 1.0523,
      "step": 9609
    },
    {
      "epoch": 89.82051282051282,
      "grad_norm": 0.5464702248573303,
      "learning_rate": 5.403284919942975e-07,
      "loss": 1.0188,
      "step": 9610
    },
    {
      "epoch": 89.82994400235779,
      "grad_norm": 0.5661144852638245,
      "learning_rate": 5.393474236746265e-07,
      "loss": 1.0236,
      "step": 9611
    },
    {
      "epoch": 89.83937518420277,
      "grad_norm": 0.5679071545600891,
      "learning_rate": 5.383672221368119e-07,
      "loss": 1.0411,
      "step": 9612
    },
    {
      "epoch": 89.84880636604774,
      "grad_norm": 0.540492832660675,
      "learning_rate": 5.373878874706607e-07,
      "loss": 1.1033,
      "step": 9613
    },
    {
      "epoch": 89.85823754789271,
      "grad_norm": 0.5240328907966614,
      "learning_rate": 5.364094197658987e-07,
      "loss": 1.1034,
      "step": 9614
    },
    {
      "epoch": 89.8676687297377,
      "grad_norm": 0.5787311792373657,
      "learning_rate": 5.35431819112171e-07,
      "loss": 1.0161,
      "step": 9615
    },
    {
      "epoch": 89.87709991158268,
      "grad_norm": 0.5651172399520874,
      "learning_rate": 5.344550855990472e-07,
      "loss": 1.0397,
      "step": 9616
    },
    {
      "epoch": 89.88653109342765,
      "grad_norm": 0.5952423214912415,
      "learning_rate": 5.334792193160155e-07,
      "loss": 1.0258,
      "step": 9617
    },
    {
      "epoch": 89.89596227527262,
      "grad_norm": 0.5754522085189819,
      "learning_rate": 5.325042203524844e-07,
      "loss": 1.0436,
      "step": 9618
    },
    {
      "epoch": 89.9053934571176,
      "grad_norm": 0.5929725170135498,
      "learning_rate": 5.315300887977815e-07,
      "loss": 1.0638,
      "step": 9619
    },
    {
      "epoch": 89.91482463896257,
      "grad_norm": 0.5214695334434509,
      "learning_rate": 5.305568247411575e-07,
      "loss": 1.0485,
      "step": 9620
    },
    {
      "epoch": 89.92425582080755,
      "grad_norm": 0.5281038880348206,
      "learning_rate": 5.295844282717821e-07,
      "loss": 1.0516,
      "step": 9621
    },
    {
      "epoch": 89.93368700265252,
      "grad_norm": 0.5856645107269287,
      "learning_rate": 5.286128994787465e-07,
      "loss": 1.0563,
      "step": 9622
    },
    {
      "epoch": 89.9431181844975,
      "grad_norm": 0.5293333530426025,
      "learning_rate": 5.276422384510604e-07,
      "loss": 1.0288,
      "step": 9623
    },
    {
      "epoch": 89.95254936634247,
      "grad_norm": 0.5632856488227844,
      "learning_rate": 5.266724452776573e-07,
      "loss": 1.089,
      "step": 9624
    },
    {
      "epoch": 89.96198054818744,
      "grad_norm": 0.5276286005973816,
      "learning_rate": 5.257035200473892e-07,
      "loss": 1.0054,
      "step": 9625
    },
    {
      "epoch": 89.97141173003241,
      "grad_norm": 0.5488169193267822,
      "learning_rate": 5.247354628490275e-07,
      "loss": 1.006,
      "step": 9626
    },
    {
      "epoch": 89.98084291187739,
      "grad_norm": 0.5413752198219299,
      "learning_rate": 5.237682737712657e-07,
      "loss": 1.024,
      "step": 9627
    },
    {
      "epoch": 89.99027409372236,
      "grad_norm": 0.532967746257782,
      "learning_rate": 5.228019529027161e-07,
      "loss": 1.0296,
      "step": 9628
    },
    {
      "epoch": 89.99970527556735,
      "grad_norm": 0.5422840714454651,
      "learning_rate": 5.218365003319159e-07,
      "loss": 1.0929,
      "step": 9629
    },
    {
      "epoch": 90.0,
      "grad_norm": 3.1854991912841797,
      "learning_rate": 5.208719161473175e-07,
      "loss": 0.6789,
      "step": 9630
    },
    {
      "epoch": 90.00943118184497,
      "grad_norm": 0.5772194862365723,
      "learning_rate": 5.199082004372958e-07,
      "loss": 1.0075,
      "step": 9631
    },
    {
      "epoch": 90.01886236368995,
      "grad_norm": 0.5301219820976257,
      "learning_rate": 5.189453532901467e-07,
      "loss": 1.053,
      "step": 9632
    },
    {
      "epoch": 90.02829354553492,
      "grad_norm": 0.566487193107605,
      "learning_rate": 5.179833747940844e-07,
      "loss": 1.0656,
      "step": 9633
    },
    {
      "epoch": 90.0377247273799,
      "grad_norm": 0.5699955224990845,
      "learning_rate": 5.17022265037247e-07,
      "loss": 1.041,
      "step": 9634
    },
    {
      "epoch": 90.04715590922487,
      "grad_norm": 0.5472379326820374,
      "learning_rate": 5.160620241076908e-07,
      "loss": 1.0013,
      "step": 9635
    },
    {
      "epoch": 90.05658709106984,
      "grad_norm": 0.6150845289230347,
      "learning_rate": 5.15102652093391e-07,
      "loss": 1.0498,
      "step": 9636
    },
    {
      "epoch": 90.06601827291482,
      "grad_norm": 0.5444390177726746,
      "learning_rate": 5.141441490822474e-07,
      "loss": 1.0387,
      "step": 9637
    },
    {
      "epoch": 90.0754494547598,
      "grad_norm": 0.5595410466194153,
      "learning_rate": 5.131865151620752e-07,
      "loss": 1.0292,
      "step": 9638
    },
    {
      "epoch": 90.08488063660478,
      "grad_norm": 0.5184100270271301,
      "learning_rate": 5.122297504206142e-07,
      "loss": 1.0645,
      "step": 9639
    },
    {
      "epoch": 90.09431181844975,
      "grad_norm": 0.5074229836463928,
      "learning_rate": 5.112738549455232e-07,
      "loss": 1.0342,
      "step": 9640
    },
    {
      "epoch": 90.10374300029473,
      "grad_norm": 0.5393104553222656,
      "learning_rate": 5.103188288243799e-07,
      "loss": 1.0361,
      "step": 9641
    },
    {
      "epoch": 90.1131741821397,
      "grad_norm": 0.5918081998825073,
      "learning_rate": 5.093646721446844e-07,
      "loss": 1.0597,
      "step": 9642
    },
    {
      "epoch": 90.12260536398468,
      "grad_norm": 0.5791653990745544,
      "learning_rate": 5.084113849938555e-07,
      "loss": 1.0223,
      "step": 9643
    },
    {
      "epoch": 90.13203654582965,
      "grad_norm": 0.5435827374458313,
      "learning_rate": 5.074589674592334e-07,
      "loss": 1.0666,
      "step": 9644
    },
    {
      "epoch": 90.14146772767462,
      "grad_norm": 0.5660974383354187,
      "learning_rate": 5.065074196280762e-07,
      "loss": 1.044,
      "step": 9645
    },
    {
      "epoch": 90.1508989095196,
      "grad_norm": 0.5674282312393188,
      "learning_rate": 5.055567415875684e-07,
      "loss": 1.031,
      "step": 9646
    },
    {
      "epoch": 90.16033009136457,
      "grad_norm": 0.5663330554962158,
      "learning_rate": 5.046069334248083e-07,
      "loss": 0.9939,
      "step": 9647
    },
    {
      "epoch": 90.16976127320955,
      "grad_norm": 0.5768174529075623,
      "learning_rate": 5.036579952268172e-07,
      "loss": 1.0471,
      "step": 9648
    },
    {
      "epoch": 90.17919245505452,
      "grad_norm": 0.5675263404846191,
      "learning_rate": 5.027099270805369e-07,
      "loss": 1.0516,
      "step": 9649
    },
    {
      "epoch": 90.1886236368995,
      "grad_norm": 0.5263115167617798,
      "learning_rate": 5.017627290728299e-07,
      "loss": 1.046,
      "step": 9650
    },
    {
      "epoch": 90.19805481874447,
      "grad_norm": 0.5428141951560974,
      "learning_rate": 5.008164012904759e-07,
      "loss": 1.024,
      "step": 9651
    },
    {
      "epoch": 90.20748600058945,
      "grad_norm": 0.5326207876205444,
      "learning_rate": 4.998709438201777e-07,
      "loss": 1.067,
      "step": 9652
    },
    {
      "epoch": 90.21691718243443,
      "grad_norm": 0.5788736343383789,
      "learning_rate": 4.989263567485603e-07,
      "loss": 1.0537,
      "step": 9653
    },
    {
      "epoch": 90.2263483642794,
      "grad_norm": 0.4757634699344635,
      "learning_rate": 4.979826401621635e-07,
      "loss": 1.0631,
      "step": 9654
    },
    {
      "epoch": 90.23577954612438,
      "grad_norm": 0.5645073652267456,
      "learning_rate": 4.970397941474526e-07,
      "loss": 1.0295,
      "step": 9655
    },
    {
      "epoch": 90.24521072796935,
      "grad_norm": 0.5571346879005432,
      "learning_rate": 4.960978187908095e-07,
      "loss": 1.0398,
      "step": 9656
    },
    {
      "epoch": 90.25464190981432,
      "grad_norm": 0.5254440903663635,
      "learning_rate": 4.951567141785374e-07,
      "loss": 1.0336,
      "step": 9657
    },
    {
      "epoch": 90.2640730916593,
      "grad_norm": 0.5860614776611328,
      "learning_rate": 4.942164803968608e-07,
      "loss": 1.0219,
      "step": 9658
    },
    {
      "epoch": 90.27350427350427,
      "grad_norm": 0.5624594688415527,
      "learning_rate": 4.932771175319228e-07,
      "loss": 1.0585,
      "step": 9659
    },
    {
      "epoch": 90.28293545534925,
      "grad_norm": 0.5628200173377991,
      "learning_rate": 4.92338625669787e-07,
      "loss": 1.0049,
      "step": 9660
    },
    {
      "epoch": 90.29236663719422,
      "grad_norm": 0.5289803147315979,
      "learning_rate": 4.914010048964412e-07,
      "loss": 1.0737,
      "step": 9661
    },
    {
      "epoch": 90.3017978190392,
      "grad_norm": 0.5400255918502808,
      "learning_rate": 4.904642552977856e-07,
      "loss": 1.0472,
      "step": 9662
    },
    {
      "epoch": 90.31122900088417,
      "grad_norm": 0.5771633386611938,
      "learning_rate": 4.895283769596481e-07,
      "loss": 1.0365,
      "step": 9663
    },
    {
      "epoch": 90.32066018272914,
      "grad_norm": 0.5682008266448975,
      "learning_rate": 4.885933699677714e-07,
      "loss": 1.0966,
      "step": 9664
    },
    {
      "epoch": 90.33009136457412,
      "grad_norm": 0.5527457594871521,
      "learning_rate": 4.876592344078224e-07,
      "loss": 1.076,
      "step": 9665
    },
    {
      "epoch": 90.3395225464191,
      "grad_norm": 0.5465532541275024,
      "learning_rate": 4.867259703653838e-07,
      "loss": 1.0384,
      "step": 9666
    },
    {
      "epoch": 90.34895372826408,
      "grad_norm": 0.5727002024650574,
      "learning_rate": 4.857935779259615e-07,
      "loss": 1.0486,
      "step": 9667
    },
    {
      "epoch": 90.35838491010905,
      "grad_norm": 0.5445257425308228,
      "learning_rate": 4.84862057174984e-07,
      "loss": 1.0793,
      "step": 9668
    },
    {
      "epoch": 90.36781609195403,
      "grad_norm": 0.5718958377838135,
      "learning_rate": 4.839314081977942e-07,
      "loss": 1.0714,
      "step": 9669
    },
    {
      "epoch": 90.377247273799,
      "grad_norm": 0.6650270223617554,
      "learning_rate": 4.830016310796581e-07,
      "loss": 1.0465,
      "step": 9670
    },
    {
      "epoch": 90.38667845564397,
      "grad_norm": 0.566076397895813,
      "learning_rate": 4.820727259057623e-07,
      "loss": 1.0803,
      "step": 9671
    },
    {
      "epoch": 90.39610963748895,
      "grad_norm": 0.5450130105018616,
      "learning_rate": 4.811446927612117e-07,
      "loss": 1.0849,
      "step": 9672
    },
    {
      "epoch": 90.40554081933392,
      "grad_norm": 0.5407994389533997,
      "learning_rate": 4.802175317310343e-07,
      "loss": 1.0668,
      "step": 9673
    },
    {
      "epoch": 90.4149720011789,
      "grad_norm": 0.5312103629112244,
      "learning_rate": 4.792912429001739e-07,
      "loss": 1.0593,
      "step": 9674
    },
    {
      "epoch": 90.42440318302387,
      "grad_norm": 0.5663963556289673,
      "learning_rate": 4.783658263534974e-07,
      "loss": 1.0598,
      "step": 9675
    },
    {
      "epoch": 90.43383436486884,
      "grad_norm": 0.5882222056388855,
      "learning_rate": 4.774412821757924e-07,
      "loss": 1.0535,
      "step": 9676
    },
    {
      "epoch": 90.44326554671382,
      "grad_norm": 0.5138576030731201,
      "learning_rate": 4.765176104517655e-07,
      "loss": 1.0583,
      "step": 9677
    },
    {
      "epoch": 90.45269672855879,
      "grad_norm": 0.5290830135345459,
      "learning_rate": 4.7559481126604137e-07,
      "loss": 1.0876,
      "step": 9678
    },
    {
      "epoch": 90.46212791040377,
      "grad_norm": 0.5272417664527893,
      "learning_rate": 4.746728847031679e-07,
      "loss": 1.0537,
      "step": 9679
    },
    {
      "epoch": 90.47155909224875,
      "grad_norm": 0.5451344847679138,
      "learning_rate": 4.7375183084761077e-07,
      "loss": 1.0485,
      "step": 9680
    },
    {
      "epoch": 90.48099027409373,
      "grad_norm": 0.5321037173271179,
      "learning_rate": 4.728316497837582e-07,
      "loss": 1.0825,
      "step": 9681
    },
    {
      "epoch": 90.4904214559387,
      "grad_norm": 0.5786333680152893,
      "learning_rate": 4.719123415959137e-07,
      "loss": 1.0218,
      "step": 9682
    },
    {
      "epoch": 90.49985263778368,
      "grad_norm": 0.5335723757743835,
      "learning_rate": 4.7099390636830776e-07,
      "loss": 1.0604,
      "step": 9683
    },
    {
      "epoch": 90.50928381962865,
      "grad_norm": 0.5679875016212463,
      "learning_rate": 4.700763441850853e-07,
      "loss": 1.0531,
      "step": 9684
    },
    {
      "epoch": 90.51871500147362,
      "grad_norm": 0.5900473594665527,
      "learning_rate": 4.691596551303135e-07,
      "loss": 1.0338,
      "step": 9685
    },
    {
      "epoch": 90.5281461833186,
      "grad_norm": 0.5459208488464355,
      "learning_rate": 4.6824383928797846e-07,
      "loss": 1.0217,
      "step": 9686
    },
    {
      "epoch": 90.53757736516357,
      "grad_norm": 0.5512142181396484,
      "learning_rate": 4.673288967419876e-07,
      "loss": 1.0284,
      "step": 9687
    },
    {
      "epoch": 90.54700854700855,
      "grad_norm": 0.562676727771759,
      "learning_rate": 4.6641482757616705e-07,
      "loss": 1.067,
      "step": 9688
    },
    {
      "epoch": 90.55643972885352,
      "grad_norm": 0.560069739818573,
      "learning_rate": 4.6550163187426444e-07,
      "loss": 1.0606,
      "step": 9689
    },
    {
      "epoch": 90.5658709106985,
      "grad_norm": 0.5881538391113281,
      "learning_rate": 4.6458930971994385e-07,
      "loss": 1.0321,
      "step": 9690
    },
    {
      "epoch": 90.57530209254347,
      "grad_norm": 0.5475137233734131,
      "learning_rate": 4.6367786119679514e-07,
      "loss": 1.0345,
      "step": 9691
    },
    {
      "epoch": 90.58473327438844,
      "grad_norm": 0.5214760899543762,
      "learning_rate": 4.627672863883248e-07,
      "loss": 1.0089,
      "step": 9692
    },
    {
      "epoch": 90.59416445623341,
      "grad_norm": 0.5516254901885986,
      "learning_rate": 4.6185758537795723e-07,
      "loss": 1.074,
      "step": 9693
    },
    {
      "epoch": 90.6035956380784,
      "grad_norm": 0.501173734664917,
      "learning_rate": 4.6094875824903907e-07,
      "loss": 1.0438,
      "step": 9694
    },
    {
      "epoch": 90.61302681992338,
      "grad_norm": 0.5454398393630981,
      "learning_rate": 4.600408050848393e-07,
      "loss": 1.0768,
      "step": 9695
    },
    {
      "epoch": 90.62245800176835,
      "grad_norm": 0.5326918959617615,
      "learning_rate": 4.5913372596854244e-07,
      "loss": 1.0807,
      "step": 9696
    },
    {
      "epoch": 90.63188918361332,
      "grad_norm": 0.5486212968826294,
      "learning_rate": 4.582275209832554e-07,
      "loss": 1.0475,
      "step": 9697
    },
    {
      "epoch": 90.6413203654583,
      "grad_norm": 0.6096934676170349,
      "learning_rate": 4.5732219021200284e-07,
      "loss": 1.062,
      "step": 9698
    },
    {
      "epoch": 90.65075154730327,
      "grad_norm": 0.5879055261611938,
      "learning_rate": 4.564177337377318e-07,
      "loss": 1.0503,
      "step": 9699
    },
    {
      "epoch": 90.66018272914825,
      "grad_norm": 0.5356754660606384,
      "learning_rate": 4.5551415164331036e-07,
      "loss": 1.0425,
      "step": 9700
    },
    {
      "epoch": 90.66961391099322,
      "grad_norm": 0.5256288647651672,
      "learning_rate": 4.5461144401152125e-07,
      "loss": 1.0828,
      "step": 9701
    },
    {
      "epoch": 90.6790450928382,
      "grad_norm": 0.5452855825424194,
      "learning_rate": 4.537096109250727e-07,
      "loss": 1.0617,
      "step": 9702
    },
    {
      "epoch": 90.68847627468317,
      "grad_norm": 0.5321938991546631,
      "learning_rate": 4.5280865246658865e-07,
      "loss": 1.0639,
      "step": 9703
    },
    {
      "epoch": 90.69790745652814,
      "grad_norm": 0.544493556022644,
      "learning_rate": 4.519085687186153e-07,
      "loss": 1.0781,
      "step": 9704
    },
    {
      "epoch": 90.70733863837312,
      "grad_norm": 0.5317196846008301,
      "learning_rate": 4.5100935976361557e-07,
      "loss": 1.0488,
      "step": 9705
    },
    {
      "epoch": 90.71676982021809,
      "grad_norm": 0.5036495923995972,
      "learning_rate": 4.5011102568398023e-07,
      "loss": 1.0628,
      "step": 9706
    },
    {
      "epoch": 90.72620100206306,
      "grad_norm": 0.5288088321685791,
      "learning_rate": 4.4921356656201007e-07,
      "loss": 1.0648,
      "step": 9707
    },
    {
      "epoch": 90.73563218390805,
      "grad_norm": 0.574184775352478,
      "learning_rate": 4.483169824799305e-07,
      "loss": 1.0742,
      "step": 9708
    },
    {
      "epoch": 90.74506336575303,
      "grad_norm": 0.5691707134246826,
      "learning_rate": 4.4742127351988794e-07,
      "loss": 1.021,
      "step": 9709
    },
    {
      "epoch": 90.754494547598,
      "grad_norm": 0.5346861481666565,
      "learning_rate": 4.465264397639457e-07,
      "loss": 1.0254,
      "step": 9710
    },
    {
      "epoch": 90.76392572944297,
      "grad_norm": 0.5610780119895935,
      "learning_rate": 4.456324812940882e-07,
      "loss": 1.0261,
      "step": 9711
    },
    {
      "epoch": 90.77335691128795,
      "grad_norm": 0.5818547010421753,
      "learning_rate": 4.447393981922177e-07,
      "loss": 1.0403,
      "step": 9712
    },
    {
      "epoch": 90.78278809313292,
      "grad_norm": 0.5671012997627258,
      "learning_rate": 4.4384719054016203e-07,
      "loss": 1.0515,
      "step": 9713
    },
    {
      "epoch": 90.7922192749779,
      "grad_norm": 0.5411532521247864,
      "learning_rate": 4.429558584196636e-07,
      "loss": 1.0838,
      "step": 9714
    },
    {
      "epoch": 90.80165045682287,
      "grad_norm": 0.5413756966590881,
      "learning_rate": 4.420654019123849e-07,
      "loss": 1.0376,
      "step": 9715
    },
    {
      "epoch": 90.81108163866784,
      "grad_norm": 0.584105908870697,
      "learning_rate": 4.411758210999106e-07,
      "loss": 1.1133,
      "step": 9716
    },
    {
      "epoch": 90.82051282051282,
      "grad_norm": 0.5889573097229004,
      "learning_rate": 4.402871160637423e-07,
      "loss": 1.0622,
      "step": 9717
    },
    {
      "epoch": 90.82994400235779,
      "grad_norm": 0.554614782333374,
      "learning_rate": 4.393992868853036e-07,
      "loss": 1.0057,
      "step": 9718
    },
    {
      "epoch": 90.83937518420277,
      "grad_norm": 0.5414983034133911,
      "learning_rate": 4.385123336459374e-07,
      "loss": 1.0127,
      "step": 9719
    },
    {
      "epoch": 90.84880636604774,
      "grad_norm": 0.5360842347145081,
      "learning_rate": 4.3762625642690514e-07,
      "loss": 1.0437,
      "step": 9720
    },
    {
      "epoch": 90.85823754789271,
      "grad_norm": 0.5601052641868591,
      "learning_rate": 4.3674105530938984e-07,
      "loss": 1.0275,
      "step": 9721
    },
    {
      "epoch": 90.8676687297377,
      "grad_norm": 0.5562275052070618,
      "learning_rate": 4.3585673037449315e-07,
      "loss": 1.0588,
      "step": 9722
    },
    {
      "epoch": 90.87709991158268,
      "grad_norm": 0.5269840955734253,
      "learning_rate": 4.3497328170323706e-07,
      "loss": 1.0282,
      "step": 9723
    },
    {
      "epoch": 90.88653109342765,
      "grad_norm": 0.5450927019119263,
      "learning_rate": 4.340907093765623e-07,
      "loss": 1.0563,
      "step": 9724
    },
    {
      "epoch": 90.89596227527262,
      "grad_norm": 0.5326151251792908,
      "learning_rate": 4.332090134753297e-07,
      "loss": 1.0385,
      "step": 9725
    },
    {
      "epoch": 90.9053934571176,
      "grad_norm": 0.537347137928009,
      "learning_rate": 4.3232819408032034e-07,
      "loss": 1.0538,
      "step": 9726
    },
    {
      "epoch": 90.91482463896257,
      "grad_norm": 0.50992351770401,
      "learning_rate": 4.3144825127223287e-07,
      "loss": 1.0536,
      "step": 9727
    },
    {
      "epoch": 90.92425582080755,
      "grad_norm": 0.5650781989097595,
      "learning_rate": 4.3056918513168957e-07,
      "loss": 1.0728,
      "step": 9728
    },
    {
      "epoch": 90.93368700265252,
      "grad_norm": 0.5518584251403809,
      "learning_rate": 4.2969099573923036e-07,
      "loss": 1.048,
      "step": 9729
    },
    {
      "epoch": 90.9431181844975,
      "grad_norm": 0.5330885052680969,
      "learning_rate": 4.288136831753131e-07,
      "loss": 1.0322,
      "step": 9730
    },
    {
      "epoch": 90.95254936634247,
      "grad_norm": 0.5438281893730164,
      "learning_rate": 4.2793724752031807e-07,
      "loss": 1.0269,
      "step": 9731
    },
    {
      "epoch": 90.96198054818744,
      "grad_norm": 0.5596216320991516,
      "learning_rate": 4.270616888545431e-07,
      "loss": 1.0355,
      "step": 9732
    },
    {
      "epoch": 90.97141173003241,
      "grad_norm": 0.5727801322937012,
      "learning_rate": 4.261870072582075e-07,
      "loss": 1.0176,
      "step": 9733
    },
    {
      "epoch": 90.98084291187739,
      "grad_norm": 0.5738047361373901,
      "learning_rate": 4.2531320281144926e-07,
      "loss": 1.0406,
      "step": 9734
    },
    {
      "epoch": 90.99027409372236,
      "grad_norm": 0.5381126403808594,
      "learning_rate": 4.244402755943233e-07,
      "loss": 1.0461,
      "step": 9735
    },
    {
      "epoch": 90.99970527556735,
      "grad_norm": 0.537429928779602,
      "learning_rate": 4.235682256868112e-07,
      "loss": 1.0232,
      "step": 9736
    },
    {
      "epoch": 91.0,
      "grad_norm": 4.120180606842041,
      "learning_rate": 4.2269705316880795e-07,
      "loss": 0.4824,
      "step": 9737
    },
    {
      "epoch": 91.00943118184497,
      "grad_norm": 0.5752577781677246,
      "learning_rate": 4.218267581201296e-07,
      "loss": 1.1103,
      "step": 9738
    },
    {
      "epoch": 91.01886236368995,
      "grad_norm": 0.5391402840614319,
      "learning_rate": 4.2095734062051253e-07,
      "loss": 1.0165,
      "step": 9739
    },
    {
      "epoch": 91.02829354553492,
      "grad_norm": 0.5363928079605103,
      "learning_rate": 4.20088800749614e-07,
      "loss": 1.0395,
      "step": 9740
    },
    {
      "epoch": 91.0377247273799,
      "grad_norm": 0.5491894483566284,
      "learning_rate": 4.192211385870071e-07,
      "loss": 1.0747,
      "step": 9741
    },
    {
      "epoch": 91.04715590922487,
      "grad_norm": 0.5083379745483398,
      "learning_rate": 4.1835435421218594e-07,
      "loss": 1.0607,
      "step": 9742
    },
    {
      "epoch": 91.05658709106984,
      "grad_norm": 0.5612678527832031,
      "learning_rate": 4.174884477045682e-07,
      "loss": 1.0163,
      "step": 9743
    },
    {
      "epoch": 91.06601827291482,
      "grad_norm": 0.5389605164527893,
      "learning_rate": 4.16623419143487e-07,
      "loss": 1.0328,
      "step": 9744
    },
    {
      "epoch": 91.0754494547598,
      "grad_norm": 0.5634843707084656,
      "learning_rate": 4.157592686081957e-07,
      "loss": 1.0613,
      "step": 9745
    },
    {
      "epoch": 91.08488063660478,
      "grad_norm": 0.5474336743354797,
      "learning_rate": 4.1489599617786646e-07,
      "loss": 1.081,
      "step": 9746
    },
    {
      "epoch": 91.09431181844975,
      "grad_norm": 0.544424295425415,
      "learning_rate": 4.1403360193159383e-07,
      "loss": 1.0154,
      "step": 9747
    },
    {
      "epoch": 91.10374300029473,
      "grad_norm": 0.551944375038147,
      "learning_rate": 4.13172085948389e-07,
      "loss": 1.0262,
      "step": 9748
    },
    {
      "epoch": 91.1131741821397,
      "grad_norm": 0.5750288367271423,
      "learning_rate": 4.123114483071822e-07,
      "loss": 1.022,
      "step": 9749
    },
    {
      "epoch": 91.12260536398468,
      "grad_norm": 0.547330915927887,
      "learning_rate": 4.11451689086827e-07,
      "loss": 1.0579,
      "step": 9750
    },
    {
      "epoch": 91.13203654582965,
      "grad_norm": 0.5275830030441284,
      "learning_rate": 4.1059280836609483e-07,
      "loss": 1.0659,
      "step": 9751
    },
    {
      "epoch": 91.14146772767462,
      "grad_norm": 0.5684303641319275,
      "learning_rate": 4.097348062236739e-07,
      "loss": 1.0423,
      "step": 9752
    },
    {
      "epoch": 91.1508989095196,
      "grad_norm": 0.5374464988708496,
      "learning_rate": 4.088776827381757e-07,
      "loss": 1.0389,
      "step": 9753
    },
    {
      "epoch": 91.16033009136457,
      "grad_norm": 0.5233131051063538,
      "learning_rate": 4.080214379881298e-07,
      "loss": 1.0459,
      "step": 9754
    },
    {
      "epoch": 91.16976127320955,
      "grad_norm": 0.5262554883956909,
      "learning_rate": 4.071660720519832e-07,
      "loss": 1.0326,
      "step": 9755
    },
    {
      "epoch": 91.17919245505452,
      "grad_norm": 0.5496211051940918,
      "learning_rate": 4.0631158500810454e-07,
      "loss": 0.9964,
      "step": 9756
    },
    {
      "epoch": 91.1886236368995,
      "grad_norm": 0.527849555015564,
      "learning_rate": 4.054579769347833e-07,
      "loss": 1.0886,
      "step": 9757
    },
    {
      "epoch": 91.19805481874447,
      "grad_norm": 0.5858221054077148,
      "learning_rate": 4.046052479102258e-07,
      "loss": 1.0839,
      "step": 9758
    },
    {
      "epoch": 91.20748600058945,
      "grad_norm": 0.562362551689148,
      "learning_rate": 4.037533980125585e-07,
      "loss": 1.0708,
      "step": 9759
    },
    {
      "epoch": 91.21691718243443,
      "grad_norm": 0.5115342140197754,
      "learning_rate": 4.02902427319829e-07,
      "loss": 1.0582,
      "step": 9760
    },
    {
      "epoch": 91.2263483642794,
      "grad_norm": 0.5433530211448669,
      "learning_rate": 4.0205233591000035e-07,
      "loss": 1.0613,
      "step": 9761
    },
    {
      "epoch": 91.23577954612438,
      "grad_norm": 0.5306485295295715,
      "learning_rate": 4.012031238609604e-07,
      "loss": 1.0427,
      "step": 9762
    },
    {
      "epoch": 91.24521072796935,
      "grad_norm": 0.5448399782180786,
      "learning_rate": 4.0035479125051235e-07,
      "loss": 1.0657,
      "step": 9763
    },
    {
      "epoch": 91.25464190981432,
      "grad_norm": 0.5831679105758667,
      "learning_rate": 3.9950733815637963e-07,
      "loss": 1.0381,
      "step": 9764
    },
    {
      "epoch": 91.2640730916593,
      "grad_norm": 0.5373132824897766,
      "learning_rate": 3.9866076465620687e-07,
      "loss": 1.0376,
      "step": 9765
    },
    {
      "epoch": 91.27350427350427,
      "grad_norm": 0.5330551266670227,
      "learning_rate": 3.978150708275552e-07,
      "loss": 1.0634,
      "step": 9766
    },
    {
      "epoch": 91.28293545534925,
      "grad_norm": 0.5612878203392029,
      "learning_rate": 3.9697025674790946e-07,
      "loss": 1.0478,
      "step": 9767
    },
    {
      "epoch": 91.29236663719422,
      "grad_norm": 0.5601304173469543,
      "learning_rate": 3.9612632249466876e-07,
      "loss": 1.102,
      "step": 9768
    },
    {
      "epoch": 91.3017978190392,
      "grad_norm": 0.5250574946403503,
      "learning_rate": 3.9528326814515574e-07,
      "loss": 1.0577,
      "step": 9769
    },
    {
      "epoch": 91.31122900088417,
      "grad_norm": 0.5640173554420471,
      "learning_rate": 3.9444109377660965e-07,
      "loss": 1.0407,
      "step": 9770
    },
    {
      "epoch": 91.32066018272914,
      "grad_norm": 0.6046317219734192,
      "learning_rate": 3.9359979946619e-07,
      "loss": 1.0515,
      "step": 9771
    },
    {
      "epoch": 91.33009136457412,
      "grad_norm": 0.5461131930351257,
      "learning_rate": 3.927593852909761e-07,
      "loss": 1.0468,
      "step": 9772
    },
    {
      "epoch": 91.3395225464191,
      "grad_norm": 0.5336334109306335,
      "learning_rate": 3.9191985132796763e-07,
      "loss": 1.0349,
      "step": 9773
    },
    {
      "epoch": 91.34895372826408,
      "grad_norm": 0.5590105652809143,
      "learning_rate": 3.910811976540818e-07,
      "loss": 1.0971,
      "step": 9774
    },
    {
      "epoch": 91.35838491010905,
      "grad_norm": 0.5379073023796082,
      "learning_rate": 3.90243424346155e-07,
      "loss": 1.0164,
      "step": 9775
    },
    {
      "epoch": 91.36781609195403,
      "grad_norm": 0.556456983089447,
      "learning_rate": 3.894065314809448e-07,
      "loss": 1.0522,
      "step": 9776
    },
    {
      "epoch": 91.377247273799,
      "grad_norm": 0.5487777590751648,
      "learning_rate": 3.8857051913512634e-07,
      "loss": 1.0445,
      "step": 9777
    },
    {
      "epoch": 91.38667845564397,
      "grad_norm": 0.5490416288375854,
      "learning_rate": 3.8773538738529517e-07,
      "loss": 1.0514,
      "step": 9778
    },
    {
      "epoch": 91.39610963748895,
      "grad_norm": 0.5342967510223389,
      "learning_rate": 3.8690113630796445e-07,
      "loss": 1.0604,
      "step": 9779
    },
    {
      "epoch": 91.40554081933392,
      "grad_norm": 0.5703282356262207,
      "learning_rate": 3.8606776597956974e-07,
      "loss": 1.0496,
      "step": 9780
    },
    {
      "epoch": 91.4149720011789,
      "grad_norm": 0.537262499332428,
      "learning_rate": 3.8523527647646444e-07,
      "loss": 1.0347,
      "step": 9781
    },
    {
      "epoch": 91.42440318302387,
      "grad_norm": 0.5261300206184387,
      "learning_rate": 3.844036678749197e-07,
      "loss": 1.0449,
      "step": 9782
    },
    {
      "epoch": 91.43383436486884,
      "grad_norm": 0.563723087310791,
      "learning_rate": 3.835729402511268e-07,
      "loss": 1.021,
      "step": 9783
    },
    {
      "epoch": 91.44326554671382,
      "grad_norm": 0.5671154856681824,
      "learning_rate": 3.8274309368119824e-07,
      "loss": 1.0039,
      "step": 9784
    },
    {
      "epoch": 91.45269672855879,
      "grad_norm": 0.5529055595397949,
      "learning_rate": 3.8191412824116426e-07,
      "loss": 1.0488,
      "step": 9785
    },
    {
      "epoch": 91.46212791040377,
      "grad_norm": 0.548615038394928,
      "learning_rate": 3.810860440069719e-07,
      "loss": 1.0415,
      "step": 9786
    },
    {
      "epoch": 91.47155909224875,
      "grad_norm": 0.5525375008583069,
      "learning_rate": 3.802588410544927e-07,
      "loss": 1.0612,
      "step": 9787
    },
    {
      "epoch": 91.48099027409373,
      "grad_norm": 0.525559663772583,
      "learning_rate": 3.794325194595139e-07,
      "loss": 1.053,
      "step": 9788
    },
    {
      "epoch": 91.4904214559387,
      "grad_norm": 0.5778462290763855,
      "learning_rate": 3.7860707929774253e-07,
      "loss": 1.0973,
      "step": 9789
    },
    {
      "epoch": 91.49985263778368,
      "grad_norm": 0.5321111679077148,
      "learning_rate": 3.777825206448049e-07,
      "loss": 1.0516,
      "step": 9790
    },
    {
      "epoch": 91.50928381962865,
      "grad_norm": 0.5331762433052063,
      "learning_rate": 3.7695884357624837e-07,
      "loss": 1.0715,
      "step": 9791
    },
    {
      "epoch": 91.51871500147362,
      "grad_norm": 0.5600031018257141,
      "learning_rate": 3.761360481675358e-07,
      "loss": 1.014,
      "step": 9792
    },
    {
      "epoch": 91.5281461833186,
      "grad_norm": 0.568644642829895,
      "learning_rate": 3.753141344940536e-07,
      "loss": 1.0369,
      "step": 9793
    },
    {
      "epoch": 91.53757736516357,
      "grad_norm": 0.5990275144577026,
      "learning_rate": 3.7449310263110163e-07,
      "loss": 1.0193,
      "step": 9794
    },
    {
      "epoch": 91.54700854700855,
      "grad_norm": 0.5301658511161804,
      "learning_rate": 3.7367295265390737e-07,
      "loss": 1.069,
      "step": 9795
    },
    {
      "epoch": 91.55643972885352,
      "grad_norm": 0.5532408356666565,
      "learning_rate": 3.7285368463760964e-07,
      "loss": 1.0357,
      "step": 9796
    },
    {
      "epoch": 91.5658709106985,
      "grad_norm": 0.5255019068717957,
      "learning_rate": 3.720352986572706e-07,
      "loss": 1.0827,
      "step": 9797
    },
    {
      "epoch": 91.57530209254347,
      "grad_norm": 0.5542953610420227,
      "learning_rate": 3.7121779478786923e-07,
      "loss": 1.0537,
      "step": 9798
    },
    {
      "epoch": 91.58473327438844,
      "grad_norm": 0.5978103876113892,
      "learning_rate": 3.7040117310430666e-07,
      "loss": 1.0828,
      "step": 9799
    },
    {
      "epoch": 91.59416445623341,
      "grad_norm": 0.5703268647193909,
      "learning_rate": 3.6958543368140086e-07,
      "loss": 0.9857,
      "step": 9800
    },
    {
      "epoch": 91.6035956380784,
      "grad_norm": 0.5444256663322449,
      "learning_rate": 3.687705765938887e-07,
      "loss": 1.0228,
      "step": 9801
    },
    {
      "epoch": 91.61302681992338,
      "grad_norm": 0.569972574710846,
      "learning_rate": 3.679566019164271e-07,
      "loss": 1.0605,
      "step": 9802
    },
    {
      "epoch": 91.62245800176835,
      "grad_norm": 0.5498142242431641,
      "learning_rate": 3.6714350972359316e-07,
      "loss": 1.0343,
      "step": 9803
    },
    {
      "epoch": 91.63188918361332,
      "grad_norm": 0.5371809601783752,
      "learning_rate": 3.663313000898827e-07,
      "loss": 1.0492,
      "step": 9804
    },
    {
      "epoch": 91.6413203654583,
      "grad_norm": 0.5362457036972046,
      "learning_rate": 3.6551997308970964e-07,
      "loss": 1.0646,
      "step": 9805
    },
    {
      "epoch": 91.65075154730327,
      "grad_norm": 0.5415758490562439,
      "learning_rate": 3.6470952879740563e-07,
      "loss": 1.0133,
      "step": 9806
    },
    {
      "epoch": 91.66018272914825,
      "grad_norm": 0.525339663028717,
      "learning_rate": 3.6389996728722565e-07,
      "loss": 1.0449,
      "step": 9807
    },
    {
      "epoch": 91.66961391099322,
      "grad_norm": 0.5920728445053101,
      "learning_rate": 3.630912886333404e-07,
      "loss": 1.0517,
      "step": 9808
    },
    {
      "epoch": 91.6790450928382,
      "grad_norm": 0.5561355352401733,
      "learning_rate": 3.6228349290984065e-07,
      "loss": 1.0486,
      "step": 9809
    },
    {
      "epoch": 91.68847627468317,
      "grad_norm": 0.5628905296325684,
      "learning_rate": 3.6147658019073717e-07,
      "loss": 1.069,
      "step": 9810
    },
    {
      "epoch": 91.69790745652814,
      "grad_norm": 0.5499079823493958,
      "learning_rate": 3.6067055054995747e-07,
      "loss": 1.079,
      "step": 9811
    },
    {
      "epoch": 91.70733863837312,
      "grad_norm": 0.5282198190689087,
      "learning_rate": 3.598654040613525e-07,
      "loss": 1.0177,
      "step": 9812
    },
    {
      "epoch": 91.71676982021809,
      "grad_norm": 0.5265623927116394,
      "learning_rate": 3.590611407986866e-07,
      "loss": 1.0678,
      "step": 9813
    },
    {
      "epoch": 91.72620100206306,
      "grad_norm": 0.5710561871528625,
      "learning_rate": 3.5825776083564746e-07,
      "loss": 1.065,
      "step": 9814
    },
    {
      "epoch": 91.73563218390805,
      "grad_norm": 0.5635309815406799,
      "learning_rate": 3.5745526424584067e-07,
      "loss": 1.0154,
      "step": 9815
    },
    {
      "epoch": 91.74506336575303,
      "grad_norm": 0.5443964004516602,
      "learning_rate": 3.566536511027907e-07,
      "loss": 1.0687,
      "step": 9816
    },
    {
      "epoch": 91.754494547598,
      "grad_norm": 0.5795230865478516,
      "learning_rate": 3.558529214799389e-07,
      "loss": 1.0113,
      "step": 9817
    },
    {
      "epoch": 91.76392572944297,
      "grad_norm": 0.5406429171562195,
      "learning_rate": 3.5505307545065094e-07,
      "loss": 1.0498,
      "step": 9818
    },
    {
      "epoch": 91.77335691128795,
      "grad_norm": 0.5756725072860718,
      "learning_rate": 3.5425411308820713e-07,
      "loss": 1.0868,
      "step": 9819
    },
    {
      "epoch": 91.78278809313292,
      "grad_norm": 0.5438240766525269,
      "learning_rate": 3.5345603446580777e-07,
      "loss": 1.0839,
      "step": 9820
    },
    {
      "epoch": 91.7922192749779,
      "grad_norm": 0.5479249954223633,
      "learning_rate": 3.526588396565733e-07,
      "loss": 1.0181,
      "step": 9821
    },
    {
      "epoch": 91.80165045682287,
      "grad_norm": 0.5557945966720581,
      "learning_rate": 3.518625287335409e-07,
      "loss": 1.0372,
      "step": 9822
    },
    {
      "epoch": 91.81108163866784,
      "grad_norm": 0.5345757007598877,
      "learning_rate": 3.5106710176967094e-07,
      "loss": 1.022,
      "step": 9823
    },
    {
      "epoch": 91.82051282051282,
      "grad_norm": 0.5344415307044983,
      "learning_rate": 3.502725588378386e-07,
      "loss": 1.0261,
      "step": 9824
    },
    {
      "epoch": 91.82994400235779,
      "grad_norm": 0.5360967516899109,
      "learning_rate": 3.4947890001084007e-07,
      "loss": 1.0529,
      "step": 9825
    },
    {
      "epoch": 91.83937518420277,
      "grad_norm": 0.5694952607154846,
      "learning_rate": 3.486861253613905e-07,
      "loss": 1.0649,
      "step": 9826
    },
    {
      "epoch": 91.84880636604774,
      "grad_norm": 0.5458844900131226,
      "learning_rate": 3.478942349621206e-07,
      "loss": 1.0338,
      "step": 9827
    },
    {
      "epoch": 91.85823754789271,
      "grad_norm": 0.5661330223083496,
      "learning_rate": 3.471032288855869e-07,
      "loss": 1.0509,
      "step": 9828
    },
    {
      "epoch": 91.8676687297377,
      "grad_norm": 0.5625718832015991,
      "learning_rate": 3.463131072042603e-07,
      "loss": 1.0222,
      "step": 9829
    },
    {
      "epoch": 91.87709991158268,
      "grad_norm": 0.5380480885505676,
      "learning_rate": 3.4552386999053054e-07,
      "loss": 1.0604,
      "step": 9830
    },
    {
      "epoch": 91.88653109342765,
      "grad_norm": 0.554636538028717,
      "learning_rate": 3.447355173167066e-07,
      "loss": 1.056,
      "step": 9831
    },
    {
      "epoch": 91.89596227527262,
      "grad_norm": 0.4955347180366516,
      "learning_rate": 3.4394804925501846e-07,
      "loss": 1.033,
      "step": 9832
    },
    {
      "epoch": 91.9053934571176,
      "grad_norm": 0.5477280020713806,
      "learning_rate": 3.431614658776139e-07,
      "loss": 1.0701,
      "step": 9833
    },
    {
      "epoch": 91.91482463896257,
      "grad_norm": 0.5570580363273621,
      "learning_rate": 3.4237576725655863e-07,
      "loss": 1.0523,
      "step": 9834
    },
    {
      "epoch": 91.92425582080755,
      "grad_norm": 0.5560683012008667,
      "learning_rate": 3.415909534638395e-07,
      "loss": 1.035,
      "step": 9835
    },
    {
      "epoch": 91.93368700265252,
      "grad_norm": 0.5566233396530151,
      "learning_rate": 3.4080702457135797e-07,
      "loss": 1.0597,
      "step": 9836
    },
    {
      "epoch": 91.9431181844975,
      "grad_norm": 0.5422714948654175,
      "learning_rate": 3.400239806509409e-07,
      "loss": 1.0507,
      "step": 9837
    },
    {
      "epoch": 91.95254936634247,
      "grad_norm": 0.4996461868286133,
      "learning_rate": 3.392418217743276e-07,
      "loss": 1.0814,
      "step": 9838
    },
    {
      "epoch": 91.96198054818744,
      "grad_norm": 0.581668496131897,
      "learning_rate": 3.384605480131786e-07,
      "loss": 1.0394,
      "step": 9839
    },
    {
      "epoch": 91.97141173003241,
      "grad_norm": 0.5226547718048096,
      "learning_rate": 3.376801594390777e-07,
      "loss": 1.0442,
      "step": 9840
    },
    {
      "epoch": 91.98084291187739,
      "grad_norm": 0.5648125410079956,
      "learning_rate": 3.369006561235222e-07,
      "loss": 1.038,
      "step": 9841
    },
    {
      "epoch": 91.99027409372236,
      "grad_norm": 0.5802344083786011,
      "learning_rate": 3.361220381379293e-07,
      "loss": 1.0291,
      "step": 9842
    },
    {
      "epoch": 91.99970527556735,
      "grad_norm": 0.5792691707611084,
      "learning_rate": 3.353443055536354e-07,
      "loss": 1.0115,
      "step": 9843
    },
    {
      "epoch": 92.0,
      "grad_norm": 2.5507891178131104,
      "learning_rate": 3.3456745844189677e-07,
      "loss": 0.7796,
      "step": 9844
    },
    {
      "epoch": 92.00943118184497,
      "grad_norm": 0.5316476821899414,
      "learning_rate": 3.3379149687388866e-07,
      "loss": 1.0659,
      "step": 9845
    },
    {
      "epoch": 92.01886236368995,
      "grad_norm": 0.5126953125,
      "learning_rate": 3.330164209207032e-07,
      "loss": 1.0432,
      "step": 9846
    },
    {
      "epoch": 92.02829354553492,
      "grad_norm": 0.5045936107635498,
      "learning_rate": 3.3224223065335125e-07,
      "loss": 1.0498,
      "step": 9847
    },
    {
      "epoch": 92.0377247273799,
      "grad_norm": 0.5381481647491455,
      "learning_rate": 3.3146892614276614e-07,
      "loss": 1.0402,
      "step": 9848
    },
    {
      "epoch": 92.04715590922487,
      "grad_norm": 0.5571381449699402,
      "learning_rate": 3.3069650745979897e-07,
      "loss": 1.0584,
      "step": 9849
    },
    {
      "epoch": 92.05658709106984,
      "grad_norm": 0.5793171525001526,
      "learning_rate": 3.2992497467521535e-07,
      "loss": 1.0823,
      "step": 9850
    },
    {
      "epoch": 92.06601827291482,
      "grad_norm": 0.5588452219963074,
      "learning_rate": 3.291543278597042e-07,
      "loss": 1.0301,
      "step": 9851
    },
    {
      "epoch": 92.0754494547598,
      "grad_norm": 0.564410388469696,
      "learning_rate": 3.2838456708387254e-07,
      "loss": 1.0373,
      "step": 9852
    },
    {
      "epoch": 92.08488063660478,
      "grad_norm": 0.5266872048377991,
      "learning_rate": 3.2761569241824497e-07,
      "loss": 1.0214,
      "step": 9853
    },
    {
      "epoch": 92.09431181844975,
      "grad_norm": 0.5412012934684753,
      "learning_rate": 3.2684770393326405e-07,
      "loss": 1.0466,
      "step": 9854
    },
    {
      "epoch": 92.10374300029473,
      "grad_norm": 0.520271897315979,
      "learning_rate": 3.2608060169929566e-07,
      "loss": 1.0752,
      "step": 9855
    },
    {
      "epoch": 92.1131741821397,
      "grad_norm": 0.5548661351203918,
      "learning_rate": 3.2531438578661924e-07,
      "loss": 1.0493,
      "step": 9856
    },
    {
      "epoch": 92.12260536398468,
      "grad_norm": 0.5447483658790588,
      "learning_rate": 3.245490562654363e-07,
      "loss": 1.023,
      "step": 9857
    },
    {
      "epoch": 92.13203654582965,
      "grad_norm": 0.5824256539344788,
      "learning_rate": 3.237846132058653e-07,
      "loss": 1.0379,
      "step": 9858
    },
    {
      "epoch": 92.14146772767462,
      "grad_norm": 0.6114533543586731,
      "learning_rate": 3.230210566779457e-07,
      "loss": 1.0218,
      "step": 9859
    },
    {
      "epoch": 92.1508989095196,
      "grad_norm": 0.5408336520195007,
      "learning_rate": 3.222583867516327e-07,
      "loss": 1.0705,
      "step": 9860
    },
    {
      "epoch": 92.16033009136457,
      "grad_norm": 0.5108163952827454,
      "learning_rate": 3.2149660349680147e-07,
      "loss": 1.0199,
      "step": 9861
    },
    {
      "epoch": 92.16976127320955,
      "grad_norm": 0.5471197366714478,
      "learning_rate": 3.207357069832473e-07,
      "loss": 1.0378,
      "step": 9862
    },
    {
      "epoch": 92.17919245505452,
      "grad_norm": 0.5493059754371643,
      "learning_rate": 3.199756972806844e-07,
      "loss": 1.0448,
      "step": 9863
    },
    {
      "epoch": 92.1886236368995,
      "grad_norm": 0.5364664793014526,
      "learning_rate": 3.1921657445874387e-07,
      "loss": 1.1002,
      "step": 9864
    },
    {
      "epoch": 92.19805481874447,
      "grad_norm": 0.5491935014724731,
      "learning_rate": 3.184583385869744e-07,
      "loss": 1.0453,
      "step": 9865
    },
    {
      "epoch": 92.20748600058945,
      "grad_norm": 0.5647826790809631,
      "learning_rate": 3.177009897348482e-07,
      "loss": 1.0072,
      "step": 9866
    },
    {
      "epoch": 92.21691718243443,
      "grad_norm": 0.5619487762451172,
      "learning_rate": 3.169445279717509e-07,
      "loss": 1.0442,
      "step": 9867
    },
    {
      "epoch": 92.2263483642794,
      "grad_norm": 0.5612677931785583,
      "learning_rate": 3.161889533669915e-07,
      "loss": 1.0804,
      "step": 9868
    },
    {
      "epoch": 92.23577954612438,
      "grad_norm": 0.5439589619636536,
      "learning_rate": 3.1543426598979244e-07,
      "loss": 1.0318,
      "step": 9869
    },
    {
      "epoch": 92.24521072796935,
      "grad_norm": 0.5475404262542725,
      "learning_rate": 3.1468046590930056e-07,
      "loss": 1.0515,
      "step": 9870
    },
    {
      "epoch": 92.25464190981432,
      "grad_norm": 0.5922518968582153,
      "learning_rate": 3.1392755319457847e-07,
      "loss": 1.0867,
      "step": 9871
    },
    {
      "epoch": 92.2640730916593,
      "grad_norm": 0.552387535572052,
      "learning_rate": 3.1317552791460646e-07,
      "loss": 1.0623,
      "step": 9872
    },
    {
      "epoch": 92.27350427350427,
      "grad_norm": 0.5814201235771179,
      "learning_rate": 3.124243901382873e-07,
      "loss": 1.0623,
      "step": 9873
    },
    {
      "epoch": 92.28293545534925,
      "grad_norm": 0.5616997480392456,
      "learning_rate": 3.1167413993443697e-07,
      "loss": 1.0684,
      "step": 9874
    },
    {
      "epoch": 92.29236663719422,
      "grad_norm": 0.559973418712616,
      "learning_rate": 3.1092477737179495e-07,
      "loss": 1.048,
      "step": 9875
    },
    {
      "epoch": 92.3017978190392,
      "grad_norm": 0.5287859439849854,
      "learning_rate": 3.1017630251901634e-07,
      "loss": 1.0275,
      "step": 9876
    },
    {
      "epoch": 92.31122900088417,
      "grad_norm": 0.5535650849342346,
      "learning_rate": 3.0942871544467846e-07,
      "loss": 1.049,
      "step": 9877
    },
    {
      "epoch": 92.32066018272914,
      "grad_norm": 0.5477479696273804,
      "learning_rate": 3.086820162172732e-07,
      "loss": 0.9806,
      "step": 9878
    },
    {
      "epoch": 92.33009136457412,
      "grad_norm": 0.5846313834190369,
      "learning_rate": 3.079362049052126e-07,
      "loss": 0.9969,
      "step": 9879
    },
    {
      "epoch": 92.3395225464191,
      "grad_norm": 0.5513225793838501,
      "learning_rate": 3.071912815768285e-07,
      "loss": 1.0904,
      "step": 9880
    },
    {
      "epoch": 92.34895372826408,
      "grad_norm": 0.5461036562919617,
      "learning_rate": 3.064472463003709e-07,
      "loss": 1.0582,
      "step": 9881
    },
    {
      "epoch": 92.35838491010905,
      "grad_norm": 0.54606032371521,
      "learning_rate": 3.0570409914400744e-07,
      "loss": 1.0277,
      "step": 9882
    },
    {
      "epoch": 92.36781609195403,
      "grad_norm": 0.5977878570556641,
      "learning_rate": 3.0496184017582477e-07,
      "loss": 1.0308,
      "step": 9883
    },
    {
      "epoch": 92.377247273799,
      "grad_norm": 0.5456553101539612,
      "learning_rate": 3.042204694638273e-07,
      "loss": 1.0746,
      "step": 9884
    },
    {
      "epoch": 92.38667845564397,
      "grad_norm": 0.5808228850364685,
      "learning_rate": 3.03479987075942e-07,
      "loss": 1.0384,
      "step": 9885
    },
    {
      "epoch": 92.39610963748895,
      "grad_norm": 0.5409799218177795,
      "learning_rate": 3.027403930800099e-07,
      "loss": 1.0334,
      "step": 9886
    },
    {
      "epoch": 92.40554081933392,
      "grad_norm": 0.558114767074585,
      "learning_rate": 3.0200168754379257e-07,
      "loss": 1.0414,
      "step": 9887
    },
    {
      "epoch": 92.4149720011789,
      "grad_norm": 0.5525909662246704,
      "learning_rate": 3.012638705349713e-07,
      "loss": 1.0294,
      "step": 9888
    },
    {
      "epoch": 92.42440318302387,
      "grad_norm": 0.5854443907737732,
      "learning_rate": 3.00526942121141e-07,
      "loss": 1.0187,
      "step": 9889
    },
    {
      "epoch": 92.43383436486884,
      "grad_norm": 0.5563718676567078,
      "learning_rate": 2.99790902369822e-07,
      "loss": 1.027,
      "step": 9890
    },
    {
      "epoch": 92.44326554671382,
      "grad_norm": 0.5204187035560608,
      "learning_rate": 2.9905575134845043e-07,
      "loss": 1.0244,
      "step": 9891
    },
    {
      "epoch": 92.45269672855879,
      "grad_norm": 0.5579203963279724,
      "learning_rate": 2.98321489124378e-07,
      "loss": 1.02,
      "step": 9892
    },
    {
      "epoch": 92.46212791040377,
      "grad_norm": 0.5419732928276062,
      "learning_rate": 2.975881157648797e-07,
      "loss": 1.0494,
      "step": 9893
    },
    {
      "epoch": 92.47155909224875,
      "grad_norm": 0.6167798042297363,
      "learning_rate": 2.968556313371451e-07,
      "loss": 1.0965,
      "step": 9894
    },
    {
      "epoch": 92.48099027409373,
      "grad_norm": 0.5541282892227173,
      "learning_rate": 2.961240359082862e-07,
      "loss": 1.0422,
      "step": 9895
    },
    {
      "epoch": 92.4904214559387,
      "grad_norm": 0.5335834622383118,
      "learning_rate": 2.953933295453315e-07,
      "loss": 1.0639,
      "step": 9896
    },
    {
      "epoch": 92.49985263778368,
      "grad_norm": 0.5392501354217529,
      "learning_rate": 2.946635123152264e-07,
      "loss": 1.043,
      "step": 9897
    },
    {
      "epoch": 92.50928381962865,
      "grad_norm": 0.5064067840576172,
      "learning_rate": 2.939345842848373e-07,
      "loss": 1.0469,
      "step": 9898
    },
    {
      "epoch": 92.51871500147362,
      "grad_norm": 0.544729471206665,
      "learning_rate": 2.932065455209465e-07,
      "loss": 1.0656,
      "step": 9899
    },
    {
      "epoch": 92.5281461833186,
      "grad_norm": 0.5542130470275879,
      "learning_rate": 2.9247939609026053e-07,
      "loss": 1.0775,
      "step": 9900
    },
    {
      "epoch": 92.53757736516357,
      "grad_norm": 0.5786254405975342,
      "learning_rate": 2.917531360593984e-07,
      "loss": 1.0367,
      "step": 9901
    },
    {
      "epoch": 92.54700854700855,
      "grad_norm": 0.5861923098564148,
      "learning_rate": 2.910277654949001e-07,
      "loss": 1.0668,
      "step": 9902
    },
    {
      "epoch": 92.55643972885352,
      "grad_norm": 0.5238251686096191,
      "learning_rate": 2.903032844632225e-07,
      "loss": 1.0508,
      "step": 9903
    },
    {
      "epoch": 92.5658709106985,
      "grad_norm": 0.5361035466194153,
      "learning_rate": 2.8957969303074484e-07,
      "loss": 1.0274,
      "step": 9904
    },
    {
      "epoch": 92.57530209254347,
      "grad_norm": 0.5264760255813599,
      "learning_rate": 2.8885699126376066e-07,
      "loss": 1.0632,
      "step": 9905
    },
    {
      "epoch": 92.58473327438844,
      "grad_norm": 0.6517781615257263,
      "learning_rate": 2.8813517922848255e-07,
      "loss": 1.0865,
      "step": 9906
    },
    {
      "epoch": 92.59416445623341,
      "grad_norm": 0.6187139749526978,
      "learning_rate": 2.874142569910454e-07,
      "loss": 1.0228,
      "step": 9907
    },
    {
      "epoch": 92.6035956380784,
      "grad_norm": 0.5393403172492981,
      "learning_rate": 2.866942246174986e-07,
      "loss": 1.0176,
      "step": 9908
    },
    {
      "epoch": 92.61302681992338,
      "grad_norm": 0.5420875549316406,
      "learning_rate": 2.8597508217381166e-07,
      "loss": 1.0396,
      "step": 9909
    },
    {
      "epoch": 92.62245800176835,
      "grad_norm": 0.5852888226509094,
      "learning_rate": 2.8525682972587175e-07,
      "loss": 1.0838,
      "step": 9910
    },
    {
      "epoch": 92.63188918361332,
      "grad_norm": 0.5494358539581299,
      "learning_rate": 2.8453946733948525e-07,
      "loss": 1.0456,
      "step": 9911
    },
    {
      "epoch": 92.6413203654583,
      "grad_norm": 0.5231800079345703,
      "learning_rate": 2.8382299508037503e-07,
      "loss": 1.0544,
      "step": 9912
    },
    {
      "epoch": 92.65075154730327,
      "grad_norm": 0.5527707934379578,
      "learning_rate": 2.831074130141864e-07,
      "loss": 1.0784,
      "step": 9913
    },
    {
      "epoch": 92.66018272914825,
      "grad_norm": 0.5620348453521729,
      "learning_rate": 2.823927212064781e-07,
      "loss": 1.0426,
      "step": 9914
    },
    {
      "epoch": 92.66961391099322,
      "grad_norm": 0.58866947889328,
      "learning_rate": 2.816789197227332e-07,
      "loss": 1.0379,
      "step": 9915
    },
    {
      "epoch": 92.6790450928382,
      "grad_norm": 0.5072562098503113,
      "learning_rate": 2.8096600862834834e-07,
      "loss": 1.057,
      "step": 9916
    },
    {
      "epoch": 92.68847627468317,
      "grad_norm": 0.5387846827507019,
      "learning_rate": 2.802539879886401e-07,
      "loss": 1.0673,
      "step": 9917
    },
    {
      "epoch": 92.69790745652814,
      "grad_norm": 0.5695700645446777,
      "learning_rate": 2.7954285786884417e-07,
      "loss": 1.0508,
      "step": 9918
    },
    {
      "epoch": 92.70733863837312,
      "grad_norm": 0.5284677743911743,
      "learning_rate": 2.788326183341128e-07,
      "loss": 1.046,
      "step": 9919
    },
    {
      "epoch": 92.71676982021809,
      "grad_norm": 0.5594667196273804,
      "learning_rate": 2.781232694495195e-07,
      "loss": 1.0493,
      "step": 9920
    },
    {
      "epoch": 92.72620100206306,
      "grad_norm": 0.5599549412727356,
      "learning_rate": 2.774148112800523e-07,
      "loss": 1.104,
      "step": 9921
    },
    {
      "epoch": 92.73563218390805,
      "grad_norm": 0.519879162311554,
      "learning_rate": 2.767072438906215e-07,
      "loss": 1.0733,
      "step": 9922
    },
    {
      "epoch": 92.74506336575303,
      "grad_norm": 0.5472522974014282,
      "learning_rate": 2.760005673460553e-07,
      "loss": 1.0464,
      "step": 9923
    },
    {
      "epoch": 92.754494547598,
      "grad_norm": 0.5580350756645203,
      "learning_rate": 2.7529478171109735e-07,
      "loss": 1.0192,
      "step": 9924
    },
    {
      "epoch": 92.76392572944297,
      "grad_norm": 0.5100231170654297,
      "learning_rate": 2.745898870504116e-07,
      "loss": 1.0481,
      "step": 9925
    },
    {
      "epoch": 92.77335691128795,
      "grad_norm": 0.5653419494628906,
      "learning_rate": 2.738858834285818e-07,
      "loss": 1.0262,
      "step": 9926
    },
    {
      "epoch": 92.78278809313292,
      "grad_norm": 0.5546894669532776,
      "learning_rate": 2.7318277091010647e-07,
      "loss": 1.0506,
      "step": 9927
    },
    {
      "epoch": 92.7922192749779,
      "grad_norm": 0.504779577255249,
      "learning_rate": 2.724805495594063e-07,
      "loss": 1.0597,
      "step": 9928
    },
    {
      "epoch": 92.80165045682287,
      "grad_norm": 0.5302361845970154,
      "learning_rate": 2.717792194408164e-07,
      "loss": 1.0635,
      "step": 9929
    },
    {
      "epoch": 92.81108163866784,
      "grad_norm": 0.5732266306877136,
      "learning_rate": 2.710787806185955e-07,
      "loss": 1.0348,
      "step": 9930
    },
    {
      "epoch": 92.82051282051282,
      "grad_norm": 0.587661623954773,
      "learning_rate": 2.703792331569155e-07,
      "loss": 1.0693,
      "step": 9931
    },
    {
      "epoch": 92.82994400235779,
      "grad_norm": 0.537749171257019,
      "learning_rate": 2.696805771198685e-07,
      "loss": 1.0554,
      "step": 9932
    },
    {
      "epoch": 92.83937518420277,
      "grad_norm": 0.5440406203269958,
      "learning_rate": 2.6898281257146666e-07,
      "loss": 1.0681,
      "step": 9933
    },
    {
      "epoch": 92.84880636604774,
      "grad_norm": 0.5203079581260681,
      "learning_rate": 2.682859395756376e-07,
      "loss": 1.0398,
      "step": 9934
    },
    {
      "epoch": 92.85823754789271,
      "grad_norm": 0.5592597723007202,
      "learning_rate": 2.6758995819622824e-07,
      "loss": 1.0761,
      "step": 9935
    },
    {
      "epoch": 92.8676687297377,
      "grad_norm": 0.5479455590248108,
      "learning_rate": 2.6689486849700516e-07,
      "loss": 1.0253,
      "step": 9936
    },
    {
      "epoch": 92.87709991158268,
      "grad_norm": 0.5689195394515991,
      "learning_rate": 2.66200670541652e-07,
      "loss": 1.0356,
      "step": 9937
    },
    {
      "epoch": 92.88653109342765,
      "grad_norm": 0.528074324131012,
      "learning_rate": 2.6550736439377113e-07,
      "loss": 1.0356,
      "step": 9938
    },
    {
      "epoch": 92.89596227527262,
      "grad_norm": 0.5327884554862976,
      "learning_rate": 2.6481495011688394e-07,
      "loss": 1.051,
      "step": 9939
    },
    {
      "epoch": 92.9053934571176,
      "grad_norm": 0.5328812599182129,
      "learning_rate": 2.6412342777442645e-07,
      "loss": 1.0235,
      "step": 9940
    },
    {
      "epoch": 92.91482463896257,
      "grad_norm": 0.5791196823120117,
      "learning_rate": 2.63432797429759e-07,
      "loss": 1.0344,
      "step": 9941
    },
    {
      "epoch": 92.92425582080755,
      "grad_norm": 0.5473344326019287,
      "learning_rate": 2.627430591461544e-07,
      "loss": 1.0095,
      "step": 9942
    },
    {
      "epoch": 92.93368700265252,
      "grad_norm": 0.5386718511581421,
      "learning_rate": 2.6205421298680754e-07,
      "loss": 1.0647,
      "step": 9943
    },
    {
      "epoch": 92.9431181844975,
      "grad_norm": 0.5742233991622925,
      "learning_rate": 2.6136625901482805e-07,
      "loss": 1.0526,
      "step": 9944
    },
    {
      "epoch": 92.95254936634247,
      "grad_norm": 0.5446848273277283,
      "learning_rate": 2.606791972932499e-07,
      "loss": 1.0606,
      "step": 9945
    },
    {
      "epoch": 92.96198054818744,
      "grad_norm": 0.5201073288917542,
      "learning_rate": 2.5999302788501826e-07,
      "loss": 1.071,
      "step": 9946
    },
    {
      "epoch": 92.97141173003241,
      "grad_norm": 0.5589050650596619,
      "learning_rate": 2.593077508530017e-07,
      "loss": 1.004,
      "step": 9947
    },
    {
      "epoch": 92.98084291187739,
      "grad_norm": 0.5473692417144775,
      "learning_rate": 2.5862336625998464e-07,
      "loss": 1.0356,
      "step": 9948
    },
    {
      "epoch": 92.99027409372236,
      "grad_norm": 0.5789026021957397,
      "learning_rate": 2.5793987416866894e-07,
      "loss": 1.0789,
      "step": 9949
    },
    {
      "epoch": 92.99970527556735,
      "grad_norm": 0.6026699542999268,
      "learning_rate": 2.572572746416768e-07,
      "loss": 1.0038,
      "step": 9950
    },
    {
      "epoch": 93.0,
      "grad_norm": 3.951387405395508,
      "learning_rate": 2.5657556774154823e-07,
      "loss": 0.6394,
      "step": 9951
    },
    {
      "epoch": 93.00943118184497,
      "grad_norm": 0.5664046406745911,
      "learning_rate": 2.5589475353073987e-07,
      "loss": 1.0893,
      "step": 9952
    },
    {
      "epoch": 93.01886236368995,
      "grad_norm": 0.5461865067481995,
      "learning_rate": 2.5521483207162857e-07,
      "loss": 1.0385,
      "step": 9953
    },
    {
      "epoch": 93.02829354553492,
      "grad_norm": 0.5170465111732483,
      "learning_rate": 2.5453580342650885e-07,
      "loss": 1.0222,
      "step": 9954
    },
    {
      "epoch": 93.0377247273799,
      "grad_norm": 0.6092643737792969,
      "learning_rate": 2.53857667657591e-07,
      "loss": 1.0497,
      "step": 9955
    },
    {
      "epoch": 93.04715590922487,
      "grad_norm": 0.5336594581604004,
      "learning_rate": 2.5318042482700866e-07,
      "loss": 1.0336,
      "step": 9956
    },
    {
      "epoch": 93.05658709106984,
      "grad_norm": 0.5656446814537048,
      "learning_rate": 2.5250407499680883e-07,
      "loss": 1.0105,
      "step": 9957
    },
    {
      "epoch": 93.06601827291482,
      "grad_norm": 0.5787628293037415,
      "learning_rate": 2.5182861822895863e-07,
      "loss": 1.0704,
      "step": 9958
    },
    {
      "epoch": 93.0754494547598,
      "grad_norm": 0.5674961805343628,
      "learning_rate": 2.5115405458534303e-07,
      "loss": 1.029,
      "step": 9959
    },
    {
      "epoch": 93.08488063660478,
      "grad_norm": 0.5210863947868347,
      "learning_rate": 2.504803841277648e-07,
      "loss": 1.0442,
      "step": 9960
    },
    {
      "epoch": 93.09431181844975,
      "grad_norm": 0.5174409747123718,
      "learning_rate": 2.498076069179467e-07,
      "loss": 1.0146,
      "step": 9961
    },
    {
      "epoch": 93.10374300029473,
      "grad_norm": 0.5837748646736145,
      "learning_rate": 2.491357230175284e-07,
      "loss": 1.0274,
      "step": 9962
    },
    {
      "epoch": 93.1131741821397,
      "grad_norm": 0.5227023959159851,
      "learning_rate": 2.484647324880674e-07,
      "loss": 1.0762,
      "step": 9963
    },
    {
      "epoch": 93.12260536398468,
      "grad_norm": 0.5646100640296936,
      "learning_rate": 2.4779463539103763e-07,
      "loss": 1.0534,
      "step": 9964
    },
    {
      "epoch": 93.13203654582965,
      "grad_norm": 0.5746174454689026,
      "learning_rate": 2.471254317878369e-07,
      "loss": 1.0539,
      "step": 9965
    },
    {
      "epoch": 93.14146772767462,
      "grad_norm": 0.5632798671722412,
      "learning_rate": 2.4645712173977267e-07,
      "loss": 1.0921,
      "step": 9966
    },
    {
      "epoch": 93.1508989095196,
      "grad_norm": 0.5367133021354675,
      "learning_rate": 2.4578970530807935e-07,
      "loss": 1.0727,
      "step": 9967
    },
    {
      "epoch": 93.16033009136457,
      "grad_norm": 0.558184027671814,
      "learning_rate": 2.4512318255390466e-07,
      "loss": 1.0543,
      "step": 9968
    },
    {
      "epoch": 93.16976127320955,
      "grad_norm": 0.5569012761116028,
      "learning_rate": 2.4445755353831424e-07,
      "loss": 1.0369,
      "step": 9969
    },
    {
      "epoch": 93.17919245505452,
      "grad_norm": 0.5598248243331909,
      "learning_rate": 2.4379281832229264e-07,
      "loss": 1.0573,
      "step": 9970
    },
    {
      "epoch": 93.1886236368995,
      "grad_norm": 0.5520763993263245,
      "learning_rate": 2.431289769667444e-07,
      "loss": 1.0094,
      "step": 9971
    },
    {
      "epoch": 93.19805481874447,
      "grad_norm": 0.5397300124168396,
      "learning_rate": 2.4246602953248765e-07,
      "loss": 1.0476,
      "step": 9972
    },
    {
      "epoch": 93.20748600058945,
      "grad_norm": 0.5539974570274353,
      "learning_rate": 2.4180397608026265e-07,
      "loss": 1.05,
      "step": 9973
    },
    {
      "epoch": 93.21691718243443,
      "grad_norm": 0.5472131967544556,
      "learning_rate": 2.411428166707286e-07,
      "loss": 1.075,
      "step": 9974
    },
    {
      "epoch": 93.2263483642794,
      "grad_norm": 0.5500419735908508,
      "learning_rate": 2.404825513644582e-07,
      "loss": 1.034,
      "step": 9975
    },
    {
      "epoch": 93.23577954612438,
      "grad_norm": 0.5432636737823486,
      "learning_rate": 2.3982318022194527e-07,
      "loss": 1.0419,
      "step": 9976
    },
    {
      "epoch": 93.24521072796935,
      "grad_norm": 0.5478053689002991,
      "learning_rate": 2.3916470330360154e-07,
      "loss": 1.0473,
      "step": 9977
    },
    {
      "epoch": 93.25464190981432,
      "grad_norm": 0.5234277248382568,
      "learning_rate": 2.3850712066975643e-07,
      "loss": 1.0552,
      "step": 9978
    },
    {
      "epoch": 93.2640730916593,
      "grad_norm": 0.5383965969085693,
      "learning_rate": 2.378504323806574e-07,
      "loss": 1.0464,
      "step": 9979
    },
    {
      "epoch": 93.27350427350427,
      "grad_norm": 0.5196697115898132,
      "learning_rate": 2.3719463849646963e-07,
      "loss": 1.0523,
      "step": 9980
    },
    {
      "epoch": 93.28293545534925,
      "grad_norm": 0.5610857605934143,
      "learning_rate": 2.3653973907727612e-07,
      "loss": 1.0557,
      "step": 9981
    },
    {
      "epoch": 93.29236663719422,
      "grad_norm": 0.5572781562805176,
      "learning_rate": 2.3588573418308113e-07,
      "loss": 1.0559,
      "step": 9982
    },
    {
      "epoch": 93.3017978190392,
      "grad_norm": 0.5363330245018005,
      "learning_rate": 2.352326238738023e-07,
      "loss": 1.0492,
      "step": 9983
    },
    {
      "epoch": 93.31122900088417,
      "grad_norm": 0.6081063747406006,
      "learning_rate": 2.345804082092773e-07,
      "loss": 1.0732,
      "step": 9984
    },
    {
      "epoch": 93.32066018272914,
      "grad_norm": 0.5925558805465698,
      "learning_rate": 2.3392908724926277e-07,
      "loss": 1.0556,
      "step": 9985
    },
    {
      "epoch": 93.33009136457412,
      "grad_norm": 0.5658618211746216,
      "learning_rate": 2.332786610534321e-07,
      "loss": 1.0538,
      "step": 9986
    },
    {
      "epoch": 93.3395225464191,
      "grad_norm": 0.5822260975837708,
      "learning_rate": 2.3262912968137652e-07,
      "loss": 1.0352,
      "step": 9987
    },
    {
      "epoch": 93.34895372826408,
      "grad_norm": 0.5079936385154724,
      "learning_rate": 2.319804931926062e-07,
      "loss": 1.0725,
      "step": 9988
    },
    {
      "epoch": 93.35838491010905,
      "grad_norm": 0.6008305549621582,
      "learning_rate": 2.3133275164655022e-07,
      "loss": 1.0233,
      "step": 9989
    },
    {
      "epoch": 93.36781609195403,
      "grad_norm": 0.5342096090316772,
      "learning_rate": 2.306859051025534e-07,
      "loss": 1.0321,
      "step": 9990
    },
    {
      "epoch": 93.377247273799,
      "grad_norm": 0.6175242066383362,
      "learning_rate": 2.3003995361988052e-07,
      "loss": 1.0124,
      "step": 9991
    },
    {
      "epoch": 93.38667845564397,
      "grad_norm": 0.5345702767372131,
      "learning_rate": 2.2939489725771202e-07,
      "loss": 1.0405,
      "step": 9992
    },
    {
      "epoch": 93.39610963748895,
      "grad_norm": 0.5397339463233948,
      "learning_rate": 2.2875073607514842e-07,
      "loss": 1.0298,
      "step": 9993
    },
    {
      "epoch": 93.40554081933392,
      "grad_norm": 0.5275248885154724,
      "learning_rate": 2.2810747013120694e-07,
      "loss": 1.0929,
      "step": 9994
    },
    {
      "epoch": 93.4149720011789,
      "grad_norm": 0.5603357553482056,
      "learning_rate": 2.2746509948482487e-07,
      "loss": 1.0973,
      "step": 9995
    },
    {
      "epoch": 93.42440318302387,
      "grad_norm": 0.5262017250061035,
      "learning_rate": 2.2682362419485403e-07,
      "loss": 1.0296,
      "step": 9996
    },
    {
      "epoch": 93.43383436486884,
      "grad_norm": 0.574286162853241,
      "learning_rate": 2.2618304432006853e-07,
      "loss": 1.002,
      "step": 9997
    },
    {
      "epoch": 93.44326554671382,
      "grad_norm": 0.5386826992034912,
      "learning_rate": 2.2554335991915587e-07,
      "loss": 1.0242,
      "step": 9998
    },
    {
      "epoch": 93.45269672855879,
      "grad_norm": 0.5636033415794373,
      "learning_rate": 2.2490457105072583e-07,
      "loss": 1.0773,
      "step": 9999
    },
    {
      "epoch": 93.46212791040377,
      "grad_norm": 0.5987045764923096,
      "learning_rate": 2.2426667777330159e-07,
      "loss": 1.0131,
      "step": 10000
    },
    {
      "epoch": 93.47155909224875,
      "grad_norm": 0.5523784160614014,
      "learning_rate": 2.236296801453286e-07,
      "loss": 1.0746,
      "step": 10001
    },
    {
      "epoch": 93.48099027409373,
      "grad_norm": 0.5177783370018005,
      "learning_rate": 2.2299357822516798e-07,
      "loss": 1.0317,
      "step": 10002
    },
    {
      "epoch": 93.4904214559387,
      "grad_norm": 0.4970565140247345,
      "learning_rate": 2.2235837207109757e-07,
      "loss": 1.0446,
      "step": 10003
    },
    {
      "epoch": 93.49985263778368,
      "grad_norm": 0.5189496874809265,
      "learning_rate": 2.2172406174131746e-07,
      "loss": 1.0602,
      "step": 10004
    },
    {
      "epoch": 93.50928381962865,
      "grad_norm": 0.5196764469146729,
      "learning_rate": 2.2109064729394224e-07,
      "loss": 1.0674,
      "step": 10005
    },
    {
      "epoch": 93.51871500147362,
      "grad_norm": 0.6255548000335693,
      "learning_rate": 2.2045812878700334e-07,
      "loss": 1.0072,
      "step": 10006
    },
    {
      "epoch": 93.5281461833186,
      "grad_norm": 0.5248503684997559,
      "learning_rate": 2.1982650627845325e-07,
      "loss": 1.0429,
      "step": 10007
    },
    {
      "epoch": 93.53757736516357,
      "grad_norm": 0.5358521342277527,
      "learning_rate": 2.1919577982616125e-07,
      "loss": 1.0267,
      "step": 10008
    },
    {
      "epoch": 93.54700854700855,
      "grad_norm": 0.541934609413147,
      "learning_rate": 2.1856594948791443e-07,
      "loss": 1.0403,
      "step": 10009
    },
    {
      "epoch": 93.55643972885352,
      "grad_norm": 0.5453541278839111,
      "learning_rate": 2.1793701532141663e-07,
      "loss": 1.0738,
      "step": 10010
    },
    {
      "epoch": 93.5658709106985,
      "grad_norm": 0.5623046159744263,
      "learning_rate": 2.1730897738428958e-07,
      "loss": 1.0783,
      "step": 10011
    },
    {
      "epoch": 93.57530209254347,
      "grad_norm": 0.5079289078712463,
      "learning_rate": 2.166818357340772e-07,
      "loss": 1.0484,
      "step": 10012
    },
    {
      "epoch": 93.58473327438844,
      "grad_norm": 0.5749897360801697,
      "learning_rate": 2.1605559042823575e-07,
      "loss": 1.0525,
      "step": 10013
    },
    {
      "epoch": 93.59416445623341,
      "grad_norm": 0.58468097448349,
      "learning_rate": 2.154302415241416e-07,
      "loss": 1.0328,
      "step": 10014
    },
    {
      "epoch": 93.6035956380784,
      "grad_norm": 0.5467545390129089,
      "learning_rate": 2.1480578907909e-07,
      "loss": 1.0452,
      "step": 10015
    },
    {
      "epoch": 93.61302681992338,
      "grad_norm": 0.5038185119628906,
      "learning_rate": 2.1418223315029295e-07,
      "loss": 1.0805,
      "step": 10016
    },
    {
      "epoch": 93.62245800176835,
      "grad_norm": 0.5486190915107727,
      "learning_rate": 2.1355957379487925e-07,
      "loss": 1.0433,
      "step": 10017
    },
    {
      "epoch": 93.63188918361332,
      "grad_norm": 0.5511045455932617,
      "learning_rate": 2.1293781106989764e-07,
      "loss": 1.0847,
      "step": 10018
    },
    {
      "epoch": 93.6413203654583,
      "grad_norm": 0.5632022619247437,
      "learning_rate": 2.123169450323137e-07,
      "loss": 1.0498,
      "step": 10019
    },
    {
      "epoch": 93.65075154730327,
      "grad_norm": 0.537287175655365,
      "learning_rate": 2.1169697573901082e-07,
      "loss": 1.0257,
      "step": 10020
    },
    {
      "epoch": 93.66018272914825,
      "grad_norm": 0.5344229936599731,
      "learning_rate": 2.1107790324679022e-07,
      "loss": 1.0366,
      "step": 10021
    },
    {
      "epoch": 93.66961391099322,
      "grad_norm": 0.5630585551261902,
      "learning_rate": 2.104597276123721e-07,
      "loss": 1.0063,
      "step": 10022
    },
    {
      "epoch": 93.6790450928382,
      "grad_norm": 0.5129135251045227,
      "learning_rate": 2.0984244889239335e-07,
      "loss": 1.0811,
      "step": 10023
    },
    {
      "epoch": 93.68847627468317,
      "grad_norm": 0.557820737361908,
      "learning_rate": 2.0922606714340766e-07,
      "loss": 1.0544,
      "step": 10024
    },
    {
      "epoch": 93.69790745652814,
      "grad_norm": 0.5684546828269958,
      "learning_rate": 2.086105824218887e-07,
      "loss": 1.0507,
      "step": 10025
    },
    {
      "epoch": 93.70733863837312,
      "grad_norm": 0.5728273987770081,
      "learning_rate": 2.0799599478422583e-07,
      "loss": 1.054,
      "step": 10026
    },
    {
      "epoch": 93.71676982021809,
      "grad_norm": 0.5394273400306702,
      "learning_rate": 2.0738230428672845e-07,
      "loss": 1.0368,
      "step": 10027
    },
    {
      "epoch": 93.72620100206306,
      "grad_norm": 0.5511100888252258,
      "learning_rate": 2.067695109856227e-07,
      "loss": 1.0456,
      "step": 10028
    },
    {
      "epoch": 93.73563218390805,
      "grad_norm": 0.5330634117126465,
      "learning_rate": 2.0615761493705255e-07,
      "loss": 1.076,
      "step": 10029
    },
    {
      "epoch": 93.74506336575303,
      "grad_norm": 0.5117498636245728,
      "learning_rate": 2.0554661619707983e-07,
      "loss": 1.049,
      "step": 10030
    },
    {
      "epoch": 93.754494547598,
      "grad_norm": 0.6276636123657227,
      "learning_rate": 2.049365148216831e-07,
      "loss": 0.9957,
      "step": 10031
    },
    {
      "epoch": 93.76392572944297,
      "grad_norm": 0.5593613386154175,
      "learning_rate": 2.0432731086675983e-07,
      "loss": 1.0587,
      "step": 10032
    },
    {
      "epoch": 93.77335691128795,
      "grad_norm": 0.5325050354003906,
      "learning_rate": 2.037190043881254e-07,
      "loss": 1.0234,
      "step": 10033
    },
    {
      "epoch": 93.78278809313292,
      "grad_norm": 0.5285716652870178,
      "learning_rate": 2.0311159544151305e-07,
      "loss": 1.0467,
      "step": 10034
    },
    {
      "epoch": 93.7922192749779,
      "grad_norm": 0.5850508809089661,
      "learning_rate": 2.0250508408257262e-07,
      "loss": 1.0835,
      "step": 10035
    },
    {
      "epoch": 93.80165045682287,
      "grad_norm": 0.5927242040634155,
      "learning_rate": 2.0189947036687306e-07,
      "loss": 1.042,
      "step": 10036
    },
    {
      "epoch": 93.81108163866784,
      "grad_norm": 0.5526715517044067,
      "learning_rate": 2.0129475434989998e-07,
      "loss": 1.0886,
      "step": 10037
    },
    {
      "epoch": 93.82051282051282,
      "grad_norm": 0.5194109678268433,
      "learning_rate": 2.0069093608705793e-07,
      "loss": 1.0567,
      "step": 10038
    },
    {
      "epoch": 93.82994400235779,
      "grad_norm": 0.544014573097229,
      "learning_rate": 2.000880156336682e-07,
      "loss": 1.0135,
      "step": 10039
    },
    {
      "epoch": 93.83937518420277,
      "grad_norm": 0.5483435392379761,
      "learning_rate": 1.9948599304497106e-07,
      "loss": 1.0614,
      "step": 10040
    },
    {
      "epoch": 93.84880636604774,
      "grad_norm": 0.589948296546936,
      "learning_rate": 1.988848683761202e-07,
      "loss": 1.05,
      "step": 10041
    },
    {
      "epoch": 93.85823754789271,
      "grad_norm": 0.5159777402877808,
      "learning_rate": 1.9828464168219486e-07,
      "loss": 1.0589,
      "step": 10042
    },
    {
      "epoch": 93.8676687297377,
      "grad_norm": 0.5323674082756042,
      "learning_rate": 1.976853130181855e-07,
      "loss": 1.0701,
      "step": 10043
    },
    {
      "epoch": 93.87709991158268,
      "grad_norm": 0.5479647517204285,
      "learning_rate": 1.9708688243900264e-07,
      "loss": 1.0401,
      "step": 10044
    },
    {
      "epoch": 93.88653109342765,
      "grad_norm": 0.5208614468574524,
      "learning_rate": 1.9648934999947466e-07,
      "loss": 1.0682,
      "step": 10045
    },
    {
      "epoch": 93.89596227527262,
      "grad_norm": 0.5899438261985779,
      "learning_rate": 1.9589271575434554e-07,
      "loss": 1.0246,
      "step": 10046
    },
    {
      "epoch": 93.9053934571176,
      "grad_norm": 0.5385288000106812,
      "learning_rate": 1.9529697975828156e-07,
      "loss": 1.0739,
      "step": 10047
    },
    {
      "epoch": 93.91482463896257,
      "grad_norm": 0.5460958480834961,
      "learning_rate": 1.9470214206586124e-07,
      "loss": 1.0612,
      "step": 10048
    },
    {
      "epoch": 93.92425582080755,
      "grad_norm": 0.627595067024231,
      "learning_rate": 1.9410820273158437e-07,
      "loss": 1.0846,
      "step": 10049
    },
    {
      "epoch": 93.93368700265252,
      "grad_norm": 0.5278986692428589,
      "learning_rate": 1.935151618098685e-07,
      "loss": 1.0595,
      "step": 10050
    },
    {
      "epoch": 93.9431181844975,
      "grad_norm": 0.57379150390625,
      "learning_rate": 1.9292301935504686e-07,
      "loss": 1.0263,
      "step": 10051
    },
    {
      "epoch": 93.95254936634247,
      "grad_norm": 0.5297086238861084,
      "learning_rate": 1.923317754213716e-07,
      "loss": 1.0225,
      "step": 10052
    },
    {
      "epoch": 93.96198054818744,
      "grad_norm": 0.51262366771698,
      "learning_rate": 1.917414300630127e-07,
      "loss": 1.0394,
      "step": 10053
    },
    {
      "epoch": 93.97141173003241,
      "grad_norm": 0.6140480637550354,
      "learning_rate": 1.9115198333405583e-07,
      "loss": 0.9677,
      "step": 10054
    },
    {
      "epoch": 93.98084291187739,
      "grad_norm": 0.5200234651565552,
      "learning_rate": 1.9056343528850773e-07,
      "loss": 1.0471,
      "step": 10055
    },
    {
      "epoch": 93.99027409372236,
      "grad_norm": 0.576202392578125,
      "learning_rate": 1.8997578598028864e-07,
      "loss": 1.049,
      "step": 10056
    },
    {
      "epoch": 93.99970527556735,
      "grad_norm": 0.5843793749809265,
      "learning_rate": 1.8938903546324216e-07,
      "loss": 1.0776,
      "step": 10057
    },
    {
      "epoch": 94.0,
      "grad_norm": 2.926607608795166,
      "learning_rate": 1.8880318379112417e-07,
      "loss": 0.4639,
      "step": 10058
    },
    {
      "epoch": 94.00943118184497,
      "grad_norm": 0.5805684328079224,
      "learning_rate": 1.8821823101760949e-07,
      "loss": 1.0482,
      "step": 10059
    },
    {
      "epoch": 94.01886236368995,
      "grad_norm": 0.5656353831291199,
      "learning_rate": 1.8763417719629306e-07,
      "loss": 1.0426,
      "step": 10060
    },
    {
      "epoch": 94.02829354553492,
      "grad_norm": 0.5429532527923584,
      "learning_rate": 1.870510223806843e-07,
      "loss": 1.029,
      "step": 10061
    },
    {
      "epoch": 94.0377247273799,
      "grad_norm": 0.6129454374313354,
      "learning_rate": 1.8646876662421264e-07,
      "loss": 1.0676,
      "step": 10062
    },
    {
      "epoch": 94.04715590922487,
      "grad_norm": 0.5609480142593384,
      "learning_rate": 1.8588740998022324e-07,
      "loss": 1.0236,
      "step": 10063
    },
    {
      "epoch": 94.05658709106984,
      "grad_norm": 0.5447580814361572,
      "learning_rate": 1.8530695250198017e-07,
      "loss": 1.0569,
      "step": 10064
    },
    {
      "epoch": 94.06601827291482,
      "grad_norm": 0.5300421118736267,
      "learning_rate": 1.847273942426664e-07,
      "loss": 1.0409,
      "step": 10065
    },
    {
      "epoch": 94.0754494547598,
      "grad_norm": 0.5747570395469666,
      "learning_rate": 1.8414873525537835e-07,
      "loss": 1.0761,
      "step": 10066
    },
    {
      "epoch": 94.08488063660478,
      "grad_norm": 0.6112523078918457,
      "learning_rate": 1.8357097559313364e-07,
      "loss": 1.0914,
      "step": 10067
    },
    {
      "epoch": 94.09431181844975,
      "grad_norm": 0.5686194896697998,
      "learning_rate": 1.8299411530886545e-07,
      "loss": 1.087,
      "step": 10068
    },
    {
      "epoch": 94.10374300029473,
      "grad_norm": 0.51290363073349,
      "learning_rate": 1.8241815445542822e-07,
      "loss": 1.0667,
      "step": 10069
    },
    {
      "epoch": 94.1131741821397,
      "grad_norm": 0.5593370795249939,
      "learning_rate": 1.8184309308558857e-07,
      "loss": 1.039,
      "step": 10070
    },
    {
      "epoch": 94.12260536398468,
      "grad_norm": 0.5279702544212341,
      "learning_rate": 1.8126893125203327e-07,
      "loss": 1.0364,
      "step": 10071
    },
    {
      "epoch": 94.13203654582965,
      "grad_norm": 0.5348281264305115,
      "learning_rate": 1.8069566900736912e-07,
      "loss": 1.0466,
      "step": 10072
    },
    {
      "epoch": 94.14146772767462,
      "grad_norm": 0.5231902003288269,
      "learning_rate": 1.8012330640411635e-07,
      "loss": 1.0634,
      "step": 10073
    },
    {
      "epoch": 94.1508989095196,
      "grad_norm": 0.532660961151123,
      "learning_rate": 1.7955184349471632e-07,
      "loss": 1.0222,
      "step": 10074
    },
    {
      "epoch": 94.16033009136457,
      "grad_norm": 0.5305367112159729,
      "learning_rate": 1.7898128033152495e-07,
      "loss": 1.0381,
      "step": 10075
    },
    {
      "epoch": 94.16976127320955,
      "grad_norm": 0.5554373860359192,
      "learning_rate": 1.7841161696681707e-07,
      "loss": 1.0704,
      "step": 10076
    },
    {
      "epoch": 94.17919245505452,
      "grad_norm": 0.5702043771743774,
      "learning_rate": 1.7784285345278542e-07,
      "loss": 1.0717,
      "step": 10077
    },
    {
      "epoch": 94.1886236368995,
      "grad_norm": 0.5146891474723816,
      "learning_rate": 1.7727498984153935e-07,
      "loss": 1.0764,
      "step": 10078
    },
    {
      "epoch": 94.19805481874447,
      "grad_norm": 0.5266506671905518,
      "learning_rate": 1.7670802618510728e-07,
      "loss": 1.0338,
      "step": 10079
    },
    {
      "epoch": 94.20748600058945,
      "grad_norm": 0.5825799703598022,
      "learning_rate": 1.7614196253543325e-07,
      "loss": 1.0291,
      "step": 10080
    },
    {
      "epoch": 94.21691718243443,
      "grad_norm": 0.48134565353393555,
      "learning_rate": 1.755767989443813e-07,
      "loss": 1.0332,
      "step": 10081
    },
    {
      "epoch": 94.2263483642794,
      "grad_norm": 0.5657888054847717,
      "learning_rate": 1.7501253546372999e-07,
      "loss": 1.0426,
      "step": 10082
    },
    {
      "epoch": 94.23577954612438,
      "grad_norm": 0.5592212677001953,
      "learning_rate": 1.7444917214517797e-07,
      "loss": 1.0822,
      "step": 10083
    },
    {
      "epoch": 94.24521072796935,
      "grad_norm": 0.5758647322654724,
      "learning_rate": 1.7388670904033955e-07,
      "loss": 1.0788,
      "step": 10084
    },
    {
      "epoch": 94.25464190981432,
      "grad_norm": 0.5762472748756409,
      "learning_rate": 1.7332514620074792e-07,
      "loss": 1.0726,
      "step": 10085
    },
    {
      "epoch": 94.2640730916593,
      "grad_norm": 0.5611564517021179,
      "learning_rate": 1.7276448367785302e-07,
      "loss": 1.0805,
      "step": 10086
    },
    {
      "epoch": 94.27350427350427,
      "grad_norm": 0.5343313217163086,
      "learning_rate": 1.722047215230238e-07,
      "loss": 1.03,
      "step": 10087
    },
    {
      "epoch": 94.28293545534925,
      "grad_norm": 0.5994386672973633,
      "learning_rate": 1.7164585978754256e-07,
      "loss": 1.0953,
      "step": 10088
    },
    {
      "epoch": 94.29236663719422,
      "grad_norm": 0.5710421204566956,
      "learning_rate": 1.71087898522615e-07,
      "loss": 1.0274,
      "step": 10089
    },
    {
      "epoch": 94.3017978190392,
      "grad_norm": 0.5828110575675964,
      "learning_rate": 1.705308377793602e-07,
      "loss": 1.0123,
      "step": 10090
    },
    {
      "epoch": 94.31122900088417,
      "grad_norm": 0.575401246547699,
      "learning_rate": 1.6997467760881626e-07,
      "loss": 1.0638,
      "step": 10091
    },
    {
      "epoch": 94.32066018272914,
      "grad_norm": 0.562066912651062,
      "learning_rate": 1.6941941806193796e-07,
      "loss": 1.0639,
      "step": 10092
    },
    {
      "epoch": 94.33009136457412,
      "grad_norm": 0.5522321462631226,
      "learning_rate": 1.6886505918959795e-07,
      "loss": 0.9998,
      "step": 10093
    },
    {
      "epoch": 94.3395225464191,
      "grad_norm": 0.5363516211509705,
      "learning_rate": 1.683116010425856e-07,
      "loss": 1.0468,
      "step": 10094
    },
    {
      "epoch": 94.34895372826408,
      "grad_norm": 0.5343202948570251,
      "learning_rate": 1.6775904367161146e-07,
      "loss": 1.0313,
      "step": 10095
    },
    {
      "epoch": 94.35838491010905,
      "grad_norm": 0.5597374439239502,
      "learning_rate": 1.6720738712729722e-07,
      "loss": 1.0676,
      "step": 10096
    },
    {
      "epoch": 94.36781609195403,
      "grad_norm": 0.5939554572105408,
      "learning_rate": 1.6665663146018807e-07,
      "loss": 1.0503,
      "step": 10097
    },
    {
      "epoch": 94.377247273799,
      "grad_norm": 0.542894184589386,
      "learning_rate": 1.6610677672074248e-07,
      "loss": 1.0686,
      "step": 10098
    },
    {
      "epoch": 94.38667845564397,
      "grad_norm": 0.5509928464889526,
      "learning_rate": 1.6555782295933797e-07,
      "loss": 1.0377,
      "step": 10099
    },
    {
      "epoch": 94.39610963748895,
      "grad_norm": 0.5914710164070129,
      "learning_rate": 1.6500977022626985e-07,
      "loss": 1.0538,
      "step": 10100
    },
    {
      "epoch": 94.40554081933392,
      "grad_norm": 0.5683331489562988,
      "learning_rate": 1.6446261857175018e-07,
      "loss": 1.0413,
      "step": 10101
    },
    {
      "epoch": 94.4149720011789,
      "grad_norm": 0.5584349632263184,
      "learning_rate": 1.6391636804591105e-07,
      "loss": 1.0563,
      "step": 10102
    },
    {
      "epoch": 94.42440318302387,
      "grad_norm": 0.54494708776474,
      "learning_rate": 1.6337101869879691e-07,
      "loss": 1.0073,
      "step": 10103
    },
    {
      "epoch": 94.43383436486884,
      "grad_norm": 0.5270097851753235,
      "learning_rate": 1.6282657058037333e-07,
      "loss": 1.0706,
      "step": 10104
    },
    {
      "epoch": 94.44326554671382,
      "grad_norm": 0.5542325377464294,
      "learning_rate": 1.6228302374052374e-07,
      "loss": 1.0427,
      "step": 10105
    },
    {
      "epoch": 94.45269672855879,
      "grad_norm": 0.5786581039428711,
      "learning_rate": 1.6174037822904609e-07,
      "loss": 1.0882,
      "step": 10106
    },
    {
      "epoch": 94.46212791040377,
      "grad_norm": 0.5459631085395813,
      "learning_rate": 1.611986340956573e-07,
      "loss": 1.024,
      "step": 10107
    },
    {
      "epoch": 94.47155909224875,
      "grad_norm": 0.5728923678398132,
      "learning_rate": 1.6065779138999204e-07,
      "loss": 1.0355,
      "step": 10108
    },
    {
      "epoch": 94.48099027409373,
      "grad_norm": 0.5137263536453247,
      "learning_rate": 1.6011785016160408e-07,
      "loss": 1.0274,
      "step": 10109
    },
    {
      "epoch": 94.4904214559387,
      "grad_norm": 0.5867922902107239,
      "learning_rate": 1.595788104599605e-07,
      "loss": 1.0165,
      "step": 10110
    },
    {
      "epoch": 94.49985263778368,
      "grad_norm": 0.5958688259124756,
      "learning_rate": 1.5904067233444842e-07,
      "loss": 1.0433,
      "step": 10111
    },
    {
      "epoch": 94.50928381962865,
      "grad_norm": 0.6047796607017517,
      "learning_rate": 1.585034358343729e-07,
      "loss": 1.0781,
      "step": 10112
    },
    {
      "epoch": 94.51871500147362,
      "grad_norm": 0.5748351812362671,
      "learning_rate": 1.579671010089534e-07,
      "loss": 1.0938,
      "step": 10113
    },
    {
      "epoch": 94.5281461833186,
      "grad_norm": 0.5439185500144958,
      "learning_rate": 1.5743166790733067e-07,
      "loss": 1.0371,
      "step": 10114
    },
    {
      "epoch": 94.53757736516357,
      "grad_norm": 0.5423473715782166,
      "learning_rate": 1.5689713657855876e-07,
      "loss": 1.029,
      "step": 10115
    },
    {
      "epoch": 94.54700854700855,
      "grad_norm": 0.5460259914398193,
      "learning_rate": 1.5636350707161406e-07,
      "loss": 1.0665,
      "step": 10116
    },
    {
      "epoch": 94.55643972885352,
      "grad_norm": 0.5027862191200256,
      "learning_rate": 1.5583077943538526e-07,
      "loss": 1.0069,
      "step": 10117
    },
    {
      "epoch": 94.5658709106985,
      "grad_norm": 0.5767611861228943,
      "learning_rate": 1.552989537186811e-07,
      "loss": 1.0294,
      "step": 10118
    },
    {
      "epoch": 94.57530209254347,
      "grad_norm": 0.5769873261451721,
      "learning_rate": 1.5476802997022812e-07,
      "loss": 1.0553,
      "step": 10119
    },
    {
      "epoch": 94.58473327438844,
      "grad_norm": 0.5720135569572449,
      "learning_rate": 1.5423800823866962e-07,
      "loss": 1.0131,
      "step": 10120
    },
    {
      "epoch": 94.59416445623341,
      "grad_norm": 0.5393176078796387,
      "learning_rate": 1.537088885725646e-07,
      "loss": 1.0753,
      "step": 10121
    },
    {
      "epoch": 94.6035956380784,
      "grad_norm": 0.5228930711746216,
      "learning_rate": 1.5318067102039203e-07,
      "loss": 1.0684,
      "step": 10122
    },
    {
      "epoch": 94.61302681992338,
      "grad_norm": 0.5383940935134888,
      "learning_rate": 1.526533556305443e-07,
      "loss": 1.0148,
      "step": 10123
    },
    {
      "epoch": 94.62245800176835,
      "grad_norm": 0.5246832966804504,
      "learning_rate": 1.521269424513383e-07,
      "loss": 1.0824,
      "step": 10124
    },
    {
      "epoch": 94.63188918361332,
      "grad_norm": 0.5532234907150269,
      "learning_rate": 1.5160143153100105e-07,
      "loss": 1.0655,
      "step": 10125
    },
    {
      "epoch": 94.6413203654583,
      "grad_norm": 0.5637873411178589,
      "learning_rate": 1.5107682291768067e-07,
      "loss": 1.0207,
      "step": 10126
    },
    {
      "epoch": 94.65075154730327,
      "grad_norm": 0.5180214643478394,
      "learning_rate": 1.5055311665944094e-07,
      "loss": 1.0648,
      "step": 10127
    },
    {
      "epoch": 94.66018272914825,
      "grad_norm": 0.5735302567481995,
      "learning_rate": 1.500303128042635e-07,
      "loss": 1.0779,
      "step": 10128
    },
    {
      "epoch": 94.66961391099322,
      "grad_norm": 0.5389357209205627,
      "learning_rate": 1.495084114000489e-07,
      "loss": 1.0401,
      "step": 10129
    },
    {
      "epoch": 94.6790450928382,
      "grad_norm": 0.5086714029312134,
      "learning_rate": 1.4898741249461112e-07,
      "loss": 1.074,
      "step": 10130
    },
    {
      "epoch": 94.68847627468317,
      "grad_norm": 0.5439126491546631,
      "learning_rate": 1.4846731613568643e-07,
      "loss": 1.0616,
      "step": 10131
    },
    {
      "epoch": 94.69790745652814,
      "grad_norm": 0.5436967611312866,
      "learning_rate": 1.4794812237092448e-07,
      "loss": 1.0882,
      "step": 10132
    },
    {
      "epoch": 94.70733863837312,
      "grad_norm": 0.5478460192680359,
      "learning_rate": 1.474298312478939e-07,
      "loss": 1.0567,
      "step": 10133
    },
    {
      "epoch": 94.71676982021809,
      "grad_norm": 0.5464642643928528,
      "learning_rate": 1.4691244281408112e-07,
      "loss": 1.0418,
      "step": 10134
    },
    {
      "epoch": 94.72620100206306,
      "grad_norm": 0.5433611273765564,
      "learning_rate": 1.4639595711688826e-07,
      "loss": 1.0199,
      "step": 10135
    },
    {
      "epoch": 94.73563218390805,
      "grad_norm": 0.5187100172042847,
      "learning_rate": 1.458803742036352e-07,
      "loss": 1.071,
      "step": 10136
    },
    {
      "epoch": 94.74506336575303,
      "grad_norm": 0.5507375001907349,
      "learning_rate": 1.4536569412155977e-07,
      "loss": 1.0498,
      "step": 10137
    },
    {
      "epoch": 94.754494547598,
      "grad_norm": 0.5331347584724426,
      "learning_rate": 1.448519169178164e-07,
      "loss": 1.0603,
      "step": 10138
    },
    {
      "epoch": 94.76392572944297,
      "grad_norm": 0.5381034016609192,
      "learning_rate": 1.4433904263947973e-07,
      "loss": 1.0584,
      "step": 10139
    },
    {
      "epoch": 94.77335691128795,
      "grad_norm": 0.4922892451286316,
      "learning_rate": 1.4382707133353545e-07,
      "loss": 1.033,
      "step": 10140
    },
    {
      "epoch": 94.78278809313292,
      "grad_norm": 0.610437273979187,
      "learning_rate": 1.4331600304689275e-07,
      "loss": 1.0511,
      "step": 10141
    },
    {
      "epoch": 94.7922192749779,
      "grad_norm": 0.5979670882225037,
      "learning_rate": 1.4280583782637525e-07,
      "loss": 1.0153,
      "step": 10142
    },
    {
      "epoch": 94.80165045682287,
      "grad_norm": 0.5152697563171387,
      "learning_rate": 1.4229657571872224e-07,
      "loss": 1.0354,
      "step": 10143
    },
    {
      "epoch": 94.81108163866784,
      "grad_norm": 0.5635868906974792,
      "learning_rate": 1.4178821677059417e-07,
      "loss": 1.0494,
      "step": 10144
    },
    {
      "epoch": 94.82051282051282,
      "grad_norm": 0.5551069974899292,
      "learning_rate": 1.4128076102856604e-07,
      "loss": 1.0458,
      "step": 10145
    },
    {
      "epoch": 94.82994400235779,
      "grad_norm": 0.5612801313400269,
      "learning_rate": 1.407742085391306e-07,
      "loss": 1.0594,
      "step": 10146
    },
    {
      "epoch": 94.83937518420277,
      "grad_norm": 0.5846097469329834,
      "learning_rate": 1.4026855934869854e-07,
      "loss": 1.005,
      "step": 10147
    },
    {
      "epoch": 94.84880636604774,
      "grad_norm": 0.5281918048858643,
      "learning_rate": 1.3976381350359723e-07,
      "loss": 1.075,
      "step": 10148
    },
    {
      "epoch": 94.85823754789271,
      "grad_norm": 0.551769495010376,
      "learning_rate": 1.3925997105006972e-07,
      "loss": 1.0078,
      "step": 10149
    },
    {
      "epoch": 94.8676687297377,
      "grad_norm": 0.5428573489189148,
      "learning_rate": 1.3875703203428015e-07,
      "loss": 1.0551,
      "step": 10150
    },
    {
      "epoch": 94.87709991158268,
      "grad_norm": 0.5441984534263611,
      "learning_rate": 1.3825499650230722e-07,
      "loss": 1.0437,
      "step": 10151
    },
    {
      "epoch": 94.88653109342765,
      "grad_norm": 0.5028769969940186,
      "learning_rate": 1.377538645001464e-07,
      "loss": 1.0545,
      "step": 10152
    },
    {
      "epoch": 94.89596227527262,
      "grad_norm": 0.563793420791626,
      "learning_rate": 1.372536360737109e-07,
      "loss": 1.0489,
      "step": 10153
    },
    {
      "epoch": 94.9053934571176,
      "grad_norm": 0.5340477824211121,
      "learning_rate": 1.3675431126883187e-07,
      "loss": 1.0292,
      "step": 10154
    },
    {
      "epoch": 94.91482463896257,
      "grad_norm": 0.57781583070755,
      "learning_rate": 1.362558901312583e-07,
      "loss": 1.05,
      "step": 10155
    },
    {
      "epoch": 94.92425582080755,
      "grad_norm": 0.5527951121330261,
      "learning_rate": 1.357583727066547e-07,
      "loss": 1.0646,
      "step": 10156
    },
    {
      "epoch": 94.93368700265252,
      "grad_norm": 0.5628953576087952,
      "learning_rate": 1.3526175904060245e-07,
      "loss": 1.0345,
      "step": 10157
    },
    {
      "epoch": 94.9431181844975,
      "grad_norm": 0.5601550340652466,
      "learning_rate": 1.3476604917860293e-07,
      "loss": 0.9989,
      "step": 10158
    },
    {
      "epoch": 94.95254936634247,
      "grad_norm": 0.5295394659042358,
      "learning_rate": 1.3427124316607199e-07,
      "loss": 1.0734,
      "step": 10159
    },
    {
      "epoch": 94.96198054818744,
      "grad_norm": 0.5406219959259033,
      "learning_rate": 1.3377734104834339e-07,
      "loss": 1.0539,
      "step": 10160
    },
    {
      "epoch": 94.97141173003241,
      "grad_norm": 0.5796838402748108,
      "learning_rate": 1.3328434287066873e-07,
      "loss": 1.0572,
      "step": 10161
    },
    {
      "epoch": 94.98084291187739,
      "grad_norm": 0.5792716145515442,
      "learning_rate": 1.3279224867821517e-07,
      "loss": 0.9971,
      "step": 10162
    },
    {
      "epoch": 94.99027409372236,
      "grad_norm": 0.53517085313797,
      "learning_rate": 1.3230105851607112e-07,
      "loss": 1.0052,
      "step": 10163
    },
    {
      "epoch": 94.99970527556735,
      "grad_norm": 0.5857735276222229,
      "learning_rate": 1.3181077242923612e-07,
      "loss": 1.0462,
      "step": 10164
    },
    {
      "epoch": 95.0,
      "grad_norm": 4.727013111114502,
      "learning_rate": 1.3132139046263092e-07,
      "loss": 0.7373,
      "step": 10165
    },
    {
      "epoch": 95.00943118184497,
      "grad_norm": 0.5204300284385681,
      "learning_rate": 1.30832912661093e-07,
      "loss": 1.0721,
      "step": 10166
    },
    {
      "epoch": 95.01886236368995,
      "grad_norm": 0.5246155858039856,
      "learning_rate": 1.3034533906937653e-07,
      "loss": 1.0666,
      "step": 10167
    },
    {
      "epoch": 95.02829354553492,
      "grad_norm": 0.5376491546630859,
      "learning_rate": 1.2985866973215245e-07,
      "loss": 1.0658,
      "step": 10168
    },
    {
      "epoch": 95.0377247273799,
      "grad_norm": 0.5844423770904541,
      "learning_rate": 1.2937290469400954e-07,
      "loss": 1.043,
      "step": 10169
    },
    {
      "epoch": 95.04715590922487,
      "grad_norm": 0.535556435585022,
      "learning_rate": 1.2888804399945441e-07,
      "loss": 1.0677,
      "step": 10170
    },
    {
      "epoch": 95.05658709106984,
      "grad_norm": 0.5681784152984619,
      "learning_rate": 1.2840408769290714e-07,
      "loss": 1.0854,
      "step": 10171
    },
    {
      "epoch": 95.06601827291482,
      "grad_norm": 0.5566914677619934,
      "learning_rate": 1.2792103581871107e-07,
      "loss": 1.0206,
      "step": 10172
    },
    {
      "epoch": 95.0754494547598,
      "grad_norm": 0.5842203497886658,
      "learning_rate": 1.2743888842111974e-07,
      "loss": 1.0883,
      "step": 10173
    },
    {
      "epoch": 95.08488063660478,
      "grad_norm": 0.5586721897125244,
      "learning_rate": 1.2695764554431002e-07,
      "loss": 1.0334,
      "step": 10174
    },
    {
      "epoch": 95.09431181844975,
      "grad_norm": 0.6109387874603271,
      "learning_rate": 1.2647730723237216e-07,
      "loss": 1.0279,
      "step": 10175
    },
    {
      "epoch": 95.10374300029473,
      "grad_norm": 0.5305789709091187,
      "learning_rate": 1.2599787352931436e-07,
      "loss": 1.0528,
      "step": 10176
    },
    {
      "epoch": 95.1131741821397,
      "grad_norm": 0.5172256231307983,
      "learning_rate": 1.2551934447906254e-07,
      "loss": 1.0781,
      "step": 10177
    },
    {
      "epoch": 95.12260536398468,
      "grad_norm": 0.5658828020095825,
      "learning_rate": 1.2504172012545946e-07,
      "loss": 1.0604,
      "step": 10178
    },
    {
      "epoch": 95.13203654582965,
      "grad_norm": 0.5224744081497192,
      "learning_rate": 1.245650005122656e-07,
      "loss": 1.081,
      "step": 10179
    },
    {
      "epoch": 95.14146772767462,
      "grad_norm": 0.5407779812812805,
      "learning_rate": 1.2408918568315608e-07,
      "loss": 1.0696,
      "step": 10180
    },
    {
      "epoch": 95.1508989095196,
      "grad_norm": 0.5510619878768921,
      "learning_rate": 1.2361427568172603e-07,
      "loss": 1.0461,
      "step": 10181
    },
    {
      "epoch": 95.16033009136457,
      "grad_norm": 0.5246671438217163,
      "learning_rate": 1.2314027055148725e-07,
      "loss": 1.0124,
      "step": 10182
    },
    {
      "epoch": 95.16976127320955,
      "grad_norm": 0.548976480960846,
      "learning_rate": 1.2266717033586617e-07,
      "loss": 1.0416,
      "step": 10183
    },
    {
      "epoch": 95.17919245505452,
      "grad_norm": 0.530665934085846,
      "learning_rate": 1.221949750782092e-07,
      "loss": 1.0423,
      "step": 10184
    },
    {
      "epoch": 95.1886236368995,
      "grad_norm": 0.5509310960769653,
      "learning_rate": 1.2172368482177843e-07,
      "loss": 1.0427,
      "step": 10185
    },
    {
      "epoch": 95.19805481874447,
      "grad_norm": 0.5665483474731445,
      "learning_rate": 1.2125329960975375e-07,
      "loss": 1.0364,
      "step": 10186
    },
    {
      "epoch": 95.20748600058945,
      "grad_norm": 0.5422153472900391,
      "learning_rate": 1.2078381948523067e-07,
      "loss": 1.0596,
      "step": 10187
    },
    {
      "epoch": 95.21691718243443,
      "grad_norm": 0.5018482804298401,
      "learning_rate": 1.2031524449122367e-07,
      "loss": 1.0686,
      "step": 10188
    },
    {
      "epoch": 95.2263483642794,
      "grad_norm": 0.5421027541160583,
      "learning_rate": 1.19847574670664e-07,
      "loss": 1.0659,
      "step": 10189
    },
    {
      "epoch": 95.23577954612438,
      "grad_norm": 0.5348107814788818,
      "learning_rate": 1.1938081006639735e-07,
      "loss": 1.0543,
      "step": 10190
    },
    {
      "epoch": 95.24521072796935,
      "grad_norm": 0.5557211637496948,
      "learning_rate": 1.1891495072119063e-07,
      "loss": 0.9824,
      "step": 10191
    },
    {
      "epoch": 95.25464190981432,
      "grad_norm": 0.5531439185142517,
      "learning_rate": 1.1844999667772417e-07,
      "loss": 1.0289,
      "step": 10192
    },
    {
      "epoch": 95.2640730916593,
      "grad_norm": 0.5507175922393799,
      "learning_rate": 1.179859479785983e-07,
      "loss": 1.059,
      "step": 10193
    },
    {
      "epoch": 95.27350427350427,
      "grad_norm": 0.5603624582290649,
      "learning_rate": 1.1752280466632903e-07,
      "loss": 1.0916,
      "step": 10194
    },
    {
      "epoch": 95.28293545534925,
      "grad_norm": 0.5141236186027527,
      "learning_rate": 1.1706056678334799e-07,
      "loss": 1.0583,
      "step": 10195
    },
    {
      "epoch": 95.29236663719422,
      "grad_norm": 0.5274965763092041,
      "learning_rate": 1.1659923437200571e-07,
      "loss": 1.0673,
      "step": 10196
    },
    {
      "epoch": 95.3017978190392,
      "grad_norm": 0.5098065137863159,
      "learning_rate": 1.1613880747456951e-07,
      "loss": 1.064,
      "step": 10197
    },
    {
      "epoch": 95.31122900088417,
      "grad_norm": 0.5451536178588867,
      "learning_rate": 1.1567928613322455e-07,
      "loss": 1.0687,
      "step": 10198
    },
    {
      "epoch": 95.32066018272914,
      "grad_norm": 0.5551556944847107,
      "learning_rate": 1.1522067039007046e-07,
      "loss": 0.992,
      "step": 10199
    },
    {
      "epoch": 95.33009136457412,
      "grad_norm": 0.5096060037612915,
      "learning_rate": 1.1476296028712697e-07,
      "loss": 1.0255,
      "step": 10200
    },
    {
      "epoch": 95.3395225464191,
      "grad_norm": 0.5835106372833252,
      "learning_rate": 1.143061558663272e-07,
      "loss": 1.0302,
      "step": 10201
    },
    {
      "epoch": 95.34895372826408,
      "grad_norm": 0.5272818803787231,
      "learning_rate": 1.1385025716952547e-07,
      "loss": 1.0158,
      "step": 10202
    },
    {
      "epoch": 95.35838491010905,
      "grad_norm": 0.5763331055641174,
      "learning_rate": 1.1339526423849057e-07,
      "loss": 1.117,
      "step": 10203
    },
    {
      "epoch": 95.36781609195403,
      "grad_norm": 0.552402138710022,
      "learning_rate": 1.1294117711490804e-07,
      "loss": 1.0542,
      "step": 10204
    },
    {
      "epoch": 95.377247273799,
      "grad_norm": 0.5366326570510864,
      "learning_rate": 1.1248799584038128e-07,
      "loss": 1.0366,
      "step": 10205
    },
    {
      "epoch": 95.38667845564397,
      "grad_norm": 0.549414873123169,
      "learning_rate": 1.1203572045643151e-07,
      "loss": 1.0572,
      "step": 10206
    },
    {
      "epoch": 95.39610963748895,
      "grad_norm": 0.5459945797920227,
      "learning_rate": 1.115843510044956e-07,
      "loss": 1.0386,
      "step": 10207
    },
    {
      "epoch": 95.40554081933392,
      "grad_norm": 0.5574752688407898,
      "learning_rate": 1.1113388752592824e-07,
      "loss": 1.0138,
      "step": 10208
    },
    {
      "epoch": 95.4149720011789,
      "grad_norm": 0.5614567995071411,
      "learning_rate": 1.1068433006199975e-07,
      "loss": 1.0366,
      "step": 10209
    },
    {
      "epoch": 95.42440318302387,
      "grad_norm": 0.5591644644737244,
      "learning_rate": 1.102356786538994e-07,
      "loss": 1.0189,
      "step": 10210
    },
    {
      "epoch": 95.43383436486884,
      "grad_norm": 0.6235273480415344,
      "learning_rate": 1.0978793334273097e-07,
      "loss": 1.0552,
      "step": 10211
    },
    {
      "epoch": 95.44326554671382,
      "grad_norm": 0.5748613476753235,
      "learning_rate": 1.0934109416951833e-07,
      "loss": 1.066,
      "step": 10212
    },
    {
      "epoch": 95.45269672855879,
      "grad_norm": 0.5548923015594482,
      "learning_rate": 1.0889516117520094e-07,
      "loss": 1.0422,
      "step": 10213
    },
    {
      "epoch": 95.46212791040377,
      "grad_norm": 0.5685740113258362,
      "learning_rate": 1.084501344006339e-07,
      "loss": 1.0632,
      "step": 10214
    },
    {
      "epoch": 95.47155909224875,
      "grad_norm": 0.5747075080871582,
      "learning_rate": 1.0800601388659127e-07,
      "loss": 1.0724,
      "step": 10215
    },
    {
      "epoch": 95.48099027409373,
      "grad_norm": 0.5565532445907593,
      "learning_rate": 1.075627996737627e-07,
      "loss": 1.0987,
      "step": 10216
    },
    {
      "epoch": 95.4904214559387,
      "grad_norm": 0.58837890625,
      "learning_rate": 1.0712049180275463e-07,
      "loss": 0.9943,
      "step": 10217
    },
    {
      "epoch": 95.49985263778368,
      "grad_norm": 0.5666906237602234,
      "learning_rate": 1.0667909031409351e-07,
      "loss": 1.0578,
      "step": 10218
    },
    {
      "epoch": 95.50928381962865,
      "grad_norm": 0.5224999785423279,
      "learning_rate": 1.0623859524821811e-07,
      "loss": 1.0445,
      "step": 10219
    },
    {
      "epoch": 95.51871500147362,
      "grad_norm": 0.521334707736969,
      "learning_rate": 1.0579900664548726e-07,
      "loss": 1.0044,
      "step": 10220
    },
    {
      "epoch": 95.5281461833186,
      "grad_norm": 0.5356010794639587,
      "learning_rate": 1.0536032454617539e-07,
      "loss": 1.0613,
      "step": 10221
    },
    {
      "epoch": 95.53757736516357,
      "grad_norm": 0.5724087953567505,
      "learning_rate": 1.0492254899047482e-07,
      "loss": 1.0505,
      "step": 10222
    },
    {
      "epoch": 95.54700854700855,
      "grad_norm": 0.512994647026062,
      "learning_rate": 1.0448568001849457e-07,
      "loss": 1.0322,
      "step": 10223
    },
    {
      "epoch": 95.55643972885352,
      "grad_norm": 0.5845134258270264,
      "learning_rate": 1.0404971767026151e-07,
      "loss": 1.0416,
      "step": 10224
    },
    {
      "epoch": 95.5658709106985,
      "grad_norm": 0.5850496888160706,
      "learning_rate": 1.036146619857159e-07,
      "loss": 1.0408,
      "step": 10225
    },
    {
      "epoch": 95.57530209254347,
      "grad_norm": 0.5280495882034302,
      "learning_rate": 1.031805130047192e-07,
      "loss": 1.0625,
      "step": 10226
    },
    {
      "epoch": 95.58473327438844,
      "grad_norm": 0.5613759160041809,
      "learning_rate": 1.0274727076704737e-07,
      "loss": 1.014,
      "step": 10227
    },
    {
      "epoch": 95.59416445623341,
      "grad_norm": 0.5560927987098694,
      "learning_rate": 1.0231493531239422e-07,
      "loss": 1.0294,
      "step": 10228
    },
    {
      "epoch": 95.6035956380784,
      "grad_norm": 0.5477227568626404,
      "learning_rate": 1.0188350668037028e-07,
      "loss": 1.0694,
      "step": 10229
    },
    {
      "epoch": 95.61302681992338,
      "grad_norm": 0.573152482509613,
      "learning_rate": 1.014529849105017e-07,
      "loss": 1.0738,
      "step": 10230
    },
    {
      "epoch": 95.62245800176835,
      "grad_norm": 0.5573518872261047,
      "learning_rate": 1.0102337004223473e-07,
      "loss": 1.0539,
      "step": 10231
    },
    {
      "epoch": 95.63188918361332,
      "grad_norm": 0.5383803844451904,
      "learning_rate": 1.0059466211492896e-07,
      "loss": 1.0518,
      "step": 10232
    },
    {
      "epoch": 95.6413203654583,
      "grad_norm": 0.5302082300186157,
      "learning_rate": 1.00166861167863e-07,
      "loss": 1.0517,
      "step": 10233
    },
    {
      "epoch": 95.65075154730327,
      "grad_norm": 0.5691105723381042,
      "learning_rate": 9.973996724023217e-08,
      "loss": 1.0842,
      "step": 10234
    },
    {
      "epoch": 95.66018272914825,
      "grad_norm": 0.5668561458587646,
      "learning_rate": 9.931398037114626e-08,
      "loss": 1.0878,
      "step": 10235
    },
    {
      "epoch": 95.66961391099322,
      "grad_norm": 0.5920138359069824,
      "learning_rate": 9.88889005996374e-08,
      "loss": 1.0258,
      "step": 10236
    },
    {
      "epoch": 95.6790450928382,
      "grad_norm": 0.495819091796875,
      "learning_rate": 9.846472796465001e-08,
      "loss": 1.0207,
      "step": 10237
    },
    {
      "epoch": 95.68847627468317,
      "grad_norm": 0.5765703916549683,
      "learning_rate": 9.80414625050452e-08,
      "loss": 1.0424,
      "step": 10238
    },
    {
      "epoch": 95.69790745652814,
      "grad_norm": 0.580758273601532,
      "learning_rate": 9.761910425960419e-08,
      "loss": 1.0288,
      "step": 10239
    },
    {
      "epoch": 95.70733863837312,
      "grad_norm": 0.589841902256012,
      "learning_rate": 9.719765326702268e-08,
      "loss": 1.0446,
      "step": 10240
    },
    {
      "epoch": 95.71676982021809,
      "grad_norm": 0.5386855006217957,
      "learning_rate": 9.677710956591313e-08,
      "loss": 1.0354,
      "step": 10241
    },
    {
      "epoch": 95.72620100206306,
      "grad_norm": 0.614464521408081,
      "learning_rate": 9.635747319480581e-08,
      "loss": 1.0683,
      "step": 10242
    },
    {
      "epoch": 95.73563218390805,
      "grad_norm": 0.5714081525802612,
      "learning_rate": 9.593874419214888e-08,
      "loss": 1.0583,
      "step": 10243
    },
    {
      "epoch": 95.74506336575303,
      "grad_norm": 0.5716217756271362,
      "learning_rate": 9.552092259630608e-08,
      "loss": 1.0583,
      "step": 10244
    },
    {
      "epoch": 95.754494547598,
      "grad_norm": 0.5519711375236511,
      "learning_rate": 9.510400844555678e-08,
      "loss": 1.0132,
      "step": 10245
    },
    {
      "epoch": 95.76392572944297,
      "grad_norm": 0.5566338300704956,
      "learning_rate": 9.468800177809823e-08,
      "loss": 1.0929,
      "step": 10246
    },
    {
      "epoch": 95.77335691128795,
      "grad_norm": 0.5601354837417603,
      "learning_rate": 9.427290263204658e-08,
      "loss": 1.0572,
      "step": 10247
    },
    {
      "epoch": 95.78278809313292,
      "grad_norm": 0.5355690717697144,
      "learning_rate": 9.385871104543254e-08,
      "loss": 1.0504,
      "step": 10248
    },
    {
      "epoch": 95.7922192749779,
      "grad_norm": 0.539692223072052,
      "learning_rate": 9.344542705620352e-08,
      "loss": 1.0376,
      "step": 10249
    },
    {
      "epoch": 95.80165045682287,
      "grad_norm": 0.49023622274398804,
      "learning_rate": 9.30330507022248e-08,
      "loss": 1.047,
      "step": 10250
    },
    {
      "epoch": 95.81108163866784,
      "grad_norm": 0.5319321751594543,
      "learning_rate": 9.262158202127835e-08,
      "loss": 1.0437,
      "step": 10251
    },
    {
      "epoch": 95.82051282051282,
      "grad_norm": 0.5497006177902222,
      "learning_rate": 9.221102105106295e-08,
      "loss": 1.0826,
      "step": 10252
    },
    {
      "epoch": 95.82994400235779,
      "grad_norm": 0.5985834002494812,
      "learning_rate": 9.180136782919291e-08,
      "loss": 1.0348,
      "step": 10253
    },
    {
      "epoch": 95.83937518420277,
      "grad_norm": 0.6017336845397949,
      "learning_rate": 9.139262239320157e-08,
      "loss": 1.0226,
      "step": 10254
    },
    {
      "epoch": 95.84880636604774,
      "grad_norm": 0.5506681203842163,
      "learning_rate": 9.098478478053896e-08,
      "loss": 1.0478,
      "step": 10255
    },
    {
      "epoch": 95.85823754789271,
      "grad_norm": 0.5881592035293579,
      "learning_rate": 9.057785502856852e-08,
      "loss": 1.0576,
      "step": 10256
    },
    {
      "epoch": 95.8676687297377,
      "grad_norm": 0.5605355501174927,
      "learning_rate": 9.017183317457489e-08,
      "loss": 1.0512,
      "step": 10257
    },
    {
      "epoch": 95.87709991158268,
      "grad_norm": 0.5842899084091187,
      "learning_rate": 8.976671925575719e-08,
      "loss": 1.0307,
      "step": 10258
    },
    {
      "epoch": 95.88653109342765,
      "grad_norm": 0.5439910888671875,
      "learning_rate": 8.936251330923129e-08,
      "loss": 1.0512,
      "step": 10259
    },
    {
      "epoch": 95.89596227527262,
      "grad_norm": 0.548497200012207,
      "learning_rate": 8.8959215372032e-08,
      "loss": 1.0128,
      "step": 10260
    },
    {
      "epoch": 95.9053934571176,
      "grad_norm": 0.5242893695831299,
      "learning_rate": 8.855682548110756e-08,
      "loss": 1.0679,
      "step": 10261
    },
    {
      "epoch": 95.91482463896257,
      "grad_norm": 0.5664992928504944,
      "learning_rate": 8.815534367332623e-08,
      "loss": 1.0088,
      "step": 10262
    },
    {
      "epoch": 95.92425582080755,
      "grad_norm": 0.5517695546150208,
      "learning_rate": 8.775476998547084e-08,
      "loss": 1.0407,
      "step": 10263
    },
    {
      "epoch": 95.93368700265252,
      "grad_norm": 0.6482529044151306,
      "learning_rate": 8.73551044542409e-08,
      "loss": 0.9936,
      "step": 10264
    },
    {
      "epoch": 95.9431181844975,
      "grad_norm": 0.53351229429245,
      "learning_rate": 8.695634711625489e-08,
      "loss": 1.0553,
      "step": 10265
    },
    {
      "epoch": 95.95254936634247,
      "grad_norm": 0.5229703783988953,
      "learning_rate": 8.655849800804694e-08,
      "loss": 1.0369,
      "step": 10266
    },
    {
      "epoch": 95.96198054818744,
      "grad_norm": 0.5193626880645752,
      "learning_rate": 8.616155716606789e-08,
      "loss": 1.0676,
      "step": 10267
    },
    {
      "epoch": 95.97141173003241,
      "grad_norm": 0.5503553748130798,
      "learning_rate": 8.576552462668419e-08,
      "loss": 1.0344,
      "step": 10268
    },
    {
      "epoch": 95.98084291187739,
      "grad_norm": 0.550044596195221,
      "learning_rate": 8.537040042618128e-08,
      "loss": 1.0346,
      "step": 10269
    },
    {
      "epoch": 95.99027409372236,
      "grad_norm": 0.5780937671661377,
      "learning_rate": 8.497618460076018e-08,
      "loss": 1.0256,
      "step": 10270
    },
    {
      "epoch": 95.99970527556735,
      "grad_norm": 0.518555760383606,
      "learning_rate": 8.458287718653868e-08,
      "loss": 1.0671,
      "step": 10271
    },
    {
      "epoch": 96.0,
      "grad_norm": 2.639683246612549,
      "learning_rate": 8.419047821955129e-08,
      "loss": 0.5889,
      "step": 10272
    },
    {
      "epoch": 96.00943118184497,
      "grad_norm": 0.5353317260742188,
      "learning_rate": 8.379898773574924e-08,
      "loss": 1.0378,
      "step": 10273
    },
    {
      "epoch": 96.01886236368995,
      "grad_norm": 0.5447752475738525,
      "learning_rate": 8.340840577100162e-08,
      "loss": 1.0354,
      "step": 10274
    },
    {
      "epoch": 96.02829354553492,
      "grad_norm": 0.5415597558021545,
      "learning_rate": 8.301873236109204e-08,
      "loss": 1.0549,
      "step": 10275
    },
    {
      "epoch": 96.0377247273799,
      "grad_norm": 0.5815937519073486,
      "learning_rate": 8.262996754172415e-08,
      "loss": 1.0386,
      "step": 10276
    },
    {
      "epoch": 96.04715590922487,
      "grad_norm": 0.5414966940879822,
      "learning_rate": 8.22421113485139e-08,
      "loss": 1.0589,
      "step": 10277
    },
    {
      "epoch": 96.05658709106984,
      "grad_norm": 0.5844919085502625,
      "learning_rate": 8.185516381699954e-08,
      "loss": 1.0187,
      "step": 10278
    },
    {
      "epoch": 96.06601827291482,
      "grad_norm": 0.5152395367622375,
      "learning_rate": 8.146912498263048e-08,
      "loss": 1.0579,
      "step": 10279
    },
    {
      "epoch": 96.0754494547598,
      "grad_norm": 0.5331611633300781,
      "learning_rate": 8.108399488077734e-08,
      "loss": 1.022,
      "step": 10280
    },
    {
      "epoch": 96.08488063660478,
      "grad_norm": 0.5301477909088135,
      "learning_rate": 8.06997735467252e-08,
      "loss": 1.0365,
      "step": 10281
    },
    {
      "epoch": 96.09431181844975,
      "grad_norm": 0.5725140571594238,
      "learning_rate": 8.03164610156748e-08,
      "loss": 1.0172,
      "step": 10282
    },
    {
      "epoch": 96.10374300029473,
      "grad_norm": 0.5727985501289368,
      "learning_rate": 7.993405732274695e-08,
      "loss": 1.0035,
      "step": 10283
    },
    {
      "epoch": 96.1131741821397,
      "grad_norm": 0.6030353307723999,
      "learning_rate": 7.955256250297694e-08,
      "loss": 1.0392,
      "step": 10284
    },
    {
      "epoch": 96.12260536398468,
      "grad_norm": 0.5657905340194702,
      "learning_rate": 7.917197659131682e-08,
      "loss": 1.0696,
      "step": 10285
    },
    {
      "epoch": 96.13203654582965,
      "grad_norm": 0.5352771878242493,
      "learning_rate": 7.879229962263535e-08,
      "loss": 1.0578,
      "step": 10286
    },
    {
      "epoch": 96.14146772767462,
      "grad_norm": 0.5623838901519775,
      "learning_rate": 7.841353163171917e-08,
      "loss": 1.0504,
      "step": 10287
    },
    {
      "epoch": 96.1508989095196,
      "grad_norm": 0.5276020169258118,
      "learning_rate": 7.80356726532705e-08,
      "loss": 1.0288,
      "step": 10288
    },
    {
      "epoch": 96.16033009136457,
      "grad_norm": 0.5476853847503662,
      "learning_rate": 7.765872272190944e-08,
      "loss": 1.0776,
      "step": 10289
    },
    {
      "epoch": 96.16976127320955,
      "grad_norm": 0.5460944771766663,
      "learning_rate": 7.728268187217058e-08,
      "loss": 1.0196,
      "step": 10290
    },
    {
      "epoch": 96.17919245505452,
      "grad_norm": 0.5450553297996521,
      "learning_rate": 7.690755013850747e-08,
      "loss": 1.0326,
      "step": 10291
    },
    {
      "epoch": 96.1886236368995,
      "grad_norm": 0.5638691782951355,
      "learning_rate": 7.653332755529042e-08,
      "loss": 1.0142,
      "step": 10292
    },
    {
      "epoch": 96.19805481874447,
      "grad_norm": 0.5564743280410767,
      "learning_rate": 7.616001415680418e-08,
      "loss": 1.0694,
      "step": 10293
    },
    {
      "epoch": 96.20748600058945,
      "grad_norm": 0.547073483467102,
      "learning_rate": 7.578760997725143e-08,
      "loss": 1.0656,
      "step": 10294
    },
    {
      "epoch": 96.21691718243443,
      "grad_norm": 0.5339043140411377,
      "learning_rate": 7.541611505075264e-08,
      "loss": 1.072,
      "step": 10295
    },
    {
      "epoch": 96.2263483642794,
      "grad_norm": 0.5362977385520935,
      "learning_rate": 7.50455294113439e-08,
      "loss": 1.0403,
      "step": 10296
    },
    {
      "epoch": 96.23577954612438,
      "grad_norm": 0.5854501128196716,
      "learning_rate": 7.467585309297809e-08,
      "loss": 1.0534,
      "step": 10297
    },
    {
      "epoch": 96.24521072796935,
      "grad_norm": 0.5305381417274475,
      "learning_rate": 7.430708612952476e-08,
      "loss": 1.0411,
      "step": 10298
    },
    {
      "epoch": 96.25464190981432,
      "grad_norm": 0.5699874758720398,
      "learning_rate": 7.39392285547702e-08,
      "loss": 1.0651,
      "step": 10299
    },
    {
      "epoch": 96.2640730916593,
      "grad_norm": 0.5214022994041443,
      "learning_rate": 7.357228040241748e-08,
      "loss": 1.0762,
      "step": 10300
    },
    {
      "epoch": 96.27350427350427,
      "grad_norm": 0.5580645799636841,
      "learning_rate": 7.320624170608636e-08,
      "loss": 1.0789,
      "step": 10301
    },
    {
      "epoch": 96.28293545534925,
      "grad_norm": 0.5479016900062561,
      "learning_rate": 7.284111249931224e-08,
      "loss": 1.0354,
      "step": 10302
    },
    {
      "epoch": 96.29236663719422,
      "grad_norm": 0.5520729422569275,
      "learning_rate": 7.247689281554949e-08,
      "loss": 1.0598,
      "step": 10303
    },
    {
      "epoch": 96.3017978190392,
      "grad_norm": 0.5347511768341064,
      "learning_rate": 7.211358268816804e-08,
      "loss": 1.0772,
      "step": 10304
    },
    {
      "epoch": 96.31122900088417,
      "grad_norm": 0.5273006558418274,
      "learning_rate": 7.17511821504524e-08,
      "loss": 1.0252,
      "step": 10305
    },
    {
      "epoch": 96.32066018272914,
      "grad_norm": 0.5994351506233215,
      "learning_rate": 7.138969123560713e-08,
      "loss": 1.0156,
      "step": 10306
    },
    {
      "epoch": 96.33009136457412,
      "grad_norm": 0.5305781364440918,
      "learning_rate": 7.102910997675127e-08,
      "loss": 1.0336,
      "step": 10307
    },
    {
      "epoch": 96.3395225464191,
      "grad_norm": 0.5480786561965942,
      "learning_rate": 7.066943840692176e-08,
      "loss": 1.0564,
      "step": 10308
    },
    {
      "epoch": 96.34895372826408,
      "grad_norm": 0.547161340713501,
      "learning_rate": 7.031067655906998e-08,
      "loss": 1.0467,
      "step": 10309
    },
    {
      "epoch": 96.35838491010905,
      "grad_norm": 0.5644565224647522,
      "learning_rate": 6.995282446606744e-08,
      "loss": 1.0496,
      "step": 10310
    },
    {
      "epoch": 96.36781609195403,
      "grad_norm": 0.6097094416618347,
      "learning_rate": 6.959588216069902e-08,
      "loss": 1.0543,
      "step": 10311
    },
    {
      "epoch": 96.377247273799,
      "grad_norm": 0.5238693356513977,
      "learning_rate": 6.923984967566966e-08,
      "loss": 1.0887,
      "step": 10312
    },
    {
      "epoch": 96.38667845564397,
      "grad_norm": 0.5213120579719543,
      "learning_rate": 6.888472704359661e-08,
      "loss": 1.0479,
      "step": 10313
    },
    {
      "epoch": 96.39610963748895,
      "grad_norm": 0.5124373435974121,
      "learning_rate": 6.853051429701718e-08,
      "loss": 1.0612,
      "step": 10314
    },
    {
      "epoch": 96.40554081933392,
      "grad_norm": 0.5564224123954773,
      "learning_rate": 6.81772114683843e-08,
      "loss": 0.9921,
      "step": 10315
    },
    {
      "epoch": 96.4149720011789,
      "grad_norm": 0.5631586313247681,
      "learning_rate": 6.782481859006762e-08,
      "loss": 1.0488,
      "step": 10316
    },
    {
      "epoch": 96.42440318302387,
      "grad_norm": 0.5380117893218994,
      "learning_rate": 6.747333569435244e-08,
      "loss": 1.0647,
      "step": 10317
    },
    {
      "epoch": 96.43383436486884,
      "grad_norm": 0.5882290601730347,
      "learning_rate": 6.712276281344188e-08,
      "loss": 1.0641,
      "step": 10318
    },
    {
      "epoch": 96.44326554671382,
      "grad_norm": 0.5828654766082764,
      "learning_rate": 6.677309997945691e-08,
      "loss": 1.0273,
      "step": 10319
    },
    {
      "epoch": 96.45269672855879,
      "grad_norm": 0.587820291519165,
      "learning_rate": 6.64243472244308e-08,
      "loss": 1.0198,
      "step": 10320
    },
    {
      "epoch": 96.46212791040377,
      "grad_norm": 0.5805363655090332,
      "learning_rate": 6.607650458031689e-08,
      "loss": 1.1023,
      "step": 10321
    },
    {
      "epoch": 96.47155909224875,
      "grad_norm": 0.5343523621559143,
      "learning_rate": 6.572957207898634e-08,
      "loss": 1.0613,
      "step": 10322
    },
    {
      "epoch": 96.48099027409373,
      "grad_norm": 0.5327289700508118,
      "learning_rate": 6.538354975222261e-08,
      "loss": 1.0583,
      "step": 10323
    },
    {
      "epoch": 96.4904214559387,
      "grad_norm": 0.5138134956359863,
      "learning_rate": 6.503843763172923e-08,
      "loss": 1.0268,
      "step": 10324
    },
    {
      "epoch": 96.49985263778368,
      "grad_norm": 0.5508319735527039,
      "learning_rate": 6.469423574912536e-08,
      "loss": 1.0466,
      "step": 10325
    },
    {
      "epoch": 96.50928381962865,
      "grad_norm": 0.5065804719924927,
      "learning_rate": 6.43509441359469e-08,
      "loss": 1.0322,
      "step": 10326
    },
    {
      "epoch": 96.51871500147362,
      "grad_norm": 0.5544131994247437,
      "learning_rate": 6.400856282364421e-08,
      "loss": 1.0687,
      "step": 10327
    },
    {
      "epoch": 96.5281461833186,
      "grad_norm": 0.5840712785720825,
      "learning_rate": 6.366709184358888e-08,
      "loss": 1.0276,
      "step": 10328
    },
    {
      "epoch": 96.53757736516357,
      "grad_norm": 0.5204203128814697,
      "learning_rate": 6.332653122706479e-08,
      "loss": 1.0578,
      "step": 10329
    },
    {
      "epoch": 96.54700854700855,
      "grad_norm": 0.5784236192703247,
      "learning_rate": 6.298688100527472e-08,
      "loss": 1.0545,
      "step": 10330
    },
    {
      "epoch": 96.55643972885352,
      "grad_norm": 0.5231592655181885,
      "learning_rate": 6.264814120933604e-08,
      "loss": 1.073,
      "step": 10331
    },
    {
      "epoch": 96.5658709106985,
      "grad_norm": 0.5440264940261841,
      "learning_rate": 6.231031187028502e-08,
      "loss": 1.0322,
      "step": 10332
    },
    {
      "epoch": 96.57530209254347,
      "grad_norm": 0.538279116153717,
      "learning_rate": 6.197339301907357e-08,
      "loss": 1.0459,
      "step": 10333
    },
    {
      "epoch": 96.58473327438844,
      "grad_norm": 0.5576492547988892,
      "learning_rate": 6.163738468657033e-08,
      "loss": 1.0305,
      "step": 10334
    },
    {
      "epoch": 96.59416445623341,
      "grad_norm": 0.5792567133903503,
      "learning_rate": 6.130228690355955e-08,
      "loss": 1.0401,
      "step": 10335
    },
    {
      "epoch": 96.6035956380784,
      "grad_norm": 0.526355504989624,
      "learning_rate": 6.096809970074224e-08,
      "loss": 1.0875,
      "step": 10336
    },
    {
      "epoch": 96.61302681992338,
      "grad_norm": 0.5311949253082275,
      "learning_rate": 6.063482310873725e-08,
      "loss": 1.0585,
      "step": 10337
    },
    {
      "epoch": 96.62245800176835,
      "grad_norm": 0.5447953343391418,
      "learning_rate": 6.030245715808014e-08,
      "loss": 1.06,
      "step": 10338
    },
    {
      "epoch": 96.63188918361332,
      "grad_norm": 0.5689484477043152,
      "learning_rate": 5.997100187921989e-08,
      "loss": 0.9978,
      "step": 10339
    },
    {
      "epoch": 96.6413203654583,
      "grad_norm": 0.5397156476974487,
      "learning_rate": 5.964045730252666e-08,
      "loss": 1.022,
      "step": 10340
    },
    {
      "epoch": 96.65075154730327,
      "grad_norm": 0.5728825926780701,
      "learning_rate": 5.931082345828398e-08,
      "loss": 1.0255,
      "step": 10341
    },
    {
      "epoch": 96.66018272914825,
      "grad_norm": 0.5552365779876709,
      "learning_rate": 5.898210037669328e-08,
      "loss": 1.0578,
      "step": 10342
    },
    {
      "epoch": 96.66961391099322,
      "grad_norm": 0.560443639755249,
      "learning_rate": 5.865428808787044e-08,
      "loss": 1.0448,
      "step": 10343
    },
    {
      "epoch": 96.6790450928382,
      "grad_norm": 0.5411701202392578,
      "learning_rate": 5.832738662185034e-08,
      "loss": 1.0547,
      "step": 10344
    },
    {
      "epoch": 96.68847627468317,
      "grad_norm": 0.5341437458992004,
      "learning_rate": 5.8001396008584565e-08,
      "loss": 1.0412,
      "step": 10345
    },
    {
      "epoch": 96.69790745652814,
      "grad_norm": 0.557102382183075,
      "learning_rate": 5.767631627793924e-08,
      "loss": 1.0652,
      "step": 10346
    },
    {
      "epoch": 96.70733863837312,
      "grad_norm": 0.5744878649711609,
      "learning_rate": 5.735214745969941e-08,
      "loss": 1.056,
      "step": 10347
    },
    {
      "epoch": 96.71676982021809,
      "grad_norm": 0.5019471645355225,
      "learning_rate": 5.702888958356356e-08,
      "loss": 1.051,
      "step": 10348
    },
    {
      "epoch": 96.72620100206306,
      "grad_norm": 0.5574447512626648,
      "learning_rate": 5.670654267914799e-08,
      "loss": 1.0427,
      "step": 10349
    },
    {
      "epoch": 96.73563218390805,
      "grad_norm": 0.522669792175293,
      "learning_rate": 5.6385106775987965e-08,
      "loss": 1.0945,
      "step": 10350
    },
    {
      "epoch": 96.74506336575303,
      "grad_norm": 0.5541548132896423,
      "learning_rate": 5.6064581903533256e-08,
      "loss": 1.0454,
      "step": 10351
    },
    {
      "epoch": 96.754494547598,
      "grad_norm": 0.6035726070404053,
      "learning_rate": 5.5744968091148154e-08,
      "loss": 1.0617,
      "step": 10352
    },
    {
      "epoch": 96.76392572944297,
      "grad_norm": 0.5555599927902222,
      "learning_rate": 5.542626536811813e-08,
      "loss": 1.0938,
      "step": 10353
    },
    {
      "epoch": 96.77335691128795,
      "grad_norm": 0.5234470367431641,
      "learning_rate": 5.510847376364093e-08,
      "loss": 1.0771,
      "step": 10354
    },
    {
      "epoch": 96.78278809313292,
      "grad_norm": 0.5474578738212585,
      "learning_rate": 5.4791593306833256e-08,
      "loss": 1.0043,
      "step": 10355
    },
    {
      "epoch": 96.7922192749779,
      "grad_norm": 0.5263083577156067,
      "learning_rate": 5.447562402672746e-08,
      "loss": 1.0618,
      "step": 10356
    },
    {
      "epoch": 96.80165045682287,
      "grad_norm": 0.5584877133369446,
      "learning_rate": 5.416056595227259e-08,
      "loss": 1.0382,
      "step": 10357
    },
    {
      "epoch": 96.81108163866784,
      "grad_norm": 0.566757321357727,
      "learning_rate": 5.3846419112333346e-08,
      "loss": 1.0362,
      "step": 10358
    },
    {
      "epoch": 96.82051282051282,
      "grad_norm": 0.5705618858337402,
      "learning_rate": 5.3533183535693364e-08,
      "loss": 1.0366,
      "step": 10359
    },
    {
      "epoch": 96.82994400235779,
      "grad_norm": 0.5187513828277588,
      "learning_rate": 5.3220859251049695e-08,
      "loss": 1.0715,
      "step": 10360
    },
    {
      "epoch": 96.83937518420277,
      "grad_norm": 0.5520278811454773,
      "learning_rate": 5.290944628701833e-08,
      "loss": 1.0064,
      "step": 10361
    },
    {
      "epoch": 96.84880636604774,
      "grad_norm": 0.5434306263923645,
      "learning_rate": 5.259894467212867e-08,
      "loss": 1.065,
      "step": 10362
    },
    {
      "epoch": 96.85823754789271,
      "grad_norm": 0.5575984716415405,
      "learning_rate": 5.228935443483241e-08,
      "loss": 1.0272,
      "step": 10363
    },
    {
      "epoch": 96.8676687297377,
      "grad_norm": 0.580423891544342,
      "learning_rate": 5.1980675603492406e-08,
      "loss": 1.0481,
      "step": 10364
    },
    {
      "epoch": 96.87709991158268,
      "grad_norm": 0.5486599206924438,
      "learning_rate": 5.167290820638826e-08,
      "loss": 1.0335,
      "step": 10365
    },
    {
      "epoch": 96.88653109342765,
      "grad_norm": 0.5605453848838806,
      "learning_rate": 5.136605227171965e-08,
      "loss": 1.0347,
      "step": 10366
    },
    {
      "epoch": 96.89596227527262,
      "grad_norm": 0.5388221144676208,
      "learning_rate": 5.106010782759962e-08,
      "loss": 1.0399,
      "step": 10367
    },
    {
      "epoch": 96.9053934571176,
      "grad_norm": 0.5497819185256958,
      "learning_rate": 5.075507490205911e-08,
      "loss": 1.0657,
      "step": 10368
    },
    {
      "epoch": 96.91482463896257,
      "grad_norm": 0.5385637283325195,
      "learning_rate": 5.045095352304463e-08,
      "loss": 1.004,
      "step": 10369
    },
    {
      "epoch": 96.92425582080755,
      "grad_norm": 0.5667353868484497,
      "learning_rate": 5.014774371841946e-08,
      "loss": 1.0703,
      "step": 10370
    },
    {
      "epoch": 96.93368700265252,
      "grad_norm": 0.5978760123252869,
      "learning_rate": 4.984544551596471e-08,
      "loss": 1.0256,
      "step": 10371
    },
    {
      "epoch": 96.9431181844975,
      "grad_norm": 0.553340494632721,
      "learning_rate": 4.954405894337599e-08,
      "loss": 1.0492,
      "step": 10372
    },
    {
      "epoch": 96.95254936634247,
      "grad_norm": 0.5434440970420837,
      "learning_rate": 4.9243584028265675e-08,
      "loss": 1.0295,
      "step": 10373
    },
    {
      "epoch": 96.96198054818744,
      "grad_norm": 0.5298152565956116,
      "learning_rate": 4.894402079816507e-08,
      "loss": 1.0612,
      "step": 10374
    },
    {
      "epoch": 96.97141173003241,
      "grad_norm": 0.5426273941993713,
      "learning_rate": 4.864536928051888e-08,
      "loss": 1.0678,
      "step": 10375
    },
    {
      "epoch": 96.98084291187739,
      "grad_norm": 0.5763762593269348,
      "learning_rate": 4.8347629502688565e-08,
      "loss": 1.0697,
      "step": 10376
    },
    {
      "epoch": 96.99027409372236,
      "grad_norm": 0.504262387752533,
      "learning_rate": 4.8050801491953405e-08,
      "loss": 1.0722,
      "step": 10377
    },
    {
      "epoch": 96.99970527556735,
      "grad_norm": 0.5657214522361755,
      "learning_rate": 4.7754885275510535e-08,
      "loss": 1.0441,
      "step": 10378
    },
    {
      "epoch": 97.0,
      "grad_norm": 3.0955970287323,
      "learning_rate": 4.745988088047049e-08,
      "loss": 0.9817,
      "step": 10379
    },
    {
      "epoch": 97.00943118184497,
      "grad_norm": 0.5227329730987549,
      "learning_rate": 4.716578833386054e-08,
      "loss": 1.0422,
      "step": 10380
    },
    {
      "epoch": 97.01886236368995,
      "grad_norm": 0.584867000579834,
      "learning_rate": 4.6872607662625804e-08,
      "loss": 1.0426,
      "step": 10381
    },
    {
      "epoch": 97.02829354553492,
      "grad_norm": 0.5616048574447632,
      "learning_rate": 4.6580338893628117e-08,
      "loss": 1.0281,
      "step": 10382
    },
    {
      "epoch": 97.0377247273799,
      "grad_norm": 0.5398125648498535,
      "learning_rate": 4.628898205364496e-08,
      "loss": 1.0414,
      "step": 10383
    },
    {
      "epoch": 97.04715590922487,
      "grad_norm": 0.5248403549194336,
      "learning_rate": 4.5998537169369414e-08,
      "loss": 1.0867,
      "step": 10384
    },
    {
      "epoch": 97.05658709106984,
      "grad_norm": 0.537821888923645,
      "learning_rate": 4.570900426741243e-08,
      "loss": 1.0537,
      "step": 10385
    },
    {
      "epoch": 97.06601827291482,
      "grad_norm": 0.5432059168815613,
      "learning_rate": 4.5420383374301655e-08,
      "loss": 1.0035,
      "step": 10386
    },
    {
      "epoch": 97.0754494547598,
      "grad_norm": 0.5535503029823303,
      "learning_rate": 4.513267451647929e-08,
      "loss": 1.1055,
      "step": 10387
    },
    {
      "epoch": 97.08488063660478,
      "grad_norm": 0.5348604321479797,
      "learning_rate": 4.484587772030646e-08,
      "loss": 1.0451,
      "step": 10388
    },
    {
      "epoch": 97.09431181844975,
      "grad_norm": 0.5519912242889404,
      "learning_rate": 4.455999301205771e-08,
      "loss": 1.0658,
      "step": 10389
    },
    {
      "epoch": 97.10374300029473,
      "grad_norm": 0.5723636746406555,
      "learning_rate": 4.427502041792764e-08,
      "loss": 1.0748,
      "step": 10390
    },
    {
      "epoch": 97.1131741821397,
      "grad_norm": 0.6066977977752686,
      "learning_rate": 4.3990959964023136e-08,
      "loss": 1.0417,
      "step": 10391
    },
    {
      "epoch": 97.12260536398468,
      "grad_norm": 0.5276549458503723,
      "learning_rate": 4.3707811676371173e-08,
      "loss": 1.0525,
      "step": 10392
    },
    {
      "epoch": 97.13203654582965,
      "grad_norm": 0.582545816898346,
      "learning_rate": 4.34255755809132e-08,
      "loss": 1.0922,
      "step": 10393
    },
    {
      "epoch": 97.14146772767462,
      "grad_norm": 0.5004217624664307,
      "learning_rate": 4.314425170350856e-08,
      "loss": 1.0707,
      "step": 10394
    },
    {
      "epoch": 97.1508989095196,
      "grad_norm": 0.5882107019424438,
      "learning_rate": 4.286384006993105e-08,
      "loss": 1.0502,
      "step": 10395
    },
    {
      "epoch": 97.16033009136457,
      "grad_norm": 0.5607461333274841,
      "learning_rate": 4.258434070587236e-08,
      "loss": 1.0498,
      "step": 10396
    },
    {
      "epoch": 97.16976127320955,
      "grad_norm": 0.5624262690544128,
      "learning_rate": 4.2305753636939784e-08,
      "loss": 1.0244,
      "step": 10397
    },
    {
      "epoch": 97.17919245505452,
      "grad_norm": 0.5707807540893555,
      "learning_rate": 4.2028078888656235e-08,
      "loss": 1.0342,
      "step": 10398
    },
    {
      "epoch": 97.1886236368995,
      "grad_norm": 0.5275754928588867,
      "learning_rate": 4.175131648646469e-08,
      "loss": 1.0656,
      "step": 10399
    },
    {
      "epoch": 97.19805481874447,
      "grad_norm": 0.5913956165313721,
      "learning_rate": 4.1475466455720424e-08,
      "loss": 1.0503,
      "step": 10400
    },
    {
      "epoch": 97.20748600058945,
      "grad_norm": 0.5331577658653259,
      "learning_rate": 4.120052882169767e-08,
      "loss": 1.069,
      "step": 10401
    },
    {
      "epoch": 97.21691718243443,
      "grad_norm": 0.5899927020072937,
      "learning_rate": 4.0926503609585164e-08,
      "loss": 1.0374,
      "step": 10402
    },
    {
      "epoch": 97.2263483642794,
      "grad_norm": 0.5878200531005859,
      "learning_rate": 4.0653390844488386e-08,
      "loss": 1.0276,
      "step": 10403
    },
    {
      "epoch": 97.23577954612438,
      "grad_norm": 0.5500660538673401,
      "learning_rate": 4.038119055143175e-08,
      "loss": 1.0196,
      "step": 10404
    },
    {
      "epoch": 97.24521072796935,
      "grad_norm": 0.5829811692237854,
      "learning_rate": 4.0109902755353094e-08,
      "loss": 1.0591,
      "step": 10405
    },
    {
      "epoch": 97.25464190981432,
      "grad_norm": 0.5791628956794739,
      "learning_rate": 3.9839527481108086e-08,
      "loss": 1.0037,
      "step": 10406
    },
    {
      "epoch": 97.2640730916593,
      "grad_norm": 0.5513231754302979,
      "learning_rate": 3.957006475346803e-08,
      "loss": 1.0602,
      "step": 10407
    },
    {
      "epoch": 97.27350427350427,
      "grad_norm": 0.5403449535369873,
      "learning_rate": 3.9301514597120946e-08,
      "loss": 1.0491,
      "step": 10408
    },
    {
      "epoch": 97.28293545534925,
      "grad_norm": 0.5093697905540466,
      "learning_rate": 3.9033877036672716e-08,
      "loss": 1.0519,
      "step": 10409
    },
    {
      "epoch": 97.29236663719422,
      "grad_norm": 0.5736224055290222,
      "learning_rate": 3.8767152096641504e-08,
      "loss": 1.0148,
      "step": 10410
    },
    {
      "epoch": 97.3017978190392,
      "grad_norm": 0.5381157994270325,
      "learning_rate": 3.850133980146664e-08,
      "loss": 1.0376,
      "step": 10411
    },
    {
      "epoch": 97.31122900088417,
      "grad_norm": 0.5592568516731262,
      "learning_rate": 3.823644017550088e-08,
      "loss": 1.0707,
      "step": 10412
    },
    {
      "epoch": 97.32066018272914,
      "grad_norm": 0.5921488404273987,
      "learning_rate": 3.79724532430148e-08,
      "loss": 1.0464,
      "step": 10413
    },
    {
      "epoch": 97.33009136457412,
      "grad_norm": 0.5404900908470154,
      "learning_rate": 3.7709379028194605e-08,
      "loss": 1.0362,
      "step": 10414
    },
    {
      "epoch": 97.3395225464191,
      "grad_norm": 0.530998170375824,
      "learning_rate": 3.744721755514325e-08,
      "loss": 1.0127,
      "step": 10415
    },
    {
      "epoch": 97.34895372826408,
      "grad_norm": 0.5661420822143555,
      "learning_rate": 3.718596884787928e-08,
      "loss": 1.034,
      "step": 10416
    },
    {
      "epoch": 97.35838491010905,
      "grad_norm": 0.5781912803649902,
      "learning_rate": 3.6925632930339125e-08,
      "loss": 1.0304,
      "step": 10417
    },
    {
      "epoch": 97.36781609195403,
      "grad_norm": 0.5698223114013672,
      "learning_rate": 3.66662098263737e-08,
      "loss": 1.0252,
      "step": 10418
    },
    {
      "epoch": 97.377247273799,
      "grad_norm": 0.5431063175201416,
      "learning_rate": 3.640769955975176e-08,
      "loss": 1.0809,
      "step": 10419
    },
    {
      "epoch": 97.38667845564397,
      "grad_norm": 0.5866710543632507,
      "learning_rate": 3.615010215415771e-08,
      "loss": 1.0628,
      "step": 10420
    },
    {
      "epoch": 97.39610963748895,
      "grad_norm": 0.5506141185760498,
      "learning_rate": 3.589341763319265e-08,
      "loss": 1.0146,
      "step": 10421
    },
    {
      "epoch": 97.40554081933392,
      "grad_norm": 0.5274965167045593,
      "learning_rate": 3.563764602037445e-08,
      "loss": 1.0634,
      "step": 10422
    },
    {
      "epoch": 97.4149720011789,
      "grad_norm": 0.5116817355155945,
      "learning_rate": 3.538278733913547e-08,
      "loss": 1.0523,
      "step": 10423
    },
    {
      "epoch": 97.42440318302387,
      "grad_norm": 0.5444389581680298,
      "learning_rate": 3.5128841612827036e-08,
      "loss": 1.0256,
      "step": 10424
    },
    {
      "epoch": 97.43383436486884,
      "grad_norm": 0.551913321018219,
      "learning_rate": 3.487580886471609e-08,
      "loss": 1.0765,
      "step": 10425
    },
    {
      "epoch": 97.44326554671382,
      "grad_norm": 0.5355167984962463,
      "learning_rate": 3.462368911798408e-08,
      "loss": 1.0513,
      "step": 10426
    },
    {
      "epoch": 97.45269672855879,
      "grad_norm": 0.5351812243461609,
      "learning_rate": 3.4372482395730325e-08,
      "loss": 1.07,
      "step": 10427
    },
    {
      "epoch": 97.46212791040377,
      "grad_norm": 0.5386921167373657,
      "learning_rate": 3.412218872096973e-08,
      "loss": 1.0235,
      "step": 10428
    },
    {
      "epoch": 97.47155909224875,
      "grad_norm": 0.584823489189148,
      "learning_rate": 3.387280811663618e-08,
      "loss": 1.0283,
      "step": 10429
    },
    {
      "epoch": 97.48099027409373,
      "grad_norm": 0.5556789040565491,
      "learning_rate": 3.362434060557584e-08,
      "loss": 1.0242,
      "step": 10430
    },
    {
      "epoch": 97.4904214559387,
      "grad_norm": 0.5228462219238281,
      "learning_rate": 3.337678621055385e-08,
      "loss": 1.0618,
      "step": 10431
    },
    {
      "epoch": 97.49985263778368,
      "grad_norm": 0.5316183567047119,
      "learning_rate": 3.3130144954252044e-08,
      "loss": 1.0525,
      "step": 10432
    },
    {
      "epoch": 97.50928381962865,
      "grad_norm": 0.5439006686210632,
      "learning_rate": 3.28844168592668e-08,
      "loss": 1.0785,
      "step": 10433
    },
    {
      "epoch": 97.51871500147362,
      "grad_norm": 0.6340062022209167,
      "learning_rate": 3.2639601948111224e-08,
      "loss": 1.015,
      "step": 10434
    },
    {
      "epoch": 97.5281461833186,
      "grad_norm": 0.5767896771430969,
      "learning_rate": 3.2395700243215146e-08,
      "loss": 1.0205,
      "step": 10435
    },
    {
      "epoch": 97.53757736516357,
      "grad_norm": 0.5303762555122375,
      "learning_rate": 3.215271176692514e-08,
      "loss": 1.0364,
      "step": 10436
    },
    {
      "epoch": 97.54700854700855,
      "grad_norm": 0.5534213781356812,
      "learning_rate": 3.19106365415045e-08,
      "loss": 1.0672,
      "step": 10437
    },
    {
      "epoch": 97.55643972885352,
      "grad_norm": 0.6164452433586121,
      "learning_rate": 3.166947458913106e-08,
      "loss": 1.0738,
      "step": 10438
    },
    {
      "epoch": 97.5658709106985,
      "grad_norm": 0.5334014296531677,
      "learning_rate": 3.142922593189934e-08,
      "loss": 1.0864,
      "step": 10439
    },
    {
      "epoch": 97.57530209254347,
      "grad_norm": 0.5339583158493042,
      "learning_rate": 3.118989059182287e-08,
      "loss": 1.083,
      "step": 10440
    },
    {
      "epoch": 97.58473327438844,
      "grad_norm": 0.5552154183387756,
      "learning_rate": 3.0951468590828537e-08,
      "loss": 1.0508,
      "step": 10441
    },
    {
      "epoch": 97.59416445623341,
      "grad_norm": 0.5315287709236145,
      "learning_rate": 3.071395995075998e-08,
      "loss": 1.0495,
      "step": 10442
    },
    {
      "epoch": 97.6035956380784,
      "grad_norm": 0.6418206095695496,
      "learning_rate": 3.047736469337759e-08,
      "loss": 1.0887,
      "step": 10443
    },
    {
      "epoch": 97.61302681992338,
      "grad_norm": 0.5801669955253601,
      "learning_rate": 3.0241682840358445e-08,
      "loss": 1.0291,
      "step": 10444
    },
    {
      "epoch": 97.62245800176835,
      "grad_norm": 0.5395966172218323,
      "learning_rate": 3.00069144132964e-08,
      "loss": 1.0751,
      "step": 10445
    },
    {
      "epoch": 97.63188918361332,
      "grad_norm": 0.5370096564292908,
      "learning_rate": 2.9773059433699792e-08,
      "loss": 1.0441,
      "step": 10446
    },
    {
      "epoch": 97.6413203654583,
      "grad_norm": 0.5661206245422363,
      "learning_rate": 2.9540117922994825e-08,
      "loss": 1.019,
      "step": 10447
    },
    {
      "epoch": 97.65075154730327,
      "grad_norm": 0.591839075088501,
      "learning_rate": 2.930808990252332e-08,
      "loss": 1.0821,
      "step": 10448
    },
    {
      "epoch": 97.66018272914825,
      "grad_norm": 0.5257779359817505,
      "learning_rate": 2.9076975393544926e-08,
      "loss": 1.0526,
      "step": 10449
    },
    {
      "epoch": 97.66961391099322,
      "grad_norm": 0.521787166595459,
      "learning_rate": 2.8846774417231606e-08,
      "loss": 1.025,
      "step": 10450
    },
    {
      "epoch": 97.6790450928382,
      "grad_norm": 0.5505455136299133,
      "learning_rate": 2.861748699467648e-08,
      "loss": 1.0432,
      "step": 10451
    },
    {
      "epoch": 97.68847627468317,
      "grad_norm": 0.5795347690582275,
      "learning_rate": 2.838911314688608e-08,
      "loss": 1.0678,
      "step": 10452
    },
    {
      "epoch": 97.69790745652814,
      "grad_norm": 0.5730823278427124,
      "learning_rate": 2.8161652894783675e-08,
      "loss": 1.0388,
      "step": 10453
    },
    {
      "epoch": 97.70733863837312,
      "grad_norm": 0.5706762671470642,
      "learning_rate": 2.7935106259209253e-08,
      "loss": 1.0395,
      "step": 10454
    },
    {
      "epoch": 97.71676982021809,
      "grad_norm": 0.5330668687820435,
      "learning_rate": 2.770947326091955e-08,
      "loss": 1.0832,
      "step": 10455
    },
    {
      "epoch": 97.72620100206306,
      "grad_norm": 0.5524596571922302,
      "learning_rate": 2.7484753920586914e-08,
      "loss": 1.0269,
      "step": 10456
    },
    {
      "epoch": 97.73563218390805,
      "grad_norm": 0.5744212865829468,
      "learning_rate": 2.7260948258799325e-08,
      "loss": 1.0393,
      "step": 10457
    },
    {
      "epoch": 97.74506336575303,
      "grad_norm": 0.6149356961250305,
      "learning_rate": 2.703805629606149e-08,
      "loss": 1.0397,
      "step": 10458
    },
    {
      "epoch": 97.754494547598,
      "grad_norm": 0.5242927670478821,
      "learning_rate": 2.681607805279596e-08,
      "loss": 1.0432,
      "step": 10459
    },
    {
      "epoch": 97.76392572944297,
      "grad_norm": 0.5408129096031189,
      "learning_rate": 2.659501354933869e-08,
      "loss": 1.0533,
      "step": 10460
    },
    {
      "epoch": 97.77335691128795,
      "grad_norm": 0.5762828588485718,
      "learning_rate": 2.63748628059457e-08,
      "loss": 1.0527,
      "step": 10461
    },
    {
      "epoch": 97.78278809313292,
      "grad_norm": 0.5169336795806885,
      "learning_rate": 2.6155625842785304e-08,
      "loss": 1.0702,
      "step": 10462
    },
    {
      "epoch": 97.7922192749779,
      "grad_norm": 0.5562761425971985,
      "learning_rate": 2.5937302679944766e-08,
      "loss": 1.0548,
      "step": 10463
    },
    {
      "epoch": 97.80165045682287,
      "grad_norm": 0.5720221400260925,
      "learning_rate": 2.571989333742697e-08,
      "loss": 1.0195,
      "step": 10464
    },
    {
      "epoch": 97.81108163866784,
      "grad_norm": 0.5370345115661621,
      "learning_rate": 2.5503397835149324e-08,
      "loss": 1.0566,
      "step": 10465
    },
    {
      "epoch": 97.82051282051282,
      "grad_norm": 0.5707229971885681,
      "learning_rate": 2.5287816192949287e-08,
      "loss": 1.0543,
      "step": 10466
    },
    {
      "epoch": 97.82994400235779,
      "grad_norm": 0.5631248354911804,
      "learning_rate": 2.5073148430576622e-08,
      "loss": 1.0666,
      "step": 10467
    },
    {
      "epoch": 97.83937518420277,
      "grad_norm": 0.5579853057861328,
      "learning_rate": 2.4859394567700034e-08,
      "loss": 1.0466,
      "step": 10468
    },
    {
      "epoch": 97.84880636604774,
      "grad_norm": 0.5884650945663452,
      "learning_rate": 2.464655462390386e-08,
      "loss": 1.0328,
      "step": 10469
    },
    {
      "epoch": 97.85823754789271,
      "grad_norm": 0.5669037699699402,
      "learning_rate": 2.4434628618688062e-08,
      "loss": 1.0358,
      "step": 10470
    },
    {
      "epoch": 97.8676687297377,
      "grad_norm": 0.569964587688446,
      "learning_rate": 2.422361657146821e-08,
      "loss": 1.0148,
      "step": 10471
    },
    {
      "epoch": 97.87709991158268,
      "grad_norm": 0.5579004287719727,
      "learning_rate": 2.401351850157885e-08,
      "loss": 1.0446,
      "step": 10472
    },
    {
      "epoch": 97.88653109342765,
      "grad_norm": 0.58517986536026,
      "learning_rate": 2.3804334428267905e-08,
      "loss": 1.0412,
      "step": 10473
    },
    {
      "epoch": 97.89596227527262,
      "grad_norm": 0.5260120034217834,
      "learning_rate": 2.3596064370701166e-08,
      "loss": 1.0218,
      "step": 10474
    },
    {
      "epoch": 97.9053934571176,
      "grad_norm": 0.5523197054862976,
      "learning_rate": 2.338870834796003e-08,
      "loss": 1.0617,
      "step": 10475
    },
    {
      "epoch": 97.91482463896257,
      "grad_norm": 0.54395991563797,
      "learning_rate": 2.318226637904264e-08,
      "loss": 1.0385,
      "step": 10476
    },
    {
      "epoch": 97.92425582080755,
      "grad_norm": 0.58058100938797,
      "learning_rate": 2.2976738482863857e-08,
      "loss": 1.0088,
      "step": 10477
    },
    {
      "epoch": 97.93368700265252,
      "grad_norm": 0.5683440566062927,
      "learning_rate": 2.2772124678253073e-08,
      "loss": 1.0279,
      "step": 10478
    },
    {
      "epoch": 97.9431181844975,
      "grad_norm": 0.5251719355583191,
      "learning_rate": 2.2568424983956392e-08,
      "loss": 1.0709,
      "step": 10479
    },
    {
      "epoch": 97.95254936634247,
      "grad_norm": 0.6025364398956299,
      "learning_rate": 2.2365639418637786e-08,
      "loss": 1.0779,
      "step": 10480
    },
    {
      "epoch": 97.96198054818744,
      "grad_norm": 0.531268835067749,
      "learning_rate": 2.216376800087683e-08,
      "loss": 1.0432,
      "step": 10481
    },
    {
      "epoch": 97.97141173003241,
      "grad_norm": 0.5309773087501526,
      "learning_rate": 2.196281074916762e-08,
      "loss": 1.0723,
      "step": 10482
    },
    {
      "epoch": 97.98084291187739,
      "grad_norm": 0.5308120250701904,
      "learning_rate": 2.176276768192209e-08,
      "loss": 1.0462,
      "step": 10483
    },
    {
      "epoch": 97.99027409372236,
      "grad_norm": 0.5412836074829102,
      "learning_rate": 2.1563638817468924e-08,
      "loss": 1.0221,
      "step": 10484
    },
    {
      "epoch": 97.99970527556735,
      "grad_norm": 0.5602365136146545,
      "learning_rate": 2.1365424174052406e-08,
      "loss": 1.0352,
      "step": 10485
    },
    {
      "epoch": 98.0,
      "grad_norm": 2.5351476669311523,
      "learning_rate": 2.116812376983135e-08,
      "loss": 0.7549,
      "step": 10486
    },
    {
      "epoch": 98.00943118184497,
      "grad_norm": 0.5206832885742188,
      "learning_rate": 2.0971737622883515e-08,
      "loss": 1.0405,
      "step": 10487
    },
    {
      "epoch": 98.01886236368995,
      "grad_norm": 0.5183752775192261,
      "learning_rate": 2.0776265751201174e-08,
      "loss": 1.0584,
      "step": 10488
    },
    {
      "epoch": 98.02829354553492,
      "grad_norm": 0.5865477323532104,
      "learning_rate": 2.0581708172693337e-08,
      "loss": 1.0635,
      "step": 10489
    },
    {
      "epoch": 98.0377247273799,
      "grad_norm": 0.5127312541007996,
      "learning_rate": 2.0388064905186855e-08,
      "loss": 1.0447,
      "step": 10490
    },
    {
      "epoch": 98.04715590922487,
      "grad_norm": 0.5638535618782043,
      "learning_rate": 2.019533596642087e-08,
      "loss": 1.0289,
      "step": 10491
    },
    {
      "epoch": 98.05658709106984,
      "grad_norm": 0.5441901683807373,
      "learning_rate": 2.000352137405459e-08,
      "loss": 1.0304,
      "step": 10492
    },
    {
      "epoch": 98.06601827291482,
      "grad_norm": 0.5450900793075562,
      "learning_rate": 1.9812621145662848e-08,
      "loss": 1.1071,
      "step": 10493
    },
    {
      "epoch": 98.0754494547598,
      "grad_norm": 0.5658814907073975,
      "learning_rate": 1.9622635298733872e-08,
      "loss": 1.0655,
      "step": 10494
    },
    {
      "epoch": 98.08488063660478,
      "grad_norm": 0.5308349132537842,
      "learning_rate": 1.943356385067485e-08,
      "loss": 1.0699,
      "step": 10495
    },
    {
      "epoch": 98.09431181844975,
      "grad_norm": 0.532016932964325,
      "learning_rate": 1.924540681880749e-08,
      "loss": 1.0434,
      "step": 10496
    },
    {
      "epoch": 98.10374300029473,
      "grad_norm": 0.5688713788986206,
      "learning_rate": 1.905816422037354e-08,
      "loss": 1.0294,
      "step": 10497
    },
    {
      "epoch": 98.1131741821397,
      "grad_norm": 0.5420016050338745,
      "learning_rate": 1.887183607252596e-08,
      "loss": 1.059,
      "step": 10498
    },
    {
      "epoch": 98.12260536398468,
      "grad_norm": 0.5440453290939331,
      "learning_rate": 1.8686422392335535e-08,
      "loss": 1.0689,
      "step": 10499
    },
    {
      "epoch": 98.13203654582965,
      "grad_norm": 0.5717858076095581,
      "learning_rate": 1.8501923196792003e-08,
      "loss": 1.0725,
      "step": 10500
    },
    {
      "epoch": 98.14146772767462,
      "grad_norm": 0.5699636340141296,
      "learning_rate": 1.8318338502796297e-08,
      "loss": 1.0506,
      "step": 10501
    },
    {
      "epoch": 98.1508989095196,
      "grad_norm": 0.5319274067878723,
      "learning_rate": 1.813566832717162e-08,
      "loss": 1.0481,
      "step": 10502
    },
    {
      "epoch": 98.16033009136457,
      "grad_norm": 0.5696085691452026,
      "learning_rate": 1.7953912686651254e-08,
      "loss": 1.0598,
      "step": 10503
    },
    {
      "epoch": 98.16976127320955,
      "grad_norm": 0.5534284114837646,
      "learning_rate": 1.7773071597888548e-08,
      "loss": 1.0453,
      "step": 10504
    },
    {
      "epoch": 98.17919245505452,
      "grad_norm": 0.5565844178199768,
      "learning_rate": 1.7593145077453577e-08,
      "loss": 1.0321,
      "step": 10505
    },
    {
      "epoch": 98.1886236368995,
      "grad_norm": 0.5834603309631348,
      "learning_rate": 1.7414133141828714e-08,
      "loss": 1.062,
      "step": 10506
    },
    {
      "epoch": 98.19805481874447,
      "grad_norm": 0.5478527545928955,
      "learning_rate": 1.7236035807416397e-08,
      "loss": 1.0981,
      "step": 10507
    },
    {
      "epoch": 98.20748600058945,
      "grad_norm": 0.5290865302085876,
      "learning_rate": 1.7058853090534677e-08,
      "loss": 1.0528,
      "step": 10508
    },
    {
      "epoch": 98.21691718243443,
      "grad_norm": 0.5524842143058777,
      "learning_rate": 1.688258500741502e-08,
      "loss": 1.0234,
      "step": 10509
    },
    {
      "epoch": 98.2263483642794,
      "grad_norm": 0.5564079284667969,
      "learning_rate": 1.670723157420784e-08,
      "loss": 1.0457,
      "step": 10510
    },
    {
      "epoch": 98.23577954612438,
      "grad_norm": 0.536429762840271,
      "learning_rate": 1.653279280697917e-08,
      "loss": 1.0788,
      "step": 10511
    },
    {
      "epoch": 98.24521072796935,
      "grad_norm": 0.5473575592041016,
      "learning_rate": 1.6359268721710674e-08,
      "loss": 1.0369,
      "step": 10512
    },
    {
      "epoch": 98.25464190981432,
      "grad_norm": 0.5201419591903687,
      "learning_rate": 1.6186659334301857e-08,
      "loss": 1.0268,
      "step": 10513
    },
    {
      "epoch": 98.2640730916593,
      "grad_norm": 0.5927953124046326,
      "learning_rate": 1.6014964660565625e-08,
      "loss": 1.0412,
      "step": 10514
    },
    {
      "epoch": 98.27350427350427,
      "grad_norm": 0.6404536366462708,
      "learning_rate": 1.5844184716232725e-08,
      "loss": 1.012,
      "step": 10515
    },
    {
      "epoch": 98.28293545534925,
      "grad_norm": 0.551613986492157,
      "learning_rate": 1.567431951695064e-08,
      "loss": 1.0617,
      "step": 10516
    },
    {
      "epoch": 98.29236663719422,
      "grad_norm": 0.519995927810669,
      "learning_rate": 1.5505369078281373e-08,
      "loss": 1.0666,
      "step": 10517
    },
    {
      "epoch": 98.3017978190392,
      "grad_norm": 0.5499351620674133,
      "learning_rate": 1.533733341570476e-08,
      "loss": 1.0601,
      "step": 10518
    },
    {
      "epoch": 98.31122900088417,
      "grad_norm": 0.5555222630500793,
      "learning_rate": 1.517021254461626e-08,
      "loss": 1.0488,
      "step": 10519
    },
    {
      "epoch": 98.32066018272914,
      "grad_norm": 0.5630189776420593,
      "learning_rate": 1.500400648032807e-08,
      "loss": 1.0605,
      "step": 10520
    },
    {
      "epoch": 98.33009136457412,
      "grad_norm": 0.5701401233673096,
      "learning_rate": 1.4838715238066903e-08,
      "loss": 1.0481,
      "step": 10521
    },
    {
      "epoch": 98.3395225464191,
      "grad_norm": 0.5650681257247925,
      "learning_rate": 1.4674338832977309e-08,
      "loss": 1.0142,
      "step": 10522
    },
    {
      "epoch": 98.34895372826408,
      "grad_norm": 0.5388820171356201,
      "learning_rate": 1.4510877280118352e-08,
      "loss": 1.0443,
      "step": 10523
    },
    {
      "epoch": 98.35838491010905,
      "grad_norm": 0.5431369543075562,
      "learning_rate": 1.4348330594466941e-08,
      "loss": 1.0392,
      "step": 10524
    },
    {
      "epoch": 98.36781609195403,
      "grad_norm": 0.5530590415000916,
      "learning_rate": 1.418669879091672e-08,
      "loss": 1.0339,
      "step": 10525
    },
    {
      "epoch": 98.377247273799,
      "grad_norm": 0.5129091143608093,
      "learning_rate": 1.4025981884273621e-08,
      "loss": 1.0385,
      "step": 10526
    },
    {
      "epoch": 98.38667845564397,
      "grad_norm": 0.5803783535957336,
      "learning_rate": 1.3866179889265863e-08,
      "loss": 1.0377,
      "step": 10527
    },
    {
      "epoch": 98.39610963748895,
      "grad_norm": 0.550931453704834,
      "learning_rate": 1.3707292820531737e-08,
      "loss": 1.0534,
      "step": 10528
    },
    {
      "epoch": 98.40554081933392,
      "grad_norm": 0.5369125008583069,
      "learning_rate": 1.3549320692629596e-08,
      "loss": 1.0623,
      "step": 10529
    },
    {
      "epoch": 98.4149720011789,
      "grad_norm": 0.5718105435371399,
      "learning_rate": 1.3392263520033422e-08,
      "loss": 1.0846,
      "step": 10530
    },
    {
      "epoch": 98.42440318302387,
      "grad_norm": 0.5490407347679138,
      "learning_rate": 1.323612131713059e-08,
      "loss": 1.0209,
      "step": 10531
    },
    {
      "epoch": 98.43383436486884,
      "grad_norm": 0.527187168598175,
      "learning_rate": 1.3080894098228547e-08,
      "loss": 1.0872,
      "step": 10532
    },
    {
      "epoch": 98.44326554671382,
      "grad_norm": 0.5366694927215576,
      "learning_rate": 1.2926581877549249e-08,
      "loss": 1.046,
      "step": 10533
    },
    {
      "epoch": 98.45269672855879,
      "grad_norm": 0.5246197581291199,
      "learning_rate": 1.2773184669229166e-08,
      "loss": 1.0573,
      "step": 10534
    },
    {
      "epoch": 98.46212791040377,
      "grad_norm": 0.5494959950447083,
      "learning_rate": 1.2620702487324832e-08,
      "loss": 1.0458,
      "step": 10535
    },
    {
      "epoch": 98.47155909224875,
      "grad_norm": 0.5304049849510193,
      "learning_rate": 1.2469135345805073e-08,
      "loss": 1.0479,
      "step": 10536
    },
    {
      "epoch": 98.48099027409373,
      "grad_norm": 0.5495455265045166,
      "learning_rate": 1.2318483258556557e-08,
      "loss": 1.0884,
      "step": 10537
    },
    {
      "epoch": 98.4904214559387,
      "grad_norm": 0.5482740998268127,
      "learning_rate": 1.2168746239381579e-08,
      "loss": 1.0755,
      "step": 10538
    },
    {
      "epoch": 98.49985263778368,
      "grad_norm": 0.5348977446556091,
      "learning_rate": 1.2019924302000275e-08,
      "loss": 1.0383,
      "step": 10539
    },
    {
      "epoch": 98.50928381962865,
      "grad_norm": 0.5916488766670227,
      "learning_rate": 1.1872017460046181e-08,
      "loss": 1.0452,
      "step": 10540
    },
    {
      "epoch": 98.51871500147362,
      "grad_norm": 0.5994441509246826,
      "learning_rate": 1.1725025727071792e-08,
      "loss": 1.0699,
      "step": 10541
    },
    {
      "epoch": 98.5281461833186,
      "grad_norm": 0.5541244149208069,
      "learning_rate": 1.1578949116543003e-08,
      "loss": 1.0447,
      "step": 10542
    },
    {
      "epoch": 98.53757736516357,
      "grad_norm": 0.5720219016075134,
      "learning_rate": 1.1433787641844662e-08,
      "loss": 1.0451,
      "step": 10543
    },
    {
      "epoch": 98.54700854700855,
      "grad_norm": 0.5401592254638672,
      "learning_rate": 1.1289541316276132e-08,
      "loss": 1.0511,
      "step": 10544
    },
    {
      "epoch": 98.55643972885352,
      "grad_norm": 0.5456350445747375,
      "learning_rate": 1.1146210153053505e-08,
      "loss": 1.0265,
      "step": 10545
    },
    {
      "epoch": 98.5658709106985,
      "grad_norm": 0.5340506434440613,
      "learning_rate": 1.1003794165307391e-08,
      "loss": 1.0281,
      "step": 10546
    },
    {
      "epoch": 98.57530209254347,
      "grad_norm": 0.5583227872848511,
      "learning_rate": 1.0862293366087351e-08,
      "loss": 1.0391,
      "step": 10547
    },
    {
      "epoch": 98.58473327438844,
      "grad_norm": 0.5670391917228699,
      "learning_rate": 1.0721707768356349e-08,
      "loss": 1.0334,
      "step": 10548
    },
    {
      "epoch": 98.59416445623341,
      "grad_norm": 0.5346813201904297,
      "learning_rate": 1.0582037384995192e-08,
      "loss": 1.0309,
      "step": 10549
    },
    {
      "epoch": 98.6035956380784,
      "grad_norm": 0.5756195783615112,
      "learning_rate": 1.0443282228801422e-08,
      "loss": 1.0277,
      "step": 10550
    },
    {
      "epoch": 98.61302681992338,
      "grad_norm": 0.5430440306663513,
      "learning_rate": 1.0305442312487091e-08,
      "loss": 1.0252,
      "step": 10551
    },
    {
      "epoch": 98.62245800176835,
      "grad_norm": 0.5381767153739929,
      "learning_rate": 1.0168517648680987e-08,
      "loss": 1.0661,
      "step": 10552
    },
    {
      "epoch": 98.63188918361332,
      "grad_norm": 0.5304387807846069,
      "learning_rate": 1.003250824992752e-08,
      "loss": 1.0352,
      "step": 10553
    },
    {
      "epoch": 98.6413203654583,
      "grad_norm": 0.5491499304771423,
      "learning_rate": 9.897414128688942e-09,
      "loss": 1.0327,
      "step": 10554
    },
    {
      "epoch": 98.65075154730327,
      "grad_norm": 0.5805649757385254,
      "learning_rate": 9.763235297342022e-09,
      "loss": 1.0785,
      "step": 10555
    },
    {
      "epoch": 98.66018272914825,
      "grad_norm": 0.5338699817657471,
      "learning_rate": 9.629971768179148e-09,
      "loss": 1.054,
      "step": 10556
    },
    {
      "epoch": 98.66961391099322,
      "grad_norm": 0.5501903295516968,
      "learning_rate": 9.497623553411662e-09,
      "loss": 1.0579,
      "step": 10557
    },
    {
      "epoch": 98.6790450928382,
      "grad_norm": 0.5360878109931946,
      "learning_rate": 9.36619066516431e-09,
      "loss": 1.0479,
      "step": 10558
    },
    {
      "epoch": 98.68847627468317,
      "grad_norm": 0.5377949476242065,
      "learning_rate": 9.235673115479682e-09,
      "loss": 1.0408,
      "step": 10559
    },
    {
      "epoch": 98.69790745652814,
      "grad_norm": 0.5579367280006409,
      "learning_rate": 9.10607091631488e-09,
      "loss": 1.058,
      "step": 10560
    },
    {
      "epoch": 98.70733863837312,
      "grad_norm": 0.5350939631462097,
      "learning_rate": 8.977384079543739e-09,
      "loss": 1.0318,
      "step": 10561
    },
    {
      "epoch": 98.71676982021809,
      "grad_norm": 0.561131477355957,
      "learning_rate": 8.849612616957936e-09,
      "loss": 1.0685,
      "step": 10562
    },
    {
      "epoch": 98.72620100206306,
      "grad_norm": 0.5676510334014893,
      "learning_rate": 8.722756540261446e-09,
      "loss": 1.0449,
      "step": 10563
    },
    {
      "epoch": 98.73563218390805,
      "grad_norm": 0.5268195867538452,
      "learning_rate": 8.59681586107941e-09,
      "loss": 1.0071,
      "step": 10564
    },
    {
      "epoch": 98.74506336575303,
      "grad_norm": 0.5524210333824158,
      "learning_rate": 8.47179059095038e-09,
      "loss": 1.0725,
      "step": 10565
    },
    {
      "epoch": 98.754494547598,
      "grad_norm": 0.5219919085502625,
      "learning_rate": 8.347680741326303e-09,
      "loss": 1.0496,
      "step": 10566
    },
    {
      "epoch": 98.76392572944297,
      "grad_norm": 0.5033782124519348,
      "learning_rate": 8.224486323581415e-09,
      "loss": 1.0412,
      "step": 10567
    },
    {
      "epoch": 98.77335691128795,
      "grad_norm": 0.6119896173477173,
      "learning_rate": 8.102207349000024e-09,
      "loss": 0.9975,
      "step": 10568
    },
    {
      "epoch": 98.78278809313292,
      "grad_norm": 0.5524001717567444,
      "learning_rate": 7.98084382878761e-09,
      "loss": 1.0682,
      "step": 10569
    },
    {
      "epoch": 98.7922192749779,
      "grad_norm": 0.564711332321167,
      "learning_rate": 7.860395774060836e-09,
      "loss": 1.0346,
      "step": 10570
    },
    {
      "epoch": 98.80165045682287,
      "grad_norm": 0.5600850582122803,
      "learning_rate": 7.74086319585754e-09,
      "loss": 1.0257,
      "step": 10571
    },
    {
      "epoch": 98.81108163866784,
      "grad_norm": 0.5253886580467224,
      "learning_rate": 7.622246105128961e-09,
      "loss": 1.0418,
      "step": 10572
    },
    {
      "epoch": 98.82051282051282,
      "grad_norm": 0.5600643754005432,
      "learning_rate": 7.504544512740852e-09,
      "loss": 1.0459,
      "step": 10573
    },
    {
      "epoch": 98.82994400235779,
      "grad_norm": 0.5620288848876953,
      "learning_rate": 7.387758429479031e-09,
      "loss": 1.0223,
      "step": 10574
    },
    {
      "epoch": 98.83937518420277,
      "grad_norm": 0.5422400236129761,
      "learning_rate": 7.271887866042715e-09,
      "loss": 1.0357,
      "step": 10575
    },
    {
      "epoch": 98.84880636604774,
      "grad_norm": 0.5341929197311401,
      "learning_rate": 7.156932833047858e-09,
      "loss": 1.0196,
      "step": 10576
    },
    {
      "epoch": 98.85823754789271,
      "grad_norm": 0.5433602929115295,
      "learning_rate": 7.042893341027146e-09,
      "loss": 1.0866,
      "step": 10577
    },
    {
      "epoch": 98.8676687297377,
      "grad_norm": 0.5048835873603821,
      "learning_rate": 6.929769400426667e-09,
      "loss": 1.059,
      "step": 10578
    },
    {
      "epoch": 98.87709991158268,
      "grad_norm": 0.558380663394928,
      "learning_rate": 6.8175610216136836e-09,
      "loss": 1.0286,
      "step": 10579
    },
    {
      "epoch": 98.88653109342765,
      "grad_norm": 0.5926192998886108,
      "learning_rate": 6.70626821486775e-09,
      "loss": 1.0556,
      "step": 10580
    },
    {
      "epoch": 98.89596227527262,
      "grad_norm": 0.604888916015625,
      "learning_rate": 6.595890990384046e-09,
      "loss": 1.0378,
      "step": 10581
    },
    {
      "epoch": 98.9053934571176,
      "grad_norm": 0.5558484792709351,
      "learning_rate": 6.486429358276703e-09,
      "loss": 1.035,
      "step": 10582
    },
    {
      "epoch": 98.91482463896257,
      "grad_norm": 0.5634481310844421,
      "learning_rate": 6.377883328575474e-09,
      "loss": 1.0735,
      "step": 10583
    },
    {
      "epoch": 98.92425582080755,
      "grad_norm": 0.5627099275588989,
      "learning_rate": 6.270252911222408e-09,
      "loss": 1.0195,
      "step": 10584
    },
    {
      "epoch": 98.93368700265252,
      "grad_norm": 0.5577492713928223,
      "learning_rate": 6.163538116081835e-09,
      "loss": 1.0505,
      "step": 10585
    },
    {
      "epoch": 98.9431181844975,
      "grad_norm": 0.5008352994918823,
      "learning_rate": 6.0577389529281605e-09,
      "loss": 1.0555,
      "step": 10586
    },
    {
      "epoch": 98.95254936634247,
      "grad_norm": 0.5294960737228394,
      "learning_rate": 5.9528554314558505e-09,
      "loss": 1.0317,
      "step": 10587
    },
    {
      "epoch": 98.96198054818744,
      "grad_norm": 0.5740886330604553,
      "learning_rate": 5.848887561274996e-09,
      "loss": 1.0512,
      "step": 10588
    },
    {
      "epoch": 98.97141173003241,
      "grad_norm": 0.528019368648529,
      "learning_rate": 5.7458353519102e-09,
      "loss": 1.0519,
      "step": 10589
    },
    {
      "epoch": 98.98084291187739,
      "grad_norm": 0.5457256436347961,
      "learning_rate": 5.64369881280391e-09,
      "loss": 1.0581,
      "step": 10590
    },
    {
      "epoch": 98.99027409372236,
      "grad_norm": 0.5586851239204407,
      "learning_rate": 5.542477953311975e-09,
      "loss": 1.0403,
      "step": 10591
    },
    {
      "epoch": 98.99970527556735,
      "grad_norm": 0.5056937336921692,
      "learning_rate": 5.4421727827103086e-09,
      "loss": 1.0725,
      "step": 10592
    },
    {
      "epoch": 99.0,
      "grad_norm": 2.4707891941070557,
      "learning_rate": 5.342783310188226e-09,
      "loss": 0.8735,
      "step": 10593
    },
    {
      "epoch": 99.00943118184497,
      "grad_norm": 0.5353374481201172,
      "learning_rate": 5.2443095448506674e-09,
      "loss": 1.0353,
      "step": 10594
    },
    {
      "epoch": 99.01886236368995,
      "grad_norm": 0.5749982595443726,
      "learning_rate": 5.146751495721525e-09,
      "loss": 1.0338,
      "step": 10595
    },
    {
      "epoch": 99.02829354553492,
      "grad_norm": 0.5840853452682495,
      "learning_rate": 5.050109171738094e-09,
      "loss": 1.0433,
      "step": 10596
    },
    {
      "epoch": 99.0377247273799,
      "grad_norm": 0.56104975938797,
      "learning_rate": 4.954382581754402e-09,
      "loss": 1.0435,
      "step": 10597
    },
    {
      "epoch": 99.04715590922487,
      "grad_norm": 0.521013617515564,
      "learning_rate": 4.859571734541213e-09,
      "loss": 1.0442,
      "step": 10598
    },
    {
      "epoch": 99.05658709106984,
      "grad_norm": 0.5664210319519043,
      "learning_rate": 4.7656766387849105e-09,
      "loss": 1.06,
      "step": 10599
    },
    {
      "epoch": 99.06601827291482,
      "grad_norm": 0.5842355489730835,
      "learning_rate": 4.672697303088613e-09,
      "loss": 1.0333,
      "step": 10600
    },
    {
      "epoch": 99.0754494547598,
      "grad_norm": 0.5805233716964722,
      "learning_rate": 4.580633735969953e-09,
      "loss": 1.0246,
      "step": 10601
    },
    {
      "epoch": 99.08488063660478,
      "grad_norm": 0.5250747799873352,
      "learning_rate": 4.489485945865513e-09,
      "loss": 1.0259,
      "step": 10602
    },
    {
      "epoch": 99.09431181844975,
      "grad_norm": 0.5486950278282166,
      "learning_rate": 4.399253941125281e-09,
      "loss": 1.0238,
      "step": 10603
    },
    {
      "epoch": 99.10374300029473,
      "grad_norm": 0.5700161457061768,
      "learning_rate": 4.309937730015978e-09,
      "loss": 1.0472,
      "step": 10604
    },
    {
      "epoch": 99.1131741821397,
      "grad_norm": 0.5656513571739197,
      "learning_rate": 4.221537320721058e-09,
      "loss": 1.0372,
      "step": 10605
    },
    {
      "epoch": 99.12260536398468,
      "grad_norm": 0.5622652769088745,
      "learning_rate": 4.134052721339599e-09,
      "loss": 1.0683,
      "step": 10606
    },
    {
      "epoch": 99.13203654582965,
      "grad_norm": 0.5922797918319702,
      "learning_rate": 4.047483939887409e-09,
      "loss": 1.0063,
      "step": 10607
    },
    {
      "epoch": 99.14146772767462,
      "grad_norm": 0.5582640171051025,
      "learning_rate": 3.961830984295922e-09,
      "loss": 1.0001,
      "step": 10608
    },
    {
      "epoch": 99.1508989095196,
      "grad_norm": 0.5662833452224731,
      "learning_rate": 3.877093862411086e-09,
      "loss": 1.0518,
      "step": 10609
    },
    {
      "epoch": 99.16033009136457,
      "grad_norm": 0.5533282160758972,
      "learning_rate": 3.7932725819977975e-09,
      "loss": 1.0638,
      "step": 10610
    },
    {
      "epoch": 99.16976127320955,
      "grad_norm": 0.5568610429763794,
      "learning_rate": 3.7103671507365822e-09,
      "loss": 1.0518,
      "step": 10611
    },
    {
      "epoch": 99.17919245505452,
      "grad_norm": 0.6113156676292419,
      "learning_rate": 3.6283775762213647e-09,
      "loss": 1.0594,
      "step": 10612
    },
    {
      "epoch": 99.1886236368995,
      "grad_norm": 0.5366033315658569,
      "learning_rate": 3.5473038659650237e-09,
      "loss": 1.0766,
      "step": 10613
    },
    {
      "epoch": 99.19805481874447,
      "grad_norm": 0.5315084457397461,
      "learning_rate": 3.467146027394952e-09,
      "loss": 1.0431,
      "step": 10614
    },
    {
      "epoch": 99.20748600058945,
      "grad_norm": 0.5593816637992859,
      "learning_rate": 3.3879040678563846e-09,
      "loss": 1.0634,
      "step": 10615
    },
    {
      "epoch": 99.21691718243443,
      "grad_norm": 0.5937647223472595,
      "learning_rate": 3.3095779946079597e-09,
      "loss": 1.0355,
      "step": 10616
    },
    {
      "epoch": 99.2263483642794,
      "grad_norm": 0.5790046453475952,
      "learning_rate": 3.2321678148261592e-09,
      "loss": 1.0617,
      "step": 10617
    },
    {
      "epoch": 99.23577954612438,
      "grad_norm": 0.5321404933929443,
      "learning_rate": 3.155673535604198e-09,
      "loss": 1.0775,
      "step": 10618
    },
    {
      "epoch": 99.24521072796935,
      "grad_norm": 0.5635793209075928,
      "learning_rate": 3.0800951639498032e-09,
      "loss": 1.0526,
      "step": 10619
    },
    {
      "epoch": 99.25464190981432,
      "grad_norm": 0.5630640983581543,
      "learning_rate": 3.0054327067874368e-09,
      "loss": 1.0462,
      "step": 10620
    },
    {
      "epoch": 99.2640730916593,
      "grad_norm": 0.5470160245895386,
      "learning_rate": 2.9316861709582924e-09,
      "loss": 1.024,
      "step": 10621
    },
    {
      "epoch": 99.27350427350427,
      "grad_norm": 0.5354623198509216,
      "learning_rate": 2.858855563218077e-09,
      "loss": 1.0797,
      "step": 10622
    },
    {
      "epoch": 99.28293545534925,
      "grad_norm": 0.5119626522064209,
      "learning_rate": 2.7869408902392313e-09,
      "loss": 1.0113,
      "step": 10623
    },
    {
      "epoch": 99.29236663719422,
      "grad_norm": 0.47846126556396484,
      "learning_rate": 2.7159421586120395e-09,
      "loss": 1.0489,
      "step": 10624
    },
    {
      "epoch": 99.3017978190392,
      "grad_norm": 0.5700739026069641,
      "learning_rate": 2.6458593748401875e-09,
      "loss": 1.0316,
      "step": 10625
    },
    {
      "epoch": 99.31122900088417,
      "grad_norm": 0.5728836059570312,
      "learning_rate": 2.576692545345205e-09,
      "loss": 1.0304,
      "step": 10626
    },
    {
      "epoch": 99.32066018272914,
      "grad_norm": 0.5692558288574219,
      "learning_rate": 2.508441676464246e-09,
      "loss": 1.0416,
      "step": 10627
    },
    {
      "epoch": 99.33009136457412,
      "grad_norm": 0.5579279661178589,
      "learning_rate": 2.441106774450086e-09,
      "loss": 1.0495,
      "step": 10628
    },
    {
      "epoch": 99.3395225464191,
      "grad_norm": 0.5402982234954834,
      "learning_rate": 2.374687845471124e-09,
      "loss": 1.0706,
      "step": 10629
    },
    {
      "epoch": 99.34895372826408,
      "grad_norm": 0.5799680352210999,
      "learning_rate": 2.309184895613603e-09,
      "loss": 1.0498,
      "step": 10630
    },
    {
      "epoch": 99.35838491010905,
      "grad_norm": 0.5526219010353088,
      "learning_rate": 2.2445979308793885e-09,
      "loss": 1.079,
      "step": 10631
    },
    {
      "epoch": 99.36781609195403,
      "grad_norm": 0.6093725562095642,
      "learning_rate": 2.180926957184859e-09,
      "loss": 1.0436,
      "step": 10632
    },
    {
      "epoch": 99.377247273799,
      "grad_norm": 0.5244584679603577,
      "learning_rate": 2.1181719803642362e-09,
      "loss": 1.0231,
      "step": 10633
    },
    {
      "epoch": 99.38667845564397,
      "grad_norm": 0.5453417301177979,
      "learning_rate": 2.0563330061673658e-09,
      "loss": 1.0363,
      "step": 10634
    },
    {
      "epoch": 99.39610963748895,
      "grad_norm": 0.5261874794960022,
      "learning_rate": 1.9954100402586052e-09,
      "loss": 1.0718,
      "step": 10635
    },
    {
      "epoch": 99.40554081933392,
      "grad_norm": 0.612838625907898,
      "learning_rate": 1.935403088220156e-09,
      "loss": 1.0386,
      "step": 10636
    },
    {
      "epoch": 99.4149720011789,
      "grad_norm": 0.5488402843475342,
      "learning_rate": 1.8763121555509522e-09,
      "loss": 1.0347,
      "step": 10637
    },
    {
      "epoch": 99.42440318302387,
      "grad_norm": 0.5228246450424194,
      "learning_rate": 1.8181372476633319e-09,
      "loss": 1.0319,
      "step": 10638
    },
    {
      "epoch": 99.43383436486884,
      "grad_norm": 0.5251003503799438,
      "learning_rate": 1.7608783698885856e-09,
      "loss": 1.0641,
      "step": 10639
    },
    {
      "epoch": 99.44326554671382,
      "grad_norm": 0.5875609517097473,
      "learning_rate": 1.704535527471407e-09,
      "loss": 1.0189,
      "step": 10640
    },
    {
      "epoch": 99.45269672855879,
      "grad_norm": 0.5657257437705994,
      "learning_rate": 1.6491087255754434e-09,
      "loss": 1.0751,
      "step": 10641
    },
    {
      "epoch": 99.46212791040377,
      "grad_norm": 0.5298722982406616,
      "learning_rate": 1.5945979692777447e-09,
      "loss": 1.0436,
      "step": 10642
    },
    {
      "epoch": 99.47155909224875,
      "grad_norm": 0.5888745188713074,
      "learning_rate": 1.5410032635732041e-09,
      "loss": 1.0582,
      "step": 10643
    },
    {
      "epoch": 99.48099027409373,
      "grad_norm": 0.5351793766021729,
      "learning_rate": 1.4883246133723384e-09,
      "loss": 1.0304,
      "step": 10644
    },
    {
      "epoch": 99.4904214559387,
      "grad_norm": 0.563421368598938,
      "learning_rate": 1.4365620235001765e-09,
      "loss": 1.0679,
      "step": 10645
    },
    {
      "epoch": 99.49985263778368,
      "grad_norm": 0.594789981842041,
      "learning_rate": 1.3857154987007016e-09,
      "loss": 1.0414,
      "step": 10646
    },
    {
      "epoch": 99.50928381962865,
      "grad_norm": 0.507377028465271,
      "learning_rate": 1.3357850436312991e-09,
      "loss": 1.0642,
      "step": 10647
    },
    {
      "epoch": 99.51871500147362,
      "grad_norm": 0.574629008769989,
      "learning_rate": 1.2867706628671984e-09,
      "loss": 0.9987,
      "step": 10648
    },
    {
      "epoch": 99.5281461833186,
      "grad_norm": 0.5545899271965027,
      "learning_rate": 1.2386723608992512e-09,
      "loss": 1.0217,
      "step": 10649
    },
    {
      "epoch": 99.53757736516357,
      "grad_norm": 0.5376085638999939,
      "learning_rate": 1.191490142133933e-09,
      "loss": 1.0382,
      "step": 10650
    },
    {
      "epoch": 99.54700854700855,
      "grad_norm": 0.52812260389328,
      "learning_rate": 1.1452240108944523e-09,
      "loss": 1.077,
      "step": 10651
    },
    {
      "epoch": 99.55643972885352,
      "grad_norm": 0.540927529335022,
      "learning_rate": 1.0998739714185303e-09,
      "loss": 1.0533,
      "step": 10652
    },
    {
      "epoch": 99.5658709106985,
      "grad_norm": 0.5306466817855835,
      "learning_rate": 1.0554400278628417e-09,
      "loss": 1.0579,
      "step": 10653
    },
    {
      "epoch": 99.57530209254347,
      "grad_norm": 0.5524740815162659,
      "learning_rate": 1.0119221842974646e-09,
      "loss": 1.0257,
      "step": 10654
    },
    {
      "epoch": 99.58473327438844,
      "grad_norm": 0.5806090831756592,
      "learning_rate": 9.693204447092098e-10,
      "loss": 1.08,
      "step": 10655
    },
    {
      "epoch": 99.59416445623341,
      "grad_norm": 0.5207688212394714,
      "learning_rate": 9.276348130016211e-10,
      "loss": 1.0427,
      "step": 10656
    },
    {
      "epoch": 99.6035956380784,
      "grad_norm": 0.5667293667793274,
      "learning_rate": 8.868652929938659e-10,
      "loss": 0.9984,
      "step": 10657
    },
    {
      "epoch": 99.61302681992338,
      "grad_norm": 0.5345813035964966,
      "learning_rate": 8.470118884207346e-10,
      "loss": 1.0278,
      "step": 10658
    },
    {
      "epoch": 99.62245800176835,
      "grad_norm": 0.4902326762676239,
      "learning_rate": 8.080746029348607e-10,
      "loss": 1.0276,
      "step": 10659
    },
    {
      "epoch": 99.63188918361332,
      "grad_norm": 0.5628032088279724,
      "learning_rate": 7.700534401022807e-10,
      "loss": 1.0422,
      "step": 10660
    },
    {
      "epoch": 99.6413203654583,
      "grad_norm": 0.5517394542694092,
      "learning_rate": 7.329484034079848e-10,
      "loss": 1.0801,
      "step": 10661
    },
    {
      "epoch": 99.65075154730327,
      "grad_norm": 0.5592719912528992,
      "learning_rate": 6.967594962492552e-10,
      "loss": 1.0667,
      "step": 10662
    },
    {
      "epoch": 99.66018272914825,
      "grad_norm": 0.5354235768318176,
      "learning_rate": 6.614867219445486e-10,
      "loss": 1.0725,
      "step": 10663
    },
    {
      "epoch": 99.66961391099322,
      "grad_norm": 0.5198949575424194,
      "learning_rate": 6.271300837235039e-10,
      "loss": 1.0412,
      "step": 10664
    },
    {
      "epoch": 99.6790450928382,
      "grad_norm": 0.5407400131225586,
      "learning_rate": 5.936895847336032e-10,
      "loss": 1.0422,
      "step": 10665
    },
    {
      "epoch": 99.68847627468317,
      "grad_norm": 0.5362202525138855,
      "learning_rate": 5.611652280401724e-10,
      "loss": 1.0598,
      "step": 10666
    },
    {
      "epoch": 99.69790745652814,
      "grad_norm": 0.5604472160339355,
      "learning_rate": 5.295570166230502e-10,
      "loss": 1.0552,
      "step": 10667
    },
    {
      "epoch": 99.70733863837312,
      "grad_norm": 0.5816450119018555,
      "learning_rate": 4.988649533765877e-10,
      "loss": 1.042,
      "step": 10668
    },
    {
      "epoch": 99.71676982021809,
      "grad_norm": 0.5320805311203003,
      "learning_rate": 4.690890411140902e-10,
      "loss": 1.0384,
      "step": 10669
    },
    {
      "epoch": 99.72620100206306,
      "grad_norm": 0.5716179609298706,
      "learning_rate": 4.4022928256337584e-10,
      "loss": 1.0579,
      "step": 10670
    },
    {
      "epoch": 99.73563218390805,
      "grad_norm": 0.5410590171813965,
      "learning_rate": 4.122856803678854e-10,
      "loss": 1.0321,
      "step": 10671
    },
    {
      "epoch": 99.74506336575303,
      "grad_norm": 0.5421226620674133,
      "learning_rate": 3.852582370889035e-10,
      "loss": 1.0316,
      "step": 10672
    },
    {
      "epoch": 99.754494547598,
      "grad_norm": 0.53605717420578,
      "learning_rate": 3.5914695520222754e-10,
      "loss": 1.0454,
      "step": 10673
    },
    {
      "epoch": 99.76392572944297,
      "grad_norm": 0.530417263507843,
      "learning_rate": 3.339518370992778e-10,
      "loss": 1.0636,
      "step": 10674
    },
    {
      "epoch": 99.77335691128795,
      "grad_norm": 0.5788940191268921,
      "learning_rate": 3.0967288508931825e-10,
      "loss": 1.0727,
      "step": 10675
    },
    {
      "epoch": 99.78278809313292,
      "grad_norm": 0.5506305694580078,
      "learning_rate": 2.863101013972358e-10,
      "loss": 1.0081,
      "step": 10676
    },
    {
      "epoch": 99.7922192749779,
      "grad_norm": 0.5155604481697083,
      "learning_rate": 2.6386348816243026e-10,
      "loss": 1.0607,
      "step": 10677
    },
    {
      "epoch": 99.80165045682287,
      "grad_norm": 0.5323470234870911,
      "learning_rate": 2.4233304744214483e-10,
      "loss": 1.0713,
      "step": 10678
    },
    {
      "epoch": 99.81108163866784,
      "grad_norm": 0.5630103945732117,
      "learning_rate": 2.2171878120924583e-10,
      "loss": 1.0661,
      "step": 10679
    },
    {
      "epoch": 99.82051282051282,
      "grad_norm": 0.5148215889930725,
      "learning_rate": 2.020206913522227e-10,
      "loss": 1.0455,
      "step": 10680
    },
    {
      "epoch": 99.82994400235779,
      "grad_norm": 0.5797673463821411,
      "learning_rate": 1.8323877967407755e-10,
      "loss": 0.9928,
      "step": 10681
    },
    {
      "epoch": 99.83937518420277,
      "grad_norm": 0.5470495223999023,
      "learning_rate": 1.6537304789787656e-10,
      "loss": 1.0808,
      "step": 10682
    },
    {
      "epoch": 99.84880636604774,
      "grad_norm": 0.5572491884231567,
      "learning_rate": 1.4842349766008846e-10,
      "loss": 1.0763,
      "step": 10683
    },
    {
      "epoch": 99.85823754789271,
      "grad_norm": 0.6315850615501404,
      "learning_rate": 1.3239013051280502e-10,
      "loss": 1.0549,
      "step": 10684
    },
    {
      "epoch": 99.8676687297377,
      "grad_norm": 0.5500911474227905,
      "learning_rate": 1.1727294792596157e-10,
      "loss": 1.0415,
      "step": 10685
    },
    {
      "epoch": 99.87709991158268,
      "grad_norm": 0.556370198726654,
      "learning_rate": 1.0307195128289593e-10,
      "loss": 1.0678,
      "step": 10686
    },
    {
      "epoch": 99.88653109342765,
      "grad_norm": 0.5769630074501038,
      "learning_rate": 8.978714188700998e-11,
      "loss": 1.0608,
      "step": 10687
    },
    {
      "epoch": 99.89596227527262,
      "grad_norm": 0.5502292513847351,
      "learning_rate": 7.741852095399794e-11,
      "loss": 1.0697,
      "step": 10688
    },
    {
      "epoch": 99.9053934571176,
      "grad_norm": 0.5210943222045898,
      "learning_rate": 6.596608961739748e-11,
      "loss": 1.0374,
      "step": 10689
    },
    {
      "epoch": 99.91482463896257,
      "grad_norm": 0.5451275706291199,
      "learning_rate": 5.542984892525916e-11,
      "loss": 1.0166,
      "step": 10690
    },
    {
      "epoch": 99.92425582080755,
      "grad_norm": 0.5047316551208496,
      "learning_rate": 4.5809799845697446e-11,
      "loss": 1.0498,
      "step": 10691
    },
    {
      "epoch": 99.93368700265252,
      "grad_norm": 0.5315130949020386,
      "learning_rate": 3.710594325800898e-11,
      "loss": 1.0563,
      "step": 10692
    },
    {
      "epoch": 99.9431181844975,
      "grad_norm": 0.5538446307182312,
      "learning_rate": 2.931827995933389e-11,
      "loss": 1.0668,
      "step": 10693
    },
    {
      "epoch": 99.95254936634247,
      "grad_norm": 0.5359312295913696,
      "learning_rate": 2.2446810664655815e-11,
      "loss": 1.0795,
      "step": 10694
    },
    {
      "epoch": 99.96198054818744,
      "grad_norm": 0.5730425715446472,
      "learning_rate": 1.6491536002360975e-11,
      "loss": 1.0144,
      "step": 10695
    },
    {
      "epoch": 99.97141173003241,
      "grad_norm": 0.5036903023719788,
      "learning_rate": 1.1452456519789323e-11,
      "loss": 1.0849,
      "step": 10696
    },
    {
      "epoch": 99.98084291187739,
      "grad_norm": 0.5539806485176086,
      "learning_rate": 7.329572675462971e-12,
      "loss": 1.0422,
      "step": 10697
    },
    {
      "epoch": 99.99027409372236,
      "grad_norm": 0.5828487277030945,
      "learning_rate": 4.122884850188414e-12,
      "loss": 1.0277,
      "step": 10698
    },
    {
      "epoch": 99.99970527556735,
      "grad_norm": 0.5338295698165894,
      "learning_rate": 1.832393337064531e-12,
      "loss": 1.0699,
      "step": 10699
    },
    {
      "epoch": 100.0,
      "grad_norm": 4.369780540466309,
      "learning_rate": 4.580983448132514e-13,
      "loss": 0.4478,
      "step": 10700
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 10700,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 100,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.685764666818822e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
