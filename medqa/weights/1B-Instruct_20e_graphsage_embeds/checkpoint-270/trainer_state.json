{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 270,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01885680612846199,
      "grad_norm": 62.62226486206055,
      "learning_rate": 0.0,
      "loss": 73.338,
      "step": 1
    },
    {
      "epoch": 0.03771361225692398,
      "grad_norm": 63.43726348876953,
      "learning_rate": 6.060606060606061e-07,
      "loss": 72.6619,
      "step": 2
    },
    {
      "epoch": 0.05657041838538598,
      "grad_norm": 64.27000427246094,
      "learning_rate": 1.2121212121212122e-06,
      "loss": 72.9868,
      "step": 3
    },
    {
      "epoch": 0.07542722451384797,
      "grad_norm": 63.0974006652832,
      "learning_rate": 1.8181818181818183e-06,
      "loss": 73.1396,
      "step": 4
    },
    {
      "epoch": 0.09428403064230996,
      "grad_norm": 80.1317367553711,
      "learning_rate": 2.4242424242424244e-06,
      "loss": 80.2556,
      "step": 5
    },
    {
      "epoch": 0.11314083677077195,
      "grad_norm": 69.28889465332031,
      "learning_rate": 3.0303030303030305e-06,
      "loss": 75.4047,
      "step": 6
    },
    {
      "epoch": 0.13199764289923394,
      "grad_norm": 72.39773559570312,
      "learning_rate": 3.6363636363636366e-06,
      "loss": 76.3931,
      "step": 7
    },
    {
      "epoch": 0.15085444902769593,
      "grad_norm": 72.52974700927734,
      "learning_rate": 4.242424242424243e-06,
      "loss": 75.1552,
      "step": 8
    },
    {
      "epoch": 0.16971125515615793,
      "grad_norm": 79.1762466430664,
      "learning_rate": 4.848484848484849e-06,
      "loss": 78.0422,
      "step": 9
    },
    {
      "epoch": 0.18856806128461992,
      "grad_norm": 69.2076416015625,
      "learning_rate": 5.4545454545454545e-06,
      "loss": 74.6092,
      "step": 10
    },
    {
      "epoch": 0.20742486741308191,
      "grad_norm": 71.61978149414062,
      "learning_rate": 6.060606060606061e-06,
      "loss": 75.1498,
      "step": 11
    },
    {
      "epoch": 0.2262816735415439,
      "grad_norm": 62.662567138671875,
      "learning_rate": 6.666666666666667e-06,
      "loss": 71.4719,
      "step": 12
    },
    {
      "epoch": 0.2451384796700059,
      "grad_norm": 82.77973175048828,
      "learning_rate": 7.272727272727273e-06,
      "loss": 79.835,
      "step": 13
    },
    {
      "epoch": 0.26399528579846787,
      "grad_norm": 56.8451042175293,
      "learning_rate": 7.87878787878788e-06,
      "loss": 69.1196,
      "step": 14
    },
    {
      "epoch": 0.2828520919269299,
      "grad_norm": 72.91389465332031,
      "learning_rate": 8.484848484848486e-06,
      "loss": 75.8186,
      "step": 15
    },
    {
      "epoch": 0.30170889805539186,
      "grad_norm": 65.0413818359375,
      "learning_rate": 9.090909090909091e-06,
      "loss": 71.4641,
      "step": 16
    },
    {
      "epoch": 0.3205657041838539,
      "grad_norm": 64.10081481933594,
      "learning_rate": 9.696969696969698e-06,
      "loss": 70.1548,
      "step": 17
    },
    {
      "epoch": 0.33942251031231585,
      "grad_norm": 73.57251739501953,
      "learning_rate": 1.0303030303030304e-05,
      "loss": 74.4056,
      "step": 18
    },
    {
      "epoch": 0.3582793164407778,
      "grad_norm": 74.13844299316406,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 73.9137,
      "step": 19
    },
    {
      "epoch": 0.37713612256923984,
      "grad_norm": 70.44461822509766,
      "learning_rate": 1.1515151515151517e-05,
      "loss": 72.5293,
      "step": 20
    },
    {
      "epoch": 0.3959929286977018,
      "grad_norm": 74.24805450439453,
      "learning_rate": 1.2121212121212122e-05,
      "loss": 73.6893,
      "step": 21
    },
    {
      "epoch": 0.41484973482616383,
      "grad_norm": 85.2084732055664,
      "learning_rate": 1.2727272727272728e-05,
      "loss": 76.7715,
      "step": 22
    },
    {
      "epoch": 0.4337065409546258,
      "grad_norm": 77.88575744628906,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 73.5509,
      "step": 23
    },
    {
      "epoch": 0.4525633470830878,
      "grad_norm": 71.67064666748047,
      "learning_rate": 1.3939393939393942e-05,
      "loss": 70.8294,
      "step": 24
    },
    {
      "epoch": 0.4714201532115498,
      "grad_norm": 78.15032958984375,
      "learning_rate": 1.4545454545454546e-05,
      "loss": 72.259,
      "step": 25
    },
    {
      "epoch": 0.4902769593400118,
      "grad_norm": 79.78480529785156,
      "learning_rate": 1.5151515151515153e-05,
      "loss": 71.4474,
      "step": 26
    },
    {
      "epoch": 0.5091337654684738,
      "grad_norm": 72.71586608886719,
      "learning_rate": 1.575757575757576e-05,
      "loss": 70.0821,
      "step": 27
    },
    {
      "epoch": 0.5279905715969357,
      "grad_norm": 78.79987335205078,
      "learning_rate": 1.6363636363636366e-05,
      "loss": 70.935,
      "step": 28
    },
    {
      "epoch": 0.5468473777253977,
      "grad_norm": 72.33824157714844,
      "learning_rate": 1.6969696969696972e-05,
      "loss": 68.0982,
      "step": 29
    },
    {
      "epoch": 0.5657041838538598,
      "grad_norm": 83.8772964477539,
      "learning_rate": 1.7575757575757576e-05,
      "loss": 70.931,
      "step": 30
    },
    {
      "epoch": 0.5845609899823218,
      "grad_norm": 87.76377868652344,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 70.9352,
      "step": 31
    },
    {
      "epoch": 0.6034177961107837,
      "grad_norm": 97.4378890991211,
      "learning_rate": 1.8787878787878792e-05,
      "loss": 72.8572,
      "step": 32
    },
    {
      "epoch": 0.6222746022392457,
      "grad_norm": 76.5610580444336,
      "learning_rate": 1.9393939393939395e-05,
      "loss": 66.1712,
      "step": 33
    },
    {
      "epoch": 0.6411314083677078,
      "grad_norm": 87.03289794921875,
      "learning_rate": 2e-05,
      "loss": 67.1866,
      "step": 34
    },
    {
      "epoch": 0.6599882144961697,
      "grad_norm": 86.66920471191406,
      "learning_rate": 1.9999954983050696e-05,
      "loss": 66.2063,
      "step": 35
    },
    {
      "epoch": 0.6788450206246317,
      "grad_norm": 69.71155548095703,
      "learning_rate": 1.999981993260809e-05,
      "loss": 60.9273,
      "step": 36
    },
    {
      "epoch": 0.6977018267530937,
      "grad_norm": 87.1468505859375,
      "learning_rate": 1.9999594849888083e-05,
      "loss": 64.3105,
      "step": 37
    },
    {
      "epoch": 0.7165586328815556,
      "grad_norm": 77.44078063964844,
      "learning_rate": 1.99992797369172e-05,
      "loss": 61.4461,
      "step": 38
    },
    {
      "epoch": 0.7354154390100177,
      "grad_norm": 91.28842163085938,
      "learning_rate": 1.9998874596532512e-05,
      "loss": 62.7726,
      "step": 39
    },
    {
      "epoch": 0.7542722451384797,
      "grad_norm": 89.93099212646484,
      "learning_rate": 1.999837943238166e-05,
      "loss": 61.5589,
      "step": 40
    },
    {
      "epoch": 0.7731290512669416,
      "grad_norm": 85.98239135742188,
      "learning_rate": 1.99977942489228e-05,
      "loss": 60.6414,
      "step": 41
    },
    {
      "epoch": 0.7919858573954036,
      "grad_norm": 87.98095703125,
      "learning_rate": 1.999711905142457e-05,
      "loss": 58.7634,
      "step": 42
    },
    {
      "epoch": 0.8108426635238657,
      "grad_norm": 79.85832977294922,
      "learning_rate": 1.9996353845966033e-05,
      "loss": 58.187,
      "step": 43
    },
    {
      "epoch": 0.8296994696523277,
      "grad_norm": 90.33596801757812,
      "learning_rate": 1.9995498639436634e-05,
      "loss": 57.4343,
      "step": 44
    },
    {
      "epoch": 0.8485562757807896,
      "grad_norm": 84.17617797851562,
      "learning_rate": 1.9994553439536125e-05,
      "loss": 56.2141,
      "step": 45
    },
    {
      "epoch": 0.8674130819092516,
      "grad_norm": 95.39984893798828,
      "learning_rate": 1.9993518254774517e-05,
      "loss": 57.049,
      "step": 46
    },
    {
      "epoch": 0.8862698880377136,
      "grad_norm": 92.82145690917969,
      "learning_rate": 1.9992393094471976e-05,
      "loss": 56.3089,
      "step": 47
    },
    {
      "epoch": 0.9051266941661756,
      "grad_norm": 95.14628601074219,
      "learning_rate": 1.9991177968758764e-05,
      "loss": 54.8917,
      "step": 48
    },
    {
      "epoch": 0.9239835002946376,
      "grad_norm": 83.65750885009766,
      "learning_rate": 1.998987288857513e-05,
      "loss": 53.039,
      "step": 49
    },
    {
      "epoch": 0.9428403064230996,
      "grad_norm": 75.34854888916016,
      "learning_rate": 1.9988477865671217e-05,
      "loss": 52.1893,
      "step": 50
    },
    {
      "epoch": 0.9616971125515615,
      "grad_norm": 80.01261138916016,
      "learning_rate": 1.998699291260696e-05,
      "loss": 52.7051,
      "step": 51
    },
    {
      "epoch": 0.9805539186800236,
      "grad_norm": 76.16426086425781,
      "learning_rate": 1.9985418042751975e-05,
      "loss": 50.8202,
      "step": 52
    },
    {
      "epoch": 0.9994107248084856,
      "grad_norm": 73.961181640625,
      "learning_rate": 1.9983753270285423e-05,
      "loss": 50.2175,
      "step": 53
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.329345226287842,
      "learning_rate": 1.998199861019591e-05,
      "loss": 1.5491,
      "step": 54
    },
    {
      "epoch": 1.018856806128462,
      "grad_norm": 81.19802856445312,
      "learning_rate": 1.998015407828131e-05,
      "loss": 49.6989,
      "step": 55
    },
    {
      "epoch": 1.037713612256924,
      "grad_norm": 75.41407775878906,
      "learning_rate": 1.9978219691148676e-05,
      "loss": 48.478,
      "step": 56
    },
    {
      "epoch": 1.056570418385386,
      "grad_norm": 78.79582214355469,
      "learning_rate": 1.9976195466214042e-05,
      "loss": 47.2407,
      "step": 57
    },
    {
      "epoch": 1.0754272245138479,
      "grad_norm": 86.61427307128906,
      "learning_rate": 1.9974081421702296e-05,
      "loss": 47.1282,
      "step": 58
    },
    {
      "epoch": 1.09428403064231,
      "grad_norm": 99.83094787597656,
      "learning_rate": 1.9971877576647005e-05,
      "loss": 46.0443,
      "step": 59
    },
    {
      "epoch": 1.113140836770772,
      "grad_norm": 91.39749908447266,
      "learning_rate": 1.9969583950890245e-05,
      "loss": 45.4884,
      "step": 60
    },
    {
      "epoch": 1.131997642899234,
      "grad_norm": 102.9197006225586,
      "learning_rate": 1.9967200565082426e-05,
      "loss": 44.7538,
      "step": 61
    },
    {
      "epoch": 1.150854449027696,
      "grad_norm": 101.64024353027344,
      "learning_rate": 1.9964727440682097e-05,
      "loss": 43.9614,
      "step": 62
    },
    {
      "epoch": 1.169711255156158,
      "grad_norm": 131.70291137695312,
      "learning_rate": 1.9962164599955762e-05,
      "loss": 42.9515,
      "step": 63
    },
    {
      "epoch": 1.18856806128462,
      "grad_norm": 110.64971923828125,
      "learning_rate": 1.9959512065977673e-05,
      "loss": 43.4927,
      "step": 64
    },
    {
      "epoch": 1.2074248674130819,
      "grad_norm": 128.70513916015625,
      "learning_rate": 1.995676986262963e-05,
      "loss": 41.6526,
      "step": 65
    },
    {
      "epoch": 1.2262816735415438,
      "grad_norm": 108.44417572021484,
      "learning_rate": 1.9953938014600757e-05,
      "loss": 41.5256,
      "step": 66
    },
    {
      "epoch": 1.2451384796700058,
      "grad_norm": 136.07569885253906,
      "learning_rate": 1.9951016547387286e-05,
      "loss": 39.7446,
      "step": 67
    },
    {
      "epoch": 1.263995285798468,
      "grad_norm": 118.0251693725586,
      "learning_rate": 1.994800548729233e-05,
      "loss": 38.5618,
      "step": 68
    },
    {
      "epoch": 1.28285209192693,
      "grad_norm": 84.94879913330078,
      "learning_rate": 1.9944904861425626e-05,
      "loss": 40.2317,
      "step": 69
    },
    {
      "epoch": 1.301708898055392,
      "grad_norm": 98.93374633789062,
      "learning_rate": 1.9941714697703333e-05,
      "loss": 38.6446,
      "step": 70
    },
    {
      "epoch": 1.3205657041838539,
      "grad_norm": 98.40473175048828,
      "learning_rate": 1.9938435024847723e-05,
      "loss": 37.212,
      "step": 71
    },
    {
      "epoch": 1.3394225103123159,
      "grad_norm": 92.71240234375,
      "learning_rate": 1.9935065872386977e-05,
      "loss": 35.4105,
      "step": 72
    },
    {
      "epoch": 1.3582793164407778,
      "grad_norm": 74.69499206542969,
      "learning_rate": 1.993160727065489e-05,
      "loss": 35.2118,
      "step": 73
    },
    {
      "epoch": 1.3771361225692398,
      "grad_norm": 63.86635971069336,
      "learning_rate": 1.99280592507906e-05,
      "loss": 34.7366,
      "step": 74
    },
    {
      "epoch": 1.3959929286977018,
      "grad_norm": 46.650848388671875,
      "learning_rate": 1.992442184473831e-05,
      "loss": 35.4217,
      "step": 75
    },
    {
      "epoch": 1.4148497348261637,
      "grad_norm": 43.84389114379883,
      "learning_rate": 1.9920695085247012e-05,
      "loss": 35.8989,
      "step": 76
    },
    {
      "epoch": 1.433706540954626,
      "grad_norm": 40.31873321533203,
      "learning_rate": 1.9916879005870164e-05,
      "loss": 34.9437,
      "step": 77
    },
    {
      "epoch": 1.4525633470830879,
      "grad_norm": 36.233238220214844,
      "learning_rate": 1.9912973640965423e-05,
      "loss": 33.8977,
      "step": 78
    },
    {
      "epoch": 1.4714201532115498,
      "grad_norm": 33.294960021972656,
      "learning_rate": 1.9908979025694312e-05,
      "loss": 32.762,
      "step": 79
    },
    {
      "epoch": 1.4902769593400118,
      "grad_norm": 32.629207611083984,
      "learning_rate": 1.990489519602191e-05,
      "loss": 34.1017,
      "step": 80
    },
    {
      "epoch": 1.5091337654684738,
      "grad_norm": 30.728254318237305,
      "learning_rate": 1.9900722188716526e-05,
      "loss": 34.618,
      "step": 81
    },
    {
      "epoch": 1.5279905715969357,
      "grad_norm": 28.548675537109375,
      "learning_rate": 1.989646004134937e-05,
      "loss": 34.6351,
      "step": 82
    },
    {
      "epoch": 1.5468473777253977,
      "grad_norm": 25.563968658447266,
      "learning_rate": 1.989210879229422e-05,
      "loss": 32.8624,
      "step": 83
    },
    {
      "epoch": 1.56570418385386,
      "grad_norm": 24.667675018310547,
      "learning_rate": 1.9887668480727066e-05,
      "loss": 32.6553,
      "step": 84
    },
    {
      "epoch": 1.5845609899823216,
      "grad_norm": 24.463743209838867,
      "learning_rate": 1.9883139146625763e-05,
      "loss": 34.7574,
      "step": 85
    },
    {
      "epoch": 1.6034177961107838,
      "grad_norm": 23.1281795501709,
      "learning_rate": 1.9878520830769675e-05,
      "loss": 33.3577,
      "step": 86
    },
    {
      "epoch": 1.6222746022392456,
      "grad_norm": 24.307430267333984,
      "learning_rate": 1.9873813574739293e-05,
      "loss": 34.5201,
      "step": 87
    },
    {
      "epoch": 1.6411314083677078,
      "grad_norm": 22.403085708618164,
      "learning_rate": 1.9869017420915888e-05,
      "loss": 31.2948,
      "step": 88
    },
    {
      "epoch": 1.6599882144961697,
      "grad_norm": 19.366302490234375,
      "learning_rate": 1.9864132412481094e-05,
      "loss": 32.1921,
      "step": 89
    },
    {
      "epoch": 1.6788450206246317,
      "grad_norm": 20.645801544189453,
      "learning_rate": 1.9859158593416554e-05,
      "loss": 31.7598,
      "step": 90
    },
    {
      "epoch": 1.6977018267530937,
      "grad_norm": 17.64431381225586,
      "learning_rate": 1.9854096008503495e-05,
      "loss": 31.92,
      "step": 91
    },
    {
      "epoch": 1.7165586328815556,
      "grad_norm": 19.259315490722656,
      "learning_rate": 1.9848944703322345e-05,
      "loss": 32.3856,
      "step": 92
    },
    {
      "epoch": 1.7354154390100178,
      "grad_norm": 16.496028900146484,
      "learning_rate": 1.9843704724252308e-05,
      "loss": 31.1344,
      "step": 93
    },
    {
      "epoch": 1.7542722451384796,
      "grad_norm": 17.07571029663086,
      "learning_rate": 1.9838376118470965e-05,
      "loss": 33.7484,
      "step": 94
    },
    {
      "epoch": 1.7731290512669418,
      "grad_norm": 14.625565528869629,
      "learning_rate": 1.983295893395383e-05,
      "loss": 30.4421,
      "step": 95
    },
    {
      "epoch": 1.7919858573954035,
      "grad_norm": 16.073848724365234,
      "learning_rate": 1.9827453219473925e-05,
      "loss": 32.8886,
      "step": 96
    },
    {
      "epoch": 1.8108426635238657,
      "grad_norm": 15.953970909118652,
      "learning_rate": 1.9821859024601345e-05,
      "loss": 32.6288,
      "step": 97
    },
    {
      "epoch": 1.8296994696523277,
      "grad_norm": 13.39419937133789,
      "learning_rate": 1.9816176399702806e-05,
      "loss": 31.4481,
      "step": 98
    },
    {
      "epoch": 1.8485562757807896,
      "grad_norm": 13.380619049072266,
      "learning_rate": 1.98104053959412e-05,
      "loss": 33.5351,
      "step": 99
    },
    {
      "epoch": 1.8674130819092516,
      "grad_norm": 13.828383445739746,
      "learning_rate": 1.9804546065275116e-05,
      "loss": 30.2244,
      "step": 100
    },
    {
      "epoch": 1.8862698880377136,
      "grad_norm": 11.966225624084473,
      "learning_rate": 1.9798598460458394e-05,
      "loss": 31.7822,
      "step": 101
    },
    {
      "epoch": 1.9051266941661757,
      "grad_norm": 12.539872169494629,
      "learning_rate": 1.979256263503965e-05,
      "loss": 31.9052,
      "step": 102
    },
    {
      "epoch": 1.9239835002946375,
      "grad_norm": 11.170974731445312,
      "learning_rate": 1.978643864336176e-05,
      "loss": 32.182,
      "step": 103
    },
    {
      "epoch": 1.9428403064230997,
      "grad_norm": 11.202855110168457,
      "learning_rate": 1.9780226540561413e-05,
      "loss": 29.7976,
      "step": 104
    },
    {
      "epoch": 1.9616971125515614,
      "grad_norm": 10.284497261047363,
      "learning_rate": 1.9773926382568592e-05,
      "loss": 31.1519,
      "step": 105
    },
    {
      "epoch": 1.9805539186800236,
      "grad_norm": 10.773138999938965,
      "learning_rate": 1.9767538226106078e-05,
      "loss": 30.1106,
      "step": 106
    },
    {
      "epoch": 1.9994107248084856,
      "grad_norm": 9.763049125671387,
      "learning_rate": 1.9761062128688932e-05,
      "loss": 31.0179,
      "step": 107
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.014212727546692,
      "learning_rate": 1.9754498148623985e-05,
      "loss": 0.7432,
      "step": 108
    },
    {
      "epoch": 2.018856806128462,
      "grad_norm": 9.082597732543945,
      "learning_rate": 1.9747846345009306e-05,
      "loss": 29.9178,
      "step": 109
    },
    {
      "epoch": 2.037713612256924,
      "grad_norm": 9.294320106506348,
      "learning_rate": 1.974110677773368e-05,
      "loss": 32.2745,
      "step": 110
    },
    {
      "epoch": 2.056570418385386,
      "grad_norm": 9.127276420593262,
      "learning_rate": 1.9734279507476057e-05,
      "loss": 30.422,
      "step": 111
    },
    {
      "epoch": 2.075427224513848,
      "grad_norm": 9.143399238586426,
      "learning_rate": 1.9727364595705012e-05,
      "loss": 32.5456,
      "step": 112
    },
    {
      "epoch": 2.09428403064231,
      "grad_norm": 9.218765258789062,
      "learning_rate": 1.9720362104678193e-05,
      "loss": 29.7395,
      "step": 113
    },
    {
      "epoch": 2.113140836770772,
      "grad_norm": 8.08279800415039,
      "learning_rate": 1.9713272097441755e-05,
      "loss": 31.6004,
      "step": 114
    },
    {
      "epoch": 2.131997642899234,
      "grad_norm": 8.291926383972168,
      "learning_rate": 1.9706094637829797e-05,
      "loss": 30.8442,
      "step": 115
    },
    {
      "epoch": 2.1508544490276957,
      "grad_norm": 7.7730278968811035,
      "learning_rate": 1.9698829790463792e-05,
      "loss": 31.8407,
      "step": 116
    },
    {
      "epoch": 2.169711255156158,
      "grad_norm": 8.251988410949707,
      "learning_rate": 1.9691477620751985e-05,
      "loss": 30.7611,
      "step": 117
    },
    {
      "epoch": 2.18856806128462,
      "grad_norm": 7.7790751457214355,
      "learning_rate": 1.9684038194888827e-05,
      "loss": 29.9647,
      "step": 118
    },
    {
      "epoch": 2.207424867413082,
      "grad_norm": 6.975891590118408,
      "learning_rate": 1.9676511579854375e-05,
      "loss": 29.4735,
      "step": 119
    },
    {
      "epoch": 2.226281673541544,
      "grad_norm": 8.047769546508789,
      "learning_rate": 1.9668897843413676e-05,
      "loss": 29.0276,
      "step": 120
    },
    {
      "epoch": 2.245138479670006,
      "grad_norm": 6.730463981628418,
      "learning_rate": 1.9661197054116165e-05,
      "loss": 30.1071,
      "step": 121
    },
    {
      "epoch": 2.263995285798468,
      "grad_norm": 7.341897964477539,
      "learning_rate": 1.9653409281295053e-05,
      "loss": 31.1005,
      "step": 122
    },
    {
      "epoch": 2.2828520919269297,
      "grad_norm": 7.122464656829834,
      "learning_rate": 1.9645534595066697e-05,
      "loss": 33.0825,
      "step": 123
    },
    {
      "epoch": 2.301708898055392,
      "grad_norm": 6.811592102050781,
      "learning_rate": 1.963757306632996e-05,
      "loss": 31.0122,
      "step": 124
    },
    {
      "epoch": 2.3205657041838537,
      "grad_norm": 6.876148700714111,
      "learning_rate": 1.9629524766765593e-05,
      "loss": 29.4824,
      "step": 125
    },
    {
      "epoch": 2.339422510312316,
      "grad_norm": 7.16985559463501,
      "learning_rate": 1.962138976883558e-05,
      "loss": 29.238,
      "step": 126
    },
    {
      "epoch": 2.358279316440778,
      "grad_norm": 7.2760844230651855,
      "learning_rate": 1.9613168145782468e-05,
      "loss": 33.1613,
      "step": 127
    },
    {
      "epoch": 2.37713612256924,
      "grad_norm": 6.3910417556762695,
      "learning_rate": 1.9604859971628743e-05,
      "loss": 29.9409,
      "step": 128
    },
    {
      "epoch": 2.395992928697702,
      "grad_norm": 6.64666223526001,
      "learning_rate": 1.9596465321176136e-05,
      "loss": 30.0353,
      "step": 129
    },
    {
      "epoch": 2.4148497348261637,
      "grad_norm": 6.621517181396484,
      "learning_rate": 1.958798427000495e-05,
      "loss": 29.784,
      "step": 130
    },
    {
      "epoch": 2.433706540954626,
      "grad_norm": 6.480105400085449,
      "learning_rate": 1.9579416894473407e-05,
      "loss": 31.1715,
      "step": 131
    },
    {
      "epoch": 2.4525633470830877,
      "grad_norm": 6.0207719802856445,
      "learning_rate": 1.957076327171692e-05,
      "loss": 29.243,
      "step": 132
    },
    {
      "epoch": 2.47142015321155,
      "grad_norm": 6.712698459625244,
      "learning_rate": 1.956202347964743e-05,
      "loss": 30.6902,
      "step": 133
    },
    {
      "epoch": 2.4902769593400116,
      "grad_norm": 5.920247554779053,
      "learning_rate": 1.955319759695269e-05,
      "loss": 29.7547,
      "step": 134
    },
    {
      "epoch": 2.5091337654684738,
      "grad_norm": 5.982396602630615,
      "learning_rate": 1.9544285703095565e-05,
      "loss": 29.5508,
      "step": 135
    },
    {
      "epoch": 2.527990571596936,
      "grad_norm": 6.326496124267578,
      "learning_rate": 1.9535287878313315e-05,
      "loss": 30.5671,
      "step": 136
    },
    {
      "epoch": 2.5468473777253977,
      "grad_norm": 6.060879230499268,
      "learning_rate": 1.952620420361686e-05,
      "loss": 29.8229,
      "step": 137
    },
    {
      "epoch": 2.56570418385386,
      "grad_norm": 7.28201150894165,
      "learning_rate": 1.9517034760790064e-05,
      "loss": 29.2164,
      "step": 138
    },
    {
      "epoch": 2.5845609899823216,
      "grad_norm": 6.593132972717285,
      "learning_rate": 1.9507779632388997e-05,
      "loss": 30.6865,
      "step": 139
    },
    {
      "epoch": 2.603417796110784,
      "grad_norm": 5.789382457733154,
      "learning_rate": 1.9498438901741186e-05,
      "loss": 30.4721,
      "step": 140
    },
    {
      "epoch": 2.6222746022392456,
      "grad_norm": 6.991436958312988,
      "learning_rate": 1.9489012652944874e-05,
      "loss": 27.5056,
      "step": 141
    },
    {
      "epoch": 2.6411314083677078,
      "grad_norm": 6.491793155670166,
      "learning_rate": 1.947950097086825e-05,
      "loss": 29.8189,
      "step": 142
    },
    {
      "epoch": 2.6599882144961695,
      "grad_norm": 6.078401565551758,
      "learning_rate": 1.94699039411487e-05,
      "loss": 30.1486,
      "step": 143
    },
    {
      "epoch": 2.6788450206246317,
      "grad_norm": 5.56458854675293,
      "learning_rate": 1.9460221650192016e-05,
      "loss": 30.6958,
      "step": 144
    },
    {
      "epoch": 2.697701826753094,
      "grad_norm": 6.432156085968018,
      "learning_rate": 1.945045418517165e-05,
      "loss": 32.3279,
      "step": 145
    },
    {
      "epoch": 2.7165586328815556,
      "grad_norm": 5.4890313148498535,
      "learning_rate": 1.9440601634027892e-05,
      "loss": 29.9276,
      "step": 146
    },
    {
      "epoch": 2.735415439010018,
      "grad_norm": 5.810276508331299,
      "learning_rate": 1.94306640854671e-05,
      "loss": 27.6622,
      "step": 147
    },
    {
      "epoch": 2.7542722451384796,
      "grad_norm": 5.737366676330566,
      "learning_rate": 1.9420641628960897e-05,
      "loss": 30.6599,
      "step": 148
    },
    {
      "epoch": 2.7731290512669418,
      "grad_norm": 5.800586700439453,
      "learning_rate": 1.9410534354745367e-05,
      "loss": 29.1975,
      "step": 149
    },
    {
      "epoch": 2.7919858573954035,
      "grad_norm": 6.012643814086914,
      "learning_rate": 1.9400342353820244e-05,
      "loss": 31.5017,
      "step": 150
    },
    {
      "epoch": 2.8108426635238657,
      "grad_norm": 5.874461650848389,
      "learning_rate": 1.9390065717948084e-05,
      "loss": 27.8245,
      "step": 151
    },
    {
      "epoch": 2.8296994696523274,
      "grad_norm": 6.426990032196045,
      "learning_rate": 1.9379704539653443e-05,
      "loss": 28.4801,
      "step": 152
    },
    {
      "epoch": 2.8485562757807896,
      "grad_norm": 6.070323944091797,
      "learning_rate": 1.9369258912222052e-05,
      "loss": 29.9537,
      "step": 153
    },
    {
      "epoch": 2.867413081909252,
      "grad_norm": 6.095250129699707,
      "learning_rate": 1.9358728929699966e-05,
      "loss": 28.7608,
      "step": 154
    },
    {
      "epoch": 2.8862698880377136,
      "grad_norm": 5.470078468322754,
      "learning_rate": 1.9348114686892722e-05,
      "loss": 30.3225,
      "step": 155
    },
    {
      "epoch": 2.9051266941661757,
      "grad_norm": 6.11785364151001,
      "learning_rate": 1.9337416279364486e-05,
      "loss": 28.2803,
      "step": 156
    },
    {
      "epoch": 2.9239835002946375,
      "grad_norm": 5.775125503540039,
      "learning_rate": 1.9326633803437197e-05,
      "loss": 29.8969,
      "step": 157
    },
    {
      "epoch": 2.9428403064230997,
      "grad_norm": 5.877498149871826,
      "learning_rate": 1.931576735618968e-05,
      "loss": 30.4683,
      "step": 158
    },
    {
      "epoch": 2.9616971125515614,
      "grad_norm": 7.098881721496582,
      "learning_rate": 1.9304817035456804e-05,
      "loss": 29.4748,
      "step": 159
    },
    {
      "epoch": 2.9805539186800236,
      "grad_norm": 6.132027626037598,
      "learning_rate": 1.929378293982857e-05,
      "loss": 29.6525,
      "step": 160
    },
    {
      "epoch": 2.9994107248084854,
      "grad_norm": 6.059079170227051,
      "learning_rate": 1.928266516864925e-05,
      "loss": 29.7888,
      "step": 161
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.9274567365646362,
      "learning_rate": 1.9271463822016465e-05,
      "loss": 1.0528,
      "step": 162
    },
    {
      "epoch": 3.018856806128462,
      "grad_norm": 7.415787696838379,
      "learning_rate": 1.926017900078031e-05,
      "loss": 29.2397,
      "step": 163
    },
    {
      "epoch": 3.037713612256924,
      "grad_norm": 6.291476249694824,
      "learning_rate": 1.924881080654243e-05,
      "loss": 27.7792,
      "step": 164
    },
    {
      "epoch": 3.056570418385386,
      "grad_norm": 5.605022430419922,
      "learning_rate": 1.9237359341655108e-05,
      "loss": 31.5991,
      "step": 165
    },
    {
      "epoch": 3.075427224513848,
      "grad_norm": 6.073587417602539,
      "learning_rate": 1.922582470922034e-05,
      "loss": 30.5208,
      "step": 166
    },
    {
      "epoch": 3.09428403064231,
      "grad_norm": 8.015741348266602,
      "learning_rate": 1.9214207013088935e-05,
      "loss": 29.5676,
      "step": 167
    },
    {
      "epoch": 3.113140836770772,
      "grad_norm": 5.530745029449463,
      "learning_rate": 1.920250635785953e-05,
      "loss": 30.6979,
      "step": 168
    },
    {
      "epoch": 3.131997642899234,
      "grad_norm": 6.337421417236328,
      "learning_rate": 1.9190722848877683e-05,
      "loss": 29.0636,
      "step": 169
    },
    {
      "epoch": 3.1508544490276957,
      "grad_norm": 5.2866530418396,
      "learning_rate": 1.9178856592234927e-05,
      "loss": 29.1127,
      "step": 170
    },
    {
      "epoch": 3.169711255156158,
      "grad_norm": 5.293655872344971,
      "learning_rate": 1.916690769476779e-05,
      "loss": 28.4485,
      "step": 171
    },
    {
      "epoch": 3.18856806128462,
      "grad_norm": 5.774308681488037,
      "learning_rate": 1.9154876264056863e-05,
      "loss": 30.0064,
      "step": 172
    },
    {
      "epoch": 3.207424867413082,
      "grad_norm": 5.795831680297852,
      "learning_rate": 1.9142762408425797e-05,
      "loss": 28.3272,
      "step": 173
    },
    {
      "epoch": 3.226281673541544,
      "grad_norm": 9.740551948547363,
      "learning_rate": 1.9130566236940363e-05,
      "loss": 28.7381,
      "step": 174
    },
    {
      "epoch": 3.245138479670006,
      "grad_norm": 5.336945056915283,
      "learning_rate": 1.911828785940745e-05,
      "loss": 30.7253,
      "step": 175
    },
    {
      "epoch": 3.263995285798468,
      "grad_norm": 5.5617499351501465,
      "learning_rate": 1.910592738637407e-05,
      "loss": 30.7394,
      "step": 176
    },
    {
      "epoch": 3.2828520919269297,
      "grad_norm": 5.992559432983398,
      "learning_rate": 1.9093484929126383e-05,
      "loss": 30.1875,
      "step": 177
    },
    {
      "epoch": 3.301708898055392,
      "grad_norm": 6.186973571777344,
      "learning_rate": 1.908096059968869e-05,
      "loss": 30.5341,
      "step": 178
    },
    {
      "epoch": 3.3205657041838537,
      "grad_norm": 5.4106831550598145,
      "learning_rate": 1.9068354510822402e-05,
      "loss": 29.5997,
      "step": 179
    },
    {
      "epoch": 3.339422510312316,
      "grad_norm": 6.253762722015381,
      "learning_rate": 1.905566677602506e-05,
      "loss": 30.8852,
      "step": 180
    },
    {
      "epoch": 3.358279316440778,
      "grad_norm": 5.7256245613098145,
      "learning_rate": 1.904289750952928e-05,
      "loss": 27.654,
      "step": 181
    },
    {
      "epoch": 3.37713612256924,
      "grad_norm": 6.161690711975098,
      "learning_rate": 1.9030046826301746e-05,
      "loss": 28.6103,
      "step": 182
    },
    {
      "epoch": 3.395992928697702,
      "grad_norm": 5.655315399169922,
      "learning_rate": 1.9017114842042174e-05,
      "loss": 29.5803,
      "step": 183
    },
    {
      "epoch": 3.4148497348261637,
      "grad_norm": 6.141570091247559,
      "learning_rate": 1.900410167318226e-05,
      "loss": 29.0428,
      "step": 184
    },
    {
      "epoch": 3.433706540954626,
      "grad_norm": 5.971486568450928,
      "learning_rate": 1.8991007436884633e-05,
      "loss": 29.1941,
      "step": 185
    },
    {
      "epoch": 3.4525633470830877,
      "grad_norm": 6.080516338348389,
      "learning_rate": 1.897783225104181e-05,
      "loss": 30.1013,
      "step": 186
    },
    {
      "epoch": 3.47142015321155,
      "grad_norm": 5.531688690185547,
      "learning_rate": 1.8964576234275123e-05,
      "loss": 30.7952,
      "step": 187
    },
    {
      "epoch": 3.4902769593400116,
      "grad_norm": 5.619970798492432,
      "learning_rate": 1.8951239505933663e-05,
      "loss": 29.4957,
      "step": 188
    },
    {
      "epoch": 3.5091337654684738,
      "grad_norm": 6.54060697555542,
      "learning_rate": 1.893782218609319e-05,
      "loss": 27.7147,
      "step": 189
    },
    {
      "epoch": 3.527990571596936,
      "grad_norm": 5.4967122077941895,
      "learning_rate": 1.8924324395555066e-05,
      "loss": 28.405,
      "step": 190
    },
    {
      "epoch": 3.5468473777253977,
      "grad_norm": 5.702770709991455,
      "learning_rate": 1.8910746255845168e-05,
      "loss": 29.0973,
      "step": 191
    },
    {
      "epoch": 3.56570418385386,
      "grad_norm": 5.716518402099609,
      "learning_rate": 1.8897087889212772e-05,
      "loss": 29.6652,
      "step": 192
    },
    {
      "epoch": 3.5845609899823216,
      "grad_norm": 5.409316062927246,
      "learning_rate": 1.8883349418629487e-05,
      "loss": 29.0534,
      "step": 193
    },
    {
      "epoch": 3.603417796110784,
      "grad_norm": 5.536512851715088,
      "learning_rate": 1.886953096778811e-05,
      "loss": 28.9359,
      "step": 194
    },
    {
      "epoch": 3.6222746022392456,
      "grad_norm": 5.557838439941406,
      "learning_rate": 1.885563266110155e-05,
      "loss": 31.3251,
      "step": 195
    },
    {
      "epoch": 3.6411314083677078,
      "grad_norm": 6.237278938293457,
      "learning_rate": 1.8841654623701673e-05,
      "loss": 29.9964,
      "step": 196
    },
    {
      "epoch": 3.6599882144961695,
      "grad_norm": 5.853729724884033,
      "learning_rate": 1.8827596981438202e-05,
      "loss": 27.6037,
      "step": 197
    },
    {
      "epoch": 3.6788450206246317,
      "grad_norm": 6.554260730743408,
      "learning_rate": 1.8813459860877575e-05,
      "loss": 28.5874,
      "step": 198
    },
    {
      "epoch": 3.697701826753094,
      "grad_norm": 5.716013431549072,
      "learning_rate": 1.8799243389301796e-05,
      "loss": 26.6776,
      "step": 199
    },
    {
      "epoch": 3.7165586328815556,
      "grad_norm": 5.954282760620117,
      "learning_rate": 1.87849476947073e-05,
      "loss": 28.6449,
      "step": 200
    },
    {
      "epoch": 3.735415439010018,
      "grad_norm": 5.599502086639404,
      "learning_rate": 1.8770572905803806e-05,
      "loss": 27.2543,
      "step": 201
    },
    {
      "epoch": 3.7542722451384796,
      "grad_norm": 6.167278289794922,
      "learning_rate": 1.8756119152013134e-05,
      "loss": 30.5196,
      "step": 202
    },
    {
      "epoch": 3.7731290512669418,
      "grad_norm": 6.045587062835693,
      "learning_rate": 1.8741586563468064e-05,
      "loss": 28.3264,
      "step": 203
    },
    {
      "epoch": 3.7919858573954035,
      "grad_norm": 7.914190292358398,
      "learning_rate": 1.8726975271011163e-05,
      "loss": 27.6603,
      "step": 204
    },
    {
      "epoch": 3.8108426635238657,
      "grad_norm": 6.3347487449646,
      "learning_rate": 1.8712285406193585e-05,
      "loss": 32.0242,
      "step": 205
    },
    {
      "epoch": 3.8296994696523274,
      "grad_norm": 5.800095081329346,
      "learning_rate": 1.869751710127392e-05,
      "loss": 30.2465,
      "step": 206
    },
    {
      "epoch": 3.8485562757807896,
      "grad_norm": 6.07722806930542,
      "learning_rate": 1.868267048921697e-05,
      "loss": 29.8326,
      "step": 207
    },
    {
      "epoch": 3.867413081909252,
      "grad_norm": 5.792522430419922,
      "learning_rate": 1.866774570369257e-05,
      "loss": 30.4469,
      "step": 208
    },
    {
      "epoch": 3.8862698880377136,
      "grad_norm": 5.497482776641846,
      "learning_rate": 1.8652742879074384e-05,
      "loss": 30.517,
      "step": 209
    },
    {
      "epoch": 3.9051266941661757,
      "grad_norm": 5.835074424743652,
      "learning_rate": 1.8637662150438695e-05,
      "loss": 30.3409,
      "step": 210
    },
    {
      "epoch": 3.9239835002946375,
      "grad_norm": 5.447529315948486,
      "learning_rate": 1.8622503653563173e-05,
      "loss": 29.652,
      "step": 211
    },
    {
      "epoch": 3.9428403064230997,
      "grad_norm": 5.713332176208496,
      "learning_rate": 1.8607267524925684e-05,
      "loss": 29.2981,
      "step": 212
    },
    {
      "epoch": 3.9616971125515614,
      "grad_norm": 5.341300964355469,
      "learning_rate": 1.8591953901703028e-05,
      "loss": 28.7348,
      "step": 213
    },
    {
      "epoch": 3.9805539186800236,
      "grad_norm": 6.711262226104736,
      "learning_rate": 1.8576562921769727e-05,
      "loss": 31.3831,
      "step": 214
    },
    {
      "epoch": 3.9994107248084854,
      "grad_norm": 5.850591659545898,
      "learning_rate": 1.8561094723696776e-05,
      "loss": 29.9802,
      "step": 215
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.9551467895507812,
      "learning_rate": 1.8545549446750392e-05,
      "loss": 0.8887,
      "step": 216
    },
    {
      "epoch": 4.018856806128462,
      "grad_norm": 5.938194751739502,
      "learning_rate": 1.8529927230890757e-05,
      "loss": 29.8595,
      "step": 217
    },
    {
      "epoch": 4.037713612256924,
      "grad_norm": 5.941755771636963,
      "learning_rate": 1.8514228216770784e-05,
      "loss": 28.2747,
      "step": 218
    },
    {
      "epoch": 4.056570418385386,
      "grad_norm": 5.786048889160156,
      "learning_rate": 1.8498452545734808e-05,
      "loss": 28.5121,
      "step": 219
    },
    {
      "epoch": 4.075427224513848,
      "grad_norm": 6.0053606033325195,
      "learning_rate": 1.8482600359817344e-05,
      "loss": 27.6703,
      "step": 220
    },
    {
      "epoch": 4.09428403064231,
      "grad_norm": 6.020656108856201,
      "learning_rate": 1.8466671801741812e-05,
      "loss": 29.9183,
      "step": 221
    },
    {
      "epoch": 4.113140836770772,
      "grad_norm": 5.832465171813965,
      "learning_rate": 1.845066701491922e-05,
      "loss": 29.4507,
      "step": 222
    },
    {
      "epoch": 4.1319976428992335,
      "grad_norm": 5.732546806335449,
      "learning_rate": 1.843458614344691e-05,
      "loss": 29.1375,
      "step": 223
    },
    {
      "epoch": 4.150854449027696,
      "grad_norm": 5.764674186706543,
      "learning_rate": 1.841842933210723e-05,
      "loss": 32.0308,
      "step": 224
    },
    {
      "epoch": 4.169711255156158,
      "grad_norm": 7.014339447021484,
      "learning_rate": 1.840219672636626e-05,
      "loss": 28.4993,
      "step": 225
    },
    {
      "epoch": 4.18856806128462,
      "grad_norm": 5.788493633270264,
      "learning_rate": 1.8385888472372474e-05,
      "loss": 28.4731,
      "step": 226
    },
    {
      "epoch": 4.207424867413082,
      "grad_norm": 5.4516682624816895,
      "learning_rate": 1.836950471695544e-05,
      "loss": 30.2475,
      "step": 227
    },
    {
      "epoch": 4.226281673541544,
      "grad_norm": 5.75223445892334,
      "learning_rate": 1.8353045607624494e-05,
      "loss": 29.2035,
      "step": 228
    },
    {
      "epoch": 4.245138479670006,
      "grad_norm": 5.979082107543945,
      "learning_rate": 1.833651129256742e-05,
      "loss": 30.7579,
      "step": 229
    },
    {
      "epoch": 4.263995285798468,
      "grad_norm": 6.404421806335449,
      "learning_rate": 1.8319901920649096e-05,
      "loss": 28.4604,
      "step": 230
    },
    {
      "epoch": 4.28285209192693,
      "grad_norm": 6.187481880187988,
      "learning_rate": 1.8303217641410174e-05,
      "loss": 29.9654,
      "step": 231
    },
    {
      "epoch": 4.3017088980553915,
      "grad_norm": 5.803898334503174,
      "learning_rate": 1.828645860506573e-05,
      "loss": 30.6545,
      "step": 232
    },
    {
      "epoch": 4.320565704183854,
      "grad_norm": 6.613586902618408,
      "learning_rate": 1.8269624962503895e-05,
      "loss": 31.1187,
      "step": 233
    },
    {
      "epoch": 4.339422510312316,
      "grad_norm": 5.967070579528809,
      "learning_rate": 1.825271686528452e-05,
      "loss": 27.0371,
      "step": 234
    },
    {
      "epoch": 4.358279316440778,
      "grad_norm": 6.227921485900879,
      "learning_rate": 1.8235734465637794e-05,
      "loss": 28.1629,
      "step": 235
    },
    {
      "epoch": 4.37713612256924,
      "grad_norm": 5.878361701965332,
      "learning_rate": 1.8218677916462882e-05,
      "loss": 28.548,
      "step": 236
    },
    {
      "epoch": 4.3959929286977015,
      "grad_norm": 6.377023696899414,
      "learning_rate": 1.8201547371326553e-05,
      "loss": 28.758,
      "step": 237
    },
    {
      "epoch": 4.414849734826164,
      "grad_norm": 5.937314510345459,
      "learning_rate": 1.8184342984461766e-05,
      "loss": 29.1732,
      "step": 238
    },
    {
      "epoch": 4.433706540954626,
      "grad_norm": 6.945167541503906,
      "learning_rate": 1.816706491076634e-05,
      "loss": 30.1351,
      "step": 239
    },
    {
      "epoch": 4.452563347083088,
      "grad_norm": 5.676613807678223,
      "learning_rate": 1.8149713305801505e-05,
      "loss": 30.404,
      "step": 240
    },
    {
      "epoch": 4.471420153211549,
      "grad_norm": 5.438294887542725,
      "learning_rate": 1.8132288325790518e-05,
      "loss": 27.3668,
      "step": 241
    },
    {
      "epoch": 4.490276959340012,
      "grad_norm": 6.052954196929932,
      "learning_rate": 1.8114790127617274e-05,
      "loss": 29.3487,
      "step": 242
    },
    {
      "epoch": 4.509133765468474,
      "grad_norm": 6.286992073059082,
      "learning_rate": 1.809721886882487e-05,
      "loss": 26.9897,
      "step": 243
    },
    {
      "epoch": 4.527990571596936,
      "grad_norm": 5.844583988189697,
      "learning_rate": 1.8079574707614202e-05,
      "loss": 29.8455,
      "step": 244
    },
    {
      "epoch": 4.546847377725397,
      "grad_norm": 6.633105754852295,
      "learning_rate": 1.806185780284253e-05,
      "loss": 27.9574,
      "step": 245
    },
    {
      "epoch": 4.5657041838538595,
      "grad_norm": 5.806082725524902,
      "learning_rate": 1.8044068314022057e-05,
      "loss": 29.1478,
      "step": 246
    },
    {
      "epoch": 4.584560989982322,
      "grad_norm": 6.550383567810059,
      "learning_rate": 1.802620640131848e-05,
      "loss": 27.8552,
      "step": 247
    },
    {
      "epoch": 4.603417796110784,
      "grad_norm": 5.67320442199707,
      "learning_rate": 1.800827222554957e-05,
      "loss": 29.1764,
      "step": 248
    },
    {
      "epoch": 4.622274602239246,
      "grad_norm": 5.52943754196167,
      "learning_rate": 1.79902659481837e-05,
      "loss": 28.5576,
      "step": 249
    },
    {
      "epoch": 4.641131408367707,
      "grad_norm": 6.66308069229126,
      "learning_rate": 1.797218773133841e-05,
      "loss": 29.4994,
      "step": 250
    },
    {
      "epoch": 4.6599882144961695,
      "grad_norm": 6.287024021148682,
      "learning_rate": 1.7954037737778927e-05,
      "loss": 28.2933,
      "step": 251
    },
    {
      "epoch": 4.678845020624632,
      "grad_norm": 6.055264472961426,
      "learning_rate": 1.7935816130916724e-05,
      "loss": 29.3869,
      "step": 252
    },
    {
      "epoch": 4.697701826753094,
      "grad_norm": 5.926159381866455,
      "learning_rate": 1.7917523074808024e-05,
      "loss": 27.9539,
      "step": 253
    },
    {
      "epoch": 4.716558632881556,
      "grad_norm": 6.345860481262207,
      "learning_rate": 1.789915873415235e-05,
      "loss": 28.5525,
      "step": 254
    },
    {
      "epoch": 4.735415439010017,
      "grad_norm": 7.237135887145996,
      "learning_rate": 1.7880723274291023e-05,
      "loss": 29.035,
      "step": 255
    },
    {
      "epoch": 4.75427224513848,
      "grad_norm": 5.795396327972412,
      "learning_rate": 1.786221686120567e-05,
      "loss": 29.7263,
      "step": 256
    },
    {
      "epoch": 4.773129051266942,
      "grad_norm": 5.708900451660156,
      "learning_rate": 1.7843639661516743e-05,
      "loss": 28.2827,
      "step": 257
    },
    {
      "epoch": 4.791985857395404,
      "grad_norm": 7.269504547119141,
      "learning_rate": 1.7824991842482014e-05,
      "loss": 30.0657,
      "step": 258
    },
    {
      "epoch": 4.810842663523866,
      "grad_norm": 5.381592750549316,
      "learning_rate": 1.7806273571995066e-05,
      "loss": 28.2911,
      "step": 259
    },
    {
      "epoch": 4.829699469652327,
      "grad_norm": 6.344857692718506,
      "learning_rate": 1.7787485018583792e-05,
      "loss": 28.2802,
      "step": 260
    },
    {
      "epoch": 4.84855627578079,
      "grad_norm": 7.4030351638793945,
      "learning_rate": 1.7768626351408856e-05,
      "loss": 29.7561,
      "step": 261
    },
    {
      "epoch": 4.867413081909252,
      "grad_norm": 5.588305473327637,
      "learning_rate": 1.7749697740262197e-05,
      "loss": 30.4978,
      "step": 262
    },
    {
      "epoch": 4.886269888037713,
      "grad_norm": 5.465107440948486,
      "learning_rate": 1.7730699355565478e-05,
      "loss": 29.5754,
      "step": 263
    },
    {
      "epoch": 4.905126694166175,
      "grad_norm": 5.770509719848633,
      "learning_rate": 1.7711631368368564e-05,
      "loss": 29.3061,
      "step": 264
    },
    {
      "epoch": 4.9239835002946375,
      "grad_norm": 5.637493133544922,
      "learning_rate": 1.769249395034797e-05,
      "loss": 29.8213,
      "step": 265
    },
    {
      "epoch": 4.9428403064231,
      "grad_norm": 5.6650285720825195,
      "learning_rate": 1.7673287273805342e-05,
      "loss": 28.9212,
      "step": 266
    },
    {
      "epoch": 4.961697112551562,
      "grad_norm": 6.288497447967529,
      "learning_rate": 1.7654011511665875e-05,
      "loss": 30.0992,
      "step": 267
    },
    {
      "epoch": 4.980553918680023,
      "grad_norm": 5.7265191078186035,
      "learning_rate": 1.7634666837476765e-05,
      "loss": 30.4485,
      "step": 268
    },
    {
      "epoch": 4.999410724808485,
      "grad_norm": 6.410874366760254,
      "learning_rate": 1.761525342540566e-05,
      "loss": 27.5523,
      "step": 269
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.057327389717102,
      "learning_rate": 1.7595771450239075e-05,
      "loss": 1.1414,
      "step": 270
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 1080,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.096080968067318e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
