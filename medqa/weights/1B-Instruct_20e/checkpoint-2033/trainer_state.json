{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 19.0,
  "eval_steps": 500,
  "global_step": 2033,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009431181844974948,
      "grad_norm": 0.7654711008071899,
      "learning_rate": 0.0,
      "loss": 1.7455,
      "step": 1
    },
    {
      "epoch": 0.018862363689949896,
      "grad_norm": 0.5938795804977417,
      "learning_rate": 3.0769230769230774e-07,
      "loss": 1.649,
      "step": 2
    },
    {
      "epoch": 0.028293545534924844,
      "grad_norm": 0.7216940522193909,
      "learning_rate": 6.153846153846155e-07,
      "loss": 1.6863,
      "step": 3
    },
    {
      "epoch": 0.03772472737989979,
      "grad_norm": 0.7538224458694458,
      "learning_rate": 9.230769230769232e-07,
      "loss": 1.7526,
      "step": 4
    },
    {
      "epoch": 0.04715590922487474,
      "grad_norm": 0.6486019492149353,
      "learning_rate": 1.230769230769231e-06,
      "loss": 1.6975,
      "step": 5
    },
    {
      "epoch": 0.05658709106984969,
      "grad_norm": 0.669456422328949,
      "learning_rate": 1.5384615384615387e-06,
      "loss": 1.7082,
      "step": 6
    },
    {
      "epoch": 0.06601827291482464,
      "grad_norm": 0.7008364200592041,
      "learning_rate": 1.8461538461538465e-06,
      "loss": 1.6665,
      "step": 7
    },
    {
      "epoch": 0.07544945475979958,
      "grad_norm": 0.7081489562988281,
      "learning_rate": 2.153846153846154e-06,
      "loss": 1.6675,
      "step": 8
    },
    {
      "epoch": 0.08488063660477453,
      "grad_norm": 0.6777686476707458,
      "learning_rate": 2.461538461538462e-06,
      "loss": 1.6669,
      "step": 9
    },
    {
      "epoch": 0.09431181844974948,
      "grad_norm": 0.6384946703910828,
      "learning_rate": 2.7692307692307697e-06,
      "loss": 1.6636,
      "step": 10
    },
    {
      "epoch": 0.10374300029472443,
      "grad_norm": 0.6854397654533386,
      "learning_rate": 3.0769230769230774e-06,
      "loss": 1.681,
      "step": 11
    },
    {
      "epoch": 0.11317418213969938,
      "grad_norm": 0.7633715867996216,
      "learning_rate": 3.384615384615385e-06,
      "loss": 1.7297,
      "step": 12
    },
    {
      "epoch": 0.12260536398467432,
      "grad_norm": 0.7567028403282166,
      "learning_rate": 3.692307692307693e-06,
      "loss": 1.7734,
      "step": 13
    },
    {
      "epoch": 0.13203654582964927,
      "grad_norm": 0.6841281652450562,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.6986,
      "step": 14
    },
    {
      "epoch": 0.14146772767462423,
      "grad_norm": 0.8022035360336304,
      "learning_rate": 4.307692307692308e-06,
      "loss": 1.7437,
      "step": 15
    },
    {
      "epoch": 0.15089890951959917,
      "grad_norm": 0.7160937786102295,
      "learning_rate": 4.615384615384616e-06,
      "loss": 1.6889,
      "step": 16
    },
    {
      "epoch": 0.16033009136457413,
      "grad_norm": 0.6825652122497559,
      "learning_rate": 4.923076923076924e-06,
      "loss": 1.7138,
      "step": 17
    },
    {
      "epoch": 0.16976127320954906,
      "grad_norm": 0.7159819006919861,
      "learning_rate": 5.230769230769232e-06,
      "loss": 1.7472,
      "step": 18
    },
    {
      "epoch": 0.17919245505452402,
      "grad_norm": 0.856762707233429,
      "learning_rate": 5.538461538461539e-06,
      "loss": 1.7539,
      "step": 19
    },
    {
      "epoch": 0.18862363689949896,
      "grad_norm": 0.7347280383110046,
      "learning_rate": 5.846153846153847e-06,
      "loss": 1.7326,
      "step": 20
    },
    {
      "epoch": 0.19805481874447392,
      "grad_norm": 0.6564392447471619,
      "learning_rate": 6.153846153846155e-06,
      "loss": 1.6699,
      "step": 21
    },
    {
      "epoch": 0.20748600058944885,
      "grad_norm": 0.8096906542778015,
      "learning_rate": 6.461538461538463e-06,
      "loss": 1.7493,
      "step": 22
    },
    {
      "epoch": 0.21691718243442382,
      "grad_norm": 0.7590331435203552,
      "learning_rate": 6.76923076923077e-06,
      "loss": 1.7038,
      "step": 23
    },
    {
      "epoch": 0.22634836427939875,
      "grad_norm": 0.6532320380210876,
      "learning_rate": 7.076923076923078e-06,
      "loss": 1.6605,
      "step": 24
    },
    {
      "epoch": 0.2357795461243737,
      "grad_norm": 0.6799933910369873,
      "learning_rate": 7.384615384615386e-06,
      "loss": 1.6572,
      "step": 25
    },
    {
      "epoch": 0.24521072796934865,
      "grad_norm": 0.6848971247673035,
      "learning_rate": 7.692307692307694e-06,
      "loss": 1.5988,
      "step": 26
    },
    {
      "epoch": 0.2546419098143236,
      "grad_norm": 0.9095935821533203,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.7173,
      "step": 27
    },
    {
      "epoch": 0.26407309165929854,
      "grad_norm": 0.6684953570365906,
      "learning_rate": 8.307692307692309e-06,
      "loss": 1.6907,
      "step": 28
    },
    {
      "epoch": 0.27350427350427353,
      "grad_norm": 0.7581301331520081,
      "learning_rate": 8.615384615384617e-06,
      "loss": 1.6751,
      "step": 29
    },
    {
      "epoch": 0.28293545534924847,
      "grad_norm": 0.6985102891921997,
      "learning_rate": 8.923076923076925e-06,
      "loss": 1.6918,
      "step": 30
    },
    {
      "epoch": 0.2923666371942234,
      "grad_norm": 0.7206470370292664,
      "learning_rate": 9.230769230769232e-06,
      "loss": 1.6721,
      "step": 31
    },
    {
      "epoch": 0.30179781903919833,
      "grad_norm": 0.7244014143943787,
      "learning_rate": 9.53846153846154e-06,
      "loss": 1.6754,
      "step": 32
    },
    {
      "epoch": 0.3112290008841733,
      "grad_norm": 0.7361248135566711,
      "learning_rate": 9.846153846153848e-06,
      "loss": 1.6822,
      "step": 33
    },
    {
      "epoch": 0.32066018272914826,
      "grad_norm": 0.6995781064033508,
      "learning_rate": 1.0153846153846154e-05,
      "loss": 1.6507,
      "step": 34
    },
    {
      "epoch": 0.3300913645741232,
      "grad_norm": 0.7522406578063965,
      "learning_rate": 1.0461538461538463e-05,
      "loss": 1.6836,
      "step": 35
    },
    {
      "epoch": 0.3395225464190981,
      "grad_norm": 0.7374791502952576,
      "learning_rate": 1.076923076923077e-05,
      "loss": 1.6676,
      "step": 36
    },
    {
      "epoch": 0.3489537282640731,
      "grad_norm": 0.8282987475395203,
      "learning_rate": 1.1076923076923079e-05,
      "loss": 1.6731,
      "step": 37
    },
    {
      "epoch": 0.35838491010904805,
      "grad_norm": 0.8065078258514404,
      "learning_rate": 1.1384615384615385e-05,
      "loss": 1.6609,
      "step": 38
    },
    {
      "epoch": 0.367816091954023,
      "grad_norm": 0.8300322890281677,
      "learning_rate": 1.1692307692307694e-05,
      "loss": 1.6438,
      "step": 39
    },
    {
      "epoch": 0.3772472737989979,
      "grad_norm": 0.751632034778595,
      "learning_rate": 1.2e-05,
      "loss": 1.6649,
      "step": 40
    },
    {
      "epoch": 0.3866784556439729,
      "grad_norm": 0.7661802768707275,
      "learning_rate": 1.230769230769231e-05,
      "loss": 1.6486,
      "step": 41
    },
    {
      "epoch": 0.39610963748894784,
      "grad_norm": 0.8092883825302124,
      "learning_rate": 1.2615384615384616e-05,
      "loss": 1.6631,
      "step": 42
    },
    {
      "epoch": 0.4055408193339228,
      "grad_norm": 0.8466928601264954,
      "learning_rate": 1.2923076923076925e-05,
      "loss": 1.6371,
      "step": 43
    },
    {
      "epoch": 0.4149720011788977,
      "grad_norm": 0.9855638146400452,
      "learning_rate": 1.3230769230769231e-05,
      "loss": 1.721,
      "step": 44
    },
    {
      "epoch": 0.4244031830238727,
      "grad_norm": 0.8279311060905457,
      "learning_rate": 1.353846153846154e-05,
      "loss": 1.6403,
      "step": 45
    },
    {
      "epoch": 0.43383436486884763,
      "grad_norm": 0.8579984307289124,
      "learning_rate": 1.3846153846153847e-05,
      "loss": 1.6287,
      "step": 46
    },
    {
      "epoch": 0.44326554671382257,
      "grad_norm": 0.8750467896461487,
      "learning_rate": 1.4153846153846156e-05,
      "loss": 1.6442,
      "step": 47
    },
    {
      "epoch": 0.4526967285587975,
      "grad_norm": 1.0644595623016357,
      "learning_rate": 1.4461538461538462e-05,
      "loss": 1.6428,
      "step": 48
    },
    {
      "epoch": 0.4621279104037725,
      "grad_norm": 0.9165589809417725,
      "learning_rate": 1.4769230769230772e-05,
      "loss": 1.5852,
      "step": 49
    },
    {
      "epoch": 0.4715590922487474,
      "grad_norm": 0.9075687527656555,
      "learning_rate": 1.5076923076923078e-05,
      "loss": 1.5863,
      "step": 50
    },
    {
      "epoch": 0.48099027409372236,
      "grad_norm": 0.8242354989051819,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 1.557,
      "step": 51
    },
    {
      "epoch": 0.4904214559386973,
      "grad_norm": 1.0253599882125854,
      "learning_rate": 1.5692307692307693e-05,
      "loss": 1.5798,
      "step": 52
    },
    {
      "epoch": 0.4998526377836723,
      "grad_norm": 0.9371146559715271,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.569,
      "step": 53
    },
    {
      "epoch": 0.5092838196286472,
      "grad_norm": 1.0246520042419434,
      "learning_rate": 1.630769230769231e-05,
      "loss": 1.6322,
      "step": 54
    },
    {
      "epoch": 0.5187150014736222,
      "grad_norm": 0.9090097546577454,
      "learning_rate": 1.6615384615384618e-05,
      "loss": 1.5657,
      "step": 55
    },
    {
      "epoch": 0.5281461833185971,
      "grad_norm": 0.9521082043647766,
      "learning_rate": 1.6923076923076924e-05,
      "loss": 1.5479,
      "step": 56
    },
    {
      "epoch": 0.537577365163572,
      "grad_norm": 0.9220258593559265,
      "learning_rate": 1.7230769230769234e-05,
      "loss": 1.5576,
      "step": 57
    },
    {
      "epoch": 0.5470085470085471,
      "grad_norm": 0.8513513803482056,
      "learning_rate": 1.753846153846154e-05,
      "loss": 1.5203,
      "step": 58
    },
    {
      "epoch": 0.556439728853522,
      "grad_norm": 0.9724740982055664,
      "learning_rate": 1.784615384615385e-05,
      "loss": 1.5397,
      "step": 59
    },
    {
      "epoch": 0.5658709106984969,
      "grad_norm": 1.1317498683929443,
      "learning_rate": 1.8153846153846155e-05,
      "loss": 1.5655,
      "step": 60
    },
    {
      "epoch": 0.5753020925434719,
      "grad_norm": 0.84391188621521,
      "learning_rate": 1.8461538461538465e-05,
      "loss": 1.5133,
      "step": 61
    },
    {
      "epoch": 0.5847332743884468,
      "grad_norm": 0.8949248194694519,
      "learning_rate": 1.876923076923077e-05,
      "loss": 1.4994,
      "step": 62
    },
    {
      "epoch": 0.5941644562334217,
      "grad_norm": 0.9082351326942444,
      "learning_rate": 1.907692307692308e-05,
      "loss": 1.5246,
      "step": 63
    },
    {
      "epoch": 0.6035956380783967,
      "grad_norm": 0.8704085946083069,
      "learning_rate": 1.9384615384615386e-05,
      "loss": 1.5067,
      "step": 64
    },
    {
      "epoch": 0.6130268199233716,
      "grad_norm": 0.7937524914741516,
      "learning_rate": 1.9692307692307696e-05,
      "loss": 1.4291,
      "step": 65
    },
    {
      "epoch": 0.6224580017683466,
      "grad_norm": 0.833602786064148,
      "learning_rate": 2e-05,
      "loss": 1.4196,
      "step": 66
    },
    {
      "epoch": 0.6318891836133216,
      "grad_norm": 0.907242476940155,
      "learning_rate": 1.999998853871097e-05,
      "loss": 1.4721,
      "step": 67
    },
    {
      "epoch": 0.6413203654582965,
      "grad_norm": 0.8051930069923401,
      "learning_rate": 1.9999954154870155e-05,
      "loss": 1.4492,
      "step": 68
    },
    {
      "epoch": 0.6507515473032714,
      "grad_norm": 0.8054486513137817,
      "learning_rate": 1.9999896848556367e-05,
      "loss": 1.4623,
      "step": 69
    },
    {
      "epoch": 0.6601827291482464,
      "grad_norm": 0.708842933177948,
      "learning_rate": 1.999981661990097e-05,
      "loss": 1.4007,
      "step": 70
    },
    {
      "epoch": 0.6696139109932213,
      "grad_norm": 0.8394299745559692,
      "learning_rate": 1.9999713469087866e-05,
      "loss": 1.4934,
      "step": 71
    },
    {
      "epoch": 0.6790450928381963,
      "grad_norm": 0.8844977021217346,
      "learning_rate": 1.9999587396353504e-05,
      "loss": 1.4546,
      "step": 72
    },
    {
      "epoch": 0.6884762746831712,
      "grad_norm": 0.8442448973655701,
      "learning_rate": 1.9999438401986882e-05,
      "loss": 1.3769,
      "step": 73
    },
    {
      "epoch": 0.6979074565281462,
      "grad_norm": 0.8486939668655396,
      "learning_rate": 1.9999266486329524e-05,
      "loss": 1.4177,
      "step": 74
    },
    {
      "epoch": 0.7073386383731212,
      "grad_norm": 0.8652157783508301,
      "learning_rate": 1.9999071649775508e-05,
      "loss": 1.397,
      "step": 75
    },
    {
      "epoch": 0.7167698202180961,
      "grad_norm": 0.8654371500015259,
      "learning_rate": 1.9998853892771453e-05,
      "loss": 1.3883,
      "step": 76
    },
    {
      "epoch": 0.726201002063071,
      "grad_norm": 0.9908559918403625,
      "learning_rate": 1.9998613215816508e-05,
      "loss": 1.4272,
      "step": 77
    },
    {
      "epoch": 0.735632183908046,
      "grad_norm": 0.8759218454360962,
      "learning_rate": 1.9998349619462373e-05,
      "loss": 1.3405,
      "step": 78
    },
    {
      "epoch": 0.7450633657530209,
      "grad_norm": 0.8894333243370056,
      "learning_rate": 1.9998063104313276e-05,
      "loss": 1.3032,
      "step": 79
    },
    {
      "epoch": 0.7544945475979958,
      "grad_norm": 0.9892635345458984,
      "learning_rate": 1.9997753671025984e-05,
      "loss": 1.3439,
      "step": 80
    },
    {
      "epoch": 0.7639257294429708,
      "grad_norm": 0.9199776649475098,
      "learning_rate": 1.9997421320309795e-05,
      "loss": 1.3401,
      "step": 81
    },
    {
      "epoch": 0.7733569112879458,
      "grad_norm": 0.979968786239624,
      "learning_rate": 1.999706605292655e-05,
      "loss": 1.3755,
      "step": 82
    },
    {
      "epoch": 0.7827880931329207,
      "grad_norm": 1.0129528045654297,
      "learning_rate": 1.9996687869690607e-05,
      "loss": 1.3053,
      "step": 83
    },
    {
      "epoch": 0.7922192749778957,
      "grad_norm": 0.9926366209983826,
      "learning_rate": 1.999628677146886e-05,
      "loss": 1.3259,
      "step": 84
    },
    {
      "epoch": 0.8016504568228706,
      "grad_norm": 0.9724948406219482,
      "learning_rate": 1.999586275918073e-05,
      "loss": 1.3526,
      "step": 85
    },
    {
      "epoch": 0.8110816386678456,
      "grad_norm": 0.9685108065605164,
      "learning_rate": 1.999541583379816e-05,
      "loss": 1.2672,
      "step": 86
    },
    {
      "epoch": 0.8205128205128205,
      "grad_norm": 0.94893878698349,
      "learning_rate": 1.9994945996345625e-05,
      "loss": 1.2915,
      "step": 87
    },
    {
      "epoch": 0.8299440023577954,
      "grad_norm": 0.8360689282417297,
      "learning_rate": 1.9994453247900107e-05,
      "loss": 1.2923,
      "step": 88
    },
    {
      "epoch": 0.8393751842027705,
      "grad_norm": 0.9599177241325378,
      "learning_rate": 1.9993937589591116e-05,
      "loss": 1.2415,
      "step": 89
    },
    {
      "epoch": 0.8488063660477454,
      "grad_norm": 0.8713008761405945,
      "learning_rate": 1.9993399022600677e-05,
      "loss": 1.2427,
      "step": 90
    },
    {
      "epoch": 0.8582375478927203,
      "grad_norm": 0.8285272717475891,
      "learning_rate": 1.9992837548163315e-05,
      "loss": 1.2044,
      "step": 91
    },
    {
      "epoch": 0.8676687297376953,
      "grad_norm": 0.7297118902206421,
      "learning_rate": 1.999225316756608e-05,
      "loss": 1.2165,
      "step": 92
    },
    {
      "epoch": 0.8770999115826702,
      "grad_norm": 0.6106252074241638,
      "learning_rate": 1.9991645882148528e-05,
      "loss": 1.2588,
      "step": 93
    },
    {
      "epoch": 0.8865310934276451,
      "grad_norm": 0.5541276335716248,
      "learning_rate": 1.9991015693302705e-05,
      "loss": 1.2429,
      "step": 94
    },
    {
      "epoch": 0.8959622752726201,
      "grad_norm": 0.5221897959709167,
      "learning_rate": 1.999036260247317e-05,
      "loss": 1.2145,
      "step": 95
    },
    {
      "epoch": 0.905393457117595,
      "grad_norm": 0.5134469866752625,
      "learning_rate": 1.9989686611156973e-05,
      "loss": 1.2295,
      "step": 96
    },
    {
      "epoch": 0.91482463896257,
      "grad_norm": 0.4935285151004791,
      "learning_rate": 1.9988987720903665e-05,
      "loss": 1.2322,
      "step": 97
    },
    {
      "epoch": 0.924255820807545,
      "grad_norm": 0.4750697910785675,
      "learning_rate": 1.9988265933315278e-05,
      "loss": 1.232,
      "step": 98
    },
    {
      "epoch": 0.9336870026525199,
      "grad_norm": 0.5253803133964539,
      "learning_rate": 1.9987521250046337e-05,
      "loss": 1.2264,
      "step": 99
    },
    {
      "epoch": 0.9431181844974948,
      "grad_norm": 0.456484854221344,
      "learning_rate": 1.998675367280385e-05,
      "loss": 1.1997,
      "step": 100
    },
    {
      "epoch": 0.9525493663424698,
      "grad_norm": 0.4444773197174072,
      "learning_rate": 1.99859632033473e-05,
      "loss": 1.2383,
      "step": 101
    },
    {
      "epoch": 0.9619805481874447,
      "grad_norm": 0.4638809561729431,
      "learning_rate": 1.998514984348865e-05,
      "loss": 1.2665,
      "step": 102
    },
    {
      "epoch": 0.9714117300324197,
      "grad_norm": 0.4478534460067749,
      "learning_rate": 1.9984313595092325e-05,
      "loss": 1.2199,
      "step": 103
    },
    {
      "epoch": 0.9808429118773946,
      "grad_norm": 0.41830265522003174,
      "learning_rate": 1.9983454460075222e-05,
      "loss": 1.2094,
      "step": 104
    },
    {
      "epoch": 0.9902740937223696,
      "grad_norm": 0.4119296371936798,
      "learning_rate": 1.9982572440406704e-05,
      "loss": 1.2311,
      "step": 105
    },
    {
      "epoch": 0.9997052755673446,
      "grad_norm": 0.4095759689807892,
      "learning_rate": 1.9981667538108588e-05,
      "loss": 1.2543,
      "step": 106
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.7893154621124268,
      "learning_rate": 1.9980739755255142e-05,
      "loss": 0.7991,
      "step": 107
    },
    {
      "epoch": 1.009431181844975,
      "grad_norm": 0.3867049515247345,
      "learning_rate": 1.997978909397308e-05,
      "loss": 1.2303,
      "step": 108
    },
    {
      "epoch": 1.0188623636899499,
      "grad_norm": 0.39822933077812195,
      "learning_rate": 1.9978815556441564e-05,
      "loss": 1.2093,
      "step": 109
    },
    {
      "epoch": 1.028293545534925,
      "grad_norm": 0.36445388197898865,
      "learning_rate": 1.9977819144892198e-05,
      "loss": 1.2356,
      "step": 110
    },
    {
      "epoch": 1.0377247273798997,
      "grad_norm": 0.35035714507102966,
      "learning_rate": 1.9976799861609008e-05,
      "loss": 1.1986,
      "step": 111
    },
    {
      "epoch": 1.0471559092248748,
      "grad_norm": 0.37209346890449524,
      "learning_rate": 1.997575770892846e-05,
      "loss": 1.1641,
      "step": 112
    },
    {
      "epoch": 1.0565870910698496,
      "grad_norm": 0.3779638409614563,
      "learning_rate": 1.997469268923943e-05,
      "loss": 1.1849,
      "step": 113
    },
    {
      "epoch": 1.0660182729148246,
      "grad_norm": 0.3417713940143585,
      "learning_rate": 1.9973604804983222e-05,
      "loss": 1.2307,
      "step": 114
    },
    {
      "epoch": 1.0754494547597995,
      "grad_norm": 0.33207541704177856,
      "learning_rate": 1.9972494058653548e-05,
      "loss": 1.1918,
      "step": 115
    },
    {
      "epoch": 1.0848806366047745,
      "grad_norm": 0.3354134261608124,
      "learning_rate": 1.9971360452796523e-05,
      "loss": 1.2213,
      "step": 116
    },
    {
      "epoch": 1.0943118184497496,
      "grad_norm": 0.3518228828907013,
      "learning_rate": 1.997020399001066e-05,
      "loss": 1.1885,
      "step": 117
    },
    {
      "epoch": 1.1037430002947244,
      "grad_norm": 0.3330670893192291,
      "learning_rate": 1.996902467294688e-05,
      "loss": 1.1648,
      "step": 118
    },
    {
      "epoch": 1.1131741821396994,
      "grad_norm": 0.30602288246154785,
      "learning_rate": 1.996782250430847e-05,
      "loss": 1.2057,
      "step": 119
    },
    {
      "epoch": 1.1226053639846743,
      "grad_norm": 0.30649417638778687,
      "learning_rate": 1.996659748685112e-05,
      "loss": 1.2556,
      "step": 120
    },
    {
      "epoch": 1.1320365458296493,
      "grad_norm": 0.3041781187057495,
      "learning_rate": 1.996534962338288e-05,
      "loss": 1.1726,
      "step": 121
    },
    {
      "epoch": 1.1414677276746241,
      "grad_norm": 0.28831028938293457,
      "learning_rate": 1.9964078916764176e-05,
      "loss": 1.2167,
      "step": 122
    },
    {
      "epoch": 1.1508989095195992,
      "grad_norm": 0.2942899465560913,
      "learning_rate": 1.99627853699078e-05,
      "loss": 1.2183,
      "step": 123
    },
    {
      "epoch": 1.1603300913645742,
      "grad_norm": 0.3021261394023895,
      "learning_rate": 1.9961468985778886e-05,
      "loss": 1.2179,
      "step": 124
    },
    {
      "epoch": 1.169761273209549,
      "grad_norm": 0.2930825352668762,
      "learning_rate": 1.9960129767394935e-05,
      "loss": 1.2202,
      "step": 125
    },
    {
      "epoch": 1.179192455054524,
      "grad_norm": 0.3076576292514801,
      "learning_rate": 1.995876771782577e-05,
      "loss": 1.1869,
      "step": 126
    },
    {
      "epoch": 1.188623636899499,
      "grad_norm": 0.2926398515701294,
      "learning_rate": 1.9957382840193573e-05,
      "loss": 1.1875,
      "step": 127
    },
    {
      "epoch": 1.198054818744474,
      "grad_norm": 0.31757989525794983,
      "learning_rate": 1.995597513767283e-05,
      "loss": 1.2074,
      "step": 128
    },
    {
      "epoch": 1.2074860005894488,
      "grad_norm": 0.2884342074394226,
      "learning_rate": 1.9954544613490362e-05,
      "loss": 1.136,
      "step": 129
    },
    {
      "epoch": 1.2169171824344238,
      "grad_norm": 0.2827122211456299,
      "learning_rate": 1.9953091270925297e-05,
      "loss": 1.2014,
      "step": 130
    },
    {
      "epoch": 1.2263483642793989,
      "grad_norm": 0.28180789947509766,
      "learning_rate": 1.9951615113309076e-05,
      "loss": 1.1949,
      "step": 131
    },
    {
      "epoch": 1.2357795461243737,
      "grad_norm": 0.2954806685447693,
      "learning_rate": 1.9950116144025428e-05,
      "loss": 1.1695,
      "step": 132
    },
    {
      "epoch": 1.2452107279693487,
      "grad_norm": 0.2792263627052307,
      "learning_rate": 1.994859436651038e-05,
      "loss": 1.1811,
      "step": 133
    },
    {
      "epoch": 1.2546419098143236,
      "grad_norm": 0.2622949481010437,
      "learning_rate": 1.9947049784252235e-05,
      "loss": 1.1507,
      "step": 134
    },
    {
      "epoch": 1.2640730916592986,
      "grad_norm": 0.2756691575050354,
      "learning_rate": 1.9945482400791577e-05,
      "loss": 1.2288,
      "step": 135
    },
    {
      "epoch": 1.2735042735042734,
      "grad_norm": 0.28202924132347107,
      "learning_rate": 1.9943892219721253e-05,
      "loss": 1.1896,
      "step": 136
    },
    {
      "epoch": 1.2829354553492485,
      "grad_norm": 0.2776024043560028,
      "learning_rate": 1.9942279244686364e-05,
      "loss": 1.2051,
      "step": 137
    },
    {
      "epoch": 1.2923666371942235,
      "grad_norm": 0.29367193579673767,
      "learning_rate": 1.994064347938427e-05,
      "loss": 1.1501,
      "step": 138
    },
    {
      "epoch": 1.3017978190391983,
      "grad_norm": 0.2763921320438385,
      "learning_rate": 1.9938984927564564e-05,
      "loss": 1.1634,
      "step": 139
    },
    {
      "epoch": 1.3112290008841734,
      "grad_norm": 0.2613625228404999,
      "learning_rate": 1.9937303593029072e-05,
      "loss": 1.1532,
      "step": 140
    },
    {
      "epoch": 1.3206601827291482,
      "grad_norm": 0.2987387478351593,
      "learning_rate": 1.993559947963185e-05,
      "loss": 1.1867,
      "step": 141
    },
    {
      "epoch": 1.3300913645741232,
      "grad_norm": 0.2564188539981842,
      "learning_rate": 1.9933872591279164e-05,
      "loss": 1.1671,
      "step": 142
    },
    {
      "epoch": 1.339522546419098,
      "grad_norm": 0.2632012367248535,
      "learning_rate": 1.993212293192949e-05,
      "loss": 1.18,
      "step": 143
    },
    {
      "epoch": 1.3489537282640731,
      "grad_norm": 0.2676357626914978,
      "learning_rate": 1.9930350505593493e-05,
      "loss": 1.212,
      "step": 144
    },
    {
      "epoch": 1.3583849101090482,
      "grad_norm": 0.2480822503566742,
      "learning_rate": 1.9928555316334034e-05,
      "loss": 1.1673,
      "step": 145
    },
    {
      "epoch": 1.367816091954023,
      "grad_norm": 0.2818484306335449,
      "learning_rate": 1.9926737368266146e-05,
      "loss": 1.2054,
      "step": 146
    },
    {
      "epoch": 1.3772472737989978,
      "grad_norm": 0.2429407238960266,
      "learning_rate": 1.992489666555704e-05,
      "loss": 1.2118,
      "step": 147
    },
    {
      "epoch": 1.3866784556439729,
      "grad_norm": 0.24313516914844513,
      "learning_rate": 1.992303321242608e-05,
      "loss": 1.1568,
      "step": 148
    },
    {
      "epoch": 1.396109637488948,
      "grad_norm": 0.2722543179988861,
      "learning_rate": 1.9921147013144782e-05,
      "loss": 1.1912,
      "step": 149
    },
    {
      "epoch": 1.4055408193339227,
      "grad_norm": 0.2365546077489853,
      "learning_rate": 1.9919238072036796e-05,
      "loss": 1.1869,
      "step": 150
    },
    {
      "epoch": 1.4149720011788978,
      "grad_norm": 0.24748913943767548,
      "learning_rate": 1.991730639347791e-05,
      "loss": 1.1511,
      "step": 151
    },
    {
      "epoch": 1.4244031830238728,
      "grad_norm": 0.2554396092891693,
      "learning_rate": 1.991535198189603e-05,
      "loss": 1.1661,
      "step": 152
    },
    {
      "epoch": 1.4338343648688476,
      "grad_norm": 0.25241604447364807,
      "learning_rate": 1.9913374841771172e-05,
      "loss": 1.1864,
      "step": 153
    },
    {
      "epoch": 1.4432655467138225,
      "grad_norm": 0.25662583112716675,
      "learning_rate": 1.991137497763545e-05,
      "loss": 1.1629,
      "step": 154
    },
    {
      "epoch": 1.4526967285587975,
      "grad_norm": 0.2663061022758484,
      "learning_rate": 1.9909352394073064e-05,
      "loss": 1.2114,
      "step": 155
    },
    {
      "epoch": 1.4621279104037725,
      "grad_norm": 0.2673744261264801,
      "learning_rate": 1.9907307095720304e-05,
      "loss": 1.1694,
      "step": 156
    },
    {
      "epoch": 1.4715590922487474,
      "grad_norm": 0.2641710638999939,
      "learning_rate": 1.9905239087265516e-05,
      "loss": 1.1605,
      "step": 157
    },
    {
      "epoch": 1.4809902740937224,
      "grad_norm": 0.25940439105033875,
      "learning_rate": 1.990314837344911e-05,
      "loss": 1.1809,
      "step": 158
    },
    {
      "epoch": 1.4904214559386972,
      "grad_norm": 0.2499089241027832,
      "learning_rate": 1.9901034959063542e-05,
      "loss": 1.2083,
      "step": 159
    },
    {
      "epoch": 1.4998526377836723,
      "grad_norm": 0.2502867877483368,
      "learning_rate": 1.9898898848953304e-05,
      "loss": 1.1491,
      "step": 160
    },
    {
      "epoch": 1.509283819628647,
      "grad_norm": 0.23572826385498047,
      "learning_rate": 1.9896740048014908e-05,
      "loss": 1.1833,
      "step": 161
    },
    {
      "epoch": 1.5187150014736222,
      "grad_norm": 0.24018700420856476,
      "learning_rate": 1.989455856119688e-05,
      "loss": 1.1574,
      "step": 162
    },
    {
      "epoch": 1.5281461833185972,
      "grad_norm": 0.24222542345523834,
      "learning_rate": 1.989235439349976e-05,
      "loss": 1.1986,
      "step": 163
    },
    {
      "epoch": 1.537577365163572,
      "grad_norm": 0.25414329767227173,
      "learning_rate": 1.989012754997606e-05,
      "loss": 1.13,
      "step": 164
    },
    {
      "epoch": 1.547008547008547,
      "grad_norm": 0.24886907637119293,
      "learning_rate": 1.988787803573028e-05,
      "loss": 1.1518,
      "step": 165
    },
    {
      "epoch": 1.556439728853522,
      "grad_norm": 0.2386523187160492,
      "learning_rate": 1.9885605855918887e-05,
      "loss": 1.1692,
      "step": 166
    },
    {
      "epoch": 1.565870910698497,
      "grad_norm": 0.2548871338367462,
      "learning_rate": 1.9883311015750307e-05,
      "loss": 1.1741,
      "step": 167
    },
    {
      "epoch": 1.5753020925434718,
      "grad_norm": 0.2470608651638031,
      "learning_rate": 1.9880993520484903e-05,
      "loss": 1.1595,
      "step": 168
    },
    {
      "epoch": 1.5847332743884468,
      "grad_norm": 0.23156823217868805,
      "learning_rate": 1.9878653375434965e-05,
      "loss": 1.1673,
      "step": 169
    },
    {
      "epoch": 1.5941644562334218,
      "grad_norm": 0.23093333840370178,
      "learning_rate": 1.987629058596472e-05,
      "loss": 1.1268,
      "step": 170
    },
    {
      "epoch": 1.6035956380783967,
      "grad_norm": 0.24878740310668945,
      "learning_rate": 1.9873905157490286e-05,
      "loss": 1.2059,
      "step": 171
    },
    {
      "epoch": 1.6130268199233715,
      "grad_norm": 0.25261634588241577,
      "learning_rate": 1.9871497095479678e-05,
      "loss": 1.1634,
      "step": 172
    },
    {
      "epoch": 1.6224580017683468,
      "grad_norm": 0.2471042275428772,
      "learning_rate": 1.9869066405452794e-05,
      "loss": 1.1957,
      "step": 173
    },
    {
      "epoch": 1.6318891836133216,
      "grad_norm": 0.2311047613620758,
      "learning_rate": 1.9866613092981408e-05,
      "loss": 1.2071,
      "step": 174
    },
    {
      "epoch": 1.6413203654582964,
      "grad_norm": 0.2542734146118164,
      "learning_rate": 1.986413716368914e-05,
      "loss": 1.1317,
      "step": 175
    },
    {
      "epoch": 1.6507515473032714,
      "grad_norm": 0.23395168781280518,
      "learning_rate": 1.986163862325146e-05,
      "loss": 1.1922,
      "step": 176
    },
    {
      "epoch": 1.6601827291482465,
      "grad_norm": 0.25499388575553894,
      "learning_rate": 1.9859117477395668e-05,
      "loss": 1.1607,
      "step": 177
    },
    {
      "epoch": 1.6696139109932213,
      "grad_norm": 0.24216996133327484,
      "learning_rate": 1.9856573731900873e-05,
      "loss": 1.1666,
      "step": 178
    },
    {
      "epoch": 1.6790450928381961,
      "grad_norm": 0.26276230812072754,
      "learning_rate": 1.9854007392598005e-05,
      "loss": 1.1835,
      "step": 179
    },
    {
      "epoch": 1.6884762746831712,
      "grad_norm": 0.23967181146144867,
      "learning_rate": 1.985141846536977e-05,
      "loss": 1.1483,
      "step": 180
    },
    {
      "epoch": 1.6979074565281462,
      "grad_norm": 0.2571614980697632,
      "learning_rate": 1.984880695615066e-05,
      "loss": 1.1807,
      "step": 181
    },
    {
      "epoch": 1.707338638373121,
      "grad_norm": 0.24450568854808807,
      "learning_rate": 1.9846172870926927e-05,
      "loss": 1.1721,
      "step": 182
    },
    {
      "epoch": 1.716769820218096,
      "grad_norm": 0.2671433985233307,
      "learning_rate": 1.9843516215736565e-05,
      "loss": 1.118,
      "step": 183
    },
    {
      "epoch": 1.7262010020630711,
      "grad_norm": 0.23806524276733398,
      "learning_rate": 1.9840836996669327e-05,
      "loss": 1.2237,
      "step": 184
    },
    {
      "epoch": 1.735632183908046,
      "grad_norm": 0.25480514764785767,
      "learning_rate": 1.983813521986666e-05,
      "loss": 1.1384,
      "step": 185
    },
    {
      "epoch": 1.7450633657530208,
      "grad_norm": 0.24067392945289612,
      "learning_rate": 1.983541089152174e-05,
      "loss": 1.1748,
      "step": 186
    },
    {
      "epoch": 1.7544945475979958,
      "grad_norm": 0.2392960786819458,
      "learning_rate": 1.983266401787943e-05,
      "loss": 1.188,
      "step": 187
    },
    {
      "epoch": 1.7639257294429709,
      "grad_norm": 0.24186831712722778,
      "learning_rate": 1.9829894605236274e-05,
      "loss": 1.2441,
      "step": 188
    },
    {
      "epoch": 1.7733569112879457,
      "grad_norm": 0.24261973798274994,
      "learning_rate": 1.9827102659940477e-05,
      "loss": 1.2042,
      "step": 189
    },
    {
      "epoch": 1.7827880931329207,
      "grad_norm": 0.24182705581188202,
      "learning_rate": 1.9824288188391894e-05,
      "loss": 1.1864,
      "step": 190
    },
    {
      "epoch": 1.7922192749778958,
      "grad_norm": 0.2541554570198059,
      "learning_rate": 1.9821451197042028e-05,
      "loss": 1.2297,
      "step": 191
    },
    {
      "epoch": 1.8016504568228706,
      "grad_norm": 0.22749127447605133,
      "learning_rate": 1.9818591692393987e-05,
      "loss": 1.18,
      "step": 192
    },
    {
      "epoch": 1.8110816386678454,
      "grad_norm": 0.2670363187789917,
      "learning_rate": 1.9815709681002498e-05,
      "loss": 1.1634,
      "step": 193
    },
    {
      "epoch": 1.8205128205128205,
      "grad_norm": 0.25845223665237427,
      "learning_rate": 1.981280516947387e-05,
      "loss": 1.187,
      "step": 194
    },
    {
      "epoch": 1.8299440023577955,
      "grad_norm": 0.24159900844097137,
      "learning_rate": 1.9809878164465993e-05,
      "loss": 1.1472,
      "step": 195
    },
    {
      "epoch": 1.8393751842027704,
      "grad_norm": 0.24192078411579132,
      "learning_rate": 1.980692867268832e-05,
      "loss": 1.171,
      "step": 196
    },
    {
      "epoch": 1.8488063660477454,
      "grad_norm": 0.2471017837524414,
      "learning_rate": 1.9803956700901842e-05,
      "loss": 1.1606,
      "step": 197
    },
    {
      "epoch": 1.8582375478927204,
      "grad_norm": 0.2658775746822357,
      "learning_rate": 1.980096225591909e-05,
      "loss": 1.1316,
      "step": 198
    },
    {
      "epoch": 1.8676687297376953,
      "grad_norm": 0.23024797439575195,
      "learning_rate": 1.9797945344604098e-05,
      "loss": 1.2035,
      "step": 199
    },
    {
      "epoch": 1.87709991158267,
      "grad_norm": 0.24485205113887787,
      "learning_rate": 1.979490597387241e-05,
      "loss": 1.1841,
      "step": 200
    },
    {
      "epoch": 1.8865310934276451,
      "grad_norm": 0.24373087286949158,
      "learning_rate": 1.9791844150691044e-05,
      "loss": 1.151,
      "step": 201
    },
    {
      "epoch": 1.8959622752726202,
      "grad_norm": 0.21955372393131256,
      "learning_rate": 1.9788759882078485e-05,
      "loss": 1.1804,
      "step": 202
    },
    {
      "epoch": 1.905393457117595,
      "grad_norm": 0.2795611321926117,
      "learning_rate": 1.978565317510468e-05,
      "loss": 1.1698,
      "step": 203
    },
    {
      "epoch": 1.91482463896257,
      "grad_norm": 0.23467381298542023,
      "learning_rate": 1.9782524036890997e-05,
      "loss": 1.151,
      "step": 204
    },
    {
      "epoch": 1.924255820807545,
      "grad_norm": 0.24418005347251892,
      "learning_rate": 1.977937247461023e-05,
      "loss": 1.213,
      "step": 205
    },
    {
      "epoch": 1.93368700265252,
      "grad_norm": 0.23331895470619202,
      "learning_rate": 1.9776198495486566e-05,
      "loss": 1.1596,
      "step": 206
    },
    {
      "epoch": 1.9431181844974947,
      "grad_norm": 0.24591346085071564,
      "learning_rate": 1.9773002106795592e-05,
      "loss": 1.1541,
      "step": 207
    },
    {
      "epoch": 1.9525493663424698,
      "grad_norm": 0.251605361700058,
      "learning_rate": 1.976978331586425e-05,
      "loss": 1.1808,
      "step": 208
    },
    {
      "epoch": 1.9619805481874448,
      "grad_norm": 0.22946709394454956,
      "learning_rate": 1.976654213007084e-05,
      "loss": 1.1547,
      "step": 209
    },
    {
      "epoch": 1.9714117300324197,
      "grad_norm": 0.23104256391525269,
      "learning_rate": 1.9763278556844997e-05,
      "loss": 1.1917,
      "step": 210
    },
    {
      "epoch": 1.9808429118773945,
      "grad_norm": 0.2331412434577942,
      "learning_rate": 1.9759992603667668e-05,
      "loss": 1.1908,
      "step": 211
    },
    {
      "epoch": 1.9902740937223697,
      "grad_norm": 0.2587920129299164,
      "learning_rate": 1.9756684278071108e-05,
      "loss": 1.1677,
      "step": 212
    },
    {
      "epoch": 1.9997052755673446,
      "grad_norm": 0.24836266040802002,
      "learning_rate": 1.975335358763885e-05,
      "loss": 1.1884,
      "step": 213
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.2394020557403564,
      "learning_rate": 1.97500005400057e-05,
      "loss": 0.9744,
      "step": 214
    },
    {
      "epoch": 2.009431181844975,
      "grad_norm": 0.2515215575695038,
      "learning_rate": 1.97466251428577e-05,
      "loss": 1.1476,
      "step": 215
    },
    {
      "epoch": 2.01886236368995,
      "grad_norm": 0.22900353372097015,
      "learning_rate": 1.9743227403932135e-05,
      "loss": 1.176,
      "step": 216
    },
    {
      "epoch": 2.028293545534925,
      "grad_norm": 0.2361432909965515,
      "learning_rate": 1.97398073310175e-05,
      "loss": 1.1586,
      "step": 217
    },
    {
      "epoch": 2.0377247273798997,
      "grad_norm": 0.24968069791793823,
      "learning_rate": 1.973636493195348e-05,
      "loss": 1.1607,
      "step": 218
    },
    {
      "epoch": 2.0471559092248746,
      "grad_norm": 0.24303221702575684,
      "learning_rate": 1.9732900214630944e-05,
      "loss": 1.1478,
      "step": 219
    },
    {
      "epoch": 2.05658709106985,
      "grad_norm": 0.2592518627643585,
      "learning_rate": 1.972941318699192e-05,
      "loss": 1.1661,
      "step": 220
    },
    {
      "epoch": 2.0660182729148246,
      "grad_norm": 0.24251288175582886,
      "learning_rate": 1.9725903857029564e-05,
      "loss": 1.1573,
      "step": 221
    },
    {
      "epoch": 2.0754494547597995,
      "grad_norm": 0.2515219748020172,
      "learning_rate": 1.9722372232788177e-05,
      "loss": 1.2258,
      "step": 222
    },
    {
      "epoch": 2.0848806366047747,
      "grad_norm": 0.24219366908073425,
      "learning_rate": 1.9718818322363143e-05,
      "loss": 1.1499,
      "step": 223
    },
    {
      "epoch": 2.0943118184497496,
      "grad_norm": 0.25317642092704773,
      "learning_rate": 1.9715242133900946e-05,
      "loss": 1.1955,
      "step": 224
    },
    {
      "epoch": 2.1037430002947244,
      "grad_norm": 0.24185709655284882,
      "learning_rate": 1.9711643675599134e-05,
      "loss": 1.1564,
      "step": 225
    },
    {
      "epoch": 2.113174182139699,
      "grad_norm": 0.24841511249542236,
      "learning_rate": 1.9708022955706294e-05,
      "loss": 1.1659,
      "step": 226
    },
    {
      "epoch": 2.1226053639846745,
      "grad_norm": 0.2595750689506531,
      "learning_rate": 1.9704379982522054e-05,
      "loss": 1.1427,
      "step": 227
    },
    {
      "epoch": 2.1320365458296493,
      "grad_norm": 0.2422349750995636,
      "learning_rate": 1.9700714764397047e-05,
      "loss": 1.0931,
      "step": 228
    },
    {
      "epoch": 2.141467727674624,
      "grad_norm": 0.2468159794807434,
      "learning_rate": 1.96970273097329e-05,
      "loss": 1.1748,
      "step": 229
    },
    {
      "epoch": 2.150898909519599,
      "grad_norm": 0.2429417371749878,
      "learning_rate": 1.9693317626982207e-05,
      "loss": 1.1776,
      "step": 230
    },
    {
      "epoch": 2.160330091364574,
      "grad_norm": 0.24727223813533783,
      "learning_rate": 1.9689585724648515e-05,
      "loss": 1.1514,
      "step": 231
    },
    {
      "epoch": 2.169761273209549,
      "grad_norm": 0.25714024901390076,
      "learning_rate": 1.9685831611286312e-05,
      "loss": 1.2066,
      "step": 232
    },
    {
      "epoch": 2.179192455054524,
      "grad_norm": 0.24081282317638397,
      "learning_rate": 1.9682055295500992e-05,
      "loss": 1.2022,
      "step": 233
    },
    {
      "epoch": 2.188623636899499,
      "grad_norm": 0.24947278201580048,
      "learning_rate": 1.9678256785948838e-05,
      "loss": 1.1507,
      "step": 234
    },
    {
      "epoch": 2.198054818744474,
      "grad_norm": 0.24169601500034332,
      "learning_rate": 1.9674436091337027e-05,
      "loss": 1.2322,
      "step": 235
    },
    {
      "epoch": 2.2074860005894488,
      "grad_norm": 0.23921944200992584,
      "learning_rate": 1.967059322042356e-05,
      "loss": 1.1348,
      "step": 236
    },
    {
      "epoch": 2.2169171824344236,
      "grad_norm": 0.26610851287841797,
      "learning_rate": 1.9666728182017296e-05,
      "loss": 1.1659,
      "step": 237
    },
    {
      "epoch": 2.226348364279399,
      "grad_norm": 0.24142496287822723,
      "learning_rate": 1.96628409849779e-05,
      "loss": 1.1642,
      "step": 238
    },
    {
      "epoch": 2.2357795461243737,
      "grad_norm": 0.23417235910892487,
      "learning_rate": 1.965893163821583e-05,
      "loss": 1.1111,
      "step": 239
    },
    {
      "epoch": 2.2452107279693485,
      "grad_norm": 0.27117064595222473,
      "learning_rate": 1.9655000150692315e-05,
      "loss": 1.1878,
      "step": 240
    },
    {
      "epoch": 2.2546419098143238,
      "grad_norm": 0.24141408503055573,
      "learning_rate": 1.9651046531419335e-05,
      "loss": 1.183,
      "step": 241
    },
    {
      "epoch": 2.2640730916592986,
      "grad_norm": 0.23481504619121552,
      "learning_rate": 1.9647070789459608e-05,
      "loss": 1.1398,
      "step": 242
    },
    {
      "epoch": 2.2735042735042734,
      "grad_norm": 0.23398709297180176,
      "learning_rate": 1.9643072933926563e-05,
      "loss": 1.0939,
      "step": 243
    },
    {
      "epoch": 2.2829354553492482,
      "grad_norm": 0.24612291157245636,
      "learning_rate": 1.963905297398431e-05,
      "loss": 1.1577,
      "step": 244
    },
    {
      "epoch": 2.2923666371942235,
      "grad_norm": 0.2406822293996811,
      "learning_rate": 1.963501091884763e-05,
      "loss": 1.1544,
      "step": 245
    },
    {
      "epoch": 2.3017978190391983,
      "grad_norm": 0.24142758548259735,
      "learning_rate": 1.9630946777781967e-05,
      "loss": 1.159,
      "step": 246
    },
    {
      "epoch": 2.311229000884173,
      "grad_norm": 0.24923396110534668,
      "learning_rate": 1.9626860560103373e-05,
      "loss": 1.1476,
      "step": 247
    },
    {
      "epoch": 2.3206601827291484,
      "grad_norm": 0.25321048498153687,
      "learning_rate": 1.9622752275178514e-05,
      "loss": 1.1316,
      "step": 248
    },
    {
      "epoch": 2.3300913645741232,
      "grad_norm": 0.23715239763259888,
      "learning_rate": 1.9618621932424635e-05,
      "loss": 1.1497,
      "step": 249
    },
    {
      "epoch": 2.339522546419098,
      "grad_norm": 0.23655402660369873,
      "learning_rate": 1.961446954130955e-05,
      "loss": 1.1539,
      "step": 250
    },
    {
      "epoch": 2.348953728264073,
      "grad_norm": 0.23993302881717682,
      "learning_rate": 1.961029511135161e-05,
      "loss": 1.1536,
      "step": 251
    },
    {
      "epoch": 2.358384910109048,
      "grad_norm": 0.23993681371212006,
      "learning_rate": 1.960609865211968e-05,
      "loss": 1.1444,
      "step": 252
    },
    {
      "epoch": 2.367816091954023,
      "grad_norm": 0.24286270141601562,
      "learning_rate": 1.9601880173233133e-05,
      "loss": 1.1355,
      "step": 253
    },
    {
      "epoch": 2.377247273798998,
      "grad_norm": 0.24435004591941833,
      "learning_rate": 1.9597639684361805e-05,
      "loss": 1.1525,
      "step": 254
    },
    {
      "epoch": 2.386678455643973,
      "grad_norm": 0.24956992268562317,
      "learning_rate": 1.959337719522599e-05,
      "loss": 1.2028,
      "step": 255
    },
    {
      "epoch": 2.396109637488948,
      "grad_norm": 0.24780336022377014,
      "learning_rate": 1.958909271559642e-05,
      "loss": 1.2064,
      "step": 256
    },
    {
      "epoch": 2.4055408193339227,
      "grad_norm": 0.24767841398715973,
      "learning_rate": 1.9584786255294217e-05,
      "loss": 1.1877,
      "step": 257
    },
    {
      "epoch": 2.4149720011788975,
      "grad_norm": 0.22663316130638123,
      "learning_rate": 1.95804578241909e-05,
      "loss": 1.1746,
      "step": 258
    },
    {
      "epoch": 2.424403183023873,
      "grad_norm": 0.25448739528656006,
      "learning_rate": 1.9576107432208353e-05,
      "loss": 1.1702,
      "step": 259
    },
    {
      "epoch": 2.4338343648688476,
      "grad_norm": 0.24593564867973328,
      "learning_rate": 1.9571735089318794e-05,
      "loss": 1.1882,
      "step": 260
    },
    {
      "epoch": 2.4432655467138225,
      "grad_norm": 0.2314690798521042,
      "learning_rate": 1.956734080554476e-05,
      "loss": 1.1169,
      "step": 261
    },
    {
      "epoch": 2.4526967285587977,
      "grad_norm": 0.26183485984802246,
      "learning_rate": 1.956292459095908e-05,
      "loss": 1.1954,
      "step": 262
    },
    {
      "epoch": 2.4621279104037725,
      "grad_norm": 0.2312699407339096,
      "learning_rate": 1.9558486455684858e-05,
      "loss": 1.1743,
      "step": 263
    },
    {
      "epoch": 2.4715590922487474,
      "grad_norm": 0.2153061181306839,
      "learning_rate": 1.955402640989545e-05,
      "loss": 1.1654,
      "step": 264
    },
    {
      "epoch": 2.480990274093722,
      "grad_norm": 0.24584868550300598,
      "learning_rate": 1.9549544463814422e-05,
      "loss": 1.1615,
      "step": 265
    },
    {
      "epoch": 2.4904214559386975,
      "grad_norm": 0.2666086256504059,
      "learning_rate": 1.9545040627715554e-05,
      "loss": 1.1207,
      "step": 266
    },
    {
      "epoch": 2.4998526377836723,
      "grad_norm": 0.26206400990486145,
      "learning_rate": 1.9540514911922798e-05,
      "loss": 1.1318,
      "step": 267
    },
    {
      "epoch": 2.509283819628647,
      "grad_norm": 0.25298890471458435,
      "learning_rate": 1.9535967326810265e-05,
      "loss": 1.1431,
      "step": 268
    },
    {
      "epoch": 2.5187150014736224,
      "grad_norm": 0.2669878304004669,
      "learning_rate": 1.953139788280219e-05,
      "loss": 1.1536,
      "step": 269
    },
    {
      "epoch": 2.528146183318597,
      "grad_norm": 0.2560335397720337,
      "learning_rate": 1.9526806590372912e-05,
      "loss": 1.1458,
      "step": 270
    },
    {
      "epoch": 2.537577365163572,
      "grad_norm": 0.23330655694007874,
      "learning_rate": 1.9522193460046862e-05,
      "loss": 1.192,
      "step": 271
    },
    {
      "epoch": 2.547008547008547,
      "grad_norm": 0.22849895060062408,
      "learning_rate": 1.9517558502398527e-05,
      "loss": 1.1912,
      "step": 272
    },
    {
      "epoch": 2.556439728853522,
      "grad_norm": 0.23537573218345642,
      "learning_rate": 1.9512901728052418e-05,
      "loss": 1.1673,
      "step": 273
    },
    {
      "epoch": 2.565870910698497,
      "grad_norm": 0.2460755854845047,
      "learning_rate": 1.9508223147683068e-05,
      "loss": 1.1471,
      "step": 274
    },
    {
      "epoch": 2.5753020925434718,
      "grad_norm": 0.2648819386959076,
      "learning_rate": 1.9503522772014984e-05,
      "loss": 1.166,
      "step": 275
    },
    {
      "epoch": 2.584733274388447,
      "grad_norm": 0.25566285848617554,
      "learning_rate": 1.9498800611822645e-05,
      "loss": 1.1502,
      "step": 276
    },
    {
      "epoch": 2.594164456233422,
      "grad_norm": 0.25419557094573975,
      "learning_rate": 1.9494056677930456e-05,
      "loss": 1.1403,
      "step": 277
    },
    {
      "epoch": 2.6035956380783967,
      "grad_norm": 0.2492426186800003,
      "learning_rate": 1.9489290981212733e-05,
      "loss": 1.1268,
      "step": 278
    },
    {
      "epoch": 2.6130268199233715,
      "grad_norm": 0.260052353143692,
      "learning_rate": 1.9484503532593688e-05,
      "loss": 1.1932,
      "step": 279
    },
    {
      "epoch": 2.6224580017683468,
      "grad_norm": 0.26052218675613403,
      "learning_rate": 1.9479694343047388e-05,
      "loss": 1.1798,
      "step": 280
    },
    {
      "epoch": 2.6318891836133216,
      "grad_norm": 0.252811461687088,
      "learning_rate": 1.947486342359773e-05,
      "loss": 1.1948,
      "step": 281
    },
    {
      "epoch": 2.6413203654582964,
      "grad_norm": 0.24094140529632568,
      "learning_rate": 1.9470010785318424e-05,
      "loss": 1.1733,
      "step": 282
    },
    {
      "epoch": 2.6507515473032717,
      "grad_norm": 0.26448315382003784,
      "learning_rate": 1.9465136439332974e-05,
      "loss": 1.1455,
      "step": 283
    },
    {
      "epoch": 2.6601827291482465,
      "grad_norm": 0.23510056734085083,
      "learning_rate": 1.946024039681464e-05,
      "loss": 1.1494,
      "step": 284
    },
    {
      "epoch": 2.6696139109932213,
      "grad_norm": 0.2556127607822418,
      "learning_rate": 1.9455322668986412e-05,
      "loss": 1.1524,
      "step": 285
    },
    {
      "epoch": 2.679045092838196,
      "grad_norm": 0.24325338006019592,
      "learning_rate": 1.9450383267120984e-05,
      "loss": 1.1948,
      "step": 286
    },
    {
      "epoch": 2.688476274683171,
      "grad_norm": 0.2522813081741333,
      "learning_rate": 1.9445422202540745e-05,
      "loss": 1.1622,
      "step": 287
    },
    {
      "epoch": 2.6979074565281462,
      "grad_norm": 0.23030245304107666,
      "learning_rate": 1.944043948661773e-05,
      "loss": 1.1765,
      "step": 288
    },
    {
      "epoch": 2.707338638373121,
      "grad_norm": 0.2435046285390854,
      "learning_rate": 1.943543513077361e-05,
      "loss": 1.1602,
      "step": 289
    },
    {
      "epoch": 2.7167698202180963,
      "grad_norm": 0.2501375377178192,
      "learning_rate": 1.9430409146479658e-05,
      "loss": 1.1641,
      "step": 290
    },
    {
      "epoch": 2.726201002063071,
      "grad_norm": 0.2555733919143677,
      "learning_rate": 1.942536154525673e-05,
      "loss": 1.1472,
      "step": 291
    },
    {
      "epoch": 2.735632183908046,
      "grad_norm": 0.22981493175029755,
      "learning_rate": 1.942029233867522e-05,
      "loss": 1.1449,
      "step": 292
    },
    {
      "epoch": 2.745063365753021,
      "grad_norm": 0.2492411583662033,
      "learning_rate": 1.941520153835507e-05,
      "loss": 1.1184,
      "step": 293
    },
    {
      "epoch": 2.7544945475979956,
      "grad_norm": 0.24750302731990814,
      "learning_rate": 1.9410089155965693e-05,
      "loss": 1.1754,
      "step": 294
    },
    {
      "epoch": 2.763925729442971,
      "grad_norm": 0.23420295119285583,
      "learning_rate": 1.9404955203225998e-05,
      "loss": 1.1564,
      "step": 295
    },
    {
      "epoch": 2.7733569112879457,
      "grad_norm": 0.2572304904460907,
      "learning_rate": 1.939979969190432e-05,
      "loss": 1.1286,
      "step": 296
    },
    {
      "epoch": 2.782788093132921,
      "grad_norm": 0.24425046145915985,
      "learning_rate": 1.9394622633818426e-05,
      "loss": 1.1565,
      "step": 297
    },
    {
      "epoch": 2.792219274977896,
      "grad_norm": 0.24395979940891266,
      "learning_rate": 1.9389424040835462e-05,
      "loss": 1.1145,
      "step": 298
    },
    {
      "epoch": 2.8016504568228706,
      "grad_norm": 0.24615716934204102,
      "learning_rate": 1.9384203924871953e-05,
      "loss": 1.1461,
      "step": 299
    },
    {
      "epoch": 2.8110816386678454,
      "grad_norm": 0.25030195713043213,
      "learning_rate": 1.937896229789374e-05,
      "loss": 1.1481,
      "step": 300
    },
    {
      "epoch": 2.8205128205128203,
      "grad_norm": 0.24526001513004303,
      "learning_rate": 1.937369917191599e-05,
      "loss": 1.1708,
      "step": 301
    },
    {
      "epoch": 2.8299440023577955,
      "grad_norm": 0.25566259026527405,
      "learning_rate": 1.9368414559003138e-05,
      "loss": 1.1591,
      "step": 302
    },
    {
      "epoch": 2.8393751842027704,
      "grad_norm": 0.26568078994750977,
      "learning_rate": 1.936310847126889e-05,
      "loss": 1.1567,
      "step": 303
    },
    {
      "epoch": 2.8488063660477456,
      "grad_norm": 0.23922915756702423,
      "learning_rate": 1.935778092087616e-05,
      "loss": 1.1802,
      "step": 304
    },
    {
      "epoch": 2.8582375478927204,
      "grad_norm": 0.23668769001960754,
      "learning_rate": 1.9352431920037063e-05,
      "loss": 1.146,
      "step": 305
    },
    {
      "epoch": 2.8676687297376953,
      "grad_norm": 0.24750137329101562,
      "learning_rate": 1.9347061481012894e-05,
      "loss": 1.1434,
      "step": 306
    },
    {
      "epoch": 2.87709991158267,
      "grad_norm": 0.25205516815185547,
      "learning_rate": 1.9341669616114083e-05,
      "loss": 1.1588,
      "step": 307
    },
    {
      "epoch": 2.886531093427645,
      "grad_norm": 0.26784518361091614,
      "learning_rate": 1.9336256337700177e-05,
      "loss": 1.1951,
      "step": 308
    },
    {
      "epoch": 2.89596227527262,
      "grad_norm": 0.2587915062904358,
      "learning_rate": 1.9330821658179798e-05,
      "loss": 1.1665,
      "step": 309
    },
    {
      "epoch": 2.905393457117595,
      "grad_norm": 0.25944218039512634,
      "learning_rate": 1.932536559001064e-05,
      "loss": 1.1436,
      "step": 310
    },
    {
      "epoch": 2.9148246389625703,
      "grad_norm": 0.23410086333751678,
      "learning_rate": 1.9319888145699416e-05,
      "loss": 1.1767,
      "step": 311
    },
    {
      "epoch": 2.924255820807545,
      "grad_norm": 0.2614251375198364,
      "learning_rate": 1.9314389337801834e-05,
      "loss": 1.1481,
      "step": 312
    },
    {
      "epoch": 2.93368700265252,
      "grad_norm": 0.24511784315109253,
      "learning_rate": 1.930886917892259e-05,
      "loss": 1.1656,
      "step": 313
    },
    {
      "epoch": 2.9431181844974947,
      "grad_norm": 0.26145365834236145,
      "learning_rate": 1.93033276817153e-05,
      "loss": 1.1834,
      "step": 314
    },
    {
      "epoch": 2.9525493663424696,
      "grad_norm": 0.2564398944377899,
      "learning_rate": 1.9297764858882516e-05,
      "loss": 1.149,
      "step": 315
    },
    {
      "epoch": 2.961980548187445,
      "grad_norm": 0.2532845437526703,
      "learning_rate": 1.9292180723175656e-05,
      "loss": 1.1313,
      "step": 316
    },
    {
      "epoch": 2.9714117300324197,
      "grad_norm": 0.24997320771217346,
      "learning_rate": 1.9286575287394997e-05,
      "loss": 1.1553,
      "step": 317
    },
    {
      "epoch": 2.9808429118773945,
      "grad_norm": 0.2599256932735443,
      "learning_rate": 1.9280948564389642e-05,
      "loss": 1.137,
      "step": 318
    },
    {
      "epoch": 2.9902740937223697,
      "grad_norm": 0.24204401671886444,
      "learning_rate": 1.9275300567057494e-05,
      "loss": 1.1407,
      "step": 319
    },
    {
      "epoch": 2.9997052755673446,
      "grad_norm": 0.2598811984062195,
      "learning_rate": 1.926963130834522e-05,
      "loss": 1.1018,
      "step": 320
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.3657182455062866,
      "learning_rate": 1.9263940801248228e-05,
      "loss": 0.7797,
      "step": 321
    },
    {
      "epoch": 3.009431181844975,
      "grad_norm": 0.24547074735164642,
      "learning_rate": 1.9258229058810613e-05,
      "loss": 1.135,
      "step": 322
    },
    {
      "epoch": 3.01886236368995,
      "grad_norm": 0.24313637614250183,
      "learning_rate": 1.925249609412517e-05,
      "loss": 1.19,
      "step": 323
    },
    {
      "epoch": 3.028293545534925,
      "grad_norm": 0.2481006383895874,
      "learning_rate": 1.924674192033333e-05,
      "loss": 1.1805,
      "step": 324
    },
    {
      "epoch": 3.0377247273798997,
      "grad_norm": 0.2369411587715149,
      "learning_rate": 1.9240966550625143e-05,
      "loss": 1.1447,
      "step": 325
    },
    {
      "epoch": 3.0471559092248746,
      "grad_norm": 0.2649562656879425,
      "learning_rate": 1.923516999823925e-05,
      "loss": 1.1112,
      "step": 326
    },
    {
      "epoch": 3.05658709106985,
      "grad_norm": 0.2506507635116577,
      "learning_rate": 1.9229352276462834e-05,
      "loss": 1.153,
      "step": 327
    },
    {
      "epoch": 3.0660182729148246,
      "grad_norm": 0.28752222657203674,
      "learning_rate": 1.922351339863162e-05,
      "loss": 1.1871,
      "step": 328
    },
    {
      "epoch": 3.0754494547597995,
      "grad_norm": 0.2614225447177887,
      "learning_rate": 1.9217653378129818e-05,
      "loss": 1.1522,
      "step": 329
    },
    {
      "epoch": 3.0848806366047747,
      "grad_norm": 0.28447508811950684,
      "learning_rate": 1.921177222839011e-05,
      "loss": 1.1435,
      "step": 330
    },
    {
      "epoch": 3.0943118184497496,
      "grad_norm": 0.24903512001037598,
      "learning_rate": 1.9205869962893607e-05,
      "loss": 1.1651,
      "step": 331
    },
    {
      "epoch": 3.1037430002947244,
      "grad_norm": 0.2607053816318512,
      "learning_rate": 1.9199946595169816e-05,
      "loss": 1.1654,
      "step": 332
    },
    {
      "epoch": 3.113174182139699,
      "grad_norm": 0.27187052369117737,
      "learning_rate": 1.919400213879663e-05,
      "loss": 1.1642,
      "step": 333
    },
    {
      "epoch": 3.1226053639846745,
      "grad_norm": 0.2784789204597473,
      "learning_rate": 1.9188036607400274e-05,
      "loss": 1.1322,
      "step": 334
    },
    {
      "epoch": 3.1320365458296493,
      "grad_norm": 0.2825380265712738,
      "learning_rate": 1.9182050014655283e-05,
      "loss": 1.1452,
      "step": 335
    },
    {
      "epoch": 3.141467727674624,
      "grad_norm": 0.22936390340328217,
      "learning_rate": 1.917604237428447e-05,
      "loss": 1.1487,
      "step": 336
    },
    {
      "epoch": 3.150898909519599,
      "grad_norm": 0.27032271027565,
      "learning_rate": 1.9170013700058897e-05,
      "loss": 1.1454,
      "step": 337
    },
    {
      "epoch": 3.160330091364574,
      "grad_norm": 0.25205641984939575,
      "learning_rate": 1.916396400579784e-05,
      "loss": 1.134,
      "step": 338
    },
    {
      "epoch": 3.169761273209549,
      "grad_norm": 0.249350905418396,
      "learning_rate": 1.9157893305368756e-05,
      "loss": 1.1208,
      "step": 339
    },
    {
      "epoch": 3.179192455054524,
      "grad_norm": 0.25476890802383423,
      "learning_rate": 1.915180161268726e-05,
      "loss": 1.1354,
      "step": 340
    },
    {
      "epoch": 3.188623636899499,
      "grad_norm": 0.2602582573890686,
      "learning_rate": 1.9145688941717074e-05,
      "loss": 1.1688,
      "step": 341
    },
    {
      "epoch": 3.198054818744474,
      "grad_norm": 0.2793292701244354,
      "learning_rate": 1.913955530647002e-05,
      "loss": 1.1405,
      "step": 342
    },
    {
      "epoch": 3.2074860005894488,
      "grad_norm": 0.2524898946285248,
      "learning_rate": 1.9133400721005977e-05,
      "loss": 1.1179,
      "step": 343
    },
    {
      "epoch": 3.2169171824344236,
      "grad_norm": 0.2739506661891937,
      "learning_rate": 1.9127225199432834e-05,
      "loss": 1.1724,
      "step": 344
    },
    {
      "epoch": 3.226348364279399,
      "grad_norm": 0.23781520128250122,
      "learning_rate": 1.9121028755906482e-05,
      "loss": 1.1459,
      "step": 345
    },
    {
      "epoch": 3.2357795461243737,
      "grad_norm": 0.2820172607898712,
      "learning_rate": 1.911481140463076e-05,
      "loss": 1.1584,
      "step": 346
    },
    {
      "epoch": 3.2452107279693485,
      "grad_norm": 0.2460946887731552,
      "learning_rate": 1.9108573159857453e-05,
      "loss": 1.1676,
      "step": 347
    },
    {
      "epoch": 3.2546419098143238,
      "grad_norm": 0.2491939812898636,
      "learning_rate": 1.9102314035886218e-05,
      "loss": 1.136,
      "step": 348
    },
    {
      "epoch": 3.2640730916592986,
      "grad_norm": 0.23142646253108978,
      "learning_rate": 1.909603404706458e-05,
      "loss": 1.1516,
      "step": 349
    },
    {
      "epoch": 3.2735042735042734,
      "grad_norm": 0.2587831914424896,
      "learning_rate": 1.9089733207787893e-05,
      "loss": 1.1392,
      "step": 350
    },
    {
      "epoch": 3.2829354553492482,
      "grad_norm": 0.3031064569950104,
      "learning_rate": 1.9083411532499308e-05,
      "loss": 1.1748,
      "step": 351
    },
    {
      "epoch": 3.2923666371942235,
      "grad_norm": 0.2710627019405365,
      "learning_rate": 1.9077069035689736e-05,
      "loss": 1.1861,
      "step": 352
    },
    {
      "epoch": 3.3017978190391983,
      "grad_norm": 0.2732830345630646,
      "learning_rate": 1.9070705731897806e-05,
      "loss": 1.0975,
      "step": 353
    },
    {
      "epoch": 3.311229000884173,
      "grad_norm": 0.2421528398990631,
      "learning_rate": 1.906432163570986e-05,
      "loss": 1.1396,
      "step": 354
    },
    {
      "epoch": 3.3206601827291484,
      "grad_norm": 0.2610471844673157,
      "learning_rate": 1.905791676175989e-05,
      "loss": 1.163,
      "step": 355
    },
    {
      "epoch": 3.3300913645741232,
      "grad_norm": 0.2717328667640686,
      "learning_rate": 1.9051491124729512e-05,
      "loss": 1.1136,
      "step": 356
    },
    {
      "epoch": 3.339522546419098,
      "grad_norm": 0.2409527599811554,
      "learning_rate": 1.9045044739347953e-05,
      "loss": 1.1031,
      "step": 357
    },
    {
      "epoch": 3.348953728264073,
      "grad_norm": 0.2720731496810913,
      "learning_rate": 1.9038577620391984e-05,
      "loss": 1.1201,
      "step": 358
    },
    {
      "epoch": 3.358384910109048,
      "grad_norm": 0.27050715684890747,
      "learning_rate": 1.9032089782685908e-05,
      "loss": 1.1635,
      "step": 359
    },
    {
      "epoch": 3.367816091954023,
      "grad_norm": 0.23995307087898254,
      "learning_rate": 1.9025581241101526e-05,
      "loss": 1.1443,
      "step": 360
    },
    {
      "epoch": 3.377247273798998,
      "grad_norm": 0.2670184075832367,
      "learning_rate": 1.9019052010558087e-05,
      "loss": 1.1454,
      "step": 361
    },
    {
      "epoch": 3.386678455643973,
      "grad_norm": 0.2620398700237274,
      "learning_rate": 1.9012502106022274e-05,
      "loss": 1.1309,
      "step": 362
    },
    {
      "epoch": 3.396109637488948,
      "grad_norm": 0.2554798126220703,
      "learning_rate": 1.900593154250816e-05,
      "loss": 1.1677,
      "step": 363
    },
    {
      "epoch": 3.4055408193339227,
      "grad_norm": 0.25955531001091003,
      "learning_rate": 1.899934033507717e-05,
      "loss": 1.201,
      "step": 364
    },
    {
      "epoch": 3.4149720011788975,
      "grad_norm": 0.24187929928302765,
      "learning_rate": 1.8992728498838047e-05,
      "loss": 1.1425,
      "step": 365
    },
    {
      "epoch": 3.424403183023873,
      "grad_norm": 0.28225719928741455,
      "learning_rate": 1.8986096048946826e-05,
      "loss": 1.1497,
      "step": 366
    },
    {
      "epoch": 3.4338343648688476,
      "grad_norm": 0.2541481852531433,
      "learning_rate": 1.8979443000606788e-05,
      "loss": 1.1854,
      "step": 367
    },
    {
      "epoch": 3.4432655467138225,
      "grad_norm": 0.256665974855423,
      "learning_rate": 1.897276936906844e-05,
      "loss": 1.0738,
      "step": 368
    },
    {
      "epoch": 3.4526967285587977,
      "grad_norm": 0.2466139942407608,
      "learning_rate": 1.8966075169629467e-05,
      "loss": 1.1682,
      "step": 369
    },
    {
      "epoch": 3.4621279104037725,
      "grad_norm": 0.25738707184791565,
      "learning_rate": 1.8959360417634694e-05,
      "loss": 1.1746,
      "step": 370
    },
    {
      "epoch": 3.4715590922487474,
      "grad_norm": 0.27860134840011597,
      "learning_rate": 1.895262512847607e-05,
      "loss": 1.1595,
      "step": 371
    },
    {
      "epoch": 3.480990274093722,
      "grad_norm": 0.25989240407943726,
      "learning_rate": 1.8945869317592614e-05,
      "loss": 1.1162,
      "step": 372
    },
    {
      "epoch": 3.4904214559386975,
      "grad_norm": 0.2761606276035309,
      "learning_rate": 1.8939093000470378e-05,
      "loss": 1.1367,
      "step": 373
    },
    {
      "epoch": 3.4998526377836723,
      "grad_norm": 0.2834632694721222,
      "learning_rate": 1.893229619264244e-05,
      "loss": 1.1518,
      "step": 374
    },
    {
      "epoch": 3.509283819628647,
      "grad_norm": 0.27298814058303833,
      "learning_rate": 1.8925478909688825e-05,
      "loss": 1.141,
      "step": 375
    },
    {
      "epoch": 3.5187150014736224,
      "grad_norm": 0.2731652557849884,
      "learning_rate": 1.8918641167236506e-05,
      "loss": 1.1123,
      "step": 376
    },
    {
      "epoch": 3.528146183318597,
      "grad_norm": 0.23891066014766693,
      "learning_rate": 1.8911782980959354e-05,
      "loss": 1.1777,
      "step": 377
    },
    {
      "epoch": 3.537577365163572,
      "grad_norm": 0.2553216218948364,
      "learning_rate": 1.89049043665781e-05,
      "loss": 1.1853,
      "step": 378
    },
    {
      "epoch": 3.547008547008547,
      "grad_norm": 0.24696451425552368,
      "learning_rate": 1.88980053398603e-05,
      "loss": 1.1627,
      "step": 379
    },
    {
      "epoch": 3.556439728853522,
      "grad_norm": 0.2576914429664612,
      "learning_rate": 1.88910859166203e-05,
      "loss": 1.1458,
      "step": 380
    },
    {
      "epoch": 3.565870910698497,
      "grad_norm": 0.23565611243247986,
      "learning_rate": 1.8884146112719208e-05,
      "loss": 1.1786,
      "step": 381
    },
    {
      "epoch": 3.5753020925434718,
      "grad_norm": 0.25541383028030396,
      "learning_rate": 1.887718594406484e-05,
      "loss": 1.137,
      "step": 382
    },
    {
      "epoch": 3.584733274388447,
      "grad_norm": 0.2581261694431305,
      "learning_rate": 1.8870205426611693e-05,
      "loss": 1.1678,
      "step": 383
    },
    {
      "epoch": 3.594164456233422,
      "grad_norm": 0.2873068153858185,
      "learning_rate": 1.886320457636092e-05,
      "loss": 1.1675,
      "step": 384
    },
    {
      "epoch": 3.6035956380783967,
      "grad_norm": 0.27282553911209106,
      "learning_rate": 1.8856183409360267e-05,
      "loss": 1.1819,
      "step": 385
    },
    {
      "epoch": 3.6130268199233715,
      "grad_norm": 0.25089573860168457,
      "learning_rate": 1.884914194170407e-05,
      "loss": 1.1597,
      "step": 386
    },
    {
      "epoch": 3.6224580017683468,
      "grad_norm": 0.28366532921791077,
      "learning_rate": 1.8842080189533175e-05,
      "loss": 1.1435,
      "step": 387
    },
    {
      "epoch": 3.6318891836133216,
      "grad_norm": 0.2619055509567261,
      "learning_rate": 1.8834998169034945e-05,
      "loss": 1.1555,
      "step": 388
    },
    {
      "epoch": 3.6413203654582964,
      "grad_norm": 0.24807323515415192,
      "learning_rate": 1.8827895896443194e-05,
      "loss": 1.1611,
      "step": 389
    },
    {
      "epoch": 3.6507515473032717,
      "grad_norm": 0.2775343954563141,
      "learning_rate": 1.882077338803817e-05,
      "loss": 1.1456,
      "step": 390
    },
    {
      "epoch": 3.6601827291482465,
      "grad_norm": 0.24452804028987885,
      "learning_rate": 1.881363066014649e-05,
      "loss": 1.1372,
      "step": 391
    },
    {
      "epoch": 3.6696139109932213,
      "grad_norm": 0.266705185174942,
      "learning_rate": 1.880646772914113e-05,
      "loss": 1.1568,
      "step": 392
    },
    {
      "epoch": 3.679045092838196,
      "grad_norm": 0.27216440439224243,
      "learning_rate": 1.8799284611441376e-05,
      "loss": 1.1474,
      "step": 393
    },
    {
      "epoch": 3.688476274683171,
      "grad_norm": 0.27131912112236023,
      "learning_rate": 1.8792081323512783e-05,
      "loss": 1.1845,
      "step": 394
    },
    {
      "epoch": 3.6979074565281462,
      "grad_norm": 0.27188339829444885,
      "learning_rate": 1.8784857881867146e-05,
      "loss": 1.1609,
      "step": 395
    },
    {
      "epoch": 3.707338638373121,
      "grad_norm": 0.28588545322418213,
      "learning_rate": 1.877761430306246e-05,
      "loss": 1.0759,
      "step": 396
    },
    {
      "epoch": 3.7167698202180963,
      "grad_norm": 0.2829708755016327,
      "learning_rate": 1.8770350603702864e-05,
      "loss": 1.1429,
      "step": 397
    },
    {
      "epoch": 3.726201002063071,
      "grad_norm": 0.25368431210517883,
      "learning_rate": 1.8763066800438638e-05,
      "loss": 1.1643,
      "step": 398
    },
    {
      "epoch": 3.735632183908046,
      "grad_norm": 0.28726157546043396,
      "learning_rate": 1.8755762909966133e-05,
      "loss": 1.1819,
      "step": 399
    },
    {
      "epoch": 3.745063365753021,
      "grad_norm": 0.25056666135787964,
      "learning_rate": 1.874843894902775e-05,
      "loss": 1.1428,
      "step": 400
    },
    {
      "epoch": 3.7544945475979956,
      "grad_norm": 0.2609914243221283,
      "learning_rate": 1.87410949344119e-05,
      "loss": 1.1159,
      "step": 401
    },
    {
      "epoch": 3.763925729442971,
      "grad_norm": 0.2424488067626953,
      "learning_rate": 1.8733730882952953e-05,
      "loss": 1.154,
      "step": 402
    },
    {
      "epoch": 3.7733569112879457,
      "grad_norm": 0.2524412274360657,
      "learning_rate": 1.8726346811531214e-05,
      "loss": 1.1743,
      "step": 403
    },
    {
      "epoch": 3.782788093132921,
      "grad_norm": 0.2699984610080719,
      "learning_rate": 1.8718942737072874e-05,
      "loss": 1.1357,
      "step": 404
    },
    {
      "epoch": 3.792219274977896,
      "grad_norm": 0.2601785659790039,
      "learning_rate": 1.871151867654999e-05,
      "loss": 1.1691,
      "step": 405
    },
    {
      "epoch": 3.8016504568228706,
      "grad_norm": 0.2567746043205261,
      "learning_rate": 1.8704074646980416e-05,
      "loss": 1.1241,
      "step": 406
    },
    {
      "epoch": 3.8110816386678454,
      "grad_norm": 0.28403162956237793,
      "learning_rate": 1.8696610665427793e-05,
      "loss": 1.1586,
      "step": 407
    },
    {
      "epoch": 3.8205128205128203,
      "grad_norm": 0.26053863763809204,
      "learning_rate": 1.8689126749001482e-05,
      "loss": 1.1522,
      "step": 408
    },
    {
      "epoch": 3.8299440023577955,
      "grad_norm": 0.2754337787628174,
      "learning_rate": 1.8681622914856557e-05,
      "loss": 1.156,
      "step": 409
    },
    {
      "epoch": 3.8393751842027704,
      "grad_norm": 0.2645362913608551,
      "learning_rate": 1.8674099180193736e-05,
      "loss": 1.1455,
      "step": 410
    },
    {
      "epoch": 3.8488063660477456,
      "grad_norm": 0.27726471424102783,
      "learning_rate": 1.8666555562259358e-05,
      "loss": 1.1325,
      "step": 411
    },
    {
      "epoch": 3.8582375478927204,
      "grad_norm": 0.2812797725200653,
      "learning_rate": 1.865899207834534e-05,
      "loss": 1.1224,
      "step": 412
    },
    {
      "epoch": 3.8676687297376953,
      "grad_norm": 0.27595317363739014,
      "learning_rate": 1.865140874578914e-05,
      "loss": 1.1111,
      "step": 413
    },
    {
      "epoch": 3.87709991158267,
      "grad_norm": 0.2577184736728668,
      "learning_rate": 1.8643805581973712e-05,
      "loss": 1.1698,
      "step": 414
    },
    {
      "epoch": 3.886531093427645,
      "grad_norm": 0.25143924355506897,
      "learning_rate": 1.8636182604327464e-05,
      "loss": 1.1724,
      "step": 415
    },
    {
      "epoch": 3.89596227527262,
      "grad_norm": 0.2733379900455475,
      "learning_rate": 1.862853983032423e-05,
      "loss": 1.1258,
      "step": 416
    },
    {
      "epoch": 3.905393457117595,
      "grad_norm": 0.258770614862442,
      "learning_rate": 1.8620877277483217e-05,
      "loss": 1.1232,
      "step": 417
    },
    {
      "epoch": 3.9148246389625703,
      "grad_norm": 0.24883943796157837,
      "learning_rate": 1.8613194963368965e-05,
      "loss": 1.1608,
      "step": 418
    },
    {
      "epoch": 3.924255820807545,
      "grad_norm": 0.2627181112766266,
      "learning_rate": 1.8605492905591328e-05,
      "loss": 1.1189,
      "step": 419
    },
    {
      "epoch": 3.93368700265252,
      "grad_norm": 0.28580594062805176,
      "learning_rate": 1.85977711218054e-05,
      "loss": 1.1318,
      "step": 420
    },
    {
      "epoch": 3.9431181844974947,
      "grad_norm": 0.2642536163330078,
      "learning_rate": 1.859002962971151e-05,
      "loss": 1.1479,
      "step": 421
    },
    {
      "epoch": 3.9525493663424696,
      "grad_norm": 0.28484898805618286,
      "learning_rate": 1.8582268447055142e-05,
      "loss": 1.1667,
      "step": 422
    },
    {
      "epoch": 3.961980548187445,
      "grad_norm": 0.27108198404312134,
      "learning_rate": 1.8574487591626934e-05,
      "loss": 1.122,
      "step": 423
    },
    {
      "epoch": 3.9714117300324197,
      "grad_norm": 0.2821853756904602,
      "learning_rate": 1.8566687081262614e-05,
      "loss": 1.16,
      "step": 424
    },
    {
      "epoch": 3.9808429118773945,
      "grad_norm": 0.2730933129787445,
      "learning_rate": 1.8558866933842962e-05,
      "loss": 1.1414,
      "step": 425
    },
    {
      "epoch": 3.9902740937223697,
      "grad_norm": 0.2529105842113495,
      "learning_rate": 1.855102716729377e-05,
      "loss": 1.1655,
      "step": 426
    },
    {
      "epoch": 3.9997052755673446,
      "grad_norm": 0.2569386661052704,
      "learning_rate": 1.8543167799585802e-05,
      "loss": 1.1506,
      "step": 427
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.2506815195083618,
      "learning_rate": 1.8535288848734764e-05,
      "loss": 0.5931,
      "step": 428
    },
    {
      "epoch": 4.009431181844975,
      "grad_norm": 0.26110681891441345,
      "learning_rate": 1.8527390332801228e-05,
      "loss": 1.1263,
      "step": 429
    },
    {
      "epoch": 4.01886236368995,
      "grad_norm": 0.26453354954719543,
      "learning_rate": 1.8519472269890645e-05,
      "loss": 1.1468,
      "step": 430
    },
    {
      "epoch": 4.0282935455349245,
      "grad_norm": 0.2667200565338135,
      "learning_rate": 1.8511534678153243e-05,
      "loss": 1.1302,
      "step": 431
    },
    {
      "epoch": 4.0377247273799,
      "grad_norm": 0.262959361076355,
      "learning_rate": 1.850357757578404e-05,
      "loss": 1.1121,
      "step": 432
    },
    {
      "epoch": 4.047155909224875,
      "grad_norm": 0.2594277858734131,
      "learning_rate": 1.8495600981022758e-05,
      "loss": 1.121,
      "step": 433
    },
    {
      "epoch": 4.05658709106985,
      "grad_norm": 0.26718199253082275,
      "learning_rate": 1.8487604912153806e-05,
      "loss": 1.1154,
      "step": 434
    },
    {
      "epoch": 4.066018272914825,
      "grad_norm": 0.3011045455932617,
      "learning_rate": 1.8479589387506245e-05,
      "loss": 1.1686,
      "step": 435
    },
    {
      "epoch": 4.0754494547597995,
      "grad_norm": 0.2941574454307556,
      "learning_rate": 1.8471554425453718e-05,
      "loss": 1.1096,
      "step": 436
    },
    {
      "epoch": 4.084880636604774,
      "grad_norm": 0.2669864296913147,
      "learning_rate": 1.846350004441443e-05,
      "loss": 1.1533,
      "step": 437
    },
    {
      "epoch": 4.094311818449749,
      "grad_norm": 0.26542890071868896,
      "learning_rate": 1.8455426262851102e-05,
      "loss": 1.1336,
      "step": 438
    },
    {
      "epoch": 4.103743000294725,
      "grad_norm": 0.2675887942314148,
      "learning_rate": 1.844733309927092e-05,
      "loss": 1.1937,
      "step": 439
    },
    {
      "epoch": 4.1131741821397,
      "grad_norm": 0.2744493782520294,
      "learning_rate": 1.84392205722255e-05,
      "loss": 1.131,
      "step": 440
    },
    {
      "epoch": 4.1226053639846745,
      "grad_norm": 0.27662011981010437,
      "learning_rate": 1.8431088700310846e-05,
      "loss": 1.1391,
      "step": 441
    },
    {
      "epoch": 4.132036545829649,
      "grad_norm": 0.2741719186306,
      "learning_rate": 1.842293750216731e-05,
      "loss": 1.1706,
      "step": 442
    },
    {
      "epoch": 4.141467727674624,
      "grad_norm": 0.27765166759490967,
      "learning_rate": 1.841476699647953e-05,
      "loss": 1.1222,
      "step": 443
    },
    {
      "epoch": 4.150898909519599,
      "grad_norm": 0.25741368532180786,
      "learning_rate": 1.840657720197642e-05,
      "loss": 1.1552,
      "step": 444
    },
    {
      "epoch": 4.160330091364574,
      "grad_norm": 0.2585431635379791,
      "learning_rate": 1.8398368137431098e-05,
      "loss": 1.1216,
      "step": 445
    },
    {
      "epoch": 4.1697612732095495,
      "grad_norm": 0.28400108218193054,
      "learning_rate": 1.8390139821660857e-05,
      "loss": 1.1031,
      "step": 446
    },
    {
      "epoch": 4.179192455054524,
      "grad_norm": 0.2780834436416626,
      "learning_rate": 1.8381892273527114e-05,
      "loss": 1.1343,
      "step": 447
    },
    {
      "epoch": 4.188623636899499,
      "grad_norm": 0.26999613642692566,
      "learning_rate": 1.8373625511935378e-05,
      "loss": 1.1588,
      "step": 448
    },
    {
      "epoch": 4.198054818744474,
      "grad_norm": 0.2631239891052246,
      "learning_rate": 1.83653395558352e-05,
      "loss": 1.1237,
      "step": 449
    },
    {
      "epoch": 4.207486000589449,
      "grad_norm": 0.30761080980300903,
      "learning_rate": 1.835703442422012e-05,
      "loss": 1.1309,
      "step": 450
    },
    {
      "epoch": 4.216917182434424,
      "grad_norm": 0.29059749841690063,
      "learning_rate": 1.8348710136127654e-05,
      "loss": 1.1329,
      "step": 451
    },
    {
      "epoch": 4.226348364279398,
      "grad_norm": 0.27588337659835815,
      "learning_rate": 1.8340366710639204e-05,
      "loss": 1.0994,
      "step": 452
    },
    {
      "epoch": 4.235779546124374,
      "grad_norm": 0.27482742071151733,
      "learning_rate": 1.833200416688006e-05,
      "loss": 1.1381,
      "step": 453
    },
    {
      "epoch": 4.245210727969349,
      "grad_norm": 0.28538644313812256,
      "learning_rate": 1.8323622524019324e-05,
      "loss": 1.1627,
      "step": 454
    },
    {
      "epoch": 4.254641909814324,
      "grad_norm": 0.282926082611084,
      "learning_rate": 1.8315221801269883e-05,
      "loss": 1.161,
      "step": 455
    },
    {
      "epoch": 4.264073091659299,
      "grad_norm": 0.25349029898643494,
      "learning_rate": 1.830680201788836e-05,
      "loss": 1.1412,
      "step": 456
    },
    {
      "epoch": 4.273504273504273,
      "grad_norm": 0.2595995366573334,
      "learning_rate": 1.829836319317507e-05,
      "loss": 1.1629,
      "step": 457
    },
    {
      "epoch": 4.282935455349248,
      "grad_norm": 0.3089168071746826,
      "learning_rate": 1.828990534647397e-05,
      "loss": 1.1595,
      "step": 458
    },
    {
      "epoch": 4.292366637194223,
      "grad_norm": 0.2760039269924164,
      "learning_rate": 1.8281428497172637e-05,
      "loss": 1.1083,
      "step": 459
    },
    {
      "epoch": 4.301797819039198,
      "grad_norm": 0.31513726711273193,
      "learning_rate": 1.827293266470218e-05,
      "loss": 1.139,
      "step": 460
    },
    {
      "epoch": 4.311229000884174,
      "grad_norm": 0.28188157081604004,
      "learning_rate": 1.8264417868537244e-05,
      "loss": 1.1365,
      "step": 461
    },
    {
      "epoch": 4.320660182729148,
      "grad_norm": 0.27063867449760437,
      "learning_rate": 1.8255884128195938e-05,
      "loss": 1.1527,
      "step": 462
    },
    {
      "epoch": 4.330091364574123,
      "grad_norm": 0.27908849716186523,
      "learning_rate": 1.8247331463239797e-05,
      "loss": 1.0977,
      "step": 463
    },
    {
      "epoch": 4.339522546419098,
      "grad_norm": 0.3085228502750397,
      "learning_rate": 1.823875989327373e-05,
      "loss": 1.124,
      "step": 464
    },
    {
      "epoch": 4.348953728264073,
      "grad_norm": 0.2908341586589813,
      "learning_rate": 1.8230169437945987e-05,
      "loss": 1.1306,
      "step": 465
    },
    {
      "epoch": 4.358384910109048,
      "grad_norm": 0.2688801884651184,
      "learning_rate": 1.8221560116948103e-05,
      "loss": 1.1492,
      "step": 466
    },
    {
      "epoch": 4.3678160919540225,
      "grad_norm": 0.2661581039428711,
      "learning_rate": 1.8212931950014862e-05,
      "loss": 1.1416,
      "step": 467
    },
    {
      "epoch": 4.377247273798998,
      "grad_norm": 0.28090181946754456,
      "learning_rate": 1.820428495692425e-05,
      "loss": 1.119,
      "step": 468
    },
    {
      "epoch": 4.386678455643973,
      "grad_norm": 0.305215060710907,
      "learning_rate": 1.819561915749741e-05,
      "loss": 1.1792,
      "step": 469
    },
    {
      "epoch": 4.396109637488948,
      "grad_norm": 0.27976512908935547,
      "learning_rate": 1.8186934571598575e-05,
      "loss": 1.1399,
      "step": 470
    },
    {
      "epoch": 4.405540819333923,
      "grad_norm": 0.28356876969337463,
      "learning_rate": 1.817823121913506e-05,
      "loss": 1.1212,
      "step": 471
    },
    {
      "epoch": 4.4149720011788975,
      "grad_norm": 0.30898600816726685,
      "learning_rate": 1.8169509120057197e-05,
      "loss": 1.1076,
      "step": 472
    },
    {
      "epoch": 4.424403183023872,
      "grad_norm": 0.2590242028236389,
      "learning_rate": 1.816076829435828e-05,
      "loss": 1.1466,
      "step": 473
    },
    {
      "epoch": 4.433834364868847,
      "grad_norm": 0.27590233087539673,
      "learning_rate": 1.8152008762074543e-05,
      "loss": 1.1573,
      "step": 474
    },
    {
      "epoch": 4.443265546713823,
      "grad_norm": 0.2646585702896118,
      "learning_rate": 1.8143230543285084e-05,
      "loss": 1.1258,
      "step": 475
    },
    {
      "epoch": 4.452696728558798,
      "grad_norm": 0.26111358404159546,
      "learning_rate": 1.8134433658111844e-05,
      "loss": 1.1529,
      "step": 476
    },
    {
      "epoch": 4.4621279104037725,
      "grad_norm": 0.2758084535598755,
      "learning_rate": 1.812561812671956e-05,
      "loss": 1.1519,
      "step": 477
    },
    {
      "epoch": 4.471559092248747,
      "grad_norm": 0.2913097143173218,
      "learning_rate": 1.811678396931569e-05,
      "loss": 1.1474,
      "step": 478
    },
    {
      "epoch": 4.480990274093722,
      "grad_norm": 0.2738874852657318,
      "learning_rate": 1.8107931206150413e-05,
      "loss": 1.1467,
      "step": 479
    },
    {
      "epoch": 4.490421455938697,
      "grad_norm": 0.2924787700176239,
      "learning_rate": 1.8099059857516534e-05,
      "loss": 1.1523,
      "step": 480
    },
    {
      "epoch": 4.499852637783672,
      "grad_norm": 0.28449758887290955,
      "learning_rate": 1.8090169943749477e-05,
      "loss": 1.0919,
      "step": 481
    },
    {
      "epoch": 4.5092838196286475,
      "grad_norm": 0.29738304018974304,
      "learning_rate": 1.8081261485227214e-05,
      "loss": 1.1425,
      "step": 482
    },
    {
      "epoch": 4.518715001473622,
      "grad_norm": 0.27795690298080444,
      "learning_rate": 1.807233450237023e-05,
      "loss": 1.139,
      "step": 483
    },
    {
      "epoch": 4.528146183318597,
      "grad_norm": 0.27351003885269165,
      "learning_rate": 1.8063389015641474e-05,
      "loss": 1.147,
      "step": 484
    },
    {
      "epoch": 4.537577365163572,
      "grad_norm": 0.27775609493255615,
      "learning_rate": 1.80544250455463e-05,
      "loss": 1.1224,
      "step": 485
    },
    {
      "epoch": 4.547008547008547,
      "grad_norm": 0.285547137260437,
      "learning_rate": 1.8045442612632446e-05,
      "loss": 1.1194,
      "step": 486
    },
    {
      "epoch": 4.556439728853522,
      "grad_norm": 0.2972468435764313,
      "learning_rate": 1.803644173748996e-05,
      "loss": 1.1557,
      "step": 487
    },
    {
      "epoch": 4.5658709106984965,
      "grad_norm": 0.26662328839302063,
      "learning_rate": 1.8027422440751165e-05,
      "loss": 1.1288,
      "step": 488
    },
    {
      "epoch": 4.575302092543472,
      "grad_norm": 0.2639901340007782,
      "learning_rate": 1.8018384743090624e-05,
      "loss": 1.1328,
      "step": 489
    },
    {
      "epoch": 4.584733274388447,
      "grad_norm": 0.2623863220214844,
      "learning_rate": 1.800932866522506e-05,
      "loss": 1.1819,
      "step": 490
    },
    {
      "epoch": 4.594164456233422,
      "grad_norm": 0.28044983744621277,
      "learning_rate": 1.8000254227913346e-05,
      "loss": 1.1161,
      "step": 491
    },
    {
      "epoch": 4.603595638078397,
      "grad_norm": 0.30056577920913696,
      "learning_rate": 1.799116145195643e-05,
      "loss": 1.1565,
      "step": 492
    },
    {
      "epoch": 4.6130268199233715,
      "grad_norm": 0.2888123095035553,
      "learning_rate": 1.7982050358197292e-05,
      "loss": 1.1589,
      "step": 493
    },
    {
      "epoch": 4.622458001768346,
      "grad_norm": 0.28865930438041687,
      "learning_rate": 1.7972920967520914e-05,
      "loss": 1.1421,
      "step": 494
    },
    {
      "epoch": 4.631889183613321,
      "grad_norm": 0.3084865212440491,
      "learning_rate": 1.7963773300854216e-05,
      "loss": 1.1407,
      "step": 495
    },
    {
      "epoch": 4.641320365458297,
      "grad_norm": 0.278693825006485,
      "learning_rate": 1.7954607379166e-05,
      "loss": 1.1292,
      "step": 496
    },
    {
      "epoch": 4.650751547303272,
      "grad_norm": 0.2928268015384674,
      "learning_rate": 1.7945423223466928e-05,
      "loss": 1.1303,
      "step": 497
    },
    {
      "epoch": 4.6601827291482465,
      "grad_norm": 0.30363571643829346,
      "learning_rate": 1.793622085480945e-05,
      "loss": 1.1575,
      "step": 498
    },
    {
      "epoch": 4.669613910993221,
      "grad_norm": 0.2752076983451843,
      "learning_rate": 1.7927000294287767e-05,
      "loss": 1.1844,
      "step": 499
    },
    {
      "epoch": 4.679045092838196,
      "grad_norm": 0.27749159932136536,
      "learning_rate": 1.7917761563037782e-05,
      "loss": 1.1327,
      "step": 500
    },
    {
      "epoch": 4.688476274683171,
      "grad_norm": 0.2541370987892151,
      "learning_rate": 1.7908504682237047e-05,
      "loss": 1.1516,
      "step": 501
    },
    {
      "epoch": 4.697907456528146,
      "grad_norm": 0.26948127150535583,
      "learning_rate": 1.7899229673104722e-05,
      "loss": 1.1085,
      "step": 502
    },
    {
      "epoch": 4.7073386383731215,
      "grad_norm": 0.28567570447921753,
      "learning_rate": 1.7889936556901518e-05,
      "loss": 1.1406,
      "step": 503
    },
    {
      "epoch": 4.716769820218096,
      "grad_norm": 0.27822500467300415,
      "learning_rate": 1.7880625354929652e-05,
      "loss": 1.1328,
      "step": 504
    },
    {
      "epoch": 4.726201002063071,
      "grad_norm": 0.27510377764701843,
      "learning_rate": 1.78712960885328e-05,
      "loss": 1.1541,
      "step": 505
    },
    {
      "epoch": 4.735632183908046,
      "grad_norm": 0.27321532368659973,
      "learning_rate": 1.7861948779096047e-05,
      "loss": 1.1261,
      "step": 506
    },
    {
      "epoch": 4.745063365753021,
      "grad_norm": 0.261831134557724,
      "learning_rate": 1.785258344804584e-05,
      "loss": 1.1638,
      "step": 507
    },
    {
      "epoch": 4.754494547597996,
      "grad_norm": 0.32160884141921997,
      "learning_rate": 1.784320011684992e-05,
      "loss": 1.1701,
      "step": 508
    },
    {
      "epoch": 4.76392572944297,
      "grad_norm": 0.27369508147239685,
      "learning_rate": 1.783379880701731e-05,
      "loss": 1.1382,
      "step": 509
    },
    {
      "epoch": 4.773356911287946,
      "grad_norm": 0.268442839384079,
      "learning_rate": 1.7824379540098237e-05,
      "loss": 1.1534,
      "step": 510
    },
    {
      "epoch": 4.782788093132921,
      "grad_norm": 0.2699986696243286,
      "learning_rate": 1.7814942337684083e-05,
      "loss": 1.1546,
      "step": 511
    },
    {
      "epoch": 4.792219274977896,
      "grad_norm": 0.29018065333366394,
      "learning_rate": 1.7805487221407354e-05,
      "loss": 1.1649,
      "step": 512
    },
    {
      "epoch": 4.801650456822871,
      "grad_norm": 0.2658344507217407,
      "learning_rate": 1.7796014212941615e-05,
      "loss": 1.111,
      "step": 513
    },
    {
      "epoch": 4.811081638667845,
      "grad_norm": 0.29831045866012573,
      "learning_rate": 1.7786523334001438e-05,
      "loss": 1.1345,
      "step": 514
    },
    {
      "epoch": 4.82051282051282,
      "grad_norm": 0.2754085958003998,
      "learning_rate": 1.7777014606342367e-05,
      "loss": 1.1902,
      "step": 515
    },
    {
      "epoch": 4.829944002357795,
      "grad_norm": 0.2751682698726654,
      "learning_rate": 1.7767488051760858e-05,
      "loss": 1.1061,
      "step": 516
    },
    {
      "epoch": 4.839375184202771,
      "grad_norm": 0.299378901720047,
      "learning_rate": 1.775794369209423e-05,
      "loss": 1.1555,
      "step": 517
    },
    {
      "epoch": 4.848806366047746,
      "grad_norm": 0.26000192761421204,
      "learning_rate": 1.7748381549220614e-05,
      "loss": 1.1763,
      "step": 518
    },
    {
      "epoch": 4.85823754789272,
      "grad_norm": 0.2627812623977661,
      "learning_rate": 1.773880164505891e-05,
      "loss": 1.1501,
      "step": 519
    },
    {
      "epoch": 4.867668729737695,
      "grad_norm": 0.297275185585022,
      "learning_rate": 1.7729204001568726e-05,
      "loss": 1.1743,
      "step": 520
    },
    {
      "epoch": 4.87709991158267,
      "grad_norm": 0.2703762948513031,
      "learning_rate": 1.7719588640750337e-05,
      "loss": 1.1524,
      "step": 521
    },
    {
      "epoch": 4.886531093427645,
      "grad_norm": 0.2971392869949341,
      "learning_rate": 1.7709955584644626e-05,
      "loss": 1.165,
      "step": 522
    },
    {
      "epoch": 4.89596227527262,
      "grad_norm": 0.2894222140312195,
      "learning_rate": 1.770030485533304e-05,
      "loss": 1.1762,
      "step": 523
    },
    {
      "epoch": 4.9053934571175954,
      "grad_norm": 0.30007466673851013,
      "learning_rate": 1.7690636474937545e-05,
      "loss": 1.2017,
      "step": 524
    },
    {
      "epoch": 4.91482463896257,
      "grad_norm": 0.3003268241882324,
      "learning_rate": 1.7680950465620556e-05,
      "loss": 1.1544,
      "step": 525
    },
    {
      "epoch": 4.924255820807545,
      "grad_norm": 0.27063900232315063,
      "learning_rate": 1.7671246849584905e-05,
      "loss": 1.1548,
      "step": 526
    },
    {
      "epoch": 4.93368700265252,
      "grad_norm": 0.29092177748680115,
      "learning_rate": 1.7661525649073784e-05,
      "loss": 1.1455,
      "step": 527
    },
    {
      "epoch": 4.943118184497495,
      "grad_norm": 0.28965577483177185,
      "learning_rate": 1.7651786886370686e-05,
      "loss": 1.1376,
      "step": 528
    },
    {
      "epoch": 4.95254936634247,
      "grad_norm": 0.28387919068336487,
      "learning_rate": 1.7642030583799366e-05,
      "loss": 1.1623,
      "step": 529
    },
    {
      "epoch": 4.961980548187444,
      "grad_norm": 0.2937987148761749,
      "learning_rate": 1.763225676372379e-05,
      "loss": 1.1035,
      "step": 530
    },
    {
      "epoch": 4.97141173003242,
      "grad_norm": 0.2891542613506317,
      "learning_rate": 1.762246544854807e-05,
      "loss": 1.1272,
      "step": 531
    },
    {
      "epoch": 4.980842911877395,
      "grad_norm": 0.2859998345375061,
      "learning_rate": 1.7612656660716424e-05,
      "loss": 1.1456,
      "step": 532
    },
    {
      "epoch": 4.99027409372237,
      "grad_norm": 0.3323535621166229,
      "learning_rate": 1.7602830422713123e-05,
      "loss": 1.1691,
      "step": 533
    },
    {
      "epoch": 4.999705275567345,
      "grad_norm": 0.3007301986217499,
      "learning_rate": 1.7592986757062443e-05,
      "loss": 1.1046,
      "step": 534
    },
    {
      "epoch": 5.0,
      "grad_norm": 2.149345874786377,
      "learning_rate": 1.7583125686328594e-05,
      "loss": 0.5181,
      "step": 535
    },
    {
      "epoch": 5.009431181844975,
      "grad_norm": 0.2432025820016861,
      "learning_rate": 1.7573247233115697e-05,
      "loss": 1.1426,
      "step": 536
    },
    {
      "epoch": 5.01886236368995,
      "grad_norm": 0.2696497142314911,
      "learning_rate": 1.7563351420067712e-05,
      "loss": 1.1508,
      "step": 537
    },
    {
      "epoch": 5.0282935455349245,
      "grad_norm": 0.27454933524131775,
      "learning_rate": 1.7553438269868398e-05,
      "loss": 1.0821,
      "step": 538
    },
    {
      "epoch": 5.0377247273799,
      "grad_norm": 0.30004847049713135,
      "learning_rate": 1.7543507805241248e-05,
      "loss": 1.0999,
      "step": 539
    },
    {
      "epoch": 5.047155909224875,
      "grad_norm": 0.27237260341644287,
      "learning_rate": 1.7533560048949446e-05,
      "loss": 1.1475,
      "step": 540
    },
    {
      "epoch": 5.05658709106985,
      "grad_norm": 0.2999335825443268,
      "learning_rate": 1.7523595023795814e-05,
      "loss": 1.125,
      "step": 541
    },
    {
      "epoch": 5.066018272914825,
      "grad_norm": 0.2663687467575073,
      "learning_rate": 1.751361275262276e-05,
      "loss": 1.2038,
      "step": 542
    },
    {
      "epoch": 5.0754494547597995,
      "grad_norm": 0.28311479091644287,
      "learning_rate": 1.7503613258312223e-05,
      "loss": 1.1349,
      "step": 543
    },
    {
      "epoch": 5.084880636604774,
      "grad_norm": 0.26742827892303467,
      "learning_rate": 1.7493596563785625e-05,
      "loss": 1.148,
      "step": 544
    },
    {
      "epoch": 5.094311818449749,
      "grad_norm": 0.2703859508037567,
      "learning_rate": 1.7483562692003806e-05,
      "loss": 1.1514,
      "step": 545
    },
    {
      "epoch": 5.103743000294725,
      "grad_norm": 0.2985357344150543,
      "learning_rate": 1.7473511665966995e-05,
      "loss": 1.134,
      "step": 546
    },
    {
      "epoch": 5.1131741821397,
      "grad_norm": 0.29315024614334106,
      "learning_rate": 1.746344350871472e-05,
      "loss": 1.1559,
      "step": 547
    },
    {
      "epoch": 5.1226053639846745,
      "grad_norm": 0.2788150906562805,
      "learning_rate": 1.745335824332581e-05,
      "loss": 1.1174,
      "step": 548
    },
    {
      "epoch": 5.132036545829649,
      "grad_norm": 0.2955104410648346,
      "learning_rate": 1.7443255892918284e-05,
      "loss": 1.1333,
      "step": 549
    },
    {
      "epoch": 5.141467727674624,
      "grad_norm": 0.3041563034057617,
      "learning_rate": 1.7433136480649337e-05,
      "loss": 1.2041,
      "step": 550
    },
    {
      "epoch": 5.150898909519599,
      "grad_norm": 0.2882762849330902,
      "learning_rate": 1.742300002971527e-05,
      "loss": 1.15,
      "step": 551
    },
    {
      "epoch": 5.160330091364574,
      "grad_norm": 0.2765505909919739,
      "learning_rate": 1.7412846563351438e-05,
      "loss": 1.1511,
      "step": 552
    },
    {
      "epoch": 5.1697612732095495,
      "grad_norm": 0.3131656050682068,
      "learning_rate": 1.740267610483221e-05,
      "loss": 1.1335,
      "step": 553
    },
    {
      "epoch": 5.179192455054524,
      "grad_norm": 0.2772139012813568,
      "learning_rate": 1.7392488677470894e-05,
      "loss": 1.15,
      "step": 554
    },
    {
      "epoch": 5.188623636899499,
      "grad_norm": 0.2920176386833191,
      "learning_rate": 1.73822843046197e-05,
      "loss": 1.1521,
      "step": 555
    },
    {
      "epoch": 5.198054818744474,
      "grad_norm": 0.2973923087120056,
      "learning_rate": 1.7372063009669685e-05,
      "loss": 1.14,
      "step": 556
    },
    {
      "epoch": 5.207486000589449,
      "grad_norm": 0.2863141894340515,
      "learning_rate": 1.736182481605069e-05,
      "loss": 1.1453,
      "step": 557
    },
    {
      "epoch": 5.216917182434424,
      "grad_norm": 0.2707030475139618,
      "learning_rate": 1.7351569747231294e-05,
      "loss": 1.1776,
      "step": 558
    },
    {
      "epoch": 5.226348364279398,
      "grad_norm": 0.2926724851131439,
      "learning_rate": 1.734129782671876e-05,
      "loss": 1.142,
      "step": 559
    },
    {
      "epoch": 5.235779546124374,
      "grad_norm": 0.29578694701194763,
      "learning_rate": 1.7331009078058977e-05,
      "loss": 1.1261,
      "step": 560
    },
    {
      "epoch": 5.245210727969349,
      "grad_norm": 0.2967230975627899,
      "learning_rate": 1.7320703524836407e-05,
      "loss": 1.1,
      "step": 561
    },
    {
      "epoch": 5.254641909814324,
      "grad_norm": 0.29843780398368835,
      "learning_rate": 1.731038119067404e-05,
      "loss": 1.0894,
      "step": 562
    },
    {
      "epoch": 5.264073091659299,
      "grad_norm": 0.2884487807750702,
      "learning_rate": 1.730004209923332e-05,
      "loss": 1.1176,
      "step": 563
    },
    {
      "epoch": 5.273504273504273,
      "grad_norm": 0.27440711855888367,
      "learning_rate": 1.7289686274214116e-05,
      "loss": 1.1507,
      "step": 564
    },
    {
      "epoch": 5.282935455349248,
      "grad_norm": 0.2923506498336792,
      "learning_rate": 1.7279313739354647e-05,
      "loss": 1.1646,
      "step": 565
    },
    {
      "epoch": 5.292366637194223,
      "grad_norm": 0.27681973576545715,
      "learning_rate": 1.7268924518431437e-05,
      "loss": 1.1755,
      "step": 566
    },
    {
      "epoch": 5.301797819039198,
      "grad_norm": 0.3040882349014282,
      "learning_rate": 1.725851863525926e-05,
      "loss": 1.1336,
      "step": 567
    },
    {
      "epoch": 5.311229000884174,
      "grad_norm": 0.27274635434150696,
      "learning_rate": 1.724809611369108e-05,
      "loss": 1.1583,
      "step": 568
    },
    {
      "epoch": 5.320660182729148,
      "grad_norm": 0.2875063121318817,
      "learning_rate": 1.7237656977618007e-05,
      "loss": 1.1253,
      "step": 569
    },
    {
      "epoch": 5.330091364574123,
      "grad_norm": 0.2737381160259247,
      "learning_rate": 1.722720125096923e-05,
      "loss": 1.1372,
      "step": 570
    },
    {
      "epoch": 5.339522546419098,
      "grad_norm": 0.31626641750335693,
      "learning_rate": 1.7216728957711968e-05,
      "loss": 1.1474,
      "step": 571
    },
    {
      "epoch": 5.348953728264073,
      "grad_norm": 0.29129737615585327,
      "learning_rate": 1.720624012185142e-05,
      "loss": 1.1035,
      "step": 572
    },
    {
      "epoch": 5.358384910109048,
      "grad_norm": 0.3042541742324829,
      "learning_rate": 1.71957347674307e-05,
      "loss": 1.16,
      "step": 573
    },
    {
      "epoch": 5.3678160919540225,
      "grad_norm": 0.28057077527046204,
      "learning_rate": 1.7185212918530794e-05,
      "loss": 1.1029,
      "step": 574
    },
    {
      "epoch": 5.377247273798998,
      "grad_norm": 0.2906835973262787,
      "learning_rate": 1.7174674599270486e-05,
      "loss": 1.1722,
      "step": 575
    },
    {
      "epoch": 5.386678455643973,
      "grad_norm": 0.280122309923172,
      "learning_rate": 1.716411983380632e-05,
      "loss": 1.13,
      "step": 576
    },
    {
      "epoch": 5.396109637488948,
      "grad_norm": 0.26865068078041077,
      "learning_rate": 1.7153548646332545e-05,
      "loss": 1.1138,
      "step": 577
    },
    {
      "epoch": 5.405540819333923,
      "grad_norm": 0.2994769513607025,
      "learning_rate": 1.714296106108104e-05,
      "loss": 1.1466,
      "step": 578
    },
    {
      "epoch": 5.4149720011788975,
      "grad_norm": 0.3138396739959717,
      "learning_rate": 1.7132357102321293e-05,
      "loss": 1.1351,
      "step": 579
    },
    {
      "epoch": 5.424403183023872,
      "grad_norm": 0.29908105731010437,
      "learning_rate": 1.71217367943603e-05,
      "loss": 1.13,
      "step": 580
    },
    {
      "epoch": 5.433834364868847,
      "grad_norm": 0.2777354419231415,
      "learning_rate": 1.7111100161542545e-05,
      "loss": 1.1362,
      "step": 581
    },
    {
      "epoch": 5.443265546713823,
      "grad_norm": 0.3044910728931427,
      "learning_rate": 1.710044722824994e-05,
      "loss": 1.1053,
      "step": 582
    },
    {
      "epoch": 5.452696728558798,
      "grad_norm": 0.29596146941185,
      "learning_rate": 1.7089778018901747e-05,
      "loss": 1.1453,
      "step": 583
    },
    {
      "epoch": 5.4621279104037725,
      "grad_norm": 0.27711713314056396,
      "learning_rate": 1.707909255795455e-05,
      "loss": 1.1499,
      "step": 584
    },
    {
      "epoch": 5.471559092248747,
      "grad_norm": 0.31857603788375854,
      "learning_rate": 1.706839086990218e-05,
      "loss": 1.1398,
      "step": 585
    },
    {
      "epoch": 5.480990274093722,
      "grad_norm": 0.29337045550346375,
      "learning_rate": 1.7057672979275656e-05,
      "loss": 1.1479,
      "step": 586
    },
    {
      "epoch": 5.490421455938697,
      "grad_norm": 0.3057938814163208,
      "learning_rate": 1.704693891064316e-05,
      "loss": 1.1507,
      "step": 587
    },
    {
      "epoch": 5.499852637783672,
      "grad_norm": 0.30440402030944824,
      "learning_rate": 1.7036188688609938e-05,
      "loss": 1.0946,
      "step": 588
    },
    {
      "epoch": 5.5092838196286475,
      "grad_norm": 0.291720449924469,
      "learning_rate": 1.7025422337818266e-05,
      "loss": 1.1508,
      "step": 589
    },
    {
      "epoch": 5.518715001473622,
      "grad_norm": 0.3024042248725891,
      "learning_rate": 1.7014639882947403e-05,
      "loss": 1.1142,
      "step": 590
    },
    {
      "epoch": 5.528146183318597,
      "grad_norm": 0.29326218366622925,
      "learning_rate": 1.700384134871351e-05,
      "loss": 1.1435,
      "step": 591
    },
    {
      "epoch": 5.537577365163572,
      "grad_norm": 0.2920026183128357,
      "learning_rate": 1.6993026759869614e-05,
      "loss": 1.1122,
      "step": 592
    },
    {
      "epoch": 5.547008547008547,
      "grad_norm": 0.30563241243362427,
      "learning_rate": 1.6982196141205543e-05,
      "loss": 1.1076,
      "step": 593
    },
    {
      "epoch": 5.556439728853522,
      "grad_norm": 0.3031493127346039,
      "learning_rate": 1.6971349517547863e-05,
      "loss": 1.0961,
      "step": 594
    },
    {
      "epoch": 5.5658709106984965,
      "grad_norm": 0.3345252275466919,
      "learning_rate": 1.696048691375983e-05,
      "loss": 1.1592,
      "step": 595
    },
    {
      "epoch": 5.575302092543472,
      "grad_norm": 0.28557053208351135,
      "learning_rate": 1.6949608354741342e-05,
      "loss": 1.1432,
      "step": 596
    },
    {
      "epoch": 5.584733274388447,
      "grad_norm": 0.28410977125167847,
      "learning_rate": 1.693871386542885e-05,
      "loss": 1.155,
      "step": 597
    },
    {
      "epoch": 5.594164456233422,
      "grad_norm": 0.28953149914741516,
      "learning_rate": 1.6927803470795338e-05,
      "loss": 1.117,
      "step": 598
    },
    {
      "epoch": 5.603595638078397,
      "grad_norm": 0.2984173595905304,
      "learning_rate": 1.691687719585024e-05,
      "loss": 1.135,
      "step": 599
    },
    {
      "epoch": 5.6130268199233715,
      "grad_norm": 0.2902682423591614,
      "learning_rate": 1.69059350656394e-05,
      "loss": 1.1495,
      "step": 600
    },
    {
      "epoch": 5.622458001768346,
      "grad_norm": 0.31897398829460144,
      "learning_rate": 1.6894977105244996e-05,
      "loss": 1.1307,
      "step": 601
    },
    {
      "epoch": 5.631889183613321,
      "grad_norm": 0.28370970487594604,
      "learning_rate": 1.6884003339785503e-05,
      "loss": 1.127,
      "step": 602
    },
    {
      "epoch": 5.641320365458297,
      "grad_norm": 0.29191261529922485,
      "learning_rate": 1.6873013794415614e-05,
      "loss": 1.1293,
      "step": 603
    },
    {
      "epoch": 5.650751547303272,
      "grad_norm": 0.29423803091049194,
      "learning_rate": 1.686200849432621e-05,
      "loss": 1.1159,
      "step": 604
    },
    {
      "epoch": 5.6601827291482465,
      "grad_norm": 0.26403361558914185,
      "learning_rate": 1.6850987464744267e-05,
      "loss": 1.1286,
      "step": 605
    },
    {
      "epoch": 5.669613910993221,
      "grad_norm": 0.28824782371520996,
      "learning_rate": 1.683995073093283e-05,
      "loss": 1.1269,
      "step": 606
    },
    {
      "epoch": 5.679045092838196,
      "grad_norm": 0.2890409231185913,
      "learning_rate": 1.6828898318190937e-05,
      "loss": 1.1298,
      "step": 607
    },
    {
      "epoch": 5.688476274683171,
      "grad_norm": 0.3020886480808258,
      "learning_rate": 1.681783025185357e-05,
      "loss": 1.1174,
      "step": 608
    },
    {
      "epoch": 5.697907456528146,
      "grad_norm": 0.285076379776001,
      "learning_rate": 1.6806746557291587e-05,
      "loss": 1.134,
      "step": 609
    },
    {
      "epoch": 5.7073386383731215,
      "grad_norm": 0.29890966415405273,
      "learning_rate": 1.6795647259911674e-05,
      "loss": 1.1912,
      "step": 610
    },
    {
      "epoch": 5.716769820218096,
      "grad_norm": 0.32472699880599976,
      "learning_rate": 1.6784532385156285e-05,
      "loss": 1.1485,
      "step": 611
    },
    {
      "epoch": 5.726201002063071,
      "grad_norm": 0.28711044788360596,
      "learning_rate": 1.6773401958503575e-05,
      "loss": 1.1146,
      "step": 612
    },
    {
      "epoch": 5.735632183908046,
      "grad_norm": 0.30163148045539856,
      "learning_rate": 1.6762256005467354e-05,
      "loss": 1.1587,
      "step": 613
    },
    {
      "epoch": 5.745063365753021,
      "grad_norm": 0.28208696842193604,
      "learning_rate": 1.6751094551597017e-05,
      "loss": 1.1277,
      "step": 614
    },
    {
      "epoch": 5.754494547597996,
      "grad_norm": 0.2789824306964874,
      "learning_rate": 1.673991762247749e-05,
      "loss": 1.1405,
      "step": 615
    },
    {
      "epoch": 5.76392572944297,
      "grad_norm": 0.3116413354873657,
      "learning_rate": 1.672872524372919e-05,
      "loss": 1.1311,
      "step": 616
    },
    {
      "epoch": 5.773356911287946,
      "grad_norm": 0.25674745440483093,
      "learning_rate": 1.671751744100792e-05,
      "loss": 1.1514,
      "step": 617
    },
    {
      "epoch": 5.782788093132921,
      "grad_norm": 0.29519253969192505,
      "learning_rate": 1.6706294240004868e-05,
      "loss": 1.1039,
      "step": 618
    },
    {
      "epoch": 5.792219274977896,
      "grad_norm": 0.28021928668022156,
      "learning_rate": 1.6695055666446488e-05,
      "loss": 1.1198,
      "step": 619
    },
    {
      "epoch": 5.801650456822871,
      "grad_norm": 0.3013998866081238,
      "learning_rate": 1.6683801746094498e-05,
      "loss": 1.1216,
      "step": 620
    },
    {
      "epoch": 5.811081638667845,
      "grad_norm": 0.3051455318927765,
      "learning_rate": 1.667253250474578e-05,
      "loss": 1.1052,
      "step": 621
    },
    {
      "epoch": 5.82051282051282,
      "grad_norm": 0.2968464493751526,
      "learning_rate": 1.666124796823234e-05,
      "loss": 1.1487,
      "step": 622
    },
    {
      "epoch": 5.829944002357795,
      "grad_norm": 0.2784920036792755,
      "learning_rate": 1.6649948162421254e-05,
      "loss": 1.1288,
      "step": 623
    },
    {
      "epoch": 5.839375184202771,
      "grad_norm": 0.2938484847545624,
      "learning_rate": 1.6638633113214577e-05,
      "loss": 1.1288,
      "step": 624
    },
    {
      "epoch": 5.848806366047746,
      "grad_norm": 0.30007779598236084,
      "learning_rate": 1.662730284654933e-05,
      "loss": 1.1274,
      "step": 625
    },
    {
      "epoch": 5.85823754789272,
      "grad_norm": 0.30561619997024536,
      "learning_rate": 1.66159573883974e-05,
      "loss": 1.1277,
      "step": 626
    },
    {
      "epoch": 5.867668729737695,
      "grad_norm": 0.31423988938331604,
      "learning_rate": 1.66045967647655e-05,
      "loss": 1.0995,
      "step": 627
    },
    {
      "epoch": 5.87709991158267,
      "grad_norm": 0.33930835127830505,
      "learning_rate": 1.6593221001695116e-05,
      "loss": 1.1372,
      "step": 628
    },
    {
      "epoch": 5.886531093427645,
      "grad_norm": 0.34827956557273865,
      "learning_rate": 1.6581830125262417e-05,
      "loss": 1.1167,
      "step": 629
    },
    {
      "epoch": 5.89596227527262,
      "grad_norm": 0.32720017433166504,
      "learning_rate": 1.657042416157824e-05,
      "loss": 1.1507,
      "step": 630
    },
    {
      "epoch": 5.9053934571175954,
      "grad_norm": 0.28394615650177,
      "learning_rate": 1.655900313678799e-05,
      "loss": 1.196,
      "step": 631
    },
    {
      "epoch": 5.91482463896257,
      "grad_norm": 0.3057882785797119,
      "learning_rate": 1.65475670770716e-05,
      "loss": 1.1687,
      "step": 632
    },
    {
      "epoch": 5.924255820807545,
      "grad_norm": 0.3126245141029358,
      "learning_rate": 1.653611600864347e-05,
      "loss": 1.1645,
      "step": 633
    },
    {
      "epoch": 5.93368700265252,
      "grad_norm": 0.27675989270210266,
      "learning_rate": 1.6524649957752393e-05,
      "loss": 1.1488,
      "step": 634
    },
    {
      "epoch": 5.943118184497495,
      "grad_norm": 0.30721315741539,
      "learning_rate": 1.6513168950681525e-05,
      "loss": 1.132,
      "step": 635
    },
    {
      "epoch": 5.95254936634247,
      "grad_norm": 0.31142857670783997,
      "learning_rate": 1.6501673013748284e-05,
      "loss": 1.1123,
      "step": 636
    },
    {
      "epoch": 5.961980548187444,
      "grad_norm": 0.30687323212623596,
      "learning_rate": 1.649016217330433e-05,
      "loss": 1.1572,
      "step": 637
    },
    {
      "epoch": 5.97141173003242,
      "grad_norm": 0.3022448718547821,
      "learning_rate": 1.647863645573547e-05,
      "loss": 1.1183,
      "step": 638
    },
    {
      "epoch": 5.980842911877395,
      "grad_norm": 0.3164305090904236,
      "learning_rate": 1.646709588746162e-05,
      "loss": 1.0839,
      "step": 639
    },
    {
      "epoch": 5.99027409372237,
      "grad_norm": 0.30133646726608276,
      "learning_rate": 1.645554049493674e-05,
      "loss": 1.1168,
      "step": 640
    },
    {
      "epoch": 5.999705275567345,
      "grad_norm": 0.28684964776039124,
      "learning_rate": 1.644397030464877e-05,
      "loss": 1.1721,
      "step": 641
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.3182268142700195,
      "learning_rate": 1.643238534311957e-05,
      "loss": 0.944,
      "step": 642
    },
    {
      "epoch": 6.009431181844975,
      "grad_norm": 0.3230450749397278,
      "learning_rate": 1.6420785636904855e-05,
      "loss": 1.1519,
      "step": 643
    },
    {
      "epoch": 6.01886236368995,
      "grad_norm": 0.29768580198287964,
      "learning_rate": 1.6409171212594142e-05,
      "loss": 1.1238,
      "step": 644
    },
    {
      "epoch": 6.0282935455349245,
      "grad_norm": 0.3050430417060852,
      "learning_rate": 1.639754209681069e-05,
      "loss": 1.167,
      "step": 645
    },
    {
      "epoch": 6.0377247273799,
      "grad_norm": 0.28335338830947876,
      "learning_rate": 1.6385898316211426e-05,
      "loss": 1.1266,
      "step": 646
    },
    {
      "epoch": 6.047155909224875,
      "grad_norm": 0.28961625695228577,
      "learning_rate": 1.63742398974869e-05,
      "loss": 1.1476,
      "step": 647
    },
    {
      "epoch": 6.05658709106985,
      "grad_norm": 0.3028390109539032,
      "learning_rate": 1.6362566867361208e-05,
      "loss": 1.1231,
      "step": 648
    },
    {
      "epoch": 6.066018272914825,
      "grad_norm": 0.27573832869529724,
      "learning_rate": 1.6350879252591953e-05,
      "loss": 1.1525,
      "step": 649
    },
    {
      "epoch": 6.0754494547597995,
      "grad_norm": 0.29435423016548157,
      "learning_rate": 1.6339177079970154e-05,
      "loss": 1.1103,
      "step": 650
    },
    {
      "epoch": 6.084880636604774,
      "grad_norm": 0.30514490604400635,
      "learning_rate": 1.632746037632021e-05,
      "loss": 1.1276,
      "step": 651
    },
    {
      "epoch": 6.094311818449749,
      "grad_norm": 0.28916510939598083,
      "learning_rate": 1.6315729168499827e-05,
      "loss": 1.145,
      "step": 652
    },
    {
      "epoch": 6.103743000294725,
      "grad_norm": 0.3298945724964142,
      "learning_rate": 1.6303983483399957e-05,
      "loss": 1.1052,
      "step": 653
    },
    {
      "epoch": 6.1131741821397,
      "grad_norm": 0.2656973600387573,
      "learning_rate": 1.629222334794474e-05,
      "loss": 1.1473,
      "step": 654
    },
    {
      "epoch": 6.1226053639846745,
      "grad_norm": 0.32597723603248596,
      "learning_rate": 1.6280448789091434e-05,
      "loss": 1.1207,
      "step": 655
    },
    {
      "epoch": 6.132036545829649,
      "grad_norm": 0.3111819326877594,
      "learning_rate": 1.626865983383037e-05,
      "loss": 1.1375,
      "step": 656
    },
    {
      "epoch": 6.141467727674624,
      "grad_norm": 0.29063090682029724,
      "learning_rate": 1.6256856509184867e-05,
      "loss": 1.1346,
      "step": 657
    },
    {
      "epoch": 6.150898909519599,
      "grad_norm": 0.30332615971565247,
      "learning_rate": 1.624503884221119e-05,
      "loss": 1.1208,
      "step": 658
    },
    {
      "epoch": 6.160330091364574,
      "grad_norm": 0.28364434838294983,
      "learning_rate": 1.6233206859998476e-05,
      "loss": 1.1218,
      "step": 659
    },
    {
      "epoch": 6.1697612732095495,
      "grad_norm": 0.29063680768013,
      "learning_rate": 1.6221360589668685e-05,
      "loss": 1.1165,
      "step": 660
    },
    {
      "epoch": 6.179192455054524,
      "grad_norm": 0.29078590869903564,
      "learning_rate": 1.6209500058376516e-05,
      "loss": 1.1477,
      "step": 661
    },
    {
      "epoch": 6.188623636899499,
      "grad_norm": 0.3035820424556732,
      "learning_rate": 1.6197625293309367e-05,
      "loss": 1.1197,
      "step": 662
    },
    {
      "epoch": 6.198054818744474,
      "grad_norm": 0.2755659520626068,
      "learning_rate": 1.6185736321687265e-05,
      "loss": 1.19,
      "step": 663
    },
    {
      "epoch": 6.207486000589449,
      "grad_norm": 0.31430545449256897,
      "learning_rate": 1.617383317076279e-05,
      "loss": 1.1095,
      "step": 664
    },
    {
      "epoch": 6.216917182434424,
      "grad_norm": 0.2913474440574646,
      "learning_rate": 1.6161915867821036e-05,
      "loss": 1.1241,
      "step": 665
    },
    {
      "epoch": 6.226348364279398,
      "grad_norm": 0.30153822898864746,
      "learning_rate": 1.614998444017954e-05,
      "loss": 1.1284,
      "step": 666
    },
    {
      "epoch": 6.235779546124374,
      "grad_norm": 0.30123600363731384,
      "learning_rate": 1.6138038915188197e-05,
      "loss": 1.1378,
      "step": 667
    },
    {
      "epoch": 6.245210727969349,
      "grad_norm": 0.3104604482650757,
      "learning_rate": 1.6126079320229243e-05,
      "loss": 1.1343,
      "step": 668
    },
    {
      "epoch": 6.254641909814324,
      "grad_norm": 0.3030853271484375,
      "learning_rate": 1.6114105682717143e-05,
      "loss": 1.1464,
      "step": 669
    },
    {
      "epoch": 6.264073091659299,
      "grad_norm": 0.32043567299842834,
      "learning_rate": 1.610211803009857e-05,
      "loss": 1.1248,
      "step": 670
    },
    {
      "epoch": 6.273504273504273,
      "grad_norm": 0.3099376857280731,
      "learning_rate": 1.6090116389852305e-05,
      "loss": 1.1478,
      "step": 671
    },
    {
      "epoch": 6.282935455349248,
      "grad_norm": 0.30996769666671753,
      "learning_rate": 1.607810078948921e-05,
      "loss": 1.1525,
      "step": 672
    },
    {
      "epoch": 6.292366637194223,
      "grad_norm": 0.3026559054851532,
      "learning_rate": 1.6066071256552133e-05,
      "loss": 1.1227,
      "step": 673
    },
    {
      "epoch": 6.301797819039198,
      "grad_norm": 0.2937197983264923,
      "learning_rate": 1.6054027818615866e-05,
      "loss": 1.1867,
      "step": 674
    },
    {
      "epoch": 6.311229000884174,
      "grad_norm": 0.2927652597427368,
      "learning_rate": 1.6041970503287077e-05,
      "loss": 1.1076,
      "step": 675
    },
    {
      "epoch": 6.320660182729148,
      "grad_norm": 0.2979874610900879,
      "learning_rate": 1.6029899338204234e-05,
      "loss": 1.1227,
      "step": 676
    },
    {
      "epoch": 6.330091364574123,
      "grad_norm": 0.31841641664505005,
      "learning_rate": 1.6017814351037568e-05,
      "loss": 1.124,
      "step": 677
    },
    {
      "epoch": 6.339522546419098,
      "grad_norm": 0.3068021535873413,
      "learning_rate": 1.600571556948898e-05,
      "loss": 1.1446,
      "step": 678
    },
    {
      "epoch": 6.348953728264073,
      "grad_norm": 0.3189489245414734,
      "learning_rate": 1.5993603021291993e-05,
      "loss": 1.1321,
      "step": 679
    },
    {
      "epoch": 6.358384910109048,
      "grad_norm": 0.26902899146080017,
      "learning_rate": 1.59814767342117e-05,
      "loss": 1.1292,
      "step": 680
    },
    {
      "epoch": 6.3678160919540225,
      "grad_norm": 0.31821268796920776,
      "learning_rate": 1.596933673604467e-05,
      "loss": 1.1006,
      "step": 681
    },
    {
      "epoch": 6.377247273798998,
      "grad_norm": 0.29199543595314026,
      "learning_rate": 1.595718305461891e-05,
      "loss": 1.1028,
      "step": 682
    },
    {
      "epoch": 6.386678455643973,
      "grad_norm": 0.29344046115875244,
      "learning_rate": 1.5945015717793797e-05,
      "loss": 1.1203,
      "step": 683
    },
    {
      "epoch": 6.396109637488948,
      "grad_norm": 0.2978805899620056,
      "learning_rate": 1.5932834753459992e-05,
      "loss": 1.1138,
      "step": 684
    },
    {
      "epoch": 6.405540819333923,
      "grad_norm": 0.2989928424358368,
      "learning_rate": 1.5920640189539415e-05,
      "loss": 1.1209,
      "step": 685
    },
    {
      "epoch": 6.4149720011788975,
      "grad_norm": 0.313240647315979,
      "learning_rate": 1.5908432053985143e-05,
      "loss": 1.1195,
      "step": 686
    },
    {
      "epoch": 6.424403183023872,
      "grad_norm": 0.30508115887641907,
      "learning_rate": 1.589621037478138e-05,
      "loss": 1.1413,
      "step": 687
    },
    {
      "epoch": 6.433834364868847,
      "grad_norm": 0.3165152966976166,
      "learning_rate": 1.5883975179943355e-05,
      "loss": 1.1166,
      "step": 688
    },
    {
      "epoch": 6.443265546713823,
      "grad_norm": 0.32457873225212097,
      "learning_rate": 1.5871726497517297e-05,
      "loss": 1.1343,
      "step": 689
    },
    {
      "epoch": 6.452696728558798,
      "grad_norm": 0.3174217641353607,
      "learning_rate": 1.5859464355580336e-05,
      "loss": 1.1535,
      "step": 690
    },
    {
      "epoch": 6.4621279104037725,
      "grad_norm": 0.28406238555908203,
      "learning_rate": 1.5847188782240473e-05,
      "loss": 1.1226,
      "step": 691
    },
    {
      "epoch": 6.471559092248747,
      "grad_norm": 0.2964501976966858,
      "learning_rate": 1.583489980563648e-05,
      "loss": 1.1291,
      "step": 692
    },
    {
      "epoch": 6.480990274093722,
      "grad_norm": 0.3050205111503601,
      "learning_rate": 1.5822597453937857e-05,
      "loss": 1.0839,
      "step": 693
    },
    {
      "epoch": 6.490421455938697,
      "grad_norm": 0.2895800769329071,
      "learning_rate": 1.5810281755344773e-05,
      "loss": 1.1744,
      "step": 694
    },
    {
      "epoch": 6.499852637783672,
      "grad_norm": 0.290942519903183,
      "learning_rate": 1.579795273808798e-05,
      "loss": 1.1292,
      "step": 695
    },
    {
      "epoch": 6.5092838196286475,
      "grad_norm": 0.2951255440711975,
      "learning_rate": 1.578561043042876e-05,
      "loss": 1.139,
      "step": 696
    },
    {
      "epoch": 6.518715001473622,
      "grad_norm": 0.2980732023715973,
      "learning_rate": 1.5773254860658873e-05,
      "loss": 1.0977,
      "step": 697
    },
    {
      "epoch": 6.528146183318597,
      "grad_norm": 0.2927853465080261,
      "learning_rate": 1.5760886057100465e-05,
      "loss": 1.1705,
      "step": 698
    },
    {
      "epoch": 6.537577365163572,
      "grad_norm": 0.3006926476955414,
      "learning_rate": 1.574850404810602e-05,
      "loss": 1.1309,
      "step": 699
    },
    {
      "epoch": 6.547008547008547,
      "grad_norm": 0.31431832909584045,
      "learning_rate": 1.57361088620583e-05,
      "loss": 1.1236,
      "step": 700
    },
    {
      "epoch": 6.556439728853522,
      "grad_norm": 0.3172517716884613,
      "learning_rate": 1.572370052737027e-05,
      "loss": 1.1895,
      "step": 701
    },
    {
      "epoch": 6.5658709106984965,
      "grad_norm": 0.30837881565093994,
      "learning_rate": 1.5711279072485023e-05,
      "loss": 1.14,
      "step": 702
    },
    {
      "epoch": 6.575302092543472,
      "grad_norm": 0.2950986325740814,
      "learning_rate": 1.569884452587574e-05,
      "loss": 1.1202,
      "step": 703
    },
    {
      "epoch": 6.584733274388447,
      "grad_norm": 0.29228341579437256,
      "learning_rate": 1.568639691604561e-05,
      "loss": 1.1492,
      "step": 704
    },
    {
      "epoch": 6.594164456233422,
      "grad_norm": 0.3057071566581726,
      "learning_rate": 1.5673936271527756e-05,
      "loss": 1.148,
      "step": 705
    },
    {
      "epoch": 6.603595638078397,
      "grad_norm": 0.3100442588329315,
      "learning_rate": 1.5661462620885198e-05,
      "loss": 1.121,
      "step": 706
    },
    {
      "epoch": 6.6130268199233715,
      "grad_norm": 0.3276217579841614,
      "learning_rate": 1.5648975992710756e-05,
      "loss": 1.1107,
      "step": 707
    },
    {
      "epoch": 6.622458001768346,
      "grad_norm": 0.288504034280777,
      "learning_rate": 1.5636476415626995e-05,
      "loss": 1.1389,
      "step": 708
    },
    {
      "epoch": 6.631889183613321,
      "grad_norm": 0.3096160590648651,
      "learning_rate": 1.5623963918286172e-05,
      "loss": 1.1008,
      "step": 709
    },
    {
      "epoch": 6.641320365458297,
      "grad_norm": 0.3110659718513489,
      "learning_rate": 1.561143852937016e-05,
      "loss": 1.1515,
      "step": 710
    },
    {
      "epoch": 6.650751547303272,
      "grad_norm": 0.2973947525024414,
      "learning_rate": 1.5598900277590373e-05,
      "loss": 1.1356,
      "step": 711
    },
    {
      "epoch": 6.6601827291482465,
      "grad_norm": 0.3222200572490692,
      "learning_rate": 1.558634919168772e-05,
      "loss": 1.1063,
      "step": 712
    },
    {
      "epoch": 6.669613910993221,
      "grad_norm": 0.32516705989837646,
      "learning_rate": 1.557378530043253e-05,
      "loss": 1.1257,
      "step": 713
    },
    {
      "epoch": 6.679045092838196,
      "grad_norm": 0.29380515217781067,
      "learning_rate": 1.556120863262447e-05,
      "loss": 1.1713,
      "step": 714
    },
    {
      "epoch": 6.688476274683171,
      "grad_norm": 0.3099794387817383,
      "learning_rate": 1.5548619217092516e-05,
      "loss": 1.1552,
      "step": 715
    },
    {
      "epoch": 6.697907456528146,
      "grad_norm": 0.2918339967727661,
      "learning_rate": 1.5536017082694846e-05,
      "loss": 1.0951,
      "step": 716
    },
    {
      "epoch": 6.7073386383731215,
      "grad_norm": 0.26660555601119995,
      "learning_rate": 1.5523402258318804e-05,
      "loss": 1.1321,
      "step": 717
    },
    {
      "epoch": 6.716769820218096,
      "grad_norm": 0.30522090196609497,
      "learning_rate": 1.5510774772880823e-05,
      "loss": 1.121,
      "step": 718
    },
    {
      "epoch": 6.726201002063071,
      "grad_norm": 0.3480081558227539,
      "learning_rate": 1.549813465532635e-05,
      "loss": 1.1514,
      "step": 719
    },
    {
      "epoch": 6.735632183908046,
      "grad_norm": 0.32115572690963745,
      "learning_rate": 1.54854819346298e-05,
      "loss": 1.1571,
      "step": 720
    },
    {
      "epoch": 6.745063365753021,
      "grad_norm": 0.31539249420166016,
      "learning_rate": 1.547281663979446e-05,
      "loss": 1.1239,
      "step": 721
    },
    {
      "epoch": 6.754494547597996,
      "grad_norm": 0.34376218914985657,
      "learning_rate": 1.546013879985246e-05,
      "loss": 1.1204,
      "step": 722
    },
    {
      "epoch": 6.76392572944297,
      "grad_norm": 0.3014904856681824,
      "learning_rate": 1.5447448443864673e-05,
      "loss": 1.1458,
      "step": 723
    },
    {
      "epoch": 6.773356911287946,
      "grad_norm": 0.3024449646472931,
      "learning_rate": 1.543474560092067e-05,
      "loss": 1.0826,
      "step": 724
    },
    {
      "epoch": 6.782788093132921,
      "grad_norm": 0.30013903975486755,
      "learning_rate": 1.5422030300138638e-05,
      "loss": 1.1694,
      "step": 725
    },
    {
      "epoch": 6.792219274977896,
      "grad_norm": 0.31281498074531555,
      "learning_rate": 1.5409302570665325e-05,
      "loss": 1.144,
      "step": 726
    },
    {
      "epoch": 6.801650456822871,
      "grad_norm": 0.2955462634563446,
      "learning_rate": 1.539656244167597e-05,
      "loss": 1.1314,
      "step": 727
    },
    {
      "epoch": 6.811081638667845,
      "grad_norm": 0.30317774415016174,
      "learning_rate": 1.5383809942374235e-05,
      "loss": 1.1459,
      "step": 728
    },
    {
      "epoch": 6.82051282051282,
      "grad_norm": 0.2920949459075928,
      "learning_rate": 1.537104510199213e-05,
      "loss": 1.143,
      "step": 729
    },
    {
      "epoch": 6.829944002357795,
      "grad_norm": 0.3022417426109314,
      "learning_rate": 1.5358267949789968e-05,
      "loss": 1.1576,
      "step": 730
    },
    {
      "epoch": 6.839375184202771,
      "grad_norm": 0.28597399592399597,
      "learning_rate": 1.534547851505627e-05,
      "loss": 1.0884,
      "step": 731
    },
    {
      "epoch": 6.848806366047746,
      "grad_norm": 0.3085551857948303,
      "learning_rate": 1.533267682710772e-05,
      "loss": 1.1718,
      "step": 732
    },
    {
      "epoch": 6.85823754789272,
      "grad_norm": 0.323281854391098,
      "learning_rate": 1.5319862915289086e-05,
      "loss": 1.0932,
      "step": 733
    },
    {
      "epoch": 6.867668729737695,
      "grad_norm": 0.30516210198402405,
      "learning_rate": 1.5307036808973154e-05,
      "loss": 1.0907,
      "step": 734
    },
    {
      "epoch": 6.87709991158267,
      "grad_norm": 0.3057391047477722,
      "learning_rate": 1.5294198537560676e-05,
      "loss": 1.1273,
      "step": 735
    },
    {
      "epoch": 6.886531093427645,
      "grad_norm": 0.29696616530418396,
      "learning_rate": 1.5281348130480272e-05,
      "loss": 1.1254,
      "step": 736
    },
    {
      "epoch": 6.89596227527262,
      "grad_norm": 0.3119654953479767,
      "learning_rate": 1.526848561718839e-05,
      "loss": 1.0965,
      "step": 737
    },
    {
      "epoch": 6.9053934571175954,
      "grad_norm": 0.3078770637512207,
      "learning_rate": 1.5255611027169224e-05,
      "loss": 1.1239,
      "step": 738
    },
    {
      "epoch": 6.91482463896257,
      "grad_norm": 0.3017534911632538,
      "learning_rate": 1.524272438993466e-05,
      "loss": 1.1689,
      "step": 739
    },
    {
      "epoch": 6.924255820807545,
      "grad_norm": 0.3093677759170532,
      "learning_rate": 1.5229825735024186e-05,
      "loss": 1.1102,
      "step": 740
    },
    {
      "epoch": 6.93368700265252,
      "grad_norm": 0.32631996273994446,
      "learning_rate": 1.5216915092004847e-05,
      "loss": 1.0805,
      "step": 741
    },
    {
      "epoch": 6.943118184497495,
      "grad_norm": 0.310660183429718,
      "learning_rate": 1.5203992490471167e-05,
      "loss": 1.1307,
      "step": 742
    },
    {
      "epoch": 6.95254936634247,
      "grad_norm": 0.31139808893203735,
      "learning_rate": 1.5191057960045077e-05,
      "loss": 1.1457,
      "step": 743
    },
    {
      "epoch": 6.961980548187444,
      "grad_norm": 0.30085116624832153,
      "learning_rate": 1.5178111530375856e-05,
      "loss": 1.1654,
      "step": 744
    },
    {
      "epoch": 6.97141173003242,
      "grad_norm": 0.30068907141685486,
      "learning_rate": 1.516515323114006e-05,
      "loss": 1.1262,
      "step": 745
    },
    {
      "epoch": 6.980842911877395,
      "grad_norm": 0.32179760932922363,
      "learning_rate": 1.515218309204145e-05,
      "loss": 1.119,
      "step": 746
    },
    {
      "epoch": 6.99027409372237,
      "grad_norm": 0.3094348609447479,
      "learning_rate": 1.5139201142810932e-05,
      "loss": 1.1401,
      "step": 747
    },
    {
      "epoch": 6.999705275567345,
      "grad_norm": 0.3243134617805481,
      "learning_rate": 1.5126207413206477e-05,
      "loss": 1.1559,
      "step": 748
    },
    {
      "epoch": 7.0,
      "grad_norm": 1.7711217403411865,
      "learning_rate": 1.5113201933013067e-05,
      "loss": 1.0118,
      "step": 749
    },
    {
      "epoch": 7.009431181844975,
      "grad_norm": 0.30755025148391724,
      "learning_rate": 1.5100184732042608e-05,
      "loss": 1.1232,
      "step": 750
    },
    {
      "epoch": 7.01886236368995,
      "grad_norm": 0.30566471815109253,
      "learning_rate": 1.508715584013389e-05,
      "loss": 1.1302,
      "step": 751
    },
    {
      "epoch": 7.0282935455349245,
      "grad_norm": 0.3090786039829254,
      "learning_rate": 1.5074115287152485e-05,
      "loss": 1.1245,
      "step": 752
    },
    {
      "epoch": 7.0377247273799,
      "grad_norm": 0.29414862394332886,
      "learning_rate": 1.5061063102990703e-05,
      "loss": 1.1647,
      "step": 753
    },
    {
      "epoch": 7.047155909224875,
      "grad_norm": 0.31586799025535583,
      "learning_rate": 1.5047999317567517e-05,
      "loss": 1.1232,
      "step": 754
    },
    {
      "epoch": 7.05658709106985,
      "grad_norm": 0.3253348469734192,
      "learning_rate": 1.5034923960828493e-05,
      "loss": 1.1197,
      "step": 755
    },
    {
      "epoch": 7.066018272914825,
      "grad_norm": 0.309334397315979,
      "learning_rate": 1.5021837062745714e-05,
      "loss": 1.1681,
      "step": 756
    },
    {
      "epoch": 7.0754494547597995,
      "grad_norm": 0.28892916440963745,
      "learning_rate": 1.500873865331773e-05,
      "loss": 1.1433,
      "step": 757
    },
    {
      "epoch": 7.084880636604774,
      "grad_norm": 0.3169523775577545,
      "learning_rate": 1.499562876256947e-05,
      "loss": 1.1239,
      "step": 758
    },
    {
      "epoch": 7.094311818449749,
      "grad_norm": 0.2976958155632019,
      "learning_rate": 1.4982507420552184e-05,
      "loss": 1.1375,
      "step": 759
    },
    {
      "epoch": 7.103743000294725,
      "grad_norm": 0.31988629698753357,
      "learning_rate": 1.4969374657343368e-05,
      "loss": 1.1647,
      "step": 760
    },
    {
      "epoch": 7.1131741821397,
      "grad_norm": 0.2924523949623108,
      "learning_rate": 1.4956230503046702e-05,
      "loss": 1.1259,
      "step": 761
    },
    {
      "epoch": 7.1226053639846745,
      "grad_norm": 0.3376644253730774,
      "learning_rate": 1.4943074987791981e-05,
      "loss": 1.0785,
      "step": 762
    },
    {
      "epoch": 7.132036545829649,
      "grad_norm": 0.28261515498161316,
      "learning_rate": 1.4929908141735032e-05,
      "loss": 1.1241,
      "step": 763
    },
    {
      "epoch": 7.141467727674624,
      "grad_norm": 0.32154542207717896,
      "learning_rate": 1.4916729995057662e-05,
      "loss": 1.1303,
      "step": 764
    },
    {
      "epoch": 7.150898909519599,
      "grad_norm": 0.31712114810943604,
      "learning_rate": 1.4903540577967582e-05,
      "loss": 1.1228,
      "step": 765
    },
    {
      "epoch": 7.160330091364574,
      "grad_norm": 0.3053187131881714,
      "learning_rate": 1.4890339920698334e-05,
      "loss": 1.1488,
      "step": 766
    },
    {
      "epoch": 7.1697612732095495,
      "grad_norm": 0.3475249111652374,
      "learning_rate": 1.4877128053509231e-05,
      "loss": 1.1173,
      "step": 767
    },
    {
      "epoch": 7.179192455054524,
      "grad_norm": 0.3249098062515259,
      "learning_rate": 1.4863905006685276e-05,
      "loss": 1.1348,
      "step": 768
    },
    {
      "epoch": 7.188623636899499,
      "grad_norm": 0.294291615486145,
      "learning_rate": 1.48506708105371e-05,
      "loss": 1.1773,
      "step": 769
    },
    {
      "epoch": 7.198054818744474,
      "grad_norm": 0.32000651955604553,
      "learning_rate": 1.4837425495400895e-05,
      "loss": 1.165,
      "step": 770
    },
    {
      "epoch": 7.207486000589449,
      "grad_norm": 0.2962455749511719,
      "learning_rate": 1.4824169091638338e-05,
      "loss": 1.1122,
      "step": 771
    },
    {
      "epoch": 7.216917182434424,
      "grad_norm": 0.2926581799983978,
      "learning_rate": 1.4810901629636526e-05,
      "loss": 1.1011,
      "step": 772
    },
    {
      "epoch": 7.226348364279398,
      "grad_norm": 0.3025626540184021,
      "learning_rate": 1.4797623139807896e-05,
      "loss": 1.1639,
      "step": 773
    },
    {
      "epoch": 7.235779546124374,
      "grad_norm": 0.3571471869945526,
      "learning_rate": 1.4784333652590177e-05,
      "loss": 1.1104,
      "step": 774
    },
    {
      "epoch": 7.245210727969349,
      "grad_norm": 0.3071790039539337,
      "learning_rate": 1.4771033198446292e-05,
      "loss": 1.1424,
      "step": 775
    },
    {
      "epoch": 7.254641909814324,
      "grad_norm": 0.3418292999267578,
      "learning_rate": 1.4757721807864318e-05,
      "loss": 1.1067,
      "step": 776
    },
    {
      "epoch": 7.264073091659299,
      "grad_norm": 0.3344862759113312,
      "learning_rate": 1.474439951135739e-05,
      "loss": 1.1247,
      "step": 777
    },
    {
      "epoch": 7.273504273504273,
      "grad_norm": 0.3085480332374573,
      "learning_rate": 1.4731066339463652e-05,
      "loss": 1.1239,
      "step": 778
    },
    {
      "epoch": 7.282935455349248,
      "grad_norm": 0.29980477690696716,
      "learning_rate": 1.4717722322746161e-05,
      "loss": 1.1315,
      "step": 779
    },
    {
      "epoch": 7.292366637194223,
      "grad_norm": 0.2933621406555176,
      "learning_rate": 1.4704367491792854e-05,
      "loss": 1.1426,
      "step": 780
    },
    {
      "epoch": 7.301797819039198,
      "grad_norm": 0.3144397735595703,
      "learning_rate": 1.469100187721644e-05,
      "loss": 1.1316,
      "step": 781
    },
    {
      "epoch": 7.311229000884174,
      "grad_norm": 0.35189858078956604,
      "learning_rate": 1.4677625509654354e-05,
      "loss": 1.1383,
      "step": 782
    },
    {
      "epoch": 7.320660182729148,
      "grad_norm": 0.3083226978778839,
      "learning_rate": 1.4664238419768681e-05,
      "loss": 1.1234,
      "step": 783
    },
    {
      "epoch": 7.330091364574123,
      "grad_norm": 0.3293572664260864,
      "learning_rate": 1.4650840638246082e-05,
      "loss": 1.1447,
      "step": 784
    },
    {
      "epoch": 7.339522546419098,
      "grad_norm": 0.31161460280418396,
      "learning_rate": 1.4637432195797724e-05,
      "loss": 1.1661,
      "step": 785
    },
    {
      "epoch": 7.348953728264073,
      "grad_norm": 0.32495802640914917,
      "learning_rate": 1.462401312315922e-05,
      "loss": 1.1291,
      "step": 786
    },
    {
      "epoch": 7.358384910109048,
      "grad_norm": 0.349248468875885,
      "learning_rate": 1.4610583451090536e-05,
      "loss": 1.0936,
      "step": 787
    },
    {
      "epoch": 7.3678160919540225,
      "grad_norm": 0.3241685628890991,
      "learning_rate": 1.4597143210375946e-05,
      "loss": 1.1349,
      "step": 788
    },
    {
      "epoch": 7.377247273798998,
      "grad_norm": 0.3153645694255829,
      "learning_rate": 1.4583692431823946e-05,
      "loss": 1.0839,
      "step": 789
    },
    {
      "epoch": 7.386678455643973,
      "grad_norm": 0.34612762928009033,
      "learning_rate": 1.4570231146267192e-05,
      "loss": 1.1164,
      "step": 790
    },
    {
      "epoch": 7.396109637488948,
      "grad_norm": 0.32165002822875977,
      "learning_rate": 1.4556759384562418e-05,
      "loss": 1.1405,
      "step": 791
    },
    {
      "epoch": 7.405540819333923,
      "grad_norm": 0.31043267250061035,
      "learning_rate": 1.4543277177590372e-05,
      "loss": 1.0944,
      "step": 792
    },
    {
      "epoch": 7.4149720011788975,
      "grad_norm": 0.34371495246887207,
      "learning_rate": 1.4529784556255751e-05,
      "loss": 1.1138,
      "step": 793
    },
    {
      "epoch": 7.424403183023872,
      "grad_norm": 0.315744549036026,
      "learning_rate": 1.4516281551487126e-05,
      "loss": 1.0568,
      "step": 794
    },
    {
      "epoch": 7.433834364868847,
      "grad_norm": 0.32337188720703125,
      "learning_rate": 1.4502768194236857e-05,
      "loss": 1.1118,
      "step": 795
    },
    {
      "epoch": 7.443265546713823,
      "grad_norm": 0.318324476480484,
      "learning_rate": 1.4489244515481047e-05,
      "loss": 1.1173,
      "step": 796
    },
    {
      "epoch": 7.452696728558798,
      "grad_norm": 0.32043954730033875,
      "learning_rate": 1.4475710546219452e-05,
      "loss": 1.1774,
      "step": 797
    },
    {
      "epoch": 7.4621279104037725,
      "grad_norm": 0.32095867395401,
      "learning_rate": 1.4462166317475425e-05,
      "loss": 1.1283,
      "step": 798
    },
    {
      "epoch": 7.471559092248747,
      "grad_norm": 0.30349722504615784,
      "learning_rate": 1.444861186029582e-05,
      "loss": 1.0861,
      "step": 799
    },
    {
      "epoch": 7.480990274093722,
      "grad_norm": 0.34250253438949585,
      "learning_rate": 1.4435047205750956e-05,
      "loss": 1.1086,
      "step": 800
    },
    {
      "epoch": 7.490421455938697,
      "grad_norm": 0.3199852406978607,
      "learning_rate": 1.4421472384934512e-05,
      "loss": 1.1479,
      "step": 801
    },
    {
      "epoch": 7.499852637783672,
      "grad_norm": 0.31055569648742676,
      "learning_rate": 1.4407887428963482e-05,
      "loss": 1.1038,
      "step": 802
    },
    {
      "epoch": 7.5092838196286475,
      "grad_norm": 0.2928882837295532,
      "learning_rate": 1.4394292368978084e-05,
      "loss": 1.0909,
      "step": 803
    },
    {
      "epoch": 7.518715001473622,
      "grad_norm": 0.32296305894851685,
      "learning_rate": 1.4380687236141704e-05,
      "loss": 1.1543,
      "step": 804
    },
    {
      "epoch": 7.528146183318597,
      "grad_norm": 0.30949264764785767,
      "learning_rate": 1.4367072061640807e-05,
      "loss": 1.1238,
      "step": 805
    },
    {
      "epoch": 7.537577365163572,
      "grad_norm": 0.30924588441848755,
      "learning_rate": 1.435344687668489e-05,
      "loss": 1.1198,
      "step": 806
    },
    {
      "epoch": 7.547008547008547,
      "grad_norm": 0.31129103899002075,
      "learning_rate": 1.4339811712506388e-05,
      "loss": 1.1114,
      "step": 807
    },
    {
      "epoch": 7.556439728853522,
      "grad_norm": 0.32940229773521423,
      "learning_rate": 1.432616660036061e-05,
      "loss": 1.1323,
      "step": 808
    },
    {
      "epoch": 7.5658709106984965,
      "grad_norm": 0.2989010214805603,
      "learning_rate": 1.4312511571525675e-05,
      "loss": 1.1407,
      "step": 809
    },
    {
      "epoch": 7.575302092543472,
      "grad_norm": 0.34893476963043213,
      "learning_rate": 1.4298846657302426e-05,
      "loss": 1.131,
      "step": 810
    },
    {
      "epoch": 7.584733274388447,
      "grad_norm": 0.3084239363670349,
      "learning_rate": 1.4285171889014369e-05,
      "loss": 1.1879,
      "step": 811
    },
    {
      "epoch": 7.594164456233422,
      "grad_norm": 0.2932817041873932,
      "learning_rate": 1.4271487298007599e-05,
      "loss": 1.1441,
      "step": 812
    },
    {
      "epoch": 7.603595638078397,
      "grad_norm": 0.3392055630683899,
      "learning_rate": 1.4257792915650728e-05,
      "loss": 1.1056,
      "step": 813
    },
    {
      "epoch": 7.6130268199233715,
      "grad_norm": 0.31714126467704773,
      "learning_rate": 1.4244088773334811e-05,
      "loss": 1.1307,
      "step": 814
    },
    {
      "epoch": 7.622458001768346,
      "grad_norm": 0.294496089220047,
      "learning_rate": 1.4230374902473272e-05,
      "loss": 1.1545,
      "step": 815
    },
    {
      "epoch": 7.631889183613321,
      "grad_norm": 0.30353477597236633,
      "learning_rate": 1.421665133450184e-05,
      "loss": 1.1292,
      "step": 816
    },
    {
      "epoch": 7.641320365458297,
      "grad_norm": 0.33766108751296997,
      "learning_rate": 1.4202918100878473e-05,
      "loss": 1.1484,
      "step": 817
    },
    {
      "epoch": 7.650751547303272,
      "grad_norm": 0.32262611389160156,
      "learning_rate": 1.4189175233083284e-05,
      "loss": 1.1111,
      "step": 818
    },
    {
      "epoch": 7.6601827291482465,
      "grad_norm": 0.3093430697917938,
      "learning_rate": 1.4175422762618462e-05,
      "loss": 1.2112,
      "step": 819
    },
    {
      "epoch": 7.669613910993221,
      "grad_norm": 0.2993868291378021,
      "learning_rate": 1.4161660721008222e-05,
      "loss": 1.1366,
      "step": 820
    },
    {
      "epoch": 7.679045092838196,
      "grad_norm": 0.30194807052612305,
      "learning_rate": 1.4147889139798709e-05,
      "loss": 1.1201,
      "step": 821
    },
    {
      "epoch": 7.688476274683171,
      "grad_norm": 0.30661603808403015,
      "learning_rate": 1.4134108050557936e-05,
      "loss": 1.0956,
      "step": 822
    },
    {
      "epoch": 7.697907456528146,
      "grad_norm": 0.33047375082969666,
      "learning_rate": 1.4120317484875712e-05,
      "loss": 1.1251,
      "step": 823
    },
    {
      "epoch": 7.7073386383731215,
      "grad_norm": 0.3087385892868042,
      "learning_rate": 1.4106517474363573e-05,
      "loss": 1.0835,
      "step": 824
    },
    {
      "epoch": 7.716769820218096,
      "grad_norm": 0.31843453645706177,
      "learning_rate": 1.4092708050654695e-05,
      "loss": 1.0908,
      "step": 825
    },
    {
      "epoch": 7.726201002063071,
      "grad_norm": 0.3221037685871124,
      "learning_rate": 1.4078889245403844e-05,
      "loss": 1.1282,
      "step": 826
    },
    {
      "epoch": 7.735632183908046,
      "grad_norm": 0.29712826013565063,
      "learning_rate": 1.4065061090287278e-05,
      "loss": 1.1274,
      "step": 827
    },
    {
      "epoch": 7.745063365753021,
      "grad_norm": 0.32145458459854126,
      "learning_rate": 1.4051223617002693e-05,
      "loss": 1.1181,
      "step": 828
    },
    {
      "epoch": 7.754494547597996,
      "grad_norm": 0.3164919912815094,
      "learning_rate": 1.4037376857269154e-05,
      "loss": 1.1011,
      "step": 829
    },
    {
      "epoch": 7.76392572944297,
      "grad_norm": 0.34030210971832275,
      "learning_rate": 1.4023520842826997e-05,
      "loss": 1.091,
      "step": 830
    },
    {
      "epoch": 7.773356911287946,
      "grad_norm": 0.35323190689086914,
      "learning_rate": 1.400965560543778e-05,
      "loss": 1.1025,
      "step": 831
    },
    {
      "epoch": 7.782788093132921,
      "grad_norm": 0.2992219030857086,
      "learning_rate": 1.39957811768842e-05,
      "loss": 1.1631,
      "step": 832
    },
    {
      "epoch": 7.792219274977896,
      "grad_norm": 0.31645816564559937,
      "learning_rate": 1.398189758897003e-05,
      "loss": 1.1289,
      "step": 833
    },
    {
      "epoch": 7.801650456822871,
      "grad_norm": 0.33037301898002625,
      "learning_rate": 1.3968004873520027e-05,
      "loss": 1.154,
      "step": 834
    },
    {
      "epoch": 7.811081638667845,
      "grad_norm": 0.3385067284107208,
      "learning_rate": 1.3954103062379878e-05,
      "loss": 1.1461,
      "step": 835
    },
    {
      "epoch": 7.82051282051282,
      "grad_norm": 0.3187876045703888,
      "learning_rate": 1.394019218741612e-05,
      "loss": 1.1601,
      "step": 836
    },
    {
      "epoch": 7.829944002357795,
      "grad_norm": 0.34470704197883606,
      "learning_rate": 1.3926272280516064e-05,
      "loss": 1.1027,
      "step": 837
    },
    {
      "epoch": 7.839375184202771,
      "grad_norm": 0.2910856008529663,
      "learning_rate": 1.3912343373587725e-05,
      "loss": 1.1569,
      "step": 838
    },
    {
      "epoch": 7.848806366047746,
      "grad_norm": 0.31687864661216736,
      "learning_rate": 1.3898405498559747e-05,
      "loss": 1.1305,
      "step": 839
    },
    {
      "epoch": 7.85823754789272,
      "grad_norm": 0.2952074706554413,
      "learning_rate": 1.3884458687381335e-05,
      "loss": 1.1176,
      "step": 840
    },
    {
      "epoch": 7.867668729737695,
      "grad_norm": 0.3394898474216461,
      "learning_rate": 1.3870502972022175e-05,
      "loss": 1.1077,
      "step": 841
    },
    {
      "epoch": 7.87709991158267,
      "grad_norm": 0.33116284012794495,
      "learning_rate": 1.3856538384472362e-05,
      "loss": 1.183,
      "step": 842
    },
    {
      "epoch": 7.886531093427645,
      "grad_norm": 0.3149014711380005,
      "learning_rate": 1.3842564956742335e-05,
      "loss": 1.1394,
      "step": 843
    },
    {
      "epoch": 7.89596227527262,
      "grad_norm": 0.3048954904079437,
      "learning_rate": 1.3828582720862791e-05,
      "loss": 1.1119,
      "step": 844
    },
    {
      "epoch": 7.9053934571175954,
      "grad_norm": 0.2944031357765198,
      "learning_rate": 1.381459170888462e-05,
      "loss": 1.1321,
      "step": 845
    },
    {
      "epoch": 7.91482463896257,
      "grad_norm": 0.31281933188438416,
      "learning_rate": 1.3800591952878826e-05,
      "loss": 1.1052,
      "step": 846
    },
    {
      "epoch": 7.924255820807545,
      "grad_norm": 0.2839208245277405,
      "learning_rate": 1.378658348493646e-05,
      "loss": 1.0998,
      "step": 847
    },
    {
      "epoch": 7.93368700265252,
      "grad_norm": 0.33669978380203247,
      "learning_rate": 1.3772566337168545e-05,
      "loss": 1.0917,
      "step": 848
    },
    {
      "epoch": 7.943118184497495,
      "grad_norm": 0.301474392414093,
      "learning_rate": 1.3758540541705996e-05,
      "loss": 1.1381,
      "step": 849
    },
    {
      "epoch": 7.95254936634247,
      "grad_norm": 0.31635749340057373,
      "learning_rate": 1.3744506130699549e-05,
      "loss": 1.1527,
      "step": 850
    },
    {
      "epoch": 7.961980548187444,
      "grad_norm": 0.3256700336933136,
      "learning_rate": 1.3730463136319694e-05,
      "loss": 1.1451,
      "step": 851
    },
    {
      "epoch": 7.97141173003242,
      "grad_norm": 0.29096299409866333,
      "learning_rate": 1.3716411590756595e-05,
      "loss": 1.1542,
      "step": 852
    },
    {
      "epoch": 7.980842911877395,
      "grad_norm": 0.28307536244392395,
      "learning_rate": 1.3702351526220019e-05,
      "loss": 1.1508,
      "step": 853
    },
    {
      "epoch": 7.99027409372237,
      "grad_norm": 0.3249085247516632,
      "learning_rate": 1.3688282974939257e-05,
      "loss": 1.0849,
      "step": 854
    },
    {
      "epoch": 7.999705275567345,
      "grad_norm": 0.31628209352493286,
      "learning_rate": 1.3674205969163054e-05,
      "loss": 1.0978,
      "step": 855
    },
    {
      "epoch": 8.0,
      "grad_norm": 1.3935823440551758,
      "learning_rate": 1.3660120541159536e-05,
      "loss": 0.8308,
      "step": 856
    },
    {
      "epoch": 8.009431181844976,
      "grad_norm": 0.3700622320175171,
      "learning_rate": 1.3646026723216142e-05,
      "loss": 1.1171,
      "step": 857
    },
    {
      "epoch": 8.01886236368995,
      "grad_norm": 0.3159230053424835,
      "learning_rate": 1.3631924547639526e-05,
      "loss": 1.1008,
      "step": 858
    },
    {
      "epoch": 8.028293545534925,
      "grad_norm": 0.29923680424690247,
      "learning_rate": 1.3617814046755516e-05,
      "loss": 1.1589,
      "step": 859
    },
    {
      "epoch": 8.0377247273799,
      "grad_norm": 0.3108072578907013,
      "learning_rate": 1.3603695252909015e-05,
      "loss": 1.1215,
      "step": 860
    },
    {
      "epoch": 8.047155909224875,
      "grad_norm": 0.2886561155319214,
      "learning_rate": 1.3589568198463945e-05,
      "loss": 1.1268,
      "step": 861
    },
    {
      "epoch": 8.056587091069849,
      "grad_norm": 0.3144734799861908,
      "learning_rate": 1.3575432915803149e-05,
      "loss": 1.118,
      "step": 862
    },
    {
      "epoch": 8.066018272914825,
      "grad_norm": 0.309955358505249,
      "learning_rate": 1.356128943732834e-05,
      "loss": 1.095,
      "step": 863
    },
    {
      "epoch": 8.0754494547598,
      "grad_norm": 0.311998188495636,
      "learning_rate": 1.3547137795460019e-05,
      "loss": 1.1336,
      "step": 864
    },
    {
      "epoch": 8.084880636604774,
      "grad_norm": 0.3382350206375122,
      "learning_rate": 1.3532978022637402e-05,
      "loss": 1.1412,
      "step": 865
    },
    {
      "epoch": 8.09431181844975,
      "grad_norm": 0.2958736717700958,
      "learning_rate": 1.351881015131833e-05,
      "loss": 1.1202,
      "step": 866
    },
    {
      "epoch": 8.103743000294724,
      "grad_norm": 0.3414778709411621,
      "learning_rate": 1.3504634213979224e-05,
      "loss": 1.1469,
      "step": 867
    },
    {
      "epoch": 8.1131741821397,
      "grad_norm": 0.3115440309047699,
      "learning_rate": 1.3490450243114985e-05,
      "loss": 1.16,
      "step": 868
    },
    {
      "epoch": 8.122605363984674,
      "grad_norm": 0.3560987114906311,
      "learning_rate": 1.3476258271238928e-05,
      "loss": 1.1088,
      "step": 869
    },
    {
      "epoch": 8.13203654582965,
      "grad_norm": 0.29825031757354736,
      "learning_rate": 1.3462058330882714e-05,
      "loss": 1.1162,
      "step": 870
    },
    {
      "epoch": 8.141467727674625,
      "grad_norm": 0.3313814699649811,
      "learning_rate": 1.3447850454596266e-05,
      "loss": 1.1489,
      "step": 871
    },
    {
      "epoch": 8.150898909519599,
      "grad_norm": 0.3192501664161682,
      "learning_rate": 1.3433634674947698e-05,
      "loss": 1.1605,
      "step": 872
    },
    {
      "epoch": 8.160330091364575,
      "grad_norm": 0.34379133582115173,
      "learning_rate": 1.3419411024523246e-05,
      "loss": 1.0983,
      "step": 873
    },
    {
      "epoch": 8.169761273209549,
      "grad_norm": 0.30814576148986816,
      "learning_rate": 1.340517953592718e-05,
      "loss": 1.1248,
      "step": 874
    },
    {
      "epoch": 8.179192455054524,
      "grad_norm": 0.30358338356018066,
      "learning_rate": 1.339094024178174e-05,
      "loss": 1.1114,
      "step": 875
    },
    {
      "epoch": 8.188623636899498,
      "grad_norm": 0.31132614612579346,
      "learning_rate": 1.3376693174727065e-05,
      "loss": 1.1401,
      "step": 876
    },
    {
      "epoch": 8.198054818744474,
      "grad_norm": 0.3118687570095062,
      "learning_rate": 1.33624383674211e-05,
      "loss": 1.143,
      "step": 877
    },
    {
      "epoch": 8.20748600058945,
      "grad_norm": 0.3168022036552429,
      "learning_rate": 1.334817585253954e-05,
      "loss": 1.1012,
      "step": 878
    },
    {
      "epoch": 8.216917182434424,
      "grad_norm": 0.3403538167476654,
      "learning_rate": 1.3333905662775746e-05,
      "loss": 1.1547,
      "step": 879
    },
    {
      "epoch": 8.2263483642794,
      "grad_norm": 0.29393312335014343,
      "learning_rate": 1.3319627830840673e-05,
      "loss": 1.0906,
      "step": 880
    },
    {
      "epoch": 8.235779546124373,
      "grad_norm": 0.3139776587486267,
      "learning_rate": 1.3305342389462792e-05,
      "loss": 1.1168,
      "step": 881
    },
    {
      "epoch": 8.245210727969349,
      "grad_norm": 0.34231069684028625,
      "learning_rate": 1.3291049371388016e-05,
      "loss": 1.1094,
      "step": 882
    },
    {
      "epoch": 8.254641909814323,
      "grad_norm": 0.33456310629844666,
      "learning_rate": 1.3276748809379631e-05,
      "loss": 1.0941,
      "step": 883
    },
    {
      "epoch": 8.264073091659299,
      "grad_norm": 0.3372575640678406,
      "learning_rate": 1.3262440736218207e-05,
      "loss": 1.1186,
      "step": 884
    },
    {
      "epoch": 8.273504273504274,
      "grad_norm": 0.30822405219078064,
      "learning_rate": 1.3248125184701544e-05,
      "loss": 1.1444,
      "step": 885
    },
    {
      "epoch": 8.282935455349248,
      "grad_norm": 0.30384281277656555,
      "learning_rate": 1.3233802187644566e-05,
      "loss": 1.1339,
      "step": 886
    },
    {
      "epoch": 8.292366637194224,
      "grad_norm": 0.3312370181083679,
      "learning_rate": 1.3219471777879282e-05,
      "loss": 1.1362,
      "step": 887
    },
    {
      "epoch": 8.301797819039198,
      "grad_norm": 0.34696099162101746,
      "learning_rate": 1.3205133988254685e-05,
      "loss": 1.1398,
      "step": 888
    },
    {
      "epoch": 8.311229000884174,
      "grad_norm": 0.3166313171386719,
      "learning_rate": 1.3190788851636684e-05,
      "loss": 1.0879,
      "step": 889
    },
    {
      "epoch": 8.320660182729148,
      "grad_norm": 0.32414036989212036,
      "learning_rate": 1.317643640090803e-05,
      "loss": 1.1132,
      "step": 890
    },
    {
      "epoch": 8.330091364574123,
      "grad_norm": 0.34643828868865967,
      "learning_rate": 1.316207666896824e-05,
      "loss": 1.0937,
      "step": 891
    },
    {
      "epoch": 8.339522546419099,
      "grad_norm": 0.33685046434402466,
      "learning_rate": 1.3147709688733525e-05,
      "loss": 1.1327,
      "step": 892
    },
    {
      "epoch": 8.348953728264073,
      "grad_norm": 0.2864835560321808,
      "learning_rate": 1.3133335493136703e-05,
      "loss": 1.1034,
      "step": 893
    },
    {
      "epoch": 8.358384910109049,
      "grad_norm": 0.3263052701950073,
      "learning_rate": 1.3118954115127138e-05,
      "loss": 1.1176,
      "step": 894
    },
    {
      "epoch": 8.367816091954023,
      "grad_norm": 0.3057129979133606,
      "learning_rate": 1.3104565587670657e-05,
      "loss": 1.1382,
      "step": 895
    },
    {
      "epoch": 8.377247273798998,
      "grad_norm": 0.32304486632347107,
      "learning_rate": 1.3090169943749475e-05,
      "loss": 1.11,
      "step": 896
    },
    {
      "epoch": 8.386678455643972,
      "grad_norm": 0.3405880630016327,
      "learning_rate": 1.3075767216362116e-05,
      "loss": 1.082,
      "step": 897
    },
    {
      "epoch": 8.396109637488948,
      "grad_norm": 0.32009947299957275,
      "learning_rate": 1.3061357438523347e-05,
      "loss": 1.1474,
      "step": 898
    },
    {
      "epoch": 8.405540819333924,
      "grad_norm": 0.3187146782875061,
      "learning_rate": 1.3046940643264093e-05,
      "loss": 1.1597,
      "step": 899
    },
    {
      "epoch": 8.414972001178898,
      "grad_norm": 0.3320246636867523,
      "learning_rate": 1.3032516863631366e-05,
      "loss": 1.0884,
      "step": 900
    },
    {
      "epoch": 8.424403183023873,
      "grad_norm": 0.33271992206573486,
      "learning_rate": 1.3018086132688184e-05,
      "loss": 1.1383,
      "step": 901
    },
    {
      "epoch": 8.433834364868847,
      "grad_norm": 0.31534168124198914,
      "learning_rate": 1.3003648483513506e-05,
      "loss": 1.1306,
      "step": 902
    },
    {
      "epoch": 8.443265546713823,
      "grad_norm": 0.3156282603740692,
      "learning_rate": 1.2989203949202145e-05,
      "loss": 1.0788,
      "step": 903
    },
    {
      "epoch": 8.452696728558797,
      "grad_norm": 0.3175128102302551,
      "learning_rate": 1.2974752562864698e-05,
      "loss": 1.1393,
      "step": 904
    },
    {
      "epoch": 8.462127910403773,
      "grad_norm": 0.31417903304100037,
      "learning_rate": 1.296029435762747e-05,
      "loss": 1.1423,
      "step": 905
    },
    {
      "epoch": 8.471559092248748,
      "grad_norm": 0.2996199131011963,
      "learning_rate": 1.294582936663239e-05,
      "loss": 1.1115,
      "step": 906
    },
    {
      "epoch": 8.480990274093722,
      "grad_norm": 0.303602397441864,
      "learning_rate": 1.2931357623036948e-05,
      "loss": 1.1346,
      "step": 907
    },
    {
      "epoch": 8.490421455938698,
      "grad_norm": 0.3087582290172577,
      "learning_rate": 1.2916879160014118e-05,
      "loss": 1.1241,
      "step": 908
    },
    {
      "epoch": 8.499852637783672,
      "grad_norm": 0.3494457006454468,
      "learning_rate": 1.2902394010752262e-05,
      "loss": 1.1478,
      "step": 909
    },
    {
      "epoch": 8.509283819628648,
      "grad_norm": 0.33505550026893616,
      "learning_rate": 1.2887902208455077e-05,
      "loss": 1.1091,
      "step": 910
    },
    {
      "epoch": 8.518715001473621,
      "grad_norm": 0.3487412929534912,
      "learning_rate": 1.2873403786341515e-05,
      "loss": 1.1179,
      "step": 911
    },
    {
      "epoch": 8.528146183318597,
      "grad_norm": 0.33286789059638977,
      "learning_rate": 1.2858898777645691e-05,
      "loss": 1.1582,
      "step": 912
    },
    {
      "epoch": 8.537577365163571,
      "grad_norm": 0.3132342994213104,
      "learning_rate": 1.284438721561683e-05,
      "loss": 1.118,
      "step": 913
    },
    {
      "epoch": 8.547008547008547,
      "grad_norm": 0.30831632018089294,
      "learning_rate": 1.2829869133519168e-05,
      "loss": 1.1553,
      "step": 914
    },
    {
      "epoch": 8.556439728853523,
      "grad_norm": 0.32725435495376587,
      "learning_rate": 1.2815344564631898e-05,
      "loss": 1.1318,
      "step": 915
    },
    {
      "epoch": 8.565870910698496,
      "grad_norm": 0.33045023679733276,
      "learning_rate": 1.2800813542249073e-05,
      "loss": 1.1233,
      "step": 916
    },
    {
      "epoch": 8.575302092543472,
      "grad_norm": 0.32784906029701233,
      "learning_rate": 1.278627609967954e-05,
      "loss": 1.1214,
      "step": 917
    },
    {
      "epoch": 8.584733274388446,
      "grad_norm": 0.3450217843055725,
      "learning_rate": 1.2771732270246869e-05,
      "loss": 1.1854,
      "step": 918
    },
    {
      "epoch": 8.594164456233422,
      "grad_norm": 0.3079494833946228,
      "learning_rate": 1.2757182087289266e-05,
      "loss": 1.0875,
      "step": 919
    },
    {
      "epoch": 8.603595638078396,
      "grad_norm": 0.3279825448989868,
      "learning_rate": 1.27426255841595e-05,
      "loss": 1.1359,
      "step": 920
    },
    {
      "epoch": 8.613026819923371,
      "grad_norm": 0.3296085298061371,
      "learning_rate": 1.2728062794224831e-05,
      "loss": 1.1033,
      "step": 921
    },
    {
      "epoch": 8.622458001768347,
      "grad_norm": 0.3281014859676361,
      "learning_rate": 1.2713493750866925e-05,
      "loss": 1.1019,
      "step": 922
    },
    {
      "epoch": 8.631889183613321,
      "grad_norm": 0.31352490186691284,
      "learning_rate": 1.269891848748179e-05,
      "loss": 1.1311,
      "step": 923
    },
    {
      "epoch": 8.641320365458297,
      "grad_norm": 0.32895007729530334,
      "learning_rate": 1.2684337037479684e-05,
      "loss": 1.1606,
      "step": 924
    },
    {
      "epoch": 8.65075154730327,
      "grad_norm": 0.29689595103263855,
      "learning_rate": 1.2669749434285048e-05,
      "loss": 1.1192,
      "step": 925
    },
    {
      "epoch": 8.660182729148246,
      "grad_norm": 0.3224624991416931,
      "learning_rate": 1.2655155711336432e-05,
      "loss": 1.0851,
      "step": 926
    },
    {
      "epoch": 8.66961391099322,
      "grad_norm": 0.3289719521999359,
      "learning_rate": 1.2640555902086412e-05,
      "loss": 1.1146,
      "step": 927
    },
    {
      "epoch": 8.679045092838196,
      "grad_norm": 0.3520483672618866,
      "learning_rate": 1.2625950040001513e-05,
      "loss": 1.1298,
      "step": 928
    },
    {
      "epoch": 8.688476274683172,
      "grad_norm": 0.34689801931381226,
      "learning_rate": 1.2611338158562136e-05,
      "loss": 1.1219,
      "step": 929
    },
    {
      "epoch": 8.697907456528146,
      "grad_norm": 0.3399101495742798,
      "learning_rate": 1.2596720291262481e-05,
      "loss": 1.118,
      "step": 930
    },
    {
      "epoch": 8.707338638373121,
      "grad_norm": 0.32971566915512085,
      "learning_rate": 1.2582096471610467e-05,
      "loss": 1.1412,
      "step": 931
    },
    {
      "epoch": 8.716769820218095,
      "grad_norm": 0.30935776233673096,
      "learning_rate": 1.2567466733127665e-05,
      "loss": 1.1278,
      "step": 932
    },
    {
      "epoch": 8.726201002063071,
      "grad_norm": 0.3198125958442688,
      "learning_rate": 1.25528311093492e-05,
      "loss": 1.1384,
      "step": 933
    },
    {
      "epoch": 8.735632183908045,
      "grad_norm": 0.3574824035167694,
      "learning_rate": 1.2538189633823693e-05,
      "loss": 1.1728,
      "step": 934
    },
    {
      "epoch": 8.74506336575302,
      "grad_norm": 0.3255365788936615,
      "learning_rate": 1.2523542340113191e-05,
      "loss": 1.1292,
      "step": 935
    },
    {
      "epoch": 8.754494547597997,
      "grad_norm": 0.30571627616882324,
      "learning_rate": 1.2508889261793057e-05,
      "loss": 1.1077,
      "step": 936
    },
    {
      "epoch": 8.76392572944297,
      "grad_norm": 0.3746596574783325,
      "learning_rate": 1.2494230432451931e-05,
      "loss": 1.1273,
      "step": 937
    },
    {
      "epoch": 8.773356911287946,
      "grad_norm": 0.3228898346424103,
      "learning_rate": 1.2479565885691625e-05,
      "loss": 1.1212,
      "step": 938
    },
    {
      "epoch": 8.78278809313292,
      "grad_norm": 0.3359968960285187,
      "learning_rate": 1.2464895655127063e-05,
      "loss": 1.1352,
      "step": 939
    },
    {
      "epoch": 8.792219274977896,
      "grad_norm": 0.32945290207862854,
      "learning_rate": 1.2450219774386194e-05,
      "loss": 1.1066,
      "step": 940
    },
    {
      "epoch": 8.80165045682287,
      "grad_norm": 0.31282103061676025,
      "learning_rate": 1.2435538277109919e-05,
      "loss": 1.1241,
      "step": 941
    },
    {
      "epoch": 8.811081638667845,
      "grad_norm": 0.3513953685760498,
      "learning_rate": 1.2420851196952022e-05,
      "loss": 1.1713,
      "step": 942
    },
    {
      "epoch": 8.820512820512821,
      "grad_norm": 0.3524232804775238,
      "learning_rate": 1.2406158567579068e-05,
      "loss": 1.1444,
      "step": 943
    },
    {
      "epoch": 8.829944002357795,
      "grad_norm": 0.3376666009426117,
      "learning_rate": 1.2391460422670356e-05,
      "loss": 1.0899,
      "step": 944
    },
    {
      "epoch": 8.83937518420277,
      "grad_norm": 0.31375840306282043,
      "learning_rate": 1.2376756795917822e-05,
      "loss": 1.1408,
      "step": 945
    },
    {
      "epoch": 8.848806366047745,
      "grad_norm": 0.3012334108352661,
      "learning_rate": 1.236204772102597e-05,
      "loss": 1.1101,
      "step": 946
    },
    {
      "epoch": 8.85823754789272,
      "grad_norm": 0.32867276668548584,
      "learning_rate": 1.2347333231711792e-05,
      "loss": 1.136,
      "step": 947
    },
    {
      "epoch": 8.867668729737694,
      "grad_norm": 0.33668282628059387,
      "learning_rate": 1.2332613361704688e-05,
      "loss": 1.1474,
      "step": 948
    },
    {
      "epoch": 8.87709991158267,
      "grad_norm": 0.30848512053489685,
      "learning_rate": 1.2317888144746397e-05,
      "loss": 1.1407,
      "step": 949
    },
    {
      "epoch": 8.886531093427646,
      "grad_norm": 0.3407306671142578,
      "learning_rate": 1.2303157614590915e-05,
      "loss": 1.1118,
      "step": 950
    },
    {
      "epoch": 8.89596227527262,
      "grad_norm": 0.32290780544281006,
      "learning_rate": 1.2288421805004414e-05,
      "loss": 1.1118,
      "step": 951
    },
    {
      "epoch": 8.905393457117595,
      "grad_norm": 0.3399868607521057,
      "learning_rate": 1.2273680749765165e-05,
      "loss": 1.1314,
      "step": 952
    },
    {
      "epoch": 8.91482463896257,
      "grad_norm": 0.32406553626060486,
      "learning_rate": 1.225893448266347e-05,
      "loss": 1.13,
      "step": 953
    },
    {
      "epoch": 8.924255820807545,
      "grad_norm": 0.3421366512775421,
      "learning_rate": 1.2244183037501571e-05,
      "loss": 1.0908,
      "step": 954
    },
    {
      "epoch": 8.933687002652519,
      "grad_norm": 0.3331695795059204,
      "learning_rate": 1.2229426448093592e-05,
      "loss": 1.167,
      "step": 955
    },
    {
      "epoch": 8.943118184497495,
      "grad_norm": 0.3018210530281067,
      "learning_rate": 1.221466474826543e-05,
      "loss": 1.1353,
      "step": 956
    },
    {
      "epoch": 8.95254936634247,
      "grad_norm": 0.3173897862434387,
      "learning_rate": 1.2199897971854712e-05,
      "loss": 1.1167,
      "step": 957
    },
    {
      "epoch": 8.961980548187444,
      "grad_norm": 0.3546077609062195,
      "learning_rate": 1.2185126152710698e-05,
      "loss": 1.1158,
      "step": 958
    },
    {
      "epoch": 8.97141173003242,
      "grad_norm": 0.3213850259780884,
      "learning_rate": 1.2170349324694202e-05,
      "loss": 1.141,
      "step": 959
    },
    {
      "epoch": 8.980842911877394,
      "grad_norm": 0.3312915563583374,
      "learning_rate": 1.2155567521677525e-05,
      "loss": 1.1033,
      "step": 960
    },
    {
      "epoch": 8.99027409372237,
      "grad_norm": 0.30589577555656433,
      "learning_rate": 1.2140780777544368e-05,
      "loss": 1.1518,
      "step": 961
    },
    {
      "epoch": 8.999705275567344,
      "grad_norm": 0.3310631215572357,
      "learning_rate": 1.2125989126189763e-05,
      "loss": 1.1174,
      "step": 962
    },
    {
      "epoch": 9.0,
      "grad_norm": 1.4951542615890503,
      "learning_rate": 1.2111192601519992e-05,
      "loss": 0.9407,
      "step": 963
    },
    {
      "epoch": 9.009431181844976,
      "grad_norm": 0.29734402894973755,
      "learning_rate": 1.2096391237452499e-05,
      "loss": 1.129,
      "step": 964
    },
    {
      "epoch": 9.01886236368995,
      "grad_norm": 0.31958967447280884,
      "learning_rate": 1.2081585067915824e-05,
      "loss": 1.1541,
      "step": 965
    },
    {
      "epoch": 9.028293545534925,
      "grad_norm": 0.3358437716960907,
      "learning_rate": 1.206677412684953e-05,
      "loss": 1.0958,
      "step": 966
    },
    {
      "epoch": 9.0377247273799,
      "grad_norm": 0.3326999843120575,
      "learning_rate": 1.2051958448204114e-05,
      "loss": 1.0924,
      "step": 967
    },
    {
      "epoch": 9.047155909224875,
      "grad_norm": 0.32359087467193604,
      "learning_rate": 1.2037138065940923e-05,
      "loss": 1.0936,
      "step": 968
    },
    {
      "epoch": 9.056587091069849,
      "grad_norm": 0.32550445199012756,
      "learning_rate": 1.2022313014032101e-05,
      "loss": 1.1684,
      "step": 969
    },
    {
      "epoch": 9.066018272914825,
      "grad_norm": 0.3236081302165985,
      "learning_rate": 1.2007483326460487e-05,
      "loss": 1.0961,
      "step": 970
    },
    {
      "epoch": 9.0754494547598,
      "grad_norm": 0.31153586506843567,
      "learning_rate": 1.1992649037219546e-05,
      "loss": 1.1274,
      "step": 971
    },
    {
      "epoch": 9.084880636604774,
      "grad_norm": 0.3324004113674164,
      "learning_rate": 1.1977810180313295e-05,
      "loss": 1.1257,
      "step": 972
    },
    {
      "epoch": 9.09431181844975,
      "grad_norm": 0.35073357820510864,
      "learning_rate": 1.1962966789756221e-05,
      "loss": 1.1045,
      "step": 973
    },
    {
      "epoch": 9.103743000294724,
      "grad_norm": 0.32239916920661926,
      "learning_rate": 1.1948118899573197e-05,
      "loss": 1.1292,
      "step": 974
    },
    {
      "epoch": 9.1131741821397,
      "grad_norm": 0.33055248856544495,
      "learning_rate": 1.1933266543799425e-05,
      "loss": 1.1453,
      "step": 975
    },
    {
      "epoch": 9.122605363984674,
      "grad_norm": 0.33346033096313477,
      "learning_rate": 1.191840975648032e-05,
      "loss": 1.1362,
      "step": 976
    },
    {
      "epoch": 9.13203654582965,
      "grad_norm": 0.3139149248600006,
      "learning_rate": 1.190354857167148e-05,
      "loss": 1.1062,
      "step": 977
    },
    {
      "epoch": 9.141467727674625,
      "grad_norm": 0.33919528126716614,
      "learning_rate": 1.1888683023438566e-05,
      "loss": 1.1211,
      "step": 978
    },
    {
      "epoch": 9.150898909519599,
      "grad_norm": 0.32962414622306824,
      "learning_rate": 1.187381314585725e-05,
      "loss": 1.1597,
      "step": 979
    },
    {
      "epoch": 9.160330091364575,
      "grad_norm": 0.3171340525150299,
      "learning_rate": 1.185893897301312e-05,
      "loss": 1.1365,
      "step": 980
    },
    {
      "epoch": 9.169761273209549,
      "grad_norm": 0.3133097290992737,
      "learning_rate": 1.1844060539001618e-05,
      "loss": 1.1182,
      "step": 981
    },
    {
      "epoch": 9.179192455054524,
      "grad_norm": 0.3222962021827698,
      "learning_rate": 1.1829177877927955e-05,
      "loss": 1.1376,
      "step": 982
    },
    {
      "epoch": 9.188623636899498,
      "grad_norm": 0.32621750235557556,
      "learning_rate": 1.181429102390702e-05,
      "loss": 1.1004,
      "step": 983
    },
    {
      "epoch": 9.198054818744474,
      "grad_norm": 0.29404494166374207,
      "learning_rate": 1.1799400011063325e-05,
      "loss": 1.1327,
      "step": 984
    },
    {
      "epoch": 9.20748600058945,
      "grad_norm": 0.2970629036426544,
      "learning_rate": 1.1784504873530906e-05,
      "loss": 1.1265,
      "step": 985
    },
    {
      "epoch": 9.216917182434424,
      "grad_norm": 0.31927889585494995,
      "learning_rate": 1.1769605645453265e-05,
      "loss": 1.1267,
      "step": 986
    },
    {
      "epoch": 9.2263483642794,
      "grad_norm": 0.33370906114578247,
      "learning_rate": 1.175470236098327e-05,
      "loss": 1.1388,
      "step": 987
    },
    {
      "epoch": 9.235779546124373,
      "grad_norm": 0.32166188955307007,
      "learning_rate": 1.1739795054283088e-05,
      "loss": 1.148,
      "step": 988
    },
    {
      "epoch": 9.245210727969349,
      "grad_norm": 0.33248716592788696,
      "learning_rate": 1.172488375952411e-05,
      "loss": 1.171,
      "step": 989
    },
    {
      "epoch": 9.254641909814323,
      "grad_norm": 0.32470059394836426,
      "learning_rate": 1.1709968510886874e-05,
      "loss": 1.1381,
      "step": 990
    },
    {
      "epoch": 9.264073091659299,
      "grad_norm": 0.3703041970729828,
      "learning_rate": 1.1695049342560969e-05,
      "loss": 1.1196,
      "step": 991
    },
    {
      "epoch": 9.273504273504274,
      "grad_norm": 0.34986549615859985,
      "learning_rate": 1.1680126288744975e-05,
      "loss": 1.1421,
      "step": 992
    },
    {
      "epoch": 9.282935455349248,
      "grad_norm": 0.3512449562549591,
      "learning_rate": 1.1665199383646384e-05,
      "loss": 1.094,
      "step": 993
    },
    {
      "epoch": 9.292366637194224,
      "grad_norm": 0.30039289593696594,
      "learning_rate": 1.1650268661481507e-05,
      "loss": 1.114,
      "step": 994
    },
    {
      "epoch": 9.301797819039198,
      "grad_norm": 0.3442722260951996,
      "learning_rate": 1.1635334156475407e-05,
      "loss": 1.1063,
      "step": 995
    },
    {
      "epoch": 9.311229000884174,
      "grad_norm": 0.3373550772666931,
      "learning_rate": 1.1620395902861823e-05,
      "loss": 1.1604,
      "step": 996
    },
    {
      "epoch": 9.320660182729148,
      "grad_norm": 0.32681170105934143,
      "learning_rate": 1.160545393488308e-05,
      "loss": 1.1032,
      "step": 997
    },
    {
      "epoch": 9.330091364574123,
      "grad_norm": 0.32633069157600403,
      "learning_rate": 1.1590508286790025e-05,
      "loss": 1.1369,
      "step": 998
    },
    {
      "epoch": 9.339522546419099,
      "grad_norm": 0.29791542887687683,
      "learning_rate": 1.1575558992841934e-05,
      "loss": 1.1274,
      "step": 999
    },
    {
      "epoch": 9.348953728264073,
      "grad_norm": 0.34093302488327026,
      "learning_rate": 1.1560606087306441e-05,
      "loss": 1.1115,
      "step": 1000
    },
    {
      "epoch": 9.358384910109049,
      "grad_norm": 0.3178524672985077,
      "learning_rate": 1.1545649604459466e-05,
      "loss": 1.0958,
      "step": 1001
    },
    {
      "epoch": 9.367816091954023,
      "grad_norm": 0.3219285011291504,
      "learning_rate": 1.1530689578585117e-05,
      "loss": 1.1068,
      "step": 1002
    },
    {
      "epoch": 9.377247273798998,
      "grad_norm": 0.30865344405174255,
      "learning_rate": 1.1515726043975633e-05,
      "loss": 1.1287,
      "step": 1003
    },
    {
      "epoch": 9.386678455643972,
      "grad_norm": 0.3239116370677948,
      "learning_rate": 1.1500759034931293e-05,
      "loss": 1.0987,
      "step": 1004
    },
    {
      "epoch": 9.396109637488948,
      "grad_norm": 0.34618687629699707,
      "learning_rate": 1.148578858576034e-05,
      "loss": 1.0632,
      "step": 1005
    },
    {
      "epoch": 9.405540819333924,
      "grad_norm": 0.3164757192134857,
      "learning_rate": 1.1470814730778905e-05,
      "loss": 1.0868,
      "step": 1006
    },
    {
      "epoch": 9.414972001178898,
      "grad_norm": 0.3466360867023468,
      "learning_rate": 1.145583750431092e-05,
      "loss": 1.0866,
      "step": 1007
    },
    {
      "epoch": 9.424403183023873,
      "grad_norm": 0.328642338514328,
      "learning_rate": 1.1440856940688053e-05,
      "loss": 1.126,
      "step": 1008
    },
    {
      "epoch": 9.433834364868847,
      "grad_norm": 0.3412288427352905,
      "learning_rate": 1.1425873074249615e-05,
      "loss": 1.1081,
      "step": 1009
    },
    {
      "epoch": 9.443265546713823,
      "grad_norm": 0.31523171067237854,
      "learning_rate": 1.1410885939342495e-05,
      "loss": 1.161,
      "step": 1010
    },
    {
      "epoch": 9.452696728558797,
      "grad_norm": 0.3322989344596863,
      "learning_rate": 1.1395895570321065e-05,
      "loss": 1.1459,
      "step": 1011
    },
    {
      "epoch": 9.462127910403773,
      "grad_norm": 0.3475552797317505,
      "learning_rate": 1.1380902001547118e-05,
      "loss": 1.1198,
      "step": 1012
    },
    {
      "epoch": 9.471559092248748,
      "grad_norm": 0.34767600893974304,
      "learning_rate": 1.1365905267389781e-05,
      "loss": 1.0932,
      "step": 1013
    },
    {
      "epoch": 9.480990274093722,
      "grad_norm": 0.3224610984325409,
      "learning_rate": 1.135090540222543e-05,
      "loss": 1.0807,
      "step": 1014
    },
    {
      "epoch": 9.490421455938698,
      "grad_norm": 0.31536930799484253,
      "learning_rate": 1.1335902440437625e-05,
      "loss": 1.0929,
      "step": 1015
    },
    {
      "epoch": 9.499852637783672,
      "grad_norm": 0.3532155752182007,
      "learning_rate": 1.1320896416417026e-05,
      "loss": 1.1244,
      "step": 1016
    },
    {
      "epoch": 9.509283819628648,
      "grad_norm": 0.3203878700733185,
      "learning_rate": 1.1305887364561302e-05,
      "loss": 1.1157,
      "step": 1017
    },
    {
      "epoch": 9.518715001473621,
      "grad_norm": 0.3296913504600525,
      "learning_rate": 1.1290875319275077e-05,
      "loss": 1.1494,
      "step": 1018
    },
    {
      "epoch": 9.528146183318597,
      "grad_norm": 0.3367222547531128,
      "learning_rate": 1.1275860314969822e-05,
      "loss": 1.1507,
      "step": 1019
    },
    {
      "epoch": 9.537577365163571,
      "grad_norm": 0.35798200964927673,
      "learning_rate": 1.12608423860638e-05,
      "loss": 1.087,
      "step": 1020
    },
    {
      "epoch": 9.547008547008547,
      "grad_norm": 0.39027705788612366,
      "learning_rate": 1.1245821566981975e-05,
      "loss": 1.1178,
      "step": 1021
    },
    {
      "epoch": 9.556439728853523,
      "grad_norm": 0.3424951434135437,
      "learning_rate": 1.1230797892155945e-05,
      "loss": 1.124,
      "step": 1022
    },
    {
      "epoch": 9.565870910698496,
      "grad_norm": 0.3108174204826355,
      "learning_rate": 1.1215771396023833e-05,
      "loss": 1.1123,
      "step": 1023
    },
    {
      "epoch": 9.575302092543472,
      "grad_norm": 0.29923543334007263,
      "learning_rate": 1.120074211303025e-05,
      "loss": 1.0957,
      "step": 1024
    },
    {
      "epoch": 9.584733274388446,
      "grad_norm": 0.30214205384254456,
      "learning_rate": 1.1185710077626188e-05,
      "loss": 1.163,
      "step": 1025
    },
    {
      "epoch": 9.594164456233422,
      "grad_norm": 0.3297635316848755,
      "learning_rate": 1.1170675324268942e-05,
      "loss": 1.1111,
      "step": 1026
    },
    {
      "epoch": 9.603595638078396,
      "grad_norm": 0.3156435787677765,
      "learning_rate": 1.1155637887422045e-05,
      "loss": 1.1312,
      "step": 1027
    },
    {
      "epoch": 9.613026819923371,
      "grad_norm": 0.3300703465938568,
      "learning_rate": 1.1140597801555179e-05,
      "loss": 1.1363,
      "step": 1028
    },
    {
      "epoch": 9.622458001768347,
      "grad_norm": 0.336516797542572,
      "learning_rate": 1.1125555101144104e-05,
      "loss": 1.0753,
      "step": 1029
    },
    {
      "epoch": 9.631889183613321,
      "grad_norm": 0.30731743574142456,
      "learning_rate": 1.111050982067056e-05,
      "loss": 1.117,
      "step": 1030
    },
    {
      "epoch": 9.641320365458297,
      "grad_norm": 0.34522855281829834,
      "learning_rate": 1.109546199462221e-05,
      "loss": 1.1397,
      "step": 1031
    },
    {
      "epoch": 9.65075154730327,
      "grad_norm": 0.3020126223564148,
      "learning_rate": 1.108041165749255e-05,
      "loss": 1.1393,
      "step": 1032
    },
    {
      "epoch": 9.660182729148246,
      "grad_norm": 0.3448754847049713,
      "learning_rate": 1.1065358843780838e-05,
      "loss": 1.151,
      "step": 1033
    },
    {
      "epoch": 9.66961391099322,
      "grad_norm": 0.3323720395565033,
      "learning_rate": 1.1050303587992e-05,
      "loss": 1.1406,
      "step": 1034
    },
    {
      "epoch": 9.679045092838196,
      "grad_norm": 0.35086122155189514,
      "learning_rate": 1.1035245924636561e-05,
      "loss": 1.0922,
      "step": 1035
    },
    {
      "epoch": 9.688476274683172,
      "grad_norm": 0.35537832975387573,
      "learning_rate": 1.1020185888230572e-05,
      "loss": 1.124,
      "step": 1036
    },
    {
      "epoch": 9.697907456528146,
      "grad_norm": 0.3013576567173004,
      "learning_rate": 1.1005123513295517e-05,
      "loss": 1.1187,
      "step": 1037
    },
    {
      "epoch": 9.707338638373121,
      "grad_norm": 0.3492630422115326,
      "learning_rate": 1.0990058834358245e-05,
      "loss": 1.113,
      "step": 1038
    },
    {
      "epoch": 9.716769820218095,
      "grad_norm": 0.30094391107559204,
      "learning_rate": 1.0974991885950877e-05,
      "loss": 1.1479,
      "step": 1039
    },
    {
      "epoch": 9.726201002063071,
      "grad_norm": 0.3099314272403717,
      "learning_rate": 1.0959922702610748e-05,
      "loss": 1.1467,
      "step": 1040
    },
    {
      "epoch": 9.735632183908045,
      "grad_norm": 0.3350242078304291,
      "learning_rate": 1.0944851318880314e-05,
      "loss": 1.0902,
      "step": 1041
    },
    {
      "epoch": 9.74506336575302,
      "grad_norm": 0.31721678376197815,
      "learning_rate": 1.0929777769307068e-05,
      "loss": 1.1051,
      "step": 1042
    },
    {
      "epoch": 9.754494547597997,
      "grad_norm": 0.31667619943618774,
      "learning_rate": 1.091470208844347e-05,
      "loss": 1.1278,
      "step": 1043
    },
    {
      "epoch": 9.76392572944297,
      "grad_norm": 0.3267480731010437,
      "learning_rate": 1.089962431084687e-05,
      "loss": 1.1201,
      "step": 1044
    },
    {
      "epoch": 9.773356911287946,
      "grad_norm": 0.366701602935791,
      "learning_rate": 1.0884544471079426e-05,
      "loss": 1.1735,
      "step": 1045
    },
    {
      "epoch": 9.78278809313292,
      "grad_norm": 0.3294471502304077,
      "learning_rate": 1.0869462603708013e-05,
      "loss": 1.1095,
      "step": 1046
    },
    {
      "epoch": 9.792219274977896,
      "grad_norm": 0.35709473490715027,
      "learning_rate": 1.0854378743304155e-05,
      "loss": 1.083,
      "step": 1047
    },
    {
      "epoch": 9.80165045682287,
      "grad_norm": 0.35598820447921753,
      "learning_rate": 1.0839292924443954e-05,
      "loss": 1.1154,
      "step": 1048
    },
    {
      "epoch": 9.811081638667845,
      "grad_norm": 0.3676482141017914,
      "learning_rate": 1.0824205181707999e-05,
      "loss": 1.1211,
      "step": 1049
    },
    {
      "epoch": 9.820512820512821,
      "grad_norm": 0.32786867022514343,
      "learning_rate": 1.0809115549681278e-05,
      "loss": 1.1246,
      "step": 1050
    },
    {
      "epoch": 9.829944002357795,
      "grad_norm": 0.3511979579925537,
      "learning_rate": 1.0794024062953123e-05,
      "loss": 1.1768,
      "step": 1051
    },
    {
      "epoch": 9.83937518420277,
      "grad_norm": 0.3167322874069214,
      "learning_rate": 1.0778930756117111e-05,
      "loss": 1.1219,
      "step": 1052
    },
    {
      "epoch": 9.848806366047745,
      "grad_norm": 0.3426837623119354,
      "learning_rate": 1.0763835663770999e-05,
      "loss": 1.144,
      "step": 1053
    },
    {
      "epoch": 9.85823754789272,
      "grad_norm": 0.3247535824775696,
      "learning_rate": 1.074873882051662e-05,
      "loss": 1.1051,
      "step": 1054
    },
    {
      "epoch": 9.867668729737694,
      "grad_norm": 0.34409400820732117,
      "learning_rate": 1.0733640260959834e-05,
      "loss": 1.1137,
      "step": 1055
    },
    {
      "epoch": 9.87709991158267,
      "grad_norm": 0.349367618560791,
      "learning_rate": 1.0718540019710433e-05,
      "loss": 1.1251,
      "step": 1056
    },
    {
      "epoch": 9.886531093427646,
      "grad_norm": 0.3259822428226471,
      "learning_rate": 1.0703438131382066e-05,
      "loss": 1.1316,
      "step": 1057
    },
    {
      "epoch": 9.89596227527262,
      "grad_norm": 0.33015039563179016,
      "learning_rate": 1.068833463059215e-05,
      "loss": 1.1273,
      "step": 1058
    },
    {
      "epoch": 9.905393457117595,
      "grad_norm": 0.33580610156059265,
      "learning_rate": 1.06732295519618e-05,
      "loss": 1.1345,
      "step": 1059
    },
    {
      "epoch": 9.91482463896257,
      "grad_norm": 0.305500864982605,
      "learning_rate": 1.0658122930115762e-05,
      "loss": 1.1321,
      "step": 1060
    },
    {
      "epoch": 9.924255820807545,
      "grad_norm": 0.34617292881011963,
      "learning_rate": 1.0643014799682296e-05,
      "loss": 1.1212,
      "step": 1061
    },
    {
      "epoch": 9.933687002652519,
      "grad_norm": 0.3194162845611572,
      "learning_rate": 1.0627905195293135e-05,
      "loss": 1.1203,
      "step": 1062
    },
    {
      "epoch": 9.943118184497495,
      "grad_norm": 0.34257733821868896,
      "learning_rate": 1.061279415158339e-05,
      "loss": 1.1404,
      "step": 1063
    },
    {
      "epoch": 9.95254936634247,
      "grad_norm": 0.3331916928291321,
      "learning_rate": 1.0597681703191467e-05,
      "loss": 1.1232,
      "step": 1064
    },
    {
      "epoch": 9.961980548187444,
      "grad_norm": 0.34811246395111084,
      "learning_rate": 1.0582567884758996e-05,
      "loss": 1.1478,
      "step": 1065
    },
    {
      "epoch": 9.97141173003242,
      "grad_norm": 0.3435853123664856,
      "learning_rate": 1.0567452730930743e-05,
      "loss": 1.1742,
      "step": 1066
    },
    {
      "epoch": 9.980842911877394,
      "grad_norm": 0.33342862129211426,
      "learning_rate": 1.0552336276354535e-05,
      "loss": 1.1454,
      "step": 1067
    },
    {
      "epoch": 9.99027409372237,
      "grad_norm": 0.3201589286327362,
      "learning_rate": 1.0537218555681188e-05,
      "loss": 1.122,
      "step": 1068
    },
    {
      "epoch": 9.999705275567344,
      "grad_norm": 0.33184337615966797,
      "learning_rate": 1.0522099603564415e-05,
      "loss": 1.1222,
      "step": 1069
    },
    {
      "epoch": 10.0,
      "grad_norm": 2.1187400817871094,
      "learning_rate": 1.0506979454660749e-05,
      "loss": 0.7213,
      "step": 1070
    },
    {
      "epoch": 10.009431181844976,
      "grad_norm": 0.3266386389732361,
      "learning_rate": 1.049185814362947e-05,
      "loss": 1.1451,
      "step": 1071
    },
    {
      "epoch": 10.01886236368995,
      "grad_norm": 0.3296206593513489,
      "learning_rate": 1.0476735705132523e-05,
      "loss": 1.1601,
      "step": 1072
    },
    {
      "epoch": 10.028293545534925,
      "grad_norm": 0.33054181933403015,
      "learning_rate": 1.0461612173834432e-05,
      "loss": 1.1401,
      "step": 1073
    },
    {
      "epoch": 10.0377247273799,
      "grad_norm": 0.3150332272052765,
      "learning_rate": 1.0446487584402234e-05,
      "loss": 1.071,
      "step": 1074
    },
    {
      "epoch": 10.047155909224875,
      "grad_norm": 0.3283747434616089,
      "learning_rate": 1.0431361971505387e-05,
      "loss": 1.102,
      "step": 1075
    },
    {
      "epoch": 10.056587091069849,
      "grad_norm": 0.3118980824947357,
      "learning_rate": 1.0416235369815692e-05,
      "loss": 1.0841,
      "step": 1076
    },
    {
      "epoch": 10.066018272914825,
      "grad_norm": 0.3197149932384491,
      "learning_rate": 1.0401107814007223e-05,
      "loss": 1.1283,
      "step": 1077
    },
    {
      "epoch": 10.0754494547598,
      "grad_norm": 0.32433927059173584,
      "learning_rate": 1.0385979338756234e-05,
      "loss": 1.1351,
      "step": 1078
    },
    {
      "epoch": 10.084880636604774,
      "grad_norm": 0.3300211429595947,
      "learning_rate": 1.0370849978741093e-05,
      "loss": 1.1229,
      "step": 1079
    },
    {
      "epoch": 10.09431181844975,
      "grad_norm": 0.3292657434940338,
      "learning_rate": 1.0355719768642194e-05,
      "loss": 1.1223,
      "step": 1080
    },
    {
      "epoch": 10.103743000294724,
      "grad_norm": 0.31007006764411926,
      "learning_rate": 1.0340588743141879e-05,
      "loss": 1.1091,
      "step": 1081
    },
    {
      "epoch": 10.1131741821397,
      "grad_norm": 0.3239206075668335,
      "learning_rate": 1.0325456936924359e-05,
      "loss": 1.1429,
      "step": 1082
    },
    {
      "epoch": 10.122605363984674,
      "grad_norm": 0.3403737545013428,
      "learning_rate": 1.0310324384675631e-05,
      "loss": 1.1422,
      "step": 1083
    },
    {
      "epoch": 10.13203654582965,
      "grad_norm": 0.33241403102874756,
      "learning_rate": 1.0295191121083417e-05,
      "loss": 1.0959,
      "step": 1084
    },
    {
      "epoch": 10.141467727674625,
      "grad_norm": 0.3413771092891693,
      "learning_rate": 1.0280057180837046e-05,
      "loss": 1.1125,
      "step": 1085
    },
    {
      "epoch": 10.150898909519599,
      "grad_norm": 0.3224530816078186,
      "learning_rate": 1.0264922598627417e-05,
      "loss": 1.1474,
      "step": 1086
    },
    {
      "epoch": 10.160330091364575,
      "grad_norm": 0.3297005593776703,
      "learning_rate": 1.0249787409146894e-05,
      "loss": 1.1419,
      "step": 1087
    },
    {
      "epoch": 10.169761273209549,
      "grad_norm": 0.32963743805885315,
      "learning_rate": 1.0234651647089232e-05,
      "loss": 1.0922,
      "step": 1088
    },
    {
      "epoch": 10.179192455054524,
      "grad_norm": 0.31437256932258606,
      "learning_rate": 1.0219515347149505e-05,
      "loss": 1.0922,
      "step": 1089
    },
    {
      "epoch": 10.188623636899498,
      "grad_norm": 0.3126941919326782,
      "learning_rate": 1.0204378544024005e-05,
      "loss": 1.1042,
      "step": 1090
    },
    {
      "epoch": 10.198054818744474,
      "grad_norm": 0.3237341046333313,
      "learning_rate": 1.0189241272410191e-05,
      "loss": 1.1438,
      "step": 1091
    },
    {
      "epoch": 10.20748600058945,
      "grad_norm": 0.33898410201072693,
      "learning_rate": 1.0174103567006597e-05,
      "loss": 1.1156,
      "step": 1092
    },
    {
      "epoch": 10.216917182434424,
      "grad_norm": 0.3235456645488739,
      "learning_rate": 1.015896546251274e-05,
      "loss": 1.2008,
      "step": 1093
    },
    {
      "epoch": 10.2263483642794,
      "grad_norm": 0.31397199630737305,
      "learning_rate": 1.014382699362906e-05,
      "loss": 1.1581,
      "step": 1094
    },
    {
      "epoch": 10.235779546124373,
      "grad_norm": 0.35640931129455566,
      "learning_rate": 1.0128688195056833e-05,
      "loss": 1.1549,
      "step": 1095
    },
    {
      "epoch": 10.245210727969349,
      "grad_norm": 0.33937862515449524,
      "learning_rate": 1.0113549101498086e-05,
      "loss": 1.1469,
      "step": 1096
    },
    {
      "epoch": 10.254641909814323,
      "grad_norm": 0.3193289339542389,
      "learning_rate": 1.0098409747655524e-05,
      "loss": 1.1155,
      "step": 1097
    },
    {
      "epoch": 10.264073091659299,
      "grad_norm": 0.33375367522239685,
      "learning_rate": 1.0083270168232448e-05,
      "loss": 1.1191,
      "step": 1098
    },
    {
      "epoch": 10.273504273504274,
      "grad_norm": 0.3270537853240967,
      "learning_rate": 1.0068130397932683e-05,
      "loss": 1.0647,
      "step": 1099
    },
    {
      "epoch": 10.282935455349248,
      "grad_norm": 0.30884405970573425,
      "learning_rate": 1.005299047146048e-05,
      "loss": 1.1438,
      "step": 1100
    },
    {
      "epoch": 10.292366637194224,
      "grad_norm": 0.3585077226161957,
      "learning_rate": 1.0037850423520454e-05,
      "loss": 1.1254,
      "step": 1101
    },
    {
      "epoch": 10.301797819039198,
      "grad_norm": 0.3391924202442169,
      "learning_rate": 1.0022710288817498e-05,
      "loss": 1.1202,
      "step": 1102
    },
    {
      "epoch": 10.311229000884174,
      "grad_norm": 0.336824893951416,
      "learning_rate": 1.0007570102056708e-05,
      "loss": 1.1557,
      "step": 1103
    },
    {
      "epoch": 10.320660182729148,
      "grad_norm": 0.33740895986557007,
      "learning_rate": 9.992429897943294e-06,
      "loss": 1.1296,
      "step": 1104
    },
    {
      "epoch": 10.330091364574123,
      "grad_norm": 0.35241520404815674,
      "learning_rate": 9.977289711182503e-06,
      "loss": 1.1238,
      "step": 1105
    },
    {
      "epoch": 10.339522546419099,
      "grad_norm": 0.3312475383281708,
      "learning_rate": 9.962149576479546e-06,
      "loss": 1.1048,
      "step": 1106
    },
    {
      "epoch": 10.348953728264073,
      "grad_norm": 0.3471001386642456,
      "learning_rate": 9.947009528539522e-06,
      "loss": 1.143,
      "step": 1107
    },
    {
      "epoch": 10.358384910109049,
      "grad_norm": 0.3474942445755005,
      "learning_rate": 9.931869602067318e-06,
      "loss": 1.1261,
      "step": 1108
    },
    {
      "epoch": 10.367816091954023,
      "grad_norm": 0.3398987650871277,
      "learning_rate": 9.916729831767554e-06,
      "loss": 1.0801,
      "step": 1109
    },
    {
      "epoch": 10.377247273798998,
      "grad_norm": 0.328056663274765,
      "learning_rate": 9.901590252344476e-06,
      "loss": 1.1006,
      "step": 1110
    },
    {
      "epoch": 10.386678455643972,
      "grad_norm": 0.325304239988327,
      "learning_rate": 9.886450898501918e-06,
      "loss": 1.085,
      "step": 1111
    },
    {
      "epoch": 10.396109637488948,
      "grad_norm": 0.31849268078804016,
      "learning_rate": 9.871311804943172e-06,
      "loss": 1.104,
      "step": 1112
    },
    {
      "epoch": 10.405540819333924,
      "grad_norm": 0.34376317262649536,
      "learning_rate": 9.856173006370943e-06,
      "loss": 1.1173,
      "step": 1113
    },
    {
      "epoch": 10.414972001178898,
      "grad_norm": 0.31951290369033813,
      "learning_rate": 9.841034537487265e-06,
      "loss": 1.1125,
      "step": 1114
    },
    {
      "epoch": 10.424403183023873,
      "grad_norm": 0.32215550541877747,
      "learning_rate": 9.825896432993406e-06,
      "loss": 1.0936,
      "step": 1115
    },
    {
      "epoch": 10.433834364868847,
      "grad_norm": 0.3134187161922455,
      "learning_rate": 9.810758727589814e-06,
      "loss": 1.1566,
      "step": 1116
    },
    {
      "epoch": 10.443265546713823,
      "grad_norm": 0.3435213267803192,
      "learning_rate": 9.795621455975998e-06,
      "loss": 1.1151,
      "step": 1117
    },
    {
      "epoch": 10.452696728558797,
      "grad_norm": 0.3152291476726532,
      "learning_rate": 9.7804846528505e-06,
      "loss": 1.145,
      "step": 1118
    },
    {
      "epoch": 10.462127910403773,
      "grad_norm": 0.3452431559562683,
      "learning_rate": 9.76534835291077e-06,
      "loss": 1.1179,
      "step": 1119
    },
    {
      "epoch": 10.471559092248748,
      "grad_norm": 0.33654704689979553,
      "learning_rate": 9.750212590853109e-06,
      "loss": 1.1335,
      "step": 1120
    },
    {
      "epoch": 10.480990274093722,
      "grad_norm": 0.3412466049194336,
      "learning_rate": 9.735077401372584e-06,
      "loss": 1.1119,
      "step": 1121
    },
    {
      "epoch": 10.490421455938698,
      "grad_norm": 0.3179161846637726,
      "learning_rate": 9.719942819162956e-06,
      "loss": 1.0874,
      "step": 1122
    },
    {
      "epoch": 10.499852637783672,
      "grad_norm": 0.33262622356414795,
      "learning_rate": 9.704808878916586e-06,
      "loss": 1.1085,
      "step": 1123
    },
    {
      "epoch": 10.509283819628648,
      "grad_norm": 0.3251478374004364,
      "learning_rate": 9.68967561532437e-06,
      "loss": 1.1299,
      "step": 1124
    },
    {
      "epoch": 10.518715001473621,
      "grad_norm": 0.33254069089889526,
      "learning_rate": 9.674543063075641e-06,
      "loss": 1.1255,
      "step": 1125
    },
    {
      "epoch": 10.528146183318597,
      "grad_norm": 0.343397855758667,
      "learning_rate": 9.659411256858123e-06,
      "loss": 1.1173,
      "step": 1126
    },
    {
      "epoch": 10.537577365163571,
      "grad_norm": 0.32353171706199646,
      "learning_rate": 9.64428023135781e-06,
      "loss": 1.1431,
      "step": 1127
    },
    {
      "epoch": 10.547008547008547,
      "grad_norm": 0.3650684952735901,
      "learning_rate": 9.62915002125891e-06,
      "loss": 1.1029,
      "step": 1128
    },
    {
      "epoch": 10.556439728853523,
      "grad_norm": 0.34292054176330566,
      "learning_rate": 9.614020661243771e-06,
      "loss": 1.1093,
      "step": 1129
    },
    {
      "epoch": 10.565870910698496,
      "grad_norm": 0.3426815867424011,
      "learning_rate": 9.598892185992779e-06,
      "loss": 1.1069,
      "step": 1130
    },
    {
      "epoch": 10.575302092543472,
      "grad_norm": 0.32432499527931213,
      "learning_rate": 9.583764630184311e-06,
      "loss": 1.1089,
      "step": 1131
    },
    {
      "epoch": 10.584733274388446,
      "grad_norm": 0.3376534879207611,
      "learning_rate": 9.568638028494616e-06,
      "loss": 1.1396,
      "step": 1132
    },
    {
      "epoch": 10.594164456233422,
      "grad_norm": 0.3319801390171051,
      "learning_rate": 9.553512415597768e-06,
      "loss": 1.1016,
      "step": 1133
    },
    {
      "epoch": 10.603595638078396,
      "grad_norm": 0.2916170060634613,
      "learning_rate": 9.538387826165567e-06,
      "loss": 1.1075,
      "step": 1134
    },
    {
      "epoch": 10.613026819923371,
      "grad_norm": 0.34474247694015503,
      "learning_rate": 9.52326429486748e-06,
      "loss": 1.071,
      "step": 1135
    },
    {
      "epoch": 10.622458001768347,
      "grad_norm": 0.3538580536842346,
      "learning_rate": 9.508141856370532e-06,
      "loss": 1.1069,
      "step": 1136
    },
    {
      "epoch": 10.631889183613321,
      "grad_norm": 0.34474286437034607,
      "learning_rate": 9.493020545339254e-06,
      "loss": 1.1143,
      "step": 1137
    },
    {
      "epoch": 10.641320365458297,
      "grad_norm": 0.343809574842453,
      "learning_rate": 9.477900396435585e-06,
      "loss": 1.1461,
      "step": 1138
    },
    {
      "epoch": 10.65075154730327,
      "grad_norm": 0.3436020612716675,
      "learning_rate": 9.462781444318815e-06,
      "loss": 1.1108,
      "step": 1139
    },
    {
      "epoch": 10.660182729148246,
      "grad_norm": 0.3304970860481262,
      "learning_rate": 9.447663723645465e-06,
      "loss": 1.1167,
      "step": 1140
    },
    {
      "epoch": 10.66961391099322,
      "grad_norm": 0.35172438621520996,
      "learning_rate": 9.43254726906926e-06,
      "loss": 1.1234,
      "step": 1141
    },
    {
      "epoch": 10.679045092838196,
      "grad_norm": 0.3447006642818451,
      "learning_rate": 9.41743211524101e-06,
      "loss": 1.075,
      "step": 1142
    },
    {
      "epoch": 10.688476274683172,
      "grad_norm": 0.3727172613143921,
      "learning_rate": 9.402318296808536e-06,
      "loss": 1.1225,
      "step": 1143
    },
    {
      "epoch": 10.697907456528146,
      "grad_norm": 0.3626832365989685,
      "learning_rate": 9.387205848416615e-06,
      "loss": 1.1449,
      "step": 1144
    },
    {
      "epoch": 10.707338638373121,
      "grad_norm": 0.3483385145664215,
      "learning_rate": 9.372094804706867e-06,
      "loss": 1.0706,
      "step": 1145
    },
    {
      "epoch": 10.716769820218095,
      "grad_norm": 0.32643449306488037,
      "learning_rate": 9.35698520031771e-06,
      "loss": 1.1468,
      "step": 1146
    },
    {
      "epoch": 10.726201002063071,
      "grad_norm": 0.32556286454200745,
      "learning_rate": 9.341877069884241e-06,
      "loss": 1.1103,
      "step": 1147
    },
    {
      "epoch": 10.735632183908045,
      "grad_norm": 0.3486495912075043,
      "learning_rate": 9.326770448038201e-06,
      "loss": 1.1063,
      "step": 1148
    },
    {
      "epoch": 10.74506336575302,
      "grad_norm": 0.3145819902420044,
      "learning_rate": 9.311665369407852e-06,
      "loss": 1.125,
      "step": 1149
    },
    {
      "epoch": 10.754494547597997,
      "grad_norm": 0.3426133692264557,
      "learning_rate": 9.296561868617937e-06,
      "loss": 1.1563,
      "step": 1150
    },
    {
      "epoch": 10.76392572944297,
      "grad_norm": 0.35767820477485657,
      "learning_rate": 9.281459980289568e-06,
      "loss": 1.084,
      "step": 1151
    },
    {
      "epoch": 10.773356911287946,
      "grad_norm": 0.3356701731681824,
      "learning_rate": 9.26635973904017e-06,
      "loss": 1.1555,
      "step": 1152
    },
    {
      "epoch": 10.78278809313292,
      "grad_norm": 0.3296436071395874,
      "learning_rate": 9.251261179483382e-06,
      "loss": 1.161,
      "step": 1153
    },
    {
      "epoch": 10.792219274977896,
      "grad_norm": 0.3460048735141754,
      "learning_rate": 9.236164336229005e-06,
      "loss": 1.1372,
      "step": 1154
    },
    {
      "epoch": 10.80165045682287,
      "grad_norm": 0.3523220419883728,
      "learning_rate": 9.221069243882887e-06,
      "loss": 1.116,
      "step": 1155
    },
    {
      "epoch": 10.811081638667845,
      "grad_norm": 0.3458026349544525,
      "learning_rate": 9.205975937046879e-06,
      "loss": 1.0909,
      "step": 1156
    },
    {
      "epoch": 10.820512820512821,
      "grad_norm": 0.3190199136734009,
      "learning_rate": 9.190884450318727e-06,
      "loss": 1.149,
      "step": 1157
    },
    {
      "epoch": 10.829944002357795,
      "grad_norm": 0.3143271207809448,
      "learning_rate": 9.175794818292006e-06,
      "loss": 1.1362,
      "step": 1158
    },
    {
      "epoch": 10.83937518420277,
      "grad_norm": 0.36365145444869995,
      "learning_rate": 9.16070707555605e-06,
      "loss": 1.0912,
      "step": 1159
    },
    {
      "epoch": 10.848806366047745,
      "grad_norm": 0.35652077198028564,
      "learning_rate": 9.145621256695849e-06,
      "loss": 1.1216,
      "step": 1160
    },
    {
      "epoch": 10.85823754789272,
      "grad_norm": 0.3505227863788605,
      "learning_rate": 9.130537396291994e-06,
      "loss": 1.1552,
      "step": 1161
    },
    {
      "epoch": 10.867668729737694,
      "grad_norm": 0.33600136637687683,
      "learning_rate": 9.115455528920577e-06,
      "loss": 1.1243,
      "step": 1162
    },
    {
      "epoch": 10.87709991158267,
      "grad_norm": 0.344382107257843,
      "learning_rate": 9.100375689153131e-06,
      "loss": 1.0958,
      "step": 1163
    },
    {
      "epoch": 10.886531093427646,
      "grad_norm": 0.38265523314476013,
      "learning_rate": 9.08529791155653e-06,
      "loss": 1.1246,
      "step": 1164
    },
    {
      "epoch": 10.89596227527262,
      "grad_norm": 0.33479782938957214,
      "learning_rate": 9.070222230692935e-06,
      "loss": 1.136,
      "step": 1165
    },
    {
      "epoch": 10.905393457117595,
      "grad_norm": 0.349907249212265,
      "learning_rate": 9.055148681119688e-06,
      "loss": 1.1204,
      "step": 1166
    },
    {
      "epoch": 10.91482463896257,
      "grad_norm": 0.31658869981765747,
      "learning_rate": 9.040077297389255e-06,
      "loss": 1.1545,
      "step": 1167
    },
    {
      "epoch": 10.924255820807545,
      "grad_norm": 0.3300524353981018,
      "learning_rate": 9.025008114049125e-06,
      "loss": 1.1309,
      "step": 1168
    },
    {
      "epoch": 10.933687002652519,
      "grad_norm": 0.3547072112560272,
      "learning_rate": 9.00994116564176e-06,
      "loss": 1.1134,
      "step": 1169
    },
    {
      "epoch": 10.943118184497495,
      "grad_norm": 0.313473105430603,
      "learning_rate": 8.994876486704486e-06,
      "loss": 1.1303,
      "step": 1170
    },
    {
      "epoch": 10.95254936634247,
      "grad_norm": 0.3168245553970337,
      "learning_rate": 8.979814111769431e-06,
      "loss": 1.1699,
      "step": 1171
    },
    {
      "epoch": 10.961980548187444,
      "grad_norm": 0.3389434218406677,
      "learning_rate": 8.964754075363444e-06,
      "loss": 1.1237,
      "step": 1172
    },
    {
      "epoch": 10.97141173003242,
      "grad_norm": 0.3387678861618042,
      "learning_rate": 8.949696412008004e-06,
      "loss": 1.0769,
      "step": 1173
    },
    {
      "epoch": 10.980842911877394,
      "grad_norm": 0.3256192207336426,
      "learning_rate": 8.934641156219165e-06,
      "loss": 1.1313,
      "step": 1174
    },
    {
      "epoch": 10.99027409372237,
      "grad_norm": 0.32680678367614746,
      "learning_rate": 8.919588342507451e-06,
      "loss": 1.1366,
      "step": 1175
    },
    {
      "epoch": 10.999705275567344,
      "grad_norm": 0.3514959216117859,
      "learning_rate": 8.904538005377793e-06,
      "loss": 1.1388,
      "step": 1176
    },
    {
      "epoch": 11.0,
      "grad_norm": 1.6496620178222656,
      "learning_rate": 8.889490179329443e-06,
      "loss": 0.6912,
      "step": 1177
    },
    {
      "epoch": 11.009431181844976,
      "grad_norm": 0.3480311334133148,
      "learning_rate": 8.874444898855898e-06,
      "loss": 1.1551,
      "step": 1178
    },
    {
      "epoch": 11.01886236368995,
      "grad_norm": 0.33256831765174866,
      "learning_rate": 8.85940219844482e-06,
      "loss": 1.131,
      "step": 1179
    },
    {
      "epoch": 11.028293545534925,
      "grad_norm": 0.3328912556171417,
      "learning_rate": 8.844362112577957e-06,
      "loss": 1.1333,
      "step": 1180
    },
    {
      "epoch": 11.0377247273799,
      "grad_norm": 0.33796241879463196,
      "learning_rate": 8.82932467573106e-06,
      "loss": 1.0946,
      "step": 1181
    },
    {
      "epoch": 11.047155909224875,
      "grad_norm": 0.34133005142211914,
      "learning_rate": 8.814289922373815e-06,
      "loss": 1.1044,
      "step": 1182
    },
    {
      "epoch": 11.056587091069849,
      "grad_norm": 0.31820395588874817,
      "learning_rate": 8.79925788696975e-06,
      "loss": 1.1476,
      "step": 1183
    },
    {
      "epoch": 11.066018272914825,
      "grad_norm": 0.35957053303718567,
      "learning_rate": 8.78422860397617e-06,
      "loss": 1.1248,
      "step": 1184
    },
    {
      "epoch": 11.0754494547598,
      "grad_norm": 0.33185043931007385,
      "learning_rate": 8.769202107844062e-06,
      "loss": 1.1311,
      "step": 1185
    },
    {
      "epoch": 11.084880636604774,
      "grad_norm": 0.3221443295478821,
      "learning_rate": 8.754178433018027e-06,
      "loss": 1.103,
      "step": 1186
    },
    {
      "epoch": 11.09431181844975,
      "grad_norm": 0.36646080017089844,
      "learning_rate": 8.739157613936206e-06,
      "loss": 1.1392,
      "step": 1187
    },
    {
      "epoch": 11.103743000294724,
      "grad_norm": 0.33302465081214905,
      "learning_rate": 8.724139685030181e-06,
      "loss": 1.1468,
      "step": 1188
    },
    {
      "epoch": 11.1131741821397,
      "grad_norm": 0.30369627475738525,
      "learning_rate": 8.709124680724928e-06,
      "loss": 1.1327,
      "step": 1189
    },
    {
      "epoch": 11.122605363984674,
      "grad_norm": 0.3361005485057831,
      "learning_rate": 8.694112635438701e-06,
      "loss": 1.1185,
      "step": 1190
    },
    {
      "epoch": 11.13203654582965,
      "grad_norm": 0.3679546117782593,
      "learning_rate": 8.67910358358298e-06,
      "loss": 1.132,
      "step": 1191
    },
    {
      "epoch": 11.141467727674625,
      "grad_norm": 0.3386375904083252,
      "learning_rate": 8.664097559562376e-06,
      "loss": 1.0976,
      "step": 1192
    },
    {
      "epoch": 11.150898909519599,
      "grad_norm": 0.31749433279037476,
      "learning_rate": 8.649094597774575e-06,
      "loss": 1.1057,
      "step": 1193
    },
    {
      "epoch": 11.160330091364575,
      "grad_norm": 0.33967849612236023,
      "learning_rate": 8.634094732610224e-06,
      "loss": 1.123,
      "step": 1194
    },
    {
      "epoch": 11.169761273209549,
      "grad_norm": 0.32213294506073,
      "learning_rate": 8.619097998452886e-06,
      "loss": 1.114,
      "step": 1195
    },
    {
      "epoch": 11.179192455054524,
      "grad_norm": 0.3584909737110138,
      "learning_rate": 8.604104429678935e-06,
      "loss": 1.1033,
      "step": 1196
    },
    {
      "epoch": 11.188623636899498,
      "grad_norm": 0.3433445692062378,
      "learning_rate": 8.589114060657507e-06,
      "loss": 1.138,
      "step": 1197
    },
    {
      "epoch": 11.198054818744474,
      "grad_norm": 0.3326849937438965,
      "learning_rate": 8.574126925750386e-06,
      "loss": 1.1536,
      "step": 1198
    },
    {
      "epoch": 11.20748600058945,
      "grad_norm": 0.31599944829940796,
      "learning_rate": 8.559143059311949e-06,
      "loss": 1.1196,
      "step": 1199
    },
    {
      "epoch": 11.216917182434424,
      "grad_norm": 0.3378796875476837,
      "learning_rate": 8.544162495689084e-06,
      "loss": 1.0824,
      "step": 1200
    },
    {
      "epoch": 11.2263483642794,
      "grad_norm": 0.33913928270339966,
      "learning_rate": 8.529185269221098e-06,
      "loss": 1.1157,
      "step": 1201
    },
    {
      "epoch": 11.235779546124373,
      "grad_norm": 0.38283541798591614,
      "learning_rate": 8.514211414239664e-06,
      "loss": 1.1332,
      "step": 1202
    },
    {
      "epoch": 11.245210727969349,
      "grad_norm": 0.342037171125412,
      "learning_rate": 8.49924096506871e-06,
      "loss": 1.0878,
      "step": 1203
    },
    {
      "epoch": 11.254641909814323,
      "grad_norm": 0.37278643250465393,
      "learning_rate": 8.484273956024372e-06,
      "loss": 1.1079,
      "step": 1204
    },
    {
      "epoch": 11.264073091659299,
      "grad_norm": 0.3321470320224762,
      "learning_rate": 8.469310421414886e-06,
      "loss": 1.0884,
      "step": 1205
    },
    {
      "epoch": 11.273504273504274,
      "grad_norm": 0.33585238456726074,
      "learning_rate": 8.45435039554054e-06,
      "loss": 1.1189,
      "step": 1206
    },
    {
      "epoch": 11.282935455349248,
      "grad_norm": 0.3503120541572571,
      "learning_rate": 8.439393912693559e-06,
      "loss": 1.1149,
      "step": 1207
    },
    {
      "epoch": 11.292366637194224,
      "grad_norm": 0.3257083594799042,
      "learning_rate": 8.42444100715807e-06,
      "loss": 1.1551,
      "step": 1208
    },
    {
      "epoch": 11.301797819039198,
      "grad_norm": 0.32141542434692383,
      "learning_rate": 8.409491713209977e-06,
      "loss": 1.1043,
      "step": 1209
    },
    {
      "epoch": 11.311229000884174,
      "grad_norm": 0.3499146103858948,
      "learning_rate": 8.394546065116923e-06,
      "loss": 1.1658,
      "step": 1210
    },
    {
      "epoch": 11.320660182729148,
      "grad_norm": 0.33942949771881104,
      "learning_rate": 8.379604097138178e-06,
      "loss": 1.1581,
      "step": 1211
    },
    {
      "epoch": 11.330091364574123,
      "grad_norm": 0.3264167904853821,
      "learning_rate": 8.364665843524595e-06,
      "loss": 1.1154,
      "step": 1212
    },
    {
      "epoch": 11.339522546419099,
      "grad_norm": 0.3284870684146881,
      "learning_rate": 8.349731338518495e-06,
      "loss": 1.136,
      "step": 1213
    },
    {
      "epoch": 11.348953728264073,
      "grad_norm": 0.3517701327800751,
      "learning_rate": 8.33480061635362e-06,
      "loss": 1.13,
      "step": 1214
    },
    {
      "epoch": 11.358384910109049,
      "grad_norm": 0.33758190274238586,
      "learning_rate": 8.319873711255029e-06,
      "loss": 1.0888,
      "step": 1215
    },
    {
      "epoch": 11.367816091954023,
      "grad_norm": 0.381600022315979,
      "learning_rate": 8.304950657439034e-06,
      "loss": 1.078,
      "step": 1216
    },
    {
      "epoch": 11.377247273798998,
      "grad_norm": 0.3279067575931549,
      "learning_rate": 8.290031489113131e-06,
      "loss": 1.0731,
      "step": 1217
    },
    {
      "epoch": 11.386678455643972,
      "grad_norm": 0.3382340669631958,
      "learning_rate": 8.275116240475893e-06,
      "loss": 1.125,
      "step": 1218
    },
    {
      "epoch": 11.396109637488948,
      "grad_norm": 0.3202723562717438,
      "learning_rate": 8.260204945716919e-06,
      "loss": 1.1419,
      "step": 1219
    },
    {
      "epoch": 11.405540819333924,
      "grad_norm": 0.3265289068222046,
      "learning_rate": 8.245297639016733e-06,
      "loss": 1.1008,
      "step": 1220
    },
    {
      "epoch": 11.414972001178898,
      "grad_norm": 0.3647773861885071,
      "learning_rate": 8.230394354546738e-06,
      "loss": 1.1309,
      "step": 1221
    },
    {
      "epoch": 11.424403183023873,
      "grad_norm": 0.3686254918575287,
      "learning_rate": 8.215495126469093e-06,
      "loss": 1.1564,
      "step": 1222
    },
    {
      "epoch": 11.433834364868847,
      "grad_norm": 0.3409377336502075,
      "learning_rate": 8.200599988936678e-06,
      "loss": 1.1164,
      "step": 1223
    },
    {
      "epoch": 11.443265546713823,
      "grad_norm": 0.32499605417251587,
      "learning_rate": 8.18570897609298e-06,
      "loss": 1.1429,
      "step": 1224
    },
    {
      "epoch": 11.452696728558797,
      "grad_norm": 0.3307386040687561,
      "learning_rate": 8.170822122072047e-06,
      "loss": 1.0839,
      "step": 1225
    },
    {
      "epoch": 11.462127910403773,
      "grad_norm": 0.3594866693019867,
      "learning_rate": 8.15593946099838e-06,
      "loss": 1.1142,
      "step": 1226
    },
    {
      "epoch": 11.471559092248748,
      "grad_norm": 0.36561664938926697,
      "learning_rate": 8.141061026986882e-06,
      "loss": 1.1103,
      "step": 1227
    },
    {
      "epoch": 11.480990274093722,
      "grad_norm": 0.3248808979988098,
      "learning_rate": 8.126186854142752e-06,
      "loss": 1.0684,
      "step": 1228
    },
    {
      "epoch": 11.490421455938698,
      "grad_norm": 0.40708082914352417,
      "learning_rate": 8.11131697656144e-06,
      "loss": 1.104,
      "step": 1229
    },
    {
      "epoch": 11.499852637783672,
      "grad_norm": 0.3357396423816681,
      "learning_rate": 8.096451428328528e-06,
      "loss": 1.1571,
      "step": 1230
    },
    {
      "epoch": 11.509283819628648,
      "grad_norm": 0.33679160475730896,
      "learning_rate": 8.081590243519682e-06,
      "loss": 1.1275,
      "step": 1231
    },
    {
      "epoch": 11.518715001473621,
      "grad_norm": 0.33311259746551514,
      "learning_rate": 8.066733456200582e-06,
      "loss": 1.1594,
      "step": 1232
    },
    {
      "epoch": 11.528146183318597,
      "grad_norm": 0.3278619945049286,
      "learning_rate": 8.051881100426804e-06,
      "loss": 1.1144,
      "step": 1233
    },
    {
      "epoch": 11.537577365163571,
      "grad_norm": 0.31852903962135315,
      "learning_rate": 8.037033210243784e-06,
      "loss": 1.1023,
      "step": 1234
    },
    {
      "epoch": 11.547008547008547,
      "grad_norm": 0.33235037326812744,
      "learning_rate": 8.022189819686706e-06,
      "loss": 1.1168,
      "step": 1235
    },
    {
      "epoch": 11.556439728853523,
      "grad_norm": 0.32985955476760864,
      "learning_rate": 8.007350962780457e-06,
      "loss": 1.1606,
      "step": 1236
    },
    {
      "epoch": 11.565870910698496,
      "grad_norm": 0.32881495356559753,
      "learning_rate": 7.992516673539516e-06,
      "loss": 1.1086,
      "step": 1237
    },
    {
      "epoch": 11.575302092543472,
      "grad_norm": 0.3670509457588196,
      "learning_rate": 7.977686985967902e-06,
      "loss": 1.1655,
      "step": 1238
    },
    {
      "epoch": 11.584733274388446,
      "grad_norm": 0.3812502324581146,
      "learning_rate": 7.962861934059077e-06,
      "loss": 1.1183,
      "step": 1239
    },
    {
      "epoch": 11.594164456233422,
      "grad_norm": 0.33364337682724,
      "learning_rate": 7.94804155179589e-06,
      "loss": 1.1164,
      "step": 1240
    },
    {
      "epoch": 11.603595638078396,
      "grad_norm": 0.35473746061325073,
      "learning_rate": 7.93322587315047e-06,
      "loss": 1.122,
      "step": 1241
    },
    {
      "epoch": 11.613026819923371,
      "grad_norm": 0.3157210350036621,
      "learning_rate": 7.918414932084178e-06,
      "loss": 1.1377,
      "step": 1242
    },
    {
      "epoch": 11.622458001768347,
      "grad_norm": 0.32867375016212463,
      "learning_rate": 7.903608762547503e-06,
      "loss": 1.1697,
      "step": 1243
    },
    {
      "epoch": 11.631889183613321,
      "grad_norm": 0.3372698426246643,
      "learning_rate": 7.88880739848001e-06,
      "loss": 1.1332,
      "step": 1244
    },
    {
      "epoch": 11.641320365458297,
      "grad_norm": 0.3351791203022003,
      "learning_rate": 7.874010873810239e-06,
      "loss": 1.1451,
      "step": 1245
    },
    {
      "epoch": 11.65075154730327,
      "grad_norm": 0.3178800344467163,
      "learning_rate": 7.859219222455634e-06,
      "loss": 1.1243,
      "step": 1246
    },
    {
      "epoch": 11.660182729148246,
      "grad_norm": 0.35754555463790894,
      "learning_rate": 7.844432478322482e-06,
      "loss": 1.0678,
      "step": 1247
    },
    {
      "epoch": 11.66961391099322,
      "grad_norm": 0.39908790588378906,
      "learning_rate": 7.8296506753058e-06,
      "loss": 1.1639,
      "step": 1248
    },
    {
      "epoch": 11.679045092838196,
      "grad_norm": 0.33906158804893494,
      "learning_rate": 7.814873847289307e-06,
      "loss": 1.0775,
      "step": 1249
    },
    {
      "epoch": 11.688476274683172,
      "grad_norm": 0.35625597834587097,
      "learning_rate": 7.80010202814529e-06,
      "loss": 1.1,
      "step": 1250
    },
    {
      "epoch": 11.697907456528146,
      "grad_norm": 0.36701834201812744,
      "learning_rate": 7.785335251734574e-06,
      "loss": 1.1224,
      "step": 1251
    },
    {
      "epoch": 11.707338638373121,
      "grad_norm": 0.33486050367355347,
      "learning_rate": 7.77057355190641e-06,
      "loss": 1.1369,
      "step": 1252
    },
    {
      "epoch": 11.716769820218095,
      "grad_norm": 0.350596159696579,
      "learning_rate": 7.755816962498432e-06,
      "loss": 1.1118,
      "step": 1253
    },
    {
      "epoch": 11.726201002063071,
      "grad_norm": 0.33755987882614136,
      "learning_rate": 7.741065517336532e-06,
      "loss": 1.0931,
      "step": 1254
    },
    {
      "epoch": 11.735632183908045,
      "grad_norm": 0.341720849275589,
      "learning_rate": 7.726319250234838e-06,
      "loss": 1.1117,
      "step": 1255
    },
    {
      "epoch": 11.74506336575302,
      "grad_norm": 0.36906757950782776,
      "learning_rate": 7.71157819499559e-06,
      "loss": 1.0961,
      "step": 1256
    },
    {
      "epoch": 11.754494547597997,
      "grad_norm": 0.3470369875431061,
      "learning_rate": 7.696842385409089e-06,
      "loss": 1.0723,
      "step": 1257
    },
    {
      "epoch": 11.76392572944297,
      "grad_norm": 0.3489816188812256,
      "learning_rate": 7.682111855253603e-06,
      "loss": 1.1327,
      "step": 1258
    },
    {
      "epoch": 11.773356911287946,
      "grad_norm": 0.34045150876045227,
      "learning_rate": 7.667386638295315e-06,
      "loss": 1.0991,
      "step": 1259
    },
    {
      "epoch": 11.78278809313292,
      "grad_norm": 0.33980002999305725,
      "learning_rate": 7.652666768288213e-06,
      "loss": 1.1247,
      "step": 1260
    },
    {
      "epoch": 11.792219274977896,
      "grad_norm": 0.34334859251976013,
      "learning_rate": 7.637952278974034e-06,
      "loss": 1.1399,
      "step": 1261
    },
    {
      "epoch": 11.80165045682287,
      "grad_norm": 0.3851679563522339,
      "learning_rate": 7.623243204082184e-06,
      "loss": 1.135,
      "step": 1262
    },
    {
      "epoch": 11.811081638667845,
      "grad_norm": 0.40343177318573,
      "learning_rate": 7.6085395773296466e-06,
      "loss": 1.1552,
      "step": 1263
    },
    {
      "epoch": 11.820512820512821,
      "grad_norm": 0.314418226480484,
      "learning_rate": 7.593841432420935e-06,
      "loss": 1.1139,
      "step": 1264
    },
    {
      "epoch": 11.829944002357795,
      "grad_norm": 0.33084627985954285,
      "learning_rate": 7.579148803047981e-06,
      "loss": 1.1349,
      "step": 1265
    },
    {
      "epoch": 11.83937518420277,
      "grad_norm": 0.33515864610671997,
      "learning_rate": 7.564461722890082e-06,
      "loss": 1.1374,
      "step": 1266
    },
    {
      "epoch": 11.848806366047745,
      "grad_norm": 0.34819719195365906,
      "learning_rate": 7.5497802256138065e-06,
      "loss": 1.1247,
      "step": 1267
    },
    {
      "epoch": 11.85823754789272,
      "grad_norm": 0.3644096851348877,
      "learning_rate": 7.535104344872939e-06,
      "loss": 1.1369,
      "step": 1268
    },
    {
      "epoch": 11.867668729737694,
      "grad_norm": 0.3449985980987549,
      "learning_rate": 7.520434114308376e-06,
      "loss": 1.1058,
      "step": 1269
    },
    {
      "epoch": 11.87709991158267,
      "grad_norm": 0.33950111269950867,
      "learning_rate": 7.505769567548072e-06,
      "loss": 1.1181,
      "step": 1270
    },
    {
      "epoch": 11.886531093427646,
      "grad_norm": 0.3474138677120209,
      "learning_rate": 7.491110738206943e-06,
      "loss": 1.1088,
      "step": 1271
    },
    {
      "epoch": 11.89596227527262,
      "grad_norm": 0.3678467273712158,
      "learning_rate": 7.4764576598868115e-06,
      "loss": 1.1362,
      "step": 1272
    },
    {
      "epoch": 11.905393457117595,
      "grad_norm": 0.34657523036003113,
      "learning_rate": 7.461810366176311e-06,
      "loss": 1.0919,
      "step": 1273
    },
    {
      "epoch": 11.91482463896257,
      "grad_norm": 0.3837311267852783,
      "learning_rate": 7.447168890650804e-06,
      "loss": 1.0848,
      "step": 1274
    },
    {
      "epoch": 11.924255820807545,
      "grad_norm": 0.35227730870246887,
      "learning_rate": 7.43253326687234e-06,
      "loss": 1.0939,
      "step": 1275
    },
    {
      "epoch": 11.933687002652519,
      "grad_norm": 0.37347298860549927,
      "learning_rate": 7.417903528389534e-06,
      "loss": 1.1155,
      "step": 1276
    },
    {
      "epoch": 11.943118184497495,
      "grad_norm": 0.34868931770324707,
      "learning_rate": 7.403279708737523e-06,
      "loss": 1.1614,
      "step": 1277
    },
    {
      "epoch": 11.95254936634247,
      "grad_norm": 0.33356237411499023,
      "learning_rate": 7.388661841437866e-06,
      "loss": 1.1258,
      "step": 1278
    },
    {
      "epoch": 11.961980548187444,
      "grad_norm": 0.37629956007003784,
      "learning_rate": 7.374049959998491e-06,
      "loss": 1.1632,
      "step": 1279
    },
    {
      "epoch": 11.97141173003242,
      "grad_norm": 0.34829387068748474,
      "learning_rate": 7.359444097913591e-06,
      "loss": 1.1092,
      "step": 1280
    },
    {
      "epoch": 11.980842911877394,
      "grad_norm": 0.37625476717948914,
      "learning_rate": 7.344844288663571e-06,
      "loss": 1.116,
      "step": 1281
    },
    {
      "epoch": 11.99027409372237,
      "grad_norm": 0.3495071828365326,
      "learning_rate": 7.330250565714953e-06,
      "loss": 1.1008,
      "step": 1282
    },
    {
      "epoch": 11.999705275567344,
      "grad_norm": 0.3376521170139313,
      "learning_rate": 7.31566296252032e-06,
      "loss": 1.107,
      "step": 1283
    },
    {
      "epoch": 12.0,
      "grad_norm": 1.6116801500320435,
      "learning_rate": 7.301081512518211e-06,
      "loss": 0.811,
      "step": 1284
    },
    {
      "epoch": 12.009431181844976,
      "grad_norm": 0.3415496349334717,
      "learning_rate": 7.286506249133076e-06,
      "loss": 1.1488,
      "step": 1285
    },
    {
      "epoch": 12.01886236368995,
      "grad_norm": 0.3251594305038452,
      "learning_rate": 7.271937205775171e-06,
      "loss": 1.1235,
      "step": 1286
    },
    {
      "epoch": 12.028293545534925,
      "grad_norm": 0.3339403569698334,
      "learning_rate": 7.257374415840502e-06,
      "loss": 1.1244,
      "step": 1287
    },
    {
      "epoch": 12.0377247273799,
      "grad_norm": 0.32362768054008484,
      "learning_rate": 7.242817912710738e-06,
      "loss": 1.0933,
      "step": 1288
    },
    {
      "epoch": 12.047155909224875,
      "grad_norm": 0.3231126368045807,
      "learning_rate": 7.228267729753134e-06,
      "loss": 1.1565,
      "step": 1289
    },
    {
      "epoch": 12.056587091069849,
      "grad_norm": 0.33315160870552063,
      "learning_rate": 7.213723900320465e-06,
      "loss": 1.1211,
      "step": 1290
    },
    {
      "epoch": 12.066018272914825,
      "grad_norm": 0.3637779951095581,
      "learning_rate": 7.199186457750931e-06,
      "loss": 1.1064,
      "step": 1291
    },
    {
      "epoch": 12.0754494547598,
      "grad_norm": 0.3315874934196472,
      "learning_rate": 7.184655435368106e-06,
      "loss": 1.1385,
      "step": 1292
    },
    {
      "epoch": 12.084880636604774,
      "grad_norm": 0.33211252093315125,
      "learning_rate": 7.170130866480834e-06,
      "loss": 1.1446,
      "step": 1293
    },
    {
      "epoch": 12.09431181844975,
      "grad_norm": 0.3721599876880646,
      "learning_rate": 7.155612784383174e-06,
      "loss": 1.1441,
      "step": 1294
    },
    {
      "epoch": 12.103743000294724,
      "grad_norm": 0.3292638063430786,
      "learning_rate": 7.141101222354309e-06,
      "loss": 1.1122,
      "step": 1295
    },
    {
      "epoch": 12.1131741821397,
      "grad_norm": 0.3411775529384613,
      "learning_rate": 7.126596213658489e-06,
      "loss": 1.1183,
      "step": 1296
    },
    {
      "epoch": 12.122605363984674,
      "grad_norm": 0.3546741008758545,
      "learning_rate": 7.1120977915449244e-06,
      "loss": 1.1687,
      "step": 1297
    },
    {
      "epoch": 12.13203654582965,
      "grad_norm": 0.36820557713508606,
      "learning_rate": 7.097605989247742e-06,
      "loss": 1.1071,
      "step": 1298
    },
    {
      "epoch": 12.141467727674625,
      "grad_norm": 0.3347208797931671,
      "learning_rate": 7.083120839985884e-06,
      "loss": 1.0881,
      "step": 1299
    },
    {
      "epoch": 12.150898909519599,
      "grad_norm": 0.3483409583568573,
      "learning_rate": 7.068642376963053e-06,
      "loss": 1.091,
      "step": 1300
    },
    {
      "epoch": 12.160330091364575,
      "grad_norm": 0.33651003241539,
      "learning_rate": 7.0541706333676104e-06,
      "loss": 1.0914,
      "step": 1301
    },
    {
      "epoch": 12.169761273209549,
      "grad_norm": 0.3408518433570862,
      "learning_rate": 7.039705642372534e-06,
      "loss": 1.1223,
      "step": 1302
    },
    {
      "epoch": 12.179192455054524,
      "grad_norm": 0.3148760199546814,
      "learning_rate": 7.025247437135305e-06,
      "loss": 1.1168,
      "step": 1303
    },
    {
      "epoch": 12.188623636899498,
      "grad_norm": 0.32042795419692993,
      "learning_rate": 7.010796050797858e-06,
      "loss": 1.1336,
      "step": 1304
    },
    {
      "epoch": 12.198054818744474,
      "grad_norm": 0.3271994888782501,
      "learning_rate": 6.9963515164865e-06,
      "loss": 1.0889,
      "step": 1305
    },
    {
      "epoch": 12.20748600058945,
      "grad_norm": 0.37911033630371094,
      "learning_rate": 6.981913867311818e-06,
      "loss": 1.1229,
      "step": 1306
    },
    {
      "epoch": 12.216917182434424,
      "grad_norm": 0.3562420904636383,
      "learning_rate": 6.967483136368639e-06,
      "loss": 1.0732,
      "step": 1307
    },
    {
      "epoch": 12.2263483642794,
      "grad_norm": 0.3432265818119049,
      "learning_rate": 6.95305935673591e-06,
      "loss": 1.1395,
      "step": 1308
    },
    {
      "epoch": 12.235779546124373,
      "grad_norm": 0.34310391545295715,
      "learning_rate": 6.938642561476656e-06,
      "loss": 1.0948,
      "step": 1309
    },
    {
      "epoch": 12.245210727969349,
      "grad_norm": 0.3494040071964264,
      "learning_rate": 6.924232783637884e-06,
      "loss": 1.1373,
      "step": 1310
    },
    {
      "epoch": 12.254641909814323,
      "grad_norm": 0.3533654808998108,
      "learning_rate": 6.909830056250527e-06,
      "loss": 1.1038,
      "step": 1311
    },
    {
      "epoch": 12.264073091659299,
      "grad_norm": 0.36803343892097473,
      "learning_rate": 6.8954344123293425e-06,
      "loss": 1.1031,
      "step": 1312
    },
    {
      "epoch": 12.273504273504274,
      "grad_norm": 0.33126166462898254,
      "learning_rate": 6.881045884872863e-06,
      "loss": 1.1043,
      "step": 1313
    },
    {
      "epoch": 12.282935455349248,
      "grad_norm": 0.3553237318992615,
      "learning_rate": 6.8666645068632964e-06,
      "loss": 1.1555,
      "step": 1314
    },
    {
      "epoch": 12.292366637194224,
      "grad_norm": 0.31992408633232117,
      "learning_rate": 6.852290311266477e-06,
      "loss": 1.1355,
      "step": 1315
    },
    {
      "epoch": 12.301797819039198,
      "grad_norm": 0.3386557996273041,
      "learning_rate": 6.837923331031761e-06,
      "loss": 1.1085,
      "step": 1316
    },
    {
      "epoch": 12.311229000884174,
      "grad_norm": 0.3767082989215851,
      "learning_rate": 6.823563599091973e-06,
      "loss": 1.11,
      "step": 1317
    },
    {
      "epoch": 12.320660182729148,
      "grad_norm": 0.3528483510017395,
      "learning_rate": 6.809211148363321e-06,
      "loss": 1.1541,
      "step": 1318
    },
    {
      "epoch": 12.330091364574123,
      "grad_norm": 0.33866173028945923,
      "learning_rate": 6.7948660117453166e-06,
      "loss": 1.1144,
      "step": 1319
    },
    {
      "epoch": 12.339522546419099,
      "grad_norm": 0.327340692281723,
      "learning_rate": 6.780528222120723e-06,
      "loss": 1.1277,
      "step": 1320
    },
    {
      "epoch": 12.348953728264073,
      "grad_norm": 0.3412814140319824,
      "learning_rate": 6.766197812355437e-06,
      "loss": 1.1084,
      "step": 1321
    },
    {
      "epoch": 12.358384910109049,
      "grad_norm": 0.33901676535606384,
      "learning_rate": 6.7518748152984626e-06,
      "loss": 1.1651,
      "step": 1322
    },
    {
      "epoch": 12.367816091954023,
      "grad_norm": 0.361865371465683,
      "learning_rate": 6.737559263781794e-06,
      "loss": 1.0737,
      "step": 1323
    },
    {
      "epoch": 12.377247273798998,
      "grad_norm": 0.3443233072757721,
      "learning_rate": 6.7232511906203735e-06,
      "loss": 1.1199,
      "step": 1324
    },
    {
      "epoch": 12.386678455643972,
      "grad_norm": 0.34266120195388794,
      "learning_rate": 6.708950628611984e-06,
      "loss": 1.1404,
      "step": 1325
    },
    {
      "epoch": 12.396109637488948,
      "grad_norm": 0.35299500823020935,
      "learning_rate": 6.694657610537211e-06,
      "loss": 1.1244,
      "step": 1326
    },
    {
      "epoch": 12.405540819333924,
      "grad_norm": 0.35287514328956604,
      "learning_rate": 6.680372169159329e-06,
      "loss": 1.1115,
      "step": 1327
    },
    {
      "epoch": 12.414972001178898,
      "grad_norm": 0.33591312170028687,
      "learning_rate": 6.6660943372242584e-06,
      "loss": 1.1113,
      "step": 1328
    },
    {
      "epoch": 12.424403183023873,
      "grad_norm": 0.32536670565605164,
      "learning_rate": 6.6518241474604615e-06,
      "loss": 1.0986,
      "step": 1329
    },
    {
      "epoch": 12.433834364868847,
      "grad_norm": 0.34965550899505615,
      "learning_rate": 6.637561632578904e-06,
      "loss": 1.1477,
      "step": 1330
    },
    {
      "epoch": 12.443265546713823,
      "grad_norm": 0.30706167221069336,
      "learning_rate": 6.623306825272937e-06,
      "loss": 1.1195,
      "step": 1331
    },
    {
      "epoch": 12.452696728558797,
      "grad_norm": 0.38720205426216125,
      "learning_rate": 6.6090597582182625e-06,
      "loss": 1.1134,
      "step": 1332
    },
    {
      "epoch": 12.462127910403773,
      "grad_norm": 0.3464304506778717,
      "learning_rate": 6.594820464072827e-06,
      "loss": 1.0854,
      "step": 1333
    },
    {
      "epoch": 12.471559092248748,
      "grad_norm": 0.34762048721313477,
      "learning_rate": 6.580588975476757e-06,
      "loss": 1.05,
      "step": 1334
    },
    {
      "epoch": 12.480990274093722,
      "grad_norm": 0.3368645906448364,
      "learning_rate": 6.566365325052307e-06,
      "loss": 1.1,
      "step": 1335
    },
    {
      "epoch": 12.490421455938698,
      "grad_norm": 0.3499802052974701,
      "learning_rate": 6.552149545403739e-06,
      "loss": 1.1455,
      "step": 1336
    },
    {
      "epoch": 12.499852637783672,
      "grad_norm": 0.3193378150463104,
      "learning_rate": 6.537941669117291e-06,
      "loss": 1.1064,
      "step": 1337
    },
    {
      "epoch": 12.509283819628648,
      "grad_norm": 0.37562841176986694,
      "learning_rate": 6.523741728761072e-06,
      "loss": 1.1163,
      "step": 1338
    },
    {
      "epoch": 12.518715001473621,
      "grad_norm": 0.3567442297935486,
      "learning_rate": 6.509549756885019e-06,
      "loss": 1.124,
      "step": 1339
    },
    {
      "epoch": 12.528146183318597,
      "grad_norm": 0.3310491740703583,
      "learning_rate": 6.495365786020777e-06,
      "loss": 1.1682,
      "step": 1340
    },
    {
      "epoch": 12.537577365163571,
      "grad_norm": 0.32315123081207275,
      "learning_rate": 6.48118984868167e-06,
      "loss": 1.0995,
      "step": 1341
    },
    {
      "epoch": 12.547008547008547,
      "grad_norm": 0.34339118003845215,
      "learning_rate": 6.4670219773625975e-06,
      "loss": 1.1652,
      "step": 1342
    },
    {
      "epoch": 12.556439728853523,
      "grad_norm": 0.3217776119709015,
      "learning_rate": 6.452862204539983e-06,
      "loss": 1.1405,
      "step": 1343
    },
    {
      "epoch": 12.565870910698496,
      "grad_norm": 0.36892077326774597,
      "learning_rate": 6.438710562671661e-06,
      "loss": 1.122,
      "step": 1344
    },
    {
      "epoch": 12.575302092543472,
      "grad_norm": 0.3635299503803253,
      "learning_rate": 6.4245670841968554e-06,
      "loss": 1.1621,
      "step": 1345
    },
    {
      "epoch": 12.584733274388446,
      "grad_norm": 0.3602728843688965,
      "learning_rate": 6.410431801536059e-06,
      "loss": 1.1105,
      "step": 1346
    },
    {
      "epoch": 12.594164456233422,
      "grad_norm": 0.3135741949081421,
      "learning_rate": 6.396304747090988e-06,
      "loss": 1.1352,
      "step": 1347
    },
    {
      "epoch": 12.603595638078396,
      "grad_norm": 0.32392317056655884,
      "learning_rate": 6.382185953244491e-06,
      "loss": 1.1584,
      "step": 1348
    },
    {
      "epoch": 12.613026819923371,
      "grad_norm": 0.3484891355037689,
      "learning_rate": 6.368075452360478e-06,
      "loss": 1.1128,
      "step": 1349
    },
    {
      "epoch": 12.622458001768347,
      "grad_norm": 0.377652108669281,
      "learning_rate": 6.353973276783864e-06,
      "loss": 1.1011,
      "step": 1350
    },
    {
      "epoch": 12.631889183613321,
      "grad_norm": 0.3515033721923828,
      "learning_rate": 6.339879458840466e-06,
      "loss": 1.156,
      "step": 1351
    },
    {
      "epoch": 12.641320365458297,
      "grad_norm": 0.3451874256134033,
      "learning_rate": 6.32579403083695e-06,
      "loss": 1.1166,
      "step": 1352
    },
    {
      "epoch": 12.65075154730327,
      "grad_norm": 0.32622668147087097,
      "learning_rate": 6.3117170250607444e-06,
      "loss": 1.1339,
      "step": 1353
    },
    {
      "epoch": 12.660182729148246,
      "grad_norm": 0.3320011496543884,
      "learning_rate": 6.297648473779982e-06,
      "loss": 1.0969,
      "step": 1354
    },
    {
      "epoch": 12.66961391099322,
      "grad_norm": 0.3668164908885956,
      "learning_rate": 6.283588409243405e-06,
      "loss": 1.1302,
      "step": 1355
    },
    {
      "epoch": 12.679045092838196,
      "grad_norm": 0.35706856846809387,
      "learning_rate": 6.2695368636803075e-06,
      "loss": 1.1171,
      "step": 1356
    },
    {
      "epoch": 12.688476274683172,
      "grad_norm": 0.334825336933136,
      "learning_rate": 6.255493869300452e-06,
      "loss": 1.1487,
      "step": 1357
    },
    {
      "epoch": 12.697907456528146,
      "grad_norm": 0.3629540205001831,
      "learning_rate": 6.241459458294008e-06,
      "loss": 1.1747,
      "step": 1358
    },
    {
      "epoch": 12.707338638373121,
      "grad_norm": 0.3326982259750366,
      "learning_rate": 6.227433662831457e-06,
      "loss": 1.1353,
      "step": 1359
    },
    {
      "epoch": 12.716769820218095,
      "grad_norm": 0.3602665662765503,
      "learning_rate": 6.213416515063541e-06,
      "loss": 1.126,
      "step": 1360
    },
    {
      "epoch": 12.726201002063071,
      "grad_norm": 0.3551085591316223,
      "learning_rate": 6.199408047121175e-06,
      "loss": 1.0883,
      "step": 1361
    },
    {
      "epoch": 12.735632183908045,
      "grad_norm": 0.3365868926048279,
      "learning_rate": 6.1854082911153835e-06,
      "loss": 1.1065,
      "step": 1362
    },
    {
      "epoch": 12.74506336575302,
      "grad_norm": 0.35992100834846497,
      "learning_rate": 6.171417279137213e-06,
      "loss": 1.101,
      "step": 1363
    },
    {
      "epoch": 12.754494547597997,
      "grad_norm": 0.32604584097862244,
      "learning_rate": 6.157435043257669e-06,
      "loss": 1.0982,
      "step": 1364
    },
    {
      "epoch": 12.76392572944297,
      "grad_norm": 0.3177421987056732,
      "learning_rate": 6.143461615527643e-06,
      "loss": 1.1067,
      "step": 1365
    },
    {
      "epoch": 12.773356911287946,
      "grad_norm": 0.372270792722702,
      "learning_rate": 6.129497027977829e-06,
      "loss": 1.1179,
      "step": 1366
    },
    {
      "epoch": 12.78278809313292,
      "grad_norm": 0.3591114282608032,
      "learning_rate": 6.115541312618671e-06,
      "loss": 1.1259,
      "step": 1367
    },
    {
      "epoch": 12.792219274977896,
      "grad_norm": 0.3484199345111847,
      "learning_rate": 6.101594501440256e-06,
      "loss": 1.0931,
      "step": 1368
    },
    {
      "epoch": 12.80165045682287,
      "grad_norm": 0.3405647873878479,
      "learning_rate": 6.087656626412279e-06,
      "loss": 1.1573,
      "step": 1369
    },
    {
      "epoch": 12.811081638667845,
      "grad_norm": 0.3447037637233734,
      "learning_rate": 6.073727719483938e-06,
      "loss": 1.1286,
      "step": 1370
    },
    {
      "epoch": 12.820512820512821,
      "grad_norm": 0.35345402359962463,
      "learning_rate": 6.0598078125838835e-06,
      "loss": 1.0742,
      "step": 1371
    },
    {
      "epoch": 12.829944002357795,
      "grad_norm": 0.36579394340515137,
      "learning_rate": 6.0458969376201215e-06,
      "loss": 1.0991,
      "step": 1372
    },
    {
      "epoch": 12.83937518420277,
      "grad_norm": 0.32078588008880615,
      "learning_rate": 6.031995126479976e-06,
      "loss": 1.1597,
      "step": 1373
    },
    {
      "epoch": 12.848806366047745,
      "grad_norm": 0.36860406398773193,
      "learning_rate": 6.018102411029973e-06,
      "loss": 1.0857,
      "step": 1374
    },
    {
      "epoch": 12.85823754789272,
      "grad_norm": 0.34012606739997864,
      "learning_rate": 6.0042188231158015e-06,
      "loss": 1.1172,
      "step": 1375
    },
    {
      "epoch": 12.867668729737694,
      "grad_norm": 0.3422607183456421,
      "learning_rate": 5.9903443945622265e-06,
      "loss": 1.0923,
      "step": 1376
    },
    {
      "epoch": 12.87709991158267,
      "grad_norm": 0.3718276619911194,
      "learning_rate": 5.976479157173006e-06,
      "loss": 1.1119,
      "step": 1377
    },
    {
      "epoch": 12.886531093427646,
      "grad_norm": 0.32002994418144226,
      "learning_rate": 5.96262314273085e-06,
      "loss": 1.1574,
      "step": 1378
    },
    {
      "epoch": 12.89596227527262,
      "grad_norm": 0.3544197678565979,
      "learning_rate": 5.948776382997307e-06,
      "loss": 1.1195,
      "step": 1379
    },
    {
      "epoch": 12.905393457117595,
      "grad_norm": 0.35053056478500366,
      "learning_rate": 5.9349389097127275e-06,
      "loss": 1.1134,
      "step": 1380
    },
    {
      "epoch": 12.91482463896257,
      "grad_norm": 0.3517555296421051,
      "learning_rate": 5.92111075459616e-06,
      "loss": 1.1195,
      "step": 1381
    },
    {
      "epoch": 12.924255820807545,
      "grad_norm": 0.34345415234565735,
      "learning_rate": 5.907291949345309e-06,
      "loss": 1.0847,
      "step": 1382
    },
    {
      "epoch": 12.933687002652519,
      "grad_norm": 0.36567458510398865,
      "learning_rate": 5.8934825256364305e-06,
      "loss": 1.1291,
      "step": 1383
    },
    {
      "epoch": 12.943118184497495,
      "grad_norm": 0.33738458156585693,
      "learning_rate": 5.879682515124292e-06,
      "loss": 1.1194,
      "step": 1384
    },
    {
      "epoch": 12.95254936634247,
      "grad_norm": 0.3190287947654724,
      "learning_rate": 5.865891949442066e-06,
      "loss": 1.088,
      "step": 1385
    },
    {
      "epoch": 12.961980548187444,
      "grad_norm": 0.3885696530342102,
      "learning_rate": 5.852110860201294e-06,
      "loss": 1.1604,
      "step": 1386
    },
    {
      "epoch": 12.97141173003242,
      "grad_norm": 0.3493114709854126,
      "learning_rate": 5.838339278991779e-06,
      "loss": 1.0599,
      "step": 1387
    },
    {
      "epoch": 12.980842911877394,
      "grad_norm": 0.35454657673835754,
      "learning_rate": 5.824577237381538e-06,
      "loss": 1.1161,
      "step": 1388
    },
    {
      "epoch": 12.99027409372237,
      "grad_norm": 0.36093929409980774,
      "learning_rate": 5.810824766916717e-06,
      "loss": 1.1422,
      "step": 1389
    },
    {
      "epoch": 12.999705275567344,
      "grad_norm": 0.32609209418296814,
      "learning_rate": 5.7970818991215285e-06,
      "loss": 1.0837,
      "step": 1390
    },
    {
      "epoch": 13.0,
      "grad_norm": 2.124236583709717,
      "learning_rate": 5.78334866549816e-06,
      "loss": 0.5272,
      "step": 1391
    },
    {
      "epoch": 13.009431181844976,
      "grad_norm": 0.36938920617103577,
      "learning_rate": 5.76962509752673e-06,
      "loss": 1.1026,
      "step": 1392
    },
    {
      "epoch": 13.01886236368995,
      "grad_norm": 0.3402775526046753,
      "learning_rate": 5.755911226665196e-06,
      "loss": 1.1088,
      "step": 1393
    },
    {
      "epoch": 13.028293545534925,
      "grad_norm": 0.3241930603981018,
      "learning_rate": 5.742207084349274e-06,
      "loss": 1.1342,
      "step": 1394
    },
    {
      "epoch": 13.0377247273799,
      "grad_norm": 0.34724974632263184,
      "learning_rate": 5.7285127019924055e-06,
      "loss": 1.1145,
      "step": 1395
    },
    {
      "epoch": 13.047155909224875,
      "grad_norm": 0.3125787079334259,
      "learning_rate": 5.714828110985635e-06,
      "loss": 1.1395,
      "step": 1396
    },
    {
      "epoch": 13.056587091069849,
      "grad_norm": 0.32095298171043396,
      "learning_rate": 5.701153342697578e-06,
      "loss": 1.0767,
      "step": 1397
    },
    {
      "epoch": 13.066018272914825,
      "grad_norm": 0.33328595757484436,
      "learning_rate": 5.687488428474328e-06,
      "loss": 1.1226,
      "step": 1398
    },
    {
      "epoch": 13.0754494547598,
      "grad_norm": 0.3525981307029724,
      "learning_rate": 5.67383339963939e-06,
      "loss": 1.1258,
      "step": 1399
    },
    {
      "epoch": 13.084880636604774,
      "grad_norm": 0.3347964882850647,
      "learning_rate": 5.660188287493612e-06,
      "loss": 1.1415,
      "step": 1400
    },
    {
      "epoch": 13.09431181844975,
      "grad_norm": 0.3440932035446167,
      "learning_rate": 5.646553123315113e-06,
      "loss": 1.1777,
      "step": 1401
    },
    {
      "epoch": 13.103743000294724,
      "grad_norm": 0.35991594195365906,
      "learning_rate": 5.632927938359191e-06,
      "loss": 1.1313,
      "step": 1402
    },
    {
      "epoch": 13.1131741821397,
      "grad_norm": 0.35012534260749817,
      "learning_rate": 5.6193127638583e-06,
      "loss": 1.1499,
      "step": 1403
    },
    {
      "epoch": 13.122605363984674,
      "grad_norm": 0.3477606177330017,
      "learning_rate": 5.605707631021917e-06,
      "loss": 1.1166,
      "step": 1404
    },
    {
      "epoch": 13.13203654582965,
      "grad_norm": 0.38470515608787537,
      "learning_rate": 5.592112571036518e-06,
      "loss": 1.1694,
      "step": 1405
    },
    {
      "epoch": 13.141467727674625,
      "grad_norm": 0.3616369664669037,
      "learning_rate": 5.5785276150654925e-06,
      "loss": 1.1108,
      "step": 1406
    },
    {
      "epoch": 13.150898909519599,
      "grad_norm": 0.3369148373603821,
      "learning_rate": 5.564952794249045e-06,
      "loss": 1.1366,
      "step": 1407
    },
    {
      "epoch": 13.160330091364575,
      "grad_norm": 0.3549255132675171,
      "learning_rate": 5.5513881397041836e-06,
      "loss": 1.1358,
      "step": 1408
    },
    {
      "epoch": 13.169761273209549,
      "grad_norm": 0.34056907892227173,
      "learning_rate": 5.537833682524581e-06,
      "loss": 1.0851,
      "step": 1409
    },
    {
      "epoch": 13.179192455054524,
      "grad_norm": 0.32777857780456543,
      "learning_rate": 5.524289453780549e-06,
      "loss": 1.1174,
      "step": 1410
    },
    {
      "epoch": 13.188623636899498,
      "grad_norm": 0.3416343927383423,
      "learning_rate": 5.510755484518955e-06,
      "loss": 1.1339,
      "step": 1411
    },
    {
      "epoch": 13.198054818744474,
      "grad_norm": 0.3691932260990143,
      "learning_rate": 5.497231805763148e-06,
      "loss": 1.1294,
      "step": 1412
    },
    {
      "epoch": 13.20748600058945,
      "grad_norm": 0.35585400462150574,
      "learning_rate": 5.483718448512876e-06,
      "loss": 1.122,
      "step": 1413
    },
    {
      "epoch": 13.216917182434424,
      "grad_norm": 0.3516830801963806,
      "learning_rate": 5.470215443744251e-06,
      "loss": 1.1101,
      "step": 1414
    },
    {
      "epoch": 13.2263483642794,
      "grad_norm": 0.33476704359054565,
      "learning_rate": 5.45672282240963e-06,
      "loss": 1.1779,
      "step": 1415
    },
    {
      "epoch": 13.235779546124373,
      "grad_norm": 0.33208152651786804,
      "learning_rate": 5.443240615437586e-06,
      "loss": 1.1283,
      "step": 1416
    },
    {
      "epoch": 13.245210727969349,
      "grad_norm": 0.3432321846485138,
      "learning_rate": 5.429768853732809e-06,
      "loss": 1.1145,
      "step": 1417
    },
    {
      "epoch": 13.254641909814323,
      "grad_norm": 0.3352908492088318,
      "learning_rate": 5.416307568176054e-06,
      "loss": 1.1303,
      "step": 1418
    },
    {
      "epoch": 13.264073091659299,
      "grad_norm": 0.36514586210250854,
      "learning_rate": 5.402856789624054e-06,
      "loss": 1.1446,
      "step": 1419
    },
    {
      "epoch": 13.273504273504274,
      "grad_norm": 0.3800867795944214,
      "learning_rate": 5.389416548909469e-06,
      "loss": 1.1465,
      "step": 1420
    },
    {
      "epoch": 13.282935455349248,
      "grad_norm": 0.33914700150489807,
      "learning_rate": 5.3759868768407845e-06,
      "loss": 1.1545,
      "step": 1421
    },
    {
      "epoch": 13.292366637194224,
      "grad_norm": 0.3261120617389679,
      "learning_rate": 5.362567804202276e-06,
      "loss": 1.1303,
      "step": 1422
    },
    {
      "epoch": 13.301797819039198,
      "grad_norm": 0.3606880009174347,
      "learning_rate": 5.349159361753919e-06,
      "loss": 1.1261,
      "step": 1423
    },
    {
      "epoch": 13.311229000884174,
      "grad_norm": 0.37921980023384094,
      "learning_rate": 5.33576158023132e-06,
      "loss": 1.0885,
      "step": 1424
    },
    {
      "epoch": 13.320660182729148,
      "grad_norm": 0.3399149179458618,
      "learning_rate": 5.32237449034565e-06,
      "loss": 1.1365,
      "step": 1425
    },
    {
      "epoch": 13.330091364574123,
      "grad_norm": 0.33401328325271606,
      "learning_rate": 5.308998122783561e-06,
      "loss": 1.1171,
      "step": 1426
    },
    {
      "epoch": 13.339522546419099,
      "grad_norm": 0.34913334250450134,
      "learning_rate": 5.29563250820715e-06,
      "loss": 1.0941,
      "step": 1427
    },
    {
      "epoch": 13.348953728264073,
      "grad_norm": 0.3355918824672699,
      "learning_rate": 5.28227767725384e-06,
      "loss": 1.1404,
      "step": 1428
    },
    {
      "epoch": 13.358384910109049,
      "grad_norm": 0.3571226894855499,
      "learning_rate": 5.268933660536352e-06,
      "loss": 1.1377,
      "step": 1429
    },
    {
      "epoch": 13.367816091954023,
      "grad_norm": 0.3477872610092163,
      "learning_rate": 5.255600488642609e-06,
      "loss": 1.1036,
      "step": 1430
    },
    {
      "epoch": 13.377247273798998,
      "grad_norm": 0.34055572748184204,
      "learning_rate": 5.242278192135682e-06,
      "loss": 1.0856,
      "step": 1431
    },
    {
      "epoch": 13.386678455643972,
      "grad_norm": 0.3439773619174957,
      "learning_rate": 5.2289668015537075e-06,
      "loss": 1.0936,
      "step": 1432
    },
    {
      "epoch": 13.396109637488948,
      "grad_norm": 0.3333064019680023,
      "learning_rate": 5.215666347409829e-06,
      "loss": 1.116,
      "step": 1433
    },
    {
      "epoch": 13.405540819333924,
      "grad_norm": 0.38837361335754395,
      "learning_rate": 5.202376860192104e-06,
      "loss": 1.1308,
      "step": 1434
    },
    {
      "epoch": 13.414972001178898,
      "grad_norm": 0.36389195919036865,
      "learning_rate": 5.189098370363479e-06,
      "loss": 1.0903,
      "step": 1435
    },
    {
      "epoch": 13.424403183023873,
      "grad_norm": 0.36172863841056824,
      "learning_rate": 5.175830908361667e-06,
      "loss": 1.1089,
      "step": 1436
    },
    {
      "epoch": 13.433834364868847,
      "grad_norm": 0.3577622175216675,
      "learning_rate": 5.162574504599106e-06,
      "loss": 1.1019,
      "step": 1437
    },
    {
      "epoch": 13.443265546713823,
      "grad_norm": 0.3372206687927246,
      "learning_rate": 5.149329189462905e-06,
      "loss": 1.137,
      "step": 1438
    },
    {
      "epoch": 13.452696728558797,
      "grad_norm": 0.33945417404174805,
      "learning_rate": 5.13609499331473e-06,
      "loss": 1.1333,
      "step": 1439
    },
    {
      "epoch": 13.462127910403773,
      "grad_norm": 0.33855709433555603,
      "learning_rate": 5.122871946490773e-06,
      "loss": 1.1344,
      "step": 1440
    },
    {
      "epoch": 13.471559092248748,
      "grad_norm": 0.33940327167510986,
      "learning_rate": 5.109660079301668e-06,
      "loss": 1.1584,
      "step": 1441
    },
    {
      "epoch": 13.480990274093722,
      "grad_norm": 0.3688458800315857,
      "learning_rate": 5.09645942203242e-06,
      "loss": 1.1013,
      "step": 1442
    },
    {
      "epoch": 13.490421455938698,
      "grad_norm": 0.3480754494667053,
      "learning_rate": 5.08327000494234e-06,
      "loss": 1.0979,
      "step": 1443
    },
    {
      "epoch": 13.499852637783672,
      "grad_norm": 0.3610903322696686,
      "learning_rate": 5.070091858264974e-06,
      "loss": 1.1185,
      "step": 1444
    },
    {
      "epoch": 13.509283819628648,
      "grad_norm": 0.3105583190917969,
      "learning_rate": 5.05692501220802e-06,
      "loss": 1.1277,
      "step": 1445
    },
    {
      "epoch": 13.518715001473621,
      "grad_norm": 0.3608260452747345,
      "learning_rate": 5.043769496953299e-06,
      "loss": 1.1067,
      "step": 1446
    },
    {
      "epoch": 13.528146183318597,
      "grad_norm": 0.34115222096443176,
      "learning_rate": 5.030625342656636e-06,
      "loss": 1.1287,
      "step": 1447
    },
    {
      "epoch": 13.537577365163571,
      "grad_norm": 0.33579736948013306,
      "learning_rate": 5.0174925794478205e-06,
      "loss": 1.1009,
      "step": 1448
    },
    {
      "epoch": 13.547008547008547,
      "grad_norm": 0.350601464509964,
      "learning_rate": 5.004371237430532e-06,
      "loss": 1.1285,
      "step": 1449
    },
    {
      "epoch": 13.556439728853523,
      "grad_norm": 0.37602609395980835,
      "learning_rate": 4.991261346682272e-06,
      "loss": 1.1428,
      "step": 1450
    },
    {
      "epoch": 13.565870910698496,
      "grad_norm": 0.3577360212802887,
      "learning_rate": 4.978162937254289e-06,
      "loss": 1.073,
      "step": 1451
    },
    {
      "epoch": 13.575302092543472,
      "grad_norm": 0.35375741124153137,
      "learning_rate": 4.9650760391715124e-06,
      "loss": 1.1057,
      "step": 1452
    },
    {
      "epoch": 13.584733274388446,
      "grad_norm": 0.3543306291103363,
      "learning_rate": 4.9520006824324865e-06,
      "loss": 1.13,
      "step": 1453
    },
    {
      "epoch": 13.594164456233422,
      "grad_norm": 0.3581714928150177,
      "learning_rate": 4.9389368970093e-06,
      "loss": 1.0877,
      "step": 1454
    },
    {
      "epoch": 13.603595638078396,
      "grad_norm": 0.37164852023124695,
      "learning_rate": 4.9258847128475185e-06,
      "loss": 1.1083,
      "step": 1455
    },
    {
      "epoch": 13.613026819923371,
      "grad_norm": 0.32438763976097107,
      "learning_rate": 4.912844159866113e-06,
      "loss": 1.1037,
      "step": 1456
    },
    {
      "epoch": 13.622458001768347,
      "grad_norm": 0.35473933815956116,
      "learning_rate": 4.899815267957395e-06,
      "loss": 1.1316,
      "step": 1457
    },
    {
      "epoch": 13.631889183613321,
      "grad_norm": 0.3378909230232239,
      "learning_rate": 4.886798066986937e-06,
      "loss": 1.1196,
      "step": 1458
    },
    {
      "epoch": 13.641320365458297,
      "grad_norm": 0.36544010043144226,
      "learning_rate": 4.873792586793524e-06,
      "loss": 1.1059,
      "step": 1459
    },
    {
      "epoch": 13.65075154730327,
      "grad_norm": 0.32906296849250793,
      "learning_rate": 4.8607988571890684e-06,
      "loss": 1.1109,
      "step": 1460
    },
    {
      "epoch": 13.660182729148246,
      "grad_norm": 0.3344363868236542,
      "learning_rate": 4.847816907958549e-06,
      "loss": 1.1466,
      "step": 1461
    },
    {
      "epoch": 13.66961391099322,
      "grad_norm": 0.337738573551178,
      "learning_rate": 4.83484676885994e-06,
      "loss": 1.1309,
      "step": 1462
    },
    {
      "epoch": 13.679045092838196,
      "grad_norm": 0.3393211364746094,
      "learning_rate": 4.821888469624147e-06,
      "loss": 1.1268,
      "step": 1463
    },
    {
      "epoch": 13.688476274683172,
      "grad_norm": 0.3642624616622925,
      "learning_rate": 4.808942039954923e-06,
      "loss": 1.108,
      "step": 1464
    },
    {
      "epoch": 13.697907456528146,
      "grad_norm": 0.3335614800453186,
      "learning_rate": 4.796007509528837e-06,
      "loss": 1.1141,
      "step": 1465
    },
    {
      "epoch": 13.707338638373121,
      "grad_norm": 0.34638094902038574,
      "learning_rate": 4.783084907995156e-06,
      "loss": 1.085,
      "step": 1466
    },
    {
      "epoch": 13.716769820218095,
      "grad_norm": 0.368318647146225,
      "learning_rate": 4.770174264975816e-06,
      "loss": 1.1056,
      "step": 1467
    },
    {
      "epoch": 13.726201002063071,
      "grad_norm": 0.322886198759079,
      "learning_rate": 4.7572756100653465e-06,
      "loss": 1.0835,
      "step": 1468
    },
    {
      "epoch": 13.735632183908045,
      "grad_norm": 0.35313138365745544,
      "learning_rate": 4.744388972830776e-06,
      "loss": 1.0989,
      "step": 1469
    },
    {
      "epoch": 13.74506336575302,
      "grad_norm": 0.3702242374420166,
      "learning_rate": 4.731514382811615e-06,
      "loss": 1.1213,
      "step": 1470
    },
    {
      "epoch": 13.754494547597997,
      "grad_norm": 0.373725026845932,
      "learning_rate": 4.718651869519732e-06,
      "loss": 1.0778,
      "step": 1471
    },
    {
      "epoch": 13.76392572944297,
      "grad_norm": 0.36794304847717285,
      "learning_rate": 4.705801462439327e-06,
      "loss": 1.0834,
      "step": 1472
    },
    {
      "epoch": 13.773356911287946,
      "grad_norm": 0.33722856640815735,
      "learning_rate": 4.692963191026846e-06,
      "loss": 1.0929,
      "step": 1473
    },
    {
      "epoch": 13.78278809313292,
      "grad_norm": 0.3581531047821045,
      "learning_rate": 4.680137084710916e-06,
      "loss": 1.1553,
      "step": 1474
    },
    {
      "epoch": 13.792219274977896,
      "grad_norm": 0.3407718241214752,
      "learning_rate": 4.667323172892282e-06,
      "loss": 1.1077,
      "step": 1475
    },
    {
      "epoch": 13.80165045682287,
      "grad_norm": 0.3218883275985718,
      "learning_rate": 4.654521484943735e-06,
      "loss": 1.0829,
      "step": 1476
    },
    {
      "epoch": 13.811081638667845,
      "grad_norm": 0.34184154868125916,
      "learning_rate": 4.641732050210032e-06,
      "loss": 1.113,
      "step": 1477
    },
    {
      "epoch": 13.820512820512821,
      "grad_norm": 0.3441900908946991,
      "learning_rate": 4.628954898007871e-06,
      "loss": 1.1255,
      "step": 1478
    },
    {
      "epoch": 13.829944002357795,
      "grad_norm": 0.3819361925125122,
      "learning_rate": 4.6161900576257725e-06,
      "loss": 1.1221,
      "step": 1479
    },
    {
      "epoch": 13.83937518420277,
      "grad_norm": 0.35164374113082886,
      "learning_rate": 4.603437558324031e-06,
      "loss": 1.1387,
      "step": 1480
    },
    {
      "epoch": 13.848806366047745,
      "grad_norm": 0.33713796734809875,
      "learning_rate": 4.59069742933468e-06,
      "loss": 1.1203,
      "step": 1481
    },
    {
      "epoch": 13.85823754789272,
      "grad_norm": 0.3546009659767151,
      "learning_rate": 4.5779696998613685e-06,
      "loss": 1.1487,
      "step": 1482
    },
    {
      "epoch": 13.867668729737694,
      "grad_norm": 0.34043604135513306,
      "learning_rate": 4.565254399079336e-06,
      "loss": 1.1284,
      "step": 1483
    },
    {
      "epoch": 13.87709991158267,
      "grad_norm": 0.3617907166481018,
      "learning_rate": 4.552551556135331e-06,
      "loss": 1.0887,
      "step": 1484
    },
    {
      "epoch": 13.886531093427646,
      "grad_norm": 0.345863401889801,
      "learning_rate": 4.539861200147543e-06,
      "loss": 1.1301,
      "step": 1485
    },
    {
      "epoch": 13.89596227527262,
      "grad_norm": 0.3643668591976166,
      "learning_rate": 4.527183360205541e-06,
      "loss": 1.1515,
      "step": 1486
    },
    {
      "epoch": 13.905393457117595,
      "grad_norm": 0.3604121804237366,
      "learning_rate": 4.514518065370206e-06,
      "loss": 1.1022,
      "step": 1487
    },
    {
      "epoch": 13.91482463896257,
      "grad_norm": 0.3606414794921875,
      "learning_rate": 4.501865344673648e-06,
      "loss": 1.111,
      "step": 1488
    },
    {
      "epoch": 13.924255820807545,
      "grad_norm": 0.3421229124069214,
      "learning_rate": 4.4892252271191795e-06,
      "loss": 1.0948,
      "step": 1489
    },
    {
      "epoch": 13.933687002652519,
      "grad_norm": 0.3499187231063843,
      "learning_rate": 4.476597741681196e-06,
      "loss": 1.0942,
      "step": 1490
    },
    {
      "epoch": 13.943118184497495,
      "grad_norm": 0.3793952763080597,
      "learning_rate": 4.463982917305155e-06,
      "loss": 1.0983,
      "step": 1491
    },
    {
      "epoch": 13.95254936634247,
      "grad_norm": 0.3380388617515564,
      "learning_rate": 4.451380782907488e-06,
      "loss": 1.0896,
      "step": 1492
    },
    {
      "epoch": 13.961980548187444,
      "grad_norm": 0.3230430781841278,
      "learning_rate": 4.4387913673755315e-06,
      "loss": 1.0815,
      "step": 1493
    },
    {
      "epoch": 13.97141173003242,
      "grad_norm": 0.36786848306655884,
      "learning_rate": 4.4262146995674775e-06,
      "loss": 1.0918,
      "step": 1494
    },
    {
      "epoch": 13.980842911877394,
      "grad_norm": 0.3571312427520752,
      "learning_rate": 4.413650808312283e-06,
      "loss": 1.1003,
      "step": 1495
    },
    {
      "epoch": 13.99027409372237,
      "grad_norm": 0.3188042938709259,
      "learning_rate": 4.401099722409631e-06,
      "loss": 1.0976,
      "step": 1496
    },
    {
      "epoch": 13.999705275567344,
      "grad_norm": 0.3454208970069885,
      "learning_rate": 4.388561470629844e-06,
      "loss": 1.1162,
      "step": 1497
    },
    {
      "epoch": 14.0,
      "grad_norm": 1.60771644115448,
      "learning_rate": 4.376036081713829e-06,
      "loss": 1.062,
      "step": 1498
    },
    {
      "epoch": 14.009431181844976,
      "grad_norm": 0.35136184096336365,
      "learning_rate": 4.363523584373007e-06,
      "loss": 1.1213,
      "step": 1499
    },
    {
      "epoch": 14.01886236368995,
      "grad_norm": 0.3938541114330292,
      "learning_rate": 4.351024007289251e-06,
      "loss": 1.1348,
      "step": 1500
    },
    {
      "epoch": 14.028293545534925,
      "grad_norm": 0.3334864377975464,
      "learning_rate": 4.338537379114801e-06,
      "loss": 1.1386,
      "step": 1501
    },
    {
      "epoch": 14.0377247273799,
      "grad_norm": 0.34509435296058655,
      "learning_rate": 4.326063728472245e-06,
      "loss": 1.1813,
      "step": 1502
    },
    {
      "epoch": 14.047155909224875,
      "grad_norm": 0.34330466389656067,
      "learning_rate": 4.313603083954394e-06,
      "loss": 1.0649,
      "step": 1503
    },
    {
      "epoch": 14.056587091069849,
      "grad_norm": 0.3266737759113312,
      "learning_rate": 4.301155474124261e-06,
      "loss": 1.0895,
      "step": 1504
    },
    {
      "epoch": 14.066018272914825,
      "grad_norm": 0.3360856771469116,
      "learning_rate": 4.2887209275149786e-06,
      "loss": 1.1127,
      "step": 1505
    },
    {
      "epoch": 14.0754494547598,
      "grad_norm": 0.34255653619766235,
      "learning_rate": 4.276299472629735e-06,
      "loss": 1.0973,
      "step": 1506
    },
    {
      "epoch": 14.084880636604774,
      "grad_norm": 0.351218044757843,
      "learning_rate": 4.263891137941697e-06,
      "loss": 1.1126,
      "step": 1507
    },
    {
      "epoch": 14.09431181844975,
      "grad_norm": 0.36341845989227295,
      "learning_rate": 4.2514959518939816e-06,
      "loss": 1.1234,
      "step": 1508
    },
    {
      "epoch": 14.103743000294724,
      "grad_norm": 0.318293035030365,
      "learning_rate": 4.2391139428995386e-06,
      "loss": 1.0883,
      "step": 1509
    },
    {
      "epoch": 14.1131741821397,
      "grad_norm": 0.40463197231292725,
      "learning_rate": 4.22674513934113e-06,
      "loss": 1.0955,
      "step": 1510
    },
    {
      "epoch": 14.122605363984674,
      "grad_norm": 0.35617130994796753,
      "learning_rate": 4.2143895695712445e-06,
      "loss": 1.1056,
      "step": 1511
    },
    {
      "epoch": 14.13203654582965,
      "grad_norm": 0.32796725630760193,
      "learning_rate": 4.202047261912023e-06,
      "loss": 1.1397,
      "step": 1512
    },
    {
      "epoch": 14.141467727674625,
      "grad_norm": 0.3669070899486542,
      "learning_rate": 4.189718244655233e-06,
      "loss": 1.1197,
      "step": 1513
    },
    {
      "epoch": 14.150898909519599,
      "grad_norm": 0.3305947482585907,
      "learning_rate": 4.177402546062146e-06,
      "loss": 1.1689,
      "step": 1514
    },
    {
      "epoch": 14.160330091364575,
      "grad_norm": 0.36043837666511536,
      "learning_rate": 4.165100194363525e-06,
      "loss": 1.0936,
      "step": 1515
    },
    {
      "epoch": 14.169761273209549,
      "grad_norm": 0.405670702457428,
      "learning_rate": 4.152811217759529e-06,
      "loss": 1.0992,
      "step": 1516
    },
    {
      "epoch": 14.179192455054524,
      "grad_norm": 0.3330479860305786,
      "learning_rate": 4.140535644419663e-06,
      "loss": 1.114,
      "step": 1517
    },
    {
      "epoch": 14.188623636899498,
      "grad_norm": 0.3295336067676544,
      "learning_rate": 4.128273502482705e-06,
      "loss": 1.1494,
      "step": 1518
    },
    {
      "epoch": 14.198054818744474,
      "grad_norm": 0.34716856479644775,
      "learning_rate": 4.116024820056649e-06,
      "loss": 1.1099,
      "step": 1519
    },
    {
      "epoch": 14.20748600058945,
      "grad_norm": 0.3763382136821747,
      "learning_rate": 4.103789625218621e-06,
      "loss": 1.1119,
      "step": 1520
    },
    {
      "epoch": 14.216917182434424,
      "grad_norm": 0.34267866611480713,
      "learning_rate": 4.091567946014858e-06,
      "loss": 1.0995,
      "step": 1521
    },
    {
      "epoch": 14.2263483642794,
      "grad_norm": 0.3565971255302429,
      "learning_rate": 4.079359810460589e-06,
      "loss": 1.1431,
      "step": 1522
    },
    {
      "epoch": 14.235779546124373,
      "grad_norm": 0.34548795223236084,
      "learning_rate": 4.06716524654001e-06,
      "loss": 1.1468,
      "step": 1523
    },
    {
      "epoch": 14.245210727969349,
      "grad_norm": 0.32870927453041077,
      "learning_rate": 4.05498428220621e-06,
      "loss": 1.1396,
      "step": 1524
    },
    {
      "epoch": 14.254641909814323,
      "grad_norm": 0.35539838671684265,
      "learning_rate": 4.042816945381088e-06,
      "loss": 1.1365,
      "step": 1525
    },
    {
      "epoch": 14.264073091659299,
      "grad_norm": 0.33437976241111755,
      "learning_rate": 4.0306632639553325e-06,
      "loss": 1.1082,
      "step": 1526
    },
    {
      "epoch": 14.273504273504274,
      "grad_norm": 0.34962207078933716,
      "learning_rate": 4.018523265788301e-06,
      "loss": 1.0935,
      "step": 1527
    },
    {
      "epoch": 14.282935455349248,
      "grad_norm": 0.3722723722457886,
      "learning_rate": 4.006396978708007e-06,
      "loss": 1.0897,
      "step": 1528
    },
    {
      "epoch": 14.292366637194224,
      "grad_norm": 0.3463619351387024,
      "learning_rate": 3.994284430511023e-06,
      "loss": 1.1251,
      "step": 1529
    },
    {
      "epoch": 14.301797819039198,
      "grad_norm": 0.35954731702804565,
      "learning_rate": 3.982185648962438e-06,
      "loss": 1.1136,
      "step": 1530
    },
    {
      "epoch": 14.311229000884174,
      "grad_norm": 0.37142932415008545,
      "learning_rate": 3.970100661795766e-06,
      "loss": 1.0967,
      "step": 1531
    },
    {
      "epoch": 14.320660182729148,
      "grad_norm": 0.3385736644268036,
      "learning_rate": 3.9580294967129285e-06,
      "loss": 1.1308,
      "step": 1532
    },
    {
      "epoch": 14.330091364574123,
      "grad_norm": 0.356680303812027,
      "learning_rate": 3.945972181384137e-06,
      "loss": 1.1081,
      "step": 1533
    },
    {
      "epoch": 14.339522546419099,
      "grad_norm": 0.3368772268295288,
      "learning_rate": 3.93392874344787e-06,
      "loss": 1.0924,
      "step": 1534
    },
    {
      "epoch": 14.348953728264073,
      "grad_norm": 0.3379823863506317,
      "learning_rate": 3.921899210510793e-06,
      "loss": 1.0878,
      "step": 1535
    },
    {
      "epoch": 14.358384910109049,
      "grad_norm": 0.35770776867866516,
      "learning_rate": 3.9098836101476955e-06,
      "loss": 1.1203,
      "step": 1536
    },
    {
      "epoch": 14.367816091954023,
      "grad_norm": 0.3625129163265228,
      "learning_rate": 3.8978819699014325e-06,
      "loss": 1.1483,
      "step": 1537
    },
    {
      "epoch": 14.377247273798998,
      "grad_norm": 0.34310105443000793,
      "learning_rate": 3.88589431728286e-06,
      "loss": 1.1079,
      "step": 1538
    },
    {
      "epoch": 14.386678455643972,
      "grad_norm": 0.33274632692337036,
      "learning_rate": 3.8739206797707615e-06,
      "loss": 1.1011,
      "step": 1539
    },
    {
      "epoch": 14.396109637488948,
      "grad_norm": 0.33831509947776794,
      "learning_rate": 3.861961084811805e-06,
      "loss": 1.1142,
      "step": 1540
    },
    {
      "epoch": 14.405540819333924,
      "grad_norm": 0.3581828474998474,
      "learning_rate": 3.850015559820465e-06,
      "loss": 1.0712,
      "step": 1541
    },
    {
      "epoch": 14.414972001178898,
      "grad_norm": 0.37382447719573975,
      "learning_rate": 3.838084132178964e-06,
      "loss": 1.1343,
      "step": 1542
    },
    {
      "epoch": 14.424403183023873,
      "grad_norm": 0.33897021412849426,
      "learning_rate": 3.826166829237215e-06,
      "loss": 1.1141,
      "step": 1543
    },
    {
      "epoch": 14.433834364868847,
      "grad_norm": 0.3608042001724243,
      "learning_rate": 3.8142636783127375e-06,
      "loss": 1.1019,
      "step": 1544
    },
    {
      "epoch": 14.443265546713823,
      "grad_norm": 0.3260062634944916,
      "learning_rate": 3.802374706690635e-06,
      "loss": 1.1024,
      "step": 1545
    },
    {
      "epoch": 14.452696728558797,
      "grad_norm": 0.3438243567943573,
      "learning_rate": 3.7904999416234866e-06,
      "loss": 1.1434,
      "step": 1546
    },
    {
      "epoch": 14.462127910403773,
      "grad_norm": 0.34604114294052124,
      "learning_rate": 3.7786394103313184e-06,
      "loss": 1.1009,
      "step": 1547
    },
    {
      "epoch": 14.471559092248748,
      "grad_norm": 0.372953861951828,
      "learning_rate": 3.7667931400015244e-06,
      "loss": 1.1498,
      "step": 1548
    },
    {
      "epoch": 14.480990274093722,
      "grad_norm": 0.36613690853118896,
      "learning_rate": 3.7549611577888156e-06,
      "loss": 1.0976,
      "step": 1549
    },
    {
      "epoch": 14.490421455938698,
      "grad_norm": 0.3253341615200043,
      "learning_rate": 3.7431434908151356e-06,
      "loss": 1.1375,
      "step": 1550
    },
    {
      "epoch": 14.499852637783672,
      "grad_norm": 0.3752797544002533,
      "learning_rate": 3.731340166169635e-06,
      "loss": 1.1059,
      "step": 1551
    },
    {
      "epoch": 14.509283819628648,
      "grad_norm": 0.32662004232406616,
      "learning_rate": 3.7195512109085684e-06,
      "loss": 1.1345,
      "step": 1552
    },
    {
      "epoch": 14.518715001473621,
      "grad_norm": 0.38950762152671814,
      "learning_rate": 3.707776652055263e-06,
      "loss": 1.1419,
      "step": 1553
    },
    {
      "epoch": 14.528146183318597,
      "grad_norm": 0.3602583706378937,
      "learning_rate": 3.696016516600047e-06,
      "loss": 1.1127,
      "step": 1554
    },
    {
      "epoch": 14.537577365163571,
      "grad_norm": 0.359601765871048,
      "learning_rate": 3.684270831500174e-06,
      "loss": 1.0784,
      "step": 1555
    },
    {
      "epoch": 14.547008547008547,
      "grad_norm": 0.31535762548446655,
      "learning_rate": 3.6725396236797937e-06,
      "loss": 1.1097,
      "step": 1556
    },
    {
      "epoch": 14.556439728853523,
      "grad_norm": 0.34746527671813965,
      "learning_rate": 3.6608229200298493e-06,
      "loss": 1.0721,
      "step": 1557
    },
    {
      "epoch": 14.565870910698496,
      "grad_norm": 0.37239280343055725,
      "learning_rate": 3.6491207474080504e-06,
      "loss": 1.1368,
      "step": 1558
    },
    {
      "epoch": 14.575302092543472,
      "grad_norm": 0.34145498275756836,
      "learning_rate": 3.6374331326387933e-06,
      "loss": 1.0911,
      "step": 1559
    },
    {
      "epoch": 14.584733274388446,
      "grad_norm": 0.3406757414340973,
      "learning_rate": 3.625760102513103e-06,
      "loss": 1.0801,
      "step": 1560
    },
    {
      "epoch": 14.594164456233422,
      "grad_norm": 0.3411194384098053,
      "learning_rate": 3.6141016837885755e-06,
      "loss": 1.1045,
      "step": 1561
    },
    {
      "epoch": 14.603595638078396,
      "grad_norm": 0.3389089107513428,
      "learning_rate": 3.6024579031893137e-06,
      "loss": 1.0968,
      "step": 1562
    },
    {
      "epoch": 14.613026819923371,
      "grad_norm": 0.37284329533576965,
      "learning_rate": 3.5908287874058566e-06,
      "loss": 1.1278,
      "step": 1563
    },
    {
      "epoch": 14.622458001768347,
      "grad_norm": 0.3459303677082062,
      "learning_rate": 3.579214363095148e-06,
      "loss": 1.1152,
      "step": 1564
    },
    {
      "epoch": 14.631889183613321,
      "grad_norm": 0.34043046832084656,
      "learning_rate": 3.5676146568804316e-06,
      "loss": 1.0887,
      "step": 1565
    },
    {
      "epoch": 14.641320365458297,
      "grad_norm": 0.3784080445766449,
      "learning_rate": 3.5560296953512296e-06,
      "loss": 1.1178,
      "step": 1566
    },
    {
      "epoch": 14.65075154730327,
      "grad_norm": 0.36441463232040405,
      "learning_rate": 3.5444595050632603e-06,
      "loss": 1.114,
      "step": 1567
    },
    {
      "epoch": 14.660182729148246,
      "grad_norm": 0.3860573470592499,
      "learning_rate": 3.532904112538381e-06,
      "loss": 1.1298,
      "step": 1568
    },
    {
      "epoch": 14.66961391099322,
      "grad_norm": 0.3433394432067871,
      "learning_rate": 3.5213635442645366e-06,
      "loss": 1.1362,
      "step": 1569
    },
    {
      "epoch": 14.679045092838196,
      "grad_norm": 0.34368088841438293,
      "learning_rate": 3.5098378266956746e-06,
      "loss": 1.1204,
      "step": 1570
    },
    {
      "epoch": 14.688476274683172,
      "grad_norm": 0.34953629970550537,
      "learning_rate": 3.4983269862517177e-06,
      "loss": 1.1265,
      "step": 1571
    },
    {
      "epoch": 14.697907456528146,
      "grad_norm": 0.36712443828582764,
      "learning_rate": 3.4868310493184777e-06,
      "loss": 1.1109,
      "step": 1572
    },
    {
      "epoch": 14.707338638373121,
      "grad_norm": 0.36869698762893677,
      "learning_rate": 3.4753500422476106e-06,
      "loss": 1.0927,
      "step": 1573
    },
    {
      "epoch": 14.716769820218095,
      "grad_norm": 0.3364191949367523,
      "learning_rate": 3.4638839913565327e-06,
      "loss": 1.1056,
      "step": 1574
    },
    {
      "epoch": 14.726201002063071,
      "grad_norm": 0.35209953784942627,
      "learning_rate": 3.4524329229284027e-06,
      "loss": 1.0952,
      "step": 1575
    },
    {
      "epoch": 14.735632183908045,
      "grad_norm": 0.34466201066970825,
      "learning_rate": 3.4409968632120127e-06,
      "loss": 1.084,
      "step": 1576
    },
    {
      "epoch": 14.74506336575302,
      "grad_norm": 0.3706128001213074,
      "learning_rate": 3.4295758384217614e-06,
      "loss": 1.0887,
      "step": 1577
    },
    {
      "epoch": 14.754494547597997,
      "grad_norm": 0.3414188325405121,
      "learning_rate": 3.4181698747375835e-06,
      "loss": 1.1046,
      "step": 1578
    },
    {
      "epoch": 14.76392572944297,
      "grad_norm": 0.3248726427555084,
      "learning_rate": 3.4067789983048882e-06,
      "loss": 1.1245,
      "step": 1579
    },
    {
      "epoch": 14.773356911287946,
      "grad_norm": 0.36411744356155396,
      "learning_rate": 3.3954032352345e-06,
      "loss": 1.1135,
      "step": 1580
    },
    {
      "epoch": 14.78278809313292,
      "grad_norm": 0.3562774062156677,
      "learning_rate": 3.384042611602605e-06,
      "loss": 1.1454,
      "step": 1581
    },
    {
      "epoch": 14.792219274977896,
      "grad_norm": 0.3781682848930359,
      "learning_rate": 3.3726971534506724e-06,
      "loss": 1.1264,
      "step": 1582
    },
    {
      "epoch": 14.80165045682287,
      "grad_norm": 0.3516963720321655,
      "learning_rate": 3.3613668867854232e-06,
      "loss": 1.1247,
      "step": 1583
    },
    {
      "epoch": 14.811081638667845,
      "grad_norm": 0.3296818137168884,
      "learning_rate": 3.3500518375787492e-06,
      "loss": 1.1522,
      "step": 1584
    },
    {
      "epoch": 14.820512820512821,
      "grad_norm": 0.3553048372268677,
      "learning_rate": 3.338752031767659e-06,
      "loss": 1.1037,
      "step": 1585
    },
    {
      "epoch": 14.829944002357795,
      "grad_norm": 0.3373537063598633,
      "learning_rate": 3.3274674952542253e-06,
      "loss": 1.1488,
      "step": 1586
    },
    {
      "epoch": 14.83937518420277,
      "grad_norm": 0.35505354404449463,
      "learning_rate": 3.316198253905505e-06,
      "loss": 1.1294,
      "step": 1587
    },
    {
      "epoch": 14.848806366047745,
      "grad_norm": 0.3587176203727722,
      "learning_rate": 3.304944333553516e-06,
      "loss": 1.1212,
      "step": 1588
    },
    {
      "epoch": 14.85823754789272,
      "grad_norm": 0.33713215589523315,
      "learning_rate": 3.2937057599951372e-06,
      "loss": 1.0998,
      "step": 1589
    },
    {
      "epoch": 14.867668729737694,
      "grad_norm": 0.3621944487094879,
      "learning_rate": 3.2824825589920784e-06,
      "loss": 1.1454,
      "step": 1590
    },
    {
      "epoch": 14.87709991158267,
      "grad_norm": 0.3224688172340393,
      "learning_rate": 3.2712747562708115e-06,
      "loss": 1.1135,
      "step": 1591
    },
    {
      "epoch": 14.886531093427646,
      "grad_norm": 0.3650525212287903,
      "learning_rate": 3.2600823775225078e-06,
      "loss": 1.1726,
      "step": 1592
    },
    {
      "epoch": 14.89596227527262,
      "grad_norm": 0.352374792098999,
      "learning_rate": 3.248905448402986e-06,
      "loss": 1.1357,
      "step": 1593
    },
    {
      "epoch": 14.905393457117595,
      "grad_norm": 0.35445305705070496,
      "learning_rate": 3.2377439945326517e-06,
      "loss": 1.1564,
      "step": 1594
    },
    {
      "epoch": 14.91482463896257,
      "grad_norm": 0.3349858224391937,
      "learning_rate": 3.226598041496426e-06,
      "loss": 1.1208,
      "step": 1595
    },
    {
      "epoch": 14.924255820807545,
      "grad_norm": 0.34068021178245544,
      "learning_rate": 3.2154676148437194e-06,
      "loss": 1.1467,
      "step": 1596
    },
    {
      "epoch": 14.933687002652519,
      "grad_norm": 0.3842608332633972,
      "learning_rate": 3.204352740088331e-06,
      "loss": 1.1537,
      "step": 1597
    },
    {
      "epoch": 14.943118184497495,
      "grad_norm": 0.33488163352012634,
      "learning_rate": 3.193253442708416e-06,
      "loss": 1.0842,
      "step": 1598
    },
    {
      "epoch": 14.95254936634247,
      "grad_norm": 0.3500538170337677,
      "learning_rate": 3.1821697481464352e-06,
      "loss": 1.1015,
      "step": 1599
    },
    {
      "epoch": 14.961980548187444,
      "grad_norm": 0.3329181671142578,
      "learning_rate": 3.171101681809067e-06,
      "loss": 1.1192,
      "step": 1600
    },
    {
      "epoch": 14.97141173003242,
      "grad_norm": 0.3501521348953247,
      "learning_rate": 3.1600492690671737e-06,
      "loss": 1.1199,
      "step": 1601
    },
    {
      "epoch": 14.980842911877394,
      "grad_norm": 0.35374051332473755,
      "learning_rate": 3.1490125352557354e-06,
      "loss": 1.1578,
      "step": 1602
    },
    {
      "epoch": 14.99027409372237,
      "grad_norm": 0.35149821639060974,
      "learning_rate": 3.137991505673792e-06,
      "loss": 1.1611,
      "step": 1603
    },
    {
      "epoch": 14.999705275567344,
      "grad_norm": 0.3659280836582184,
      "learning_rate": 3.1269862055843847e-06,
      "loss": 1.105,
      "step": 1604
    },
    {
      "epoch": 15.0,
      "grad_norm": 1.4072672128677368,
      "learning_rate": 3.115996660214502e-06,
      "loss": 0.7725,
      "step": 1605
    },
    {
      "epoch": 15.009431181844976,
      "grad_norm": 0.340688019990921,
      "learning_rate": 3.105022894755003e-06,
      "loss": 1.0861,
      "step": 1606
    },
    {
      "epoch": 15.01886236368995,
      "grad_norm": 0.36730918288230896,
      "learning_rate": 3.0940649343606033e-06,
      "loss": 1.1073,
      "step": 1607
    },
    {
      "epoch": 15.028293545534925,
      "grad_norm": 0.35178643465042114,
      "learning_rate": 3.0831228041497617e-06,
      "loss": 1.1029,
      "step": 1608
    },
    {
      "epoch": 15.0377247273799,
      "grad_norm": 0.38027024269104004,
      "learning_rate": 3.072196529204665e-06,
      "loss": 1.1104,
      "step": 1609
    },
    {
      "epoch": 15.047155909224875,
      "grad_norm": 0.36249473690986633,
      "learning_rate": 3.061286134571152e-06,
      "loss": 1.114,
      "step": 1610
    },
    {
      "epoch": 15.056587091069849,
      "grad_norm": 0.4128088355064392,
      "learning_rate": 3.050391645258661e-06,
      "loss": 1.1212,
      "step": 1611
    },
    {
      "epoch": 15.066018272914825,
      "grad_norm": 0.3456946015357971,
      "learning_rate": 3.039513086240172e-06,
      "loss": 1.1344,
      "step": 1612
    },
    {
      "epoch": 15.0754494547598,
      "grad_norm": 0.31603121757507324,
      "learning_rate": 3.0286504824521424e-06,
      "loss": 1.0944,
      "step": 1613
    },
    {
      "epoch": 15.084880636604774,
      "grad_norm": 0.34002959728240967,
      "learning_rate": 3.0178038587944614e-06,
      "loss": 1.1487,
      "step": 1614
    },
    {
      "epoch": 15.09431181844975,
      "grad_norm": 0.36576294898986816,
      "learning_rate": 3.0069732401303877e-06,
      "loss": 1.1271,
      "step": 1615
    },
    {
      "epoch": 15.103743000294724,
      "grad_norm": 0.37929442524909973,
      "learning_rate": 2.9961586512864947e-06,
      "loss": 1.1307,
      "step": 1616
    },
    {
      "epoch": 15.1131741821397,
      "grad_norm": 0.37773844599723816,
      "learning_rate": 2.9853601170526e-06,
      "loss": 1.1366,
      "step": 1617
    },
    {
      "epoch": 15.122605363984674,
      "grad_norm": 0.33048635721206665,
      "learning_rate": 2.974577662181738e-06,
      "loss": 1.1031,
      "step": 1618
    },
    {
      "epoch": 15.13203654582965,
      "grad_norm": 0.33881351351737976,
      "learning_rate": 2.963811311390068e-06,
      "loss": 1.0841,
      "step": 1619
    },
    {
      "epoch": 15.141467727674625,
      "grad_norm": 0.3667095899581909,
      "learning_rate": 2.953061089356842e-06,
      "loss": 1.1435,
      "step": 1620
    },
    {
      "epoch": 15.150898909519599,
      "grad_norm": 0.393202006816864,
      "learning_rate": 2.9423270207243436e-06,
      "loss": 1.1246,
      "step": 1621
    },
    {
      "epoch": 15.160330091364575,
      "grad_norm": 0.34361159801483154,
      "learning_rate": 2.9316091300978233e-06,
      "loss": 1.1508,
      "step": 1622
    },
    {
      "epoch": 15.169761273209549,
      "grad_norm": 0.4088674485683441,
      "learning_rate": 2.9209074420454496e-06,
      "loss": 1.0818,
      "step": 1623
    },
    {
      "epoch": 15.179192455054524,
      "grad_norm": 0.3337348401546478,
      "learning_rate": 2.9102219810982547e-06,
      "loss": 1.1253,
      "step": 1624
    },
    {
      "epoch": 15.188623636899498,
      "grad_norm": 0.35035043954849243,
      "learning_rate": 2.899552771750059e-06,
      "loss": 1.1178,
      "step": 1625
    },
    {
      "epoch": 15.198054818744474,
      "grad_norm": 0.3859728276729584,
      "learning_rate": 2.888899838457455e-06,
      "loss": 1.0834,
      "step": 1626
    },
    {
      "epoch": 15.20748600058945,
      "grad_norm": 0.36818957328796387,
      "learning_rate": 2.878263205639703e-06,
      "loss": 1.0984,
      "step": 1627
    },
    {
      "epoch": 15.216917182434424,
      "grad_norm": 0.3658128082752228,
      "learning_rate": 2.8676428976787084e-06,
      "loss": 1.088,
      "step": 1628
    },
    {
      "epoch": 15.2263483642794,
      "grad_norm": 0.3482513725757599,
      "learning_rate": 2.85703893891896e-06,
      "loss": 1.1254,
      "step": 1629
    },
    {
      "epoch": 15.235779546124373,
      "grad_norm": 0.35254690051078796,
      "learning_rate": 2.8464513536674567e-06,
      "loss": 1.0975,
      "step": 1630
    },
    {
      "epoch": 15.245210727969349,
      "grad_norm": 0.36266961693763733,
      "learning_rate": 2.835880166193683e-06,
      "loss": 1.1136,
      "step": 1631
    },
    {
      "epoch": 15.254641909814323,
      "grad_norm": 0.3400875926017761,
      "learning_rate": 2.8253254007295182e-06,
      "loss": 1.1211,
      "step": 1632
    },
    {
      "epoch": 15.264073091659299,
      "grad_norm": 0.3596057891845703,
      "learning_rate": 2.814787081469209e-06,
      "loss": 1.1477,
      "step": 1633
    },
    {
      "epoch": 15.273504273504274,
      "grad_norm": 0.34476661682128906,
      "learning_rate": 2.8042652325693e-06,
      "loss": 1.1086,
      "step": 1634
    },
    {
      "epoch": 15.282935455349248,
      "grad_norm": 0.3225494623184204,
      "learning_rate": 2.7937598781485808e-06,
      "loss": 1.145,
      "step": 1635
    },
    {
      "epoch": 15.292366637194224,
      "grad_norm": 0.3415917158126831,
      "learning_rate": 2.7832710422880326e-06,
      "loss": 1.1117,
      "step": 1636
    },
    {
      "epoch": 15.301797819039198,
      "grad_norm": 0.3497355878353119,
      "learning_rate": 2.772798749030774e-06,
      "loss": 1.0603,
      "step": 1637
    },
    {
      "epoch": 15.311229000884174,
      "grad_norm": 0.3777298033237457,
      "learning_rate": 2.7623430223819926e-06,
      "loss": 1.1295,
      "step": 1638
    },
    {
      "epoch": 15.320660182729148,
      "grad_norm": 0.3446938097476959,
      "learning_rate": 2.7519038863089207e-06,
      "loss": 1.1094,
      "step": 1639
    },
    {
      "epoch": 15.330091364574123,
      "grad_norm": 0.32494696974754333,
      "learning_rate": 2.7414813647407414e-06,
      "loss": 1.0839,
      "step": 1640
    },
    {
      "epoch": 15.339522546419099,
      "grad_norm": 0.35107168555259705,
      "learning_rate": 2.7310754815685627e-06,
      "loss": 1.1255,
      "step": 1641
    },
    {
      "epoch": 15.348953728264073,
      "grad_norm": 0.33943653106689453,
      "learning_rate": 2.7206862606453554e-06,
      "loss": 1.0938,
      "step": 1642
    },
    {
      "epoch": 15.358384910109049,
      "grad_norm": 0.38162264227867126,
      "learning_rate": 2.7103137257858867e-06,
      "loss": 1.085,
      "step": 1643
    },
    {
      "epoch": 15.367816091954023,
      "grad_norm": 0.3492495119571686,
      "learning_rate": 2.699957900766683e-06,
      "loss": 1.1132,
      "step": 1644
    },
    {
      "epoch": 15.377247273798998,
      "grad_norm": 0.3554603159427643,
      "learning_rate": 2.6896188093259645e-06,
      "loss": 1.1254,
      "step": 1645
    },
    {
      "epoch": 15.386678455643972,
      "grad_norm": 0.3477567434310913,
      "learning_rate": 2.6792964751635955e-06,
      "loss": 1.1595,
      "step": 1646
    },
    {
      "epoch": 15.396109637488948,
      "grad_norm": 0.3589915335178375,
      "learning_rate": 2.668990921941026e-06,
      "loss": 1.1086,
      "step": 1647
    },
    {
      "epoch": 15.405540819333924,
      "grad_norm": 0.3202964961528778,
      "learning_rate": 2.6587021732812444e-06,
      "loss": 1.1064,
      "step": 1648
    },
    {
      "epoch": 15.414972001178898,
      "grad_norm": 0.3695574104785919,
      "learning_rate": 2.6484302527687057e-06,
      "loss": 1.1345,
      "step": 1649
    },
    {
      "epoch": 15.424403183023873,
      "grad_norm": 0.3507002890110016,
      "learning_rate": 2.638175183949313e-06,
      "loss": 1.1534,
      "step": 1650
    },
    {
      "epoch": 15.433834364868847,
      "grad_norm": 0.3716087341308594,
      "learning_rate": 2.6279369903303174e-06,
      "loss": 1.1205,
      "step": 1651
    },
    {
      "epoch": 15.443265546713823,
      "grad_norm": 0.3843843936920166,
      "learning_rate": 2.617715695380302e-06,
      "loss": 1.1226,
      "step": 1652
    },
    {
      "epoch": 15.452696728558797,
      "grad_norm": 0.38515859842300415,
      "learning_rate": 2.6075113225291082e-06,
      "loss": 1.1449,
      "step": 1653
    },
    {
      "epoch": 15.462127910403773,
      "grad_norm": 0.3399384915828705,
      "learning_rate": 2.5973238951677926e-06,
      "loss": 1.175,
      "step": 1654
    },
    {
      "epoch": 15.471559092248748,
      "grad_norm": 0.34171628952026367,
      "learning_rate": 2.587153436648563e-06,
      "loss": 1.1189,
      "step": 1655
    },
    {
      "epoch": 15.480990274093722,
      "grad_norm": 0.3829136788845062,
      "learning_rate": 2.576999970284735e-06,
      "loss": 1.1281,
      "step": 1656
    },
    {
      "epoch": 15.490421455938698,
      "grad_norm": 0.38113701343536377,
      "learning_rate": 2.5668635193506664e-06,
      "loss": 1.0999,
      "step": 1657
    },
    {
      "epoch": 15.499852637783672,
      "grad_norm": 0.38662320375442505,
      "learning_rate": 2.556744107081718e-06,
      "loss": 1.1273,
      "step": 1658
    },
    {
      "epoch": 15.509283819628648,
      "grad_norm": 0.3636048138141632,
      "learning_rate": 2.546641756674192e-06,
      "loss": 1.1127,
      "step": 1659
    },
    {
      "epoch": 15.518715001473621,
      "grad_norm": 0.3445219397544861,
      "learning_rate": 2.5365564912852792e-06,
      "loss": 1.1425,
      "step": 1660
    },
    {
      "epoch": 15.528146183318597,
      "grad_norm": 0.3628080189228058,
      "learning_rate": 2.5264883340330115e-06,
      "loss": 1.1025,
      "step": 1661
    },
    {
      "epoch": 15.537577365163571,
      "grad_norm": 0.3349055349826813,
      "learning_rate": 2.5164373079961934e-06,
      "loss": 1.1191,
      "step": 1662
    },
    {
      "epoch": 15.547008547008547,
      "grad_norm": 0.322834312915802,
      "learning_rate": 2.5064034362143773e-06,
      "loss": 1.1136,
      "step": 1663
    },
    {
      "epoch": 15.556439728853523,
      "grad_norm": 0.34810563921928406,
      "learning_rate": 2.4963867416877764e-06,
      "loss": 1.1108,
      "step": 1664
    },
    {
      "epoch": 15.565870910698496,
      "grad_norm": 0.3489898443222046,
      "learning_rate": 2.4863872473772397e-06,
      "loss": 1.1211,
      "step": 1665
    },
    {
      "epoch": 15.575302092543472,
      "grad_norm": 0.3453870415687561,
      "learning_rate": 2.4764049762041874e-06,
      "loss": 1.1131,
      "step": 1666
    },
    {
      "epoch": 15.584733274388446,
      "grad_norm": 0.35650166869163513,
      "learning_rate": 2.466439951050559e-06,
      "loss": 1.1012,
      "step": 1667
    },
    {
      "epoch": 15.594164456233422,
      "grad_norm": 0.33855557441711426,
      "learning_rate": 2.456492194758754e-06,
      "loss": 1.1421,
      "step": 1668
    },
    {
      "epoch": 15.603595638078396,
      "grad_norm": 0.3623739778995514,
      "learning_rate": 2.446561730131606e-06,
      "loss": 1.0722,
      "step": 1669
    },
    {
      "epoch": 15.613026819923371,
      "grad_norm": 0.34737953543663025,
      "learning_rate": 2.4366485799322904e-06,
      "loss": 1.0932,
      "step": 1670
    },
    {
      "epoch": 15.622458001768347,
      "grad_norm": 0.3442149758338928,
      "learning_rate": 2.426752766884306e-06,
      "loss": 1.1209,
      "step": 1671
    },
    {
      "epoch": 15.631889183613321,
      "grad_norm": 0.3495137095451355,
      "learning_rate": 2.4168743136714125e-06,
      "loss": 1.1223,
      "step": 1672
    },
    {
      "epoch": 15.641320365458297,
      "grad_norm": 0.3809995651245117,
      "learning_rate": 2.4070132429375604e-06,
      "loss": 1.0821,
      "step": 1673
    },
    {
      "epoch": 15.65075154730327,
      "grad_norm": 0.3526795208454132,
      "learning_rate": 2.397169577286879e-06,
      "loss": 1.094,
      "step": 1674
    },
    {
      "epoch": 15.660182729148246,
      "grad_norm": 0.3602689206600189,
      "learning_rate": 2.3873433392835788e-06,
      "loss": 1.1422,
      "step": 1675
    },
    {
      "epoch": 15.66961391099322,
      "grad_norm": 0.3364054560661316,
      "learning_rate": 2.377534551451932e-06,
      "loss": 1.1106,
      "step": 1676
    },
    {
      "epoch": 15.679045092838196,
      "grad_norm": 0.38335609436035156,
      "learning_rate": 2.367743236276212e-06,
      "loss": 1.1369,
      "step": 1677
    },
    {
      "epoch": 15.688476274683172,
      "grad_norm": 0.3321736752986908,
      "learning_rate": 2.357969416200634e-06,
      "loss": 1.1189,
      "step": 1678
    },
    {
      "epoch": 15.697907456528146,
      "grad_norm": 0.3843548595905304,
      "learning_rate": 2.348213113629316e-06,
      "loss": 1.1345,
      "step": 1679
    },
    {
      "epoch": 15.707338638373121,
      "grad_norm": 0.38216516375541687,
      "learning_rate": 2.338474350926221e-06,
      "loss": 1.1466,
      "step": 1680
    },
    {
      "epoch": 15.716769820218095,
      "grad_norm": 0.35551631450653076,
      "learning_rate": 2.328753150415094e-06,
      "loss": 1.1169,
      "step": 1681
    },
    {
      "epoch": 15.726201002063071,
      "grad_norm": 0.35245680809020996,
      "learning_rate": 2.3190495343794463e-06,
      "loss": 1.1377,
      "step": 1682
    },
    {
      "epoch": 15.735632183908045,
      "grad_norm": 0.38224953413009644,
      "learning_rate": 2.309363525062457e-06,
      "loss": 1.1102,
      "step": 1683
    },
    {
      "epoch": 15.74506336575302,
      "grad_norm": 0.3764696419239044,
      "learning_rate": 2.29969514466696e-06,
      "loss": 1.1109,
      "step": 1684
    },
    {
      "epoch": 15.754494547597997,
      "grad_norm": 0.39266031980514526,
      "learning_rate": 2.290044415355379e-06,
      "loss": 1.1246,
      "step": 1685
    },
    {
      "epoch": 15.76392572944297,
      "grad_norm": 0.35101670026779175,
      "learning_rate": 2.280411359249668e-06,
      "loss": 1.1103,
      "step": 1686
    },
    {
      "epoch": 15.773356911287946,
      "grad_norm": 0.3307812809944153,
      "learning_rate": 2.2707959984312766e-06,
      "loss": 1.123,
      "step": 1687
    },
    {
      "epoch": 15.78278809313292,
      "grad_norm": 0.39610207080841064,
      "learning_rate": 2.261198354941092e-06,
      "loss": 1.1198,
      "step": 1688
    },
    {
      "epoch": 15.792219274977896,
      "grad_norm": 0.362895131111145,
      "learning_rate": 2.251618450779388e-06,
      "loss": 1.1649,
      "step": 1689
    },
    {
      "epoch": 15.80165045682287,
      "grad_norm": 0.375703901052475,
      "learning_rate": 2.2420563079057735e-06,
      "loss": 1.1319,
      "step": 1690
    },
    {
      "epoch": 15.811081638667845,
      "grad_norm": 0.33624517917633057,
      "learning_rate": 2.2325119482391466e-06,
      "loss": 1.1175,
      "step": 1691
    },
    {
      "epoch": 15.820512820512821,
      "grad_norm": 0.3347674310207367,
      "learning_rate": 2.222985393657635e-06,
      "loss": 1.1143,
      "step": 1692
    },
    {
      "epoch": 15.829944002357795,
      "grad_norm": 0.34878313541412354,
      "learning_rate": 2.213476665998567e-06,
      "loss": 1.1076,
      "step": 1693
    },
    {
      "epoch": 15.83937518420277,
      "grad_norm": 0.3811147212982178,
      "learning_rate": 2.2039857870583904e-06,
      "loss": 1.0811,
      "step": 1694
    },
    {
      "epoch": 15.848806366047745,
      "grad_norm": 0.34394752979278564,
      "learning_rate": 2.194512778592648e-06,
      "loss": 1.1339,
      "step": 1695
    },
    {
      "epoch": 15.85823754789272,
      "grad_norm": 0.3521111309528351,
      "learning_rate": 2.1850576623159183e-06,
      "loss": 1.1014,
      "step": 1696
    },
    {
      "epoch": 15.867668729737694,
      "grad_norm": 0.380003422498703,
      "learning_rate": 2.1756204599017663e-06,
      "loss": 1.1099,
      "step": 1697
    },
    {
      "epoch": 15.87709991158267,
      "grad_norm": 0.3538569211959839,
      "learning_rate": 2.1662011929826897e-06,
      "loss": 1.11,
      "step": 1698
    },
    {
      "epoch": 15.886531093427646,
      "grad_norm": 0.34423625469207764,
      "learning_rate": 2.1567998831500836e-06,
      "loss": 1.1568,
      "step": 1699
    },
    {
      "epoch": 15.89596227527262,
      "grad_norm": 0.36764979362487793,
      "learning_rate": 2.1474165519541657e-06,
      "loss": 1.1147,
      "step": 1700
    },
    {
      "epoch": 15.905393457117595,
      "grad_norm": 0.3479544222354889,
      "learning_rate": 2.1380512209039527e-06,
      "loss": 1.1347,
      "step": 1701
    },
    {
      "epoch": 15.91482463896257,
      "grad_norm": 0.36533689498901367,
      "learning_rate": 2.128703911467199e-06,
      "loss": 1.1277,
      "step": 1702
    },
    {
      "epoch": 15.924255820807545,
      "grad_norm": 0.3550501763820648,
      "learning_rate": 2.119374645070348e-06,
      "loss": 1.0962,
      "step": 1703
    },
    {
      "epoch": 15.933687002652519,
      "grad_norm": 0.3474718928337097,
      "learning_rate": 2.1100634430984858e-06,
      "loss": 1.106,
      "step": 1704
    },
    {
      "epoch": 15.943118184497495,
      "grad_norm": 0.34881043434143066,
      "learning_rate": 2.1007703268952783e-06,
      "loss": 1.1084,
      "step": 1705
    },
    {
      "epoch": 15.95254936634247,
      "grad_norm": 0.3447684943675995,
      "learning_rate": 2.091495317762955e-06,
      "loss": 1.1206,
      "step": 1706
    },
    {
      "epoch": 15.961980548187444,
      "grad_norm": 0.3674579858779907,
      "learning_rate": 2.0822384369622208e-06,
      "loss": 1.1043,
      "step": 1707
    },
    {
      "epoch": 15.97141173003242,
      "grad_norm": 0.35090121626853943,
      "learning_rate": 2.0729997057122353e-06,
      "loss": 1.1507,
      "step": 1708
    },
    {
      "epoch": 15.980842911877394,
      "grad_norm": 0.3386728763580322,
      "learning_rate": 2.063779145190552e-06,
      "loss": 1.0667,
      "step": 1709
    },
    {
      "epoch": 15.99027409372237,
      "grad_norm": 0.3389531970024109,
      "learning_rate": 2.0545767765330758e-06,
      "loss": 1.1281,
      "step": 1710
    },
    {
      "epoch": 15.999705275567344,
      "grad_norm": 0.3650829493999481,
      "learning_rate": 2.0453926208340003e-06,
      "loss": 1.1111,
      "step": 1711
    },
    {
      "epoch": 16.0,
      "grad_norm": 1.7831753492355347,
      "learning_rate": 2.036226699145788e-06,
      "loss": 0.8538,
      "step": 1712
    },
    {
      "epoch": 16.009431181844974,
      "grad_norm": 0.35009175539016724,
      "learning_rate": 2.0270790324790867e-06,
      "loss": 1.1093,
      "step": 1713
    },
    {
      "epoch": 16.01886236368995,
      "grad_norm": 0.33155152201652527,
      "learning_rate": 2.01794964180271e-06,
      "loss": 1.1207,
      "step": 1714
    },
    {
      "epoch": 16.028293545534925,
      "grad_norm": 0.3413483500480652,
      "learning_rate": 2.0088385480435767e-06,
      "loss": 1.089,
      "step": 1715
    },
    {
      "epoch": 16.0377247273799,
      "grad_norm": 0.36470261216163635,
      "learning_rate": 1.9997457720866554e-06,
      "loss": 1.0973,
      "step": 1716
    },
    {
      "epoch": 16.047155909224873,
      "grad_norm": 0.3420538306236267,
      "learning_rate": 1.990671334774942e-06,
      "loss": 1.1053,
      "step": 1717
    },
    {
      "epoch": 16.05658709106985,
      "grad_norm": 0.34583890438079834,
      "learning_rate": 1.9816152569093806e-06,
      "loss": 1.0919,
      "step": 1718
    },
    {
      "epoch": 16.066018272914825,
      "grad_norm": 0.3550310730934143,
      "learning_rate": 1.972577559248837e-06,
      "loss": 1.1432,
      "step": 1719
    },
    {
      "epoch": 16.0754494547598,
      "grad_norm": 0.33012548089027405,
      "learning_rate": 1.963558262510045e-06,
      "loss": 1.132,
      "step": 1720
    },
    {
      "epoch": 16.084880636604776,
      "grad_norm": 0.350027859210968,
      "learning_rate": 1.9545573873675572e-06,
      "loss": 1.0963,
      "step": 1721
    },
    {
      "epoch": 16.09431181844975,
      "grad_norm": 0.37556686997413635,
      "learning_rate": 1.945574954453702e-06,
      "loss": 1.0872,
      "step": 1722
    },
    {
      "epoch": 16.103743000294724,
      "grad_norm": 0.35171952843666077,
      "learning_rate": 1.936610984358531e-06,
      "loss": 1.1688,
      "step": 1723
    },
    {
      "epoch": 16.113174182139698,
      "grad_norm": 0.39991816878318787,
      "learning_rate": 1.9276654976297706e-06,
      "loss": 1.032,
      "step": 1724
    },
    {
      "epoch": 16.122605363984675,
      "grad_norm": 0.3280753493309021,
      "learning_rate": 1.918738514772789e-06,
      "loss": 1.1261,
      "step": 1725
    },
    {
      "epoch": 16.13203654582965,
      "grad_norm": 0.3582812249660492,
      "learning_rate": 1.9098300562505266e-06,
      "loss": 1.0993,
      "step": 1726
    },
    {
      "epoch": 16.141467727674623,
      "grad_norm": 0.3760313391685486,
      "learning_rate": 1.9009401424834695e-06,
      "loss": 1.0996,
      "step": 1727
    },
    {
      "epoch": 16.1508989095196,
      "grad_norm": 0.3278001844882965,
      "learning_rate": 1.8920687938495919e-06,
      "loss": 1.1471,
      "step": 1728
    },
    {
      "epoch": 16.160330091364575,
      "grad_norm": 0.3245578706264496,
      "learning_rate": 1.8832160306843117e-06,
      "loss": 1.1064,
      "step": 1729
    },
    {
      "epoch": 16.16976127320955,
      "grad_norm": 0.3410113751888275,
      "learning_rate": 1.8743818732804465e-06,
      "loss": 1.1113,
      "step": 1730
    },
    {
      "epoch": 16.179192455054523,
      "grad_norm": 0.38526684045791626,
      "learning_rate": 1.8655663418881587e-06,
      "loss": 1.1378,
      "step": 1731
    },
    {
      "epoch": 16.1886236368995,
      "grad_norm": 0.36144426465034485,
      "learning_rate": 1.8567694567149196e-06,
      "loss": 1.1063,
      "step": 1732
    },
    {
      "epoch": 16.198054818744474,
      "grad_norm": 0.3369310200214386,
      "learning_rate": 1.8479912379254606e-06,
      "loss": 1.1114,
      "step": 1733
    },
    {
      "epoch": 16.207486000589448,
      "grad_norm": 0.345212459564209,
      "learning_rate": 1.839231705641722e-06,
      "loss": 1.135,
      "step": 1734
    },
    {
      "epoch": 16.216917182434425,
      "grad_norm": 0.3759314715862274,
      "learning_rate": 1.8304908799428046e-06,
      "loss": 1.1193,
      "step": 1735
    },
    {
      "epoch": 16.2263483642794,
      "grad_norm": 0.3540313243865967,
      "learning_rate": 1.8217687808649431e-06,
      "loss": 1.1091,
      "step": 1736
    },
    {
      "epoch": 16.235779546124373,
      "grad_norm": 0.3581109941005707,
      "learning_rate": 1.8130654284014292e-06,
      "loss": 1.121,
      "step": 1737
    },
    {
      "epoch": 16.245210727969347,
      "grad_norm": 0.35828468203544617,
      "learning_rate": 1.8043808425025955e-06,
      "loss": 1.1523,
      "step": 1738
    },
    {
      "epoch": 16.254641909814325,
      "grad_norm": 0.37198835611343384,
      "learning_rate": 1.7957150430757498e-06,
      "loss": 1.1182,
      "step": 1739
    },
    {
      "epoch": 16.2640730916593,
      "grad_norm": 0.331823468208313,
      "learning_rate": 1.787068049985139e-06,
      "loss": 1.1443,
      "step": 1740
    },
    {
      "epoch": 16.273504273504273,
      "grad_norm": 0.3873201906681061,
      "learning_rate": 1.7784398830519002e-06,
      "loss": 1.1284,
      "step": 1741
    },
    {
      "epoch": 16.28293545534925,
      "grad_norm": 0.3405406177043915,
      "learning_rate": 1.769830562054019e-06,
      "loss": 1.0833,
      "step": 1742
    },
    {
      "epoch": 16.292366637194224,
      "grad_norm": 0.3610421419143677,
      "learning_rate": 1.7612401067262708e-06,
      "loss": 1.119,
      "step": 1743
    },
    {
      "epoch": 16.301797819039198,
      "grad_norm": 0.3430520296096802,
      "learning_rate": 1.7526685367602048e-06,
      "loss": 1.1017,
      "step": 1744
    },
    {
      "epoch": 16.311229000884172,
      "grad_norm": 0.3744744062423706,
      "learning_rate": 1.7441158718040617e-06,
      "loss": 1.15,
      "step": 1745
    },
    {
      "epoch": 16.32066018272915,
      "grad_norm": 0.3704509735107422,
      "learning_rate": 1.7355821314627564e-06,
      "loss": 1.0987,
      "step": 1746
    },
    {
      "epoch": 16.330091364574123,
      "grad_norm": 0.33928290009498596,
      "learning_rate": 1.727067335297824e-06,
      "loss": 1.1351,
      "step": 1747
    },
    {
      "epoch": 16.339522546419097,
      "grad_norm": 0.3730381429195404,
      "learning_rate": 1.7185715028273664e-06,
      "loss": 1.0786,
      "step": 1748
    },
    {
      "epoch": 16.348953728264075,
      "grad_norm": 0.362169474363327,
      "learning_rate": 1.7100946535260298e-06,
      "loss": 1.1503,
      "step": 1749
    },
    {
      "epoch": 16.35838491010905,
      "grad_norm": 0.364210844039917,
      "learning_rate": 1.7016368068249322e-06,
      "loss": 1.1835,
      "step": 1750
    },
    {
      "epoch": 16.367816091954023,
      "grad_norm": 0.3401619493961334,
      "learning_rate": 1.6931979821116417e-06,
      "loss": 1.1217,
      "step": 1751
    },
    {
      "epoch": 16.377247273798996,
      "grad_norm": 0.3602297306060791,
      "learning_rate": 1.684778198730118e-06,
      "loss": 1.1299,
      "step": 1752
    },
    {
      "epoch": 16.386678455643974,
      "grad_norm": 0.3387836813926697,
      "learning_rate": 1.6763774759806794e-06,
      "loss": 1.0994,
      "step": 1753
    },
    {
      "epoch": 16.396109637488948,
      "grad_norm": 0.3351861536502838,
      "learning_rate": 1.667995833119941e-06,
      "loss": 1.0973,
      "step": 1754
    },
    {
      "epoch": 16.405540819333922,
      "grad_norm": 0.3399495780467987,
      "learning_rate": 1.659633289360798e-06,
      "loss": 1.0995,
      "step": 1755
    },
    {
      "epoch": 16.4149720011789,
      "grad_norm": 0.37297725677490234,
      "learning_rate": 1.6512898638723496e-06,
      "loss": 1.1203,
      "step": 1756
    },
    {
      "epoch": 16.424403183023873,
      "grad_norm": 0.3857313394546509,
      "learning_rate": 1.6429655757798801e-06,
      "loss": 1.1164,
      "step": 1757
    },
    {
      "epoch": 16.433834364868847,
      "grad_norm": 0.323589026927948,
      "learning_rate": 1.6346604441648039e-06,
      "loss": 1.1017,
      "step": 1758
    },
    {
      "epoch": 16.44326554671382,
      "grad_norm": 0.3609319031238556,
      "learning_rate": 1.6263744880646248e-06,
      "loss": 1.1202,
      "step": 1759
    },
    {
      "epoch": 16.4526967285588,
      "grad_norm": 0.35221144556999207,
      "learning_rate": 1.6181077264728918e-06,
      "loss": 1.1279,
      "step": 1760
    },
    {
      "epoch": 16.462127910403773,
      "grad_norm": 0.375021368265152,
      "learning_rate": 1.6098601783391488e-06,
      "loss": 1.0942,
      "step": 1761
    },
    {
      "epoch": 16.471559092248746,
      "grad_norm": 0.333278626203537,
      "learning_rate": 1.6016318625689055e-06,
      "loss": 1.1039,
      "step": 1762
    },
    {
      "epoch": 16.480990274093724,
      "grad_norm": 0.34693455696105957,
      "learning_rate": 1.5934227980235828e-06,
      "loss": 1.1161,
      "step": 1763
    },
    {
      "epoch": 16.490421455938698,
      "grad_norm": 0.3608114421367645,
      "learning_rate": 1.5852330035204722e-06,
      "loss": 1.0826,
      "step": 1764
    },
    {
      "epoch": 16.499852637783672,
      "grad_norm": 0.33449846506118774,
      "learning_rate": 1.5770624978326943e-06,
      "loss": 1.1273,
      "step": 1765
    },
    {
      "epoch": 16.509283819628646,
      "grad_norm": 0.3372788727283478,
      "learning_rate": 1.5689112996891576e-06,
      "loss": 1.1209,
      "step": 1766
    },
    {
      "epoch": 16.518715001473623,
      "grad_norm": 0.3480289876461029,
      "learning_rate": 1.5607794277745025e-06,
      "loss": 1.1147,
      "step": 1767
    },
    {
      "epoch": 16.528146183318597,
      "grad_norm": 0.3429708182811737,
      "learning_rate": 1.552666900729085e-06,
      "loss": 1.1119,
      "step": 1768
    },
    {
      "epoch": 16.53757736516357,
      "grad_norm": 0.3606266975402832,
      "learning_rate": 1.5445737371489011e-06,
      "loss": 1.1376,
      "step": 1769
    },
    {
      "epoch": 16.54700854700855,
      "grad_norm": 0.3840964734554291,
      "learning_rate": 1.5364999555855708e-06,
      "loss": 1.1272,
      "step": 1770
    },
    {
      "epoch": 16.556439728853523,
      "grad_norm": 0.3351229727268219,
      "learning_rate": 1.5284455745462833e-06,
      "loss": 1.1401,
      "step": 1771
    },
    {
      "epoch": 16.565870910698496,
      "grad_norm": 0.3707326054573059,
      "learning_rate": 1.5204106124937567e-06,
      "loss": 1.0939,
      "step": 1772
    },
    {
      "epoch": 16.57530209254347,
      "grad_norm": 0.34324657917022705,
      "learning_rate": 1.5123950878461935e-06,
      "loss": 1.1631,
      "step": 1773
    },
    {
      "epoch": 16.584733274388448,
      "grad_norm": 0.35224759578704834,
      "learning_rate": 1.5043990189772473e-06,
      "loss": 1.1482,
      "step": 1774
    },
    {
      "epoch": 16.594164456233422,
      "grad_norm": 0.3806590735912323,
      "learning_rate": 1.4964224242159642e-06,
      "loss": 1.1112,
      "step": 1775
    },
    {
      "epoch": 16.603595638078396,
      "grad_norm": 0.3435921370983124,
      "learning_rate": 1.4884653218467571e-06,
      "loss": 1.0914,
      "step": 1776
    },
    {
      "epoch": 16.613026819923373,
      "grad_norm": 0.3314414918422699,
      "learning_rate": 1.48052773010936e-06,
      "loss": 1.1197,
      "step": 1777
    },
    {
      "epoch": 16.622458001768347,
      "grad_norm": 0.3552112281322479,
      "learning_rate": 1.472609667198771e-06,
      "loss": 1.1038,
      "step": 1778
    },
    {
      "epoch": 16.63188918361332,
      "grad_norm": 0.34535863995552063,
      "learning_rate": 1.4647111512652412e-06,
      "loss": 1.0688,
      "step": 1779
    },
    {
      "epoch": 16.641320365458295,
      "grad_norm": 0.3782249987125397,
      "learning_rate": 1.4568322004141988e-06,
      "loss": 1.1013,
      "step": 1780
    },
    {
      "epoch": 16.650751547303273,
      "grad_norm": 0.3508602976799011,
      "learning_rate": 1.4489728327062325e-06,
      "loss": 1.1302,
      "step": 1781
    },
    {
      "epoch": 16.660182729148246,
      "grad_norm": 0.3710654079914093,
      "learning_rate": 1.44113306615704e-06,
      "loss": 1.1366,
      "step": 1782
    },
    {
      "epoch": 16.66961391099322,
      "grad_norm": 0.34381216764450073,
      "learning_rate": 1.4333129187373862e-06,
      "loss": 1.1293,
      "step": 1783
    },
    {
      "epoch": 16.679045092838198,
      "grad_norm": 0.33751678466796875,
      "learning_rate": 1.4255124083730653e-06,
      "loss": 1.1416,
      "step": 1784
    },
    {
      "epoch": 16.688476274683172,
      "grad_norm": 0.3644719123840332,
      "learning_rate": 1.4177315529448622e-06,
      "loss": 1.107,
      "step": 1785
    },
    {
      "epoch": 16.697907456528146,
      "grad_norm": 0.34390977025032043,
      "learning_rate": 1.4099703702884936e-06,
      "loss": 1.1091,
      "step": 1786
    },
    {
      "epoch": 16.70733863837312,
      "grad_norm": 0.3386477530002594,
      "learning_rate": 1.4022288781946025e-06,
      "loss": 1.1407,
      "step": 1787
    },
    {
      "epoch": 16.716769820218097,
      "grad_norm": 0.37030303478240967,
      "learning_rate": 1.3945070944086759e-06,
      "loss": 1.082,
      "step": 1788
    },
    {
      "epoch": 16.72620100206307,
      "grad_norm": 0.3759078085422516,
      "learning_rate": 1.386805036631037e-06,
      "loss": 1.1511,
      "step": 1789
    },
    {
      "epoch": 16.735632183908045,
      "grad_norm": 0.38027074933052063,
      "learning_rate": 1.3791227225167902e-06,
      "loss": 1.1292,
      "step": 1790
    },
    {
      "epoch": 16.745063365753023,
      "grad_norm": 0.3780059218406677,
      "learning_rate": 1.3714601696757713e-06,
      "loss": 1.1268,
      "step": 1791
    },
    {
      "epoch": 16.754494547597997,
      "grad_norm": 0.3541305959224701,
      "learning_rate": 1.3638173956725377e-06,
      "loss": 1.1248,
      "step": 1792
    },
    {
      "epoch": 16.76392572944297,
      "grad_norm": 0.35068437457084656,
      "learning_rate": 1.35619441802629e-06,
      "loss": 1.0856,
      "step": 1793
    },
    {
      "epoch": 16.773356911287944,
      "grad_norm": 0.36488524079322815,
      "learning_rate": 1.3485912542108603e-06,
      "loss": 1.0991,
      "step": 1794
    },
    {
      "epoch": 16.782788093132922,
      "grad_norm": 0.33173027634620667,
      "learning_rate": 1.3410079216546612e-06,
      "loss": 1.1364,
      "step": 1795
    },
    {
      "epoch": 16.792219274977896,
      "grad_norm": 0.3411335051059723,
      "learning_rate": 1.3334444377406454e-06,
      "loss": 1.1001,
      "step": 1796
    },
    {
      "epoch": 16.80165045682287,
      "grad_norm": 0.3256993889808655,
      "learning_rate": 1.3259008198062674e-06,
      "loss": 1.0691,
      "step": 1797
    },
    {
      "epoch": 16.811081638667847,
      "grad_norm": 0.3815590739250183,
      "learning_rate": 1.3183770851434475e-06,
      "loss": 1.0807,
      "step": 1798
    },
    {
      "epoch": 16.82051282051282,
      "grad_norm": 0.3503197133541107,
      "learning_rate": 1.3108732509985178e-06,
      "loss": 1.1052,
      "step": 1799
    },
    {
      "epoch": 16.829944002357795,
      "grad_norm": 0.3802257180213928,
      "learning_rate": 1.3033893345722103e-06,
      "loss": 1.1229,
      "step": 1800
    },
    {
      "epoch": 16.83937518420277,
      "grad_norm": 0.36096107959747314,
      "learning_rate": 1.2959253530195837e-06,
      "loss": 1.115,
      "step": 1801
    },
    {
      "epoch": 16.848806366047747,
      "grad_norm": 0.33833277225494385,
      "learning_rate": 1.2884813234500103e-06,
      "loss": 1.1331,
      "step": 1802
    },
    {
      "epoch": 16.85823754789272,
      "grad_norm": 0.3495444655418396,
      "learning_rate": 1.2810572629271278e-06,
      "loss": 1.1276,
      "step": 1803
    },
    {
      "epoch": 16.867668729737694,
      "grad_norm": 0.33592161536216736,
      "learning_rate": 1.2736531884687908e-06,
      "loss": 1.0939,
      "step": 1804
    },
    {
      "epoch": 16.877099911582672,
      "grad_norm": 0.3294423520565033,
      "learning_rate": 1.266269117047051e-06,
      "loss": 1.0936,
      "step": 1805
    },
    {
      "epoch": 16.886531093427646,
      "grad_norm": 0.3493350148200989,
      "learning_rate": 1.258905065588103e-06,
      "loss": 1.1285,
      "step": 1806
    },
    {
      "epoch": 16.89596227527262,
      "grad_norm": 0.34495699405670166,
      "learning_rate": 1.2515610509722498e-06,
      "loss": 1.099,
      "step": 1807
    },
    {
      "epoch": 16.905393457117594,
      "grad_norm": 0.34956464171409607,
      "learning_rate": 1.2442370900338686e-06,
      "loss": 1.113,
      "step": 1808
    },
    {
      "epoch": 16.91482463896257,
      "grad_norm": 0.3257807791233063,
      "learning_rate": 1.2369331995613664e-06,
      "loss": 1.1379,
      "step": 1809
    },
    {
      "epoch": 16.924255820807545,
      "grad_norm": 0.37315237522125244,
      "learning_rate": 1.2296493962971367e-06,
      "loss": 1.1286,
      "step": 1810
    },
    {
      "epoch": 16.93368700265252,
      "grad_norm": 0.3701653480529785,
      "learning_rate": 1.2223856969375447e-06,
      "loss": 1.1171,
      "step": 1811
    },
    {
      "epoch": 16.943118184497497,
      "grad_norm": 0.3291904926300049,
      "learning_rate": 1.2151421181328539e-06,
      "loss": 1.1019,
      "step": 1812
    },
    {
      "epoch": 16.95254936634247,
      "grad_norm": 0.373241126537323,
      "learning_rate": 1.2079186764872175e-06,
      "loss": 1.0914,
      "step": 1813
    },
    {
      "epoch": 16.961980548187444,
      "grad_norm": 0.3569602370262146,
      "learning_rate": 1.2007153885586253e-06,
      "loss": 1.1081,
      "step": 1814
    },
    {
      "epoch": 16.97141173003242,
      "grad_norm": 0.3479671776294708,
      "learning_rate": 1.1935322708588703e-06,
      "loss": 1.1267,
      "step": 1815
    },
    {
      "epoch": 16.980842911877396,
      "grad_norm": 0.35279160737991333,
      "learning_rate": 1.1863693398535115e-06,
      "loss": 1.1287,
      "step": 1816
    },
    {
      "epoch": 16.99027409372237,
      "grad_norm": 0.3607306480407715,
      "learning_rate": 1.1792266119618334e-06,
      "loss": 1.1086,
      "step": 1817
    },
    {
      "epoch": 16.999705275567344,
      "grad_norm": 0.36222150921821594,
      "learning_rate": 1.172104103556806e-06,
      "loss": 1.1739,
      "step": 1818
    },
    {
      "epoch": 17.0,
      "grad_norm": 2.2313601970672607,
      "learning_rate": 1.165001830965058e-06,
      "loss": 0.3769,
      "step": 1819
    },
    {
      "epoch": 17.009431181844974,
      "grad_norm": 0.3456123471260071,
      "learning_rate": 1.1579198104668287e-06,
      "loss": 1.1167,
      "step": 1820
    },
    {
      "epoch": 17.01886236368995,
      "grad_norm": 0.32605215907096863,
      "learning_rate": 1.150858058295935e-06,
      "loss": 1.14,
      "step": 1821
    },
    {
      "epoch": 17.028293545534925,
      "grad_norm": 0.36011457443237305,
      "learning_rate": 1.1438165906397348e-06,
      "loss": 1.1398,
      "step": 1822
    },
    {
      "epoch": 17.0377247273799,
      "grad_norm": 0.33487728238105774,
      "learning_rate": 1.1367954236390843e-06,
      "loss": 1.146,
      "step": 1823
    },
    {
      "epoch": 17.047155909224873,
      "grad_norm": 0.34093713760375977,
      "learning_rate": 1.1297945733883086e-06,
      "loss": 1.1454,
      "step": 1824
    },
    {
      "epoch": 17.05658709106985,
      "grad_norm": 0.34662437438964844,
      "learning_rate": 1.122814055935164e-06,
      "loss": 1.1184,
      "step": 1825
    },
    {
      "epoch": 17.066018272914825,
      "grad_norm": 0.3423305153846741,
      "learning_rate": 1.1158538872807934e-06,
      "loss": 1.1025,
      "step": 1826
    },
    {
      "epoch": 17.0754494547598,
      "grad_norm": 0.3641010522842407,
      "learning_rate": 1.1089140833796997e-06,
      "loss": 1.0969,
      "step": 1827
    },
    {
      "epoch": 17.084880636604776,
      "grad_norm": 0.34901928901672363,
      "learning_rate": 1.1019946601397036e-06,
      "loss": 1.1239,
      "step": 1828
    },
    {
      "epoch": 17.09431181844975,
      "grad_norm": 0.39329469203948975,
      "learning_rate": 1.095095633421901e-06,
      "loss": 1.1443,
      "step": 1829
    },
    {
      "epoch": 17.103743000294724,
      "grad_norm": 0.36189329624176025,
      "learning_rate": 1.0882170190406483e-06,
      "loss": 1.1139,
      "step": 1830
    },
    {
      "epoch": 17.113174182139698,
      "grad_norm": 0.35596123337745667,
      "learning_rate": 1.0813588327634961e-06,
      "loss": 1.1377,
      "step": 1831
    },
    {
      "epoch": 17.122605363984675,
      "grad_norm": 0.37620946764945984,
      "learning_rate": 1.074521090311179e-06,
      "loss": 1.1864,
      "step": 1832
    },
    {
      "epoch": 17.13203654582965,
      "grad_norm": 0.3714730441570282,
      "learning_rate": 1.0677038073575664e-06,
      "loss": 1.18,
      "step": 1833
    },
    {
      "epoch": 17.141467727674623,
      "grad_norm": 0.37780091166496277,
      "learning_rate": 1.0609069995296217e-06,
      "loss": 1.1307,
      "step": 1834
    },
    {
      "epoch": 17.1508989095196,
      "grad_norm": 0.3392076790332794,
      "learning_rate": 1.0541306824073906e-06,
      "loss": 1.1414,
      "step": 1835
    },
    {
      "epoch": 17.160330091364575,
      "grad_norm": 0.32350391149520874,
      "learning_rate": 1.0473748715239307e-06,
      "loss": 1.102,
      "step": 1836
    },
    {
      "epoch": 17.16976127320955,
      "grad_norm": 0.355346143245697,
      "learning_rate": 1.0406395823653059e-06,
      "loss": 1.1262,
      "step": 1837
    },
    {
      "epoch": 17.179192455054523,
      "grad_norm": 0.3360930383205414,
      "learning_rate": 1.0339248303705352e-06,
      "loss": 1.1189,
      "step": 1838
    },
    {
      "epoch": 17.1886236368995,
      "grad_norm": 0.36781632900238037,
      "learning_rate": 1.0272306309315605e-06,
      "loss": 1.0959,
      "step": 1839
    },
    {
      "epoch": 17.198054818744474,
      "grad_norm": 0.3368598222732544,
      "learning_rate": 1.020556999393214e-06,
      "loss": 1.1507,
      "step": 1840
    },
    {
      "epoch": 17.207486000589448,
      "grad_norm": 0.36761733889579773,
      "learning_rate": 1.01390395105318e-06,
      "loss": 1.1395,
      "step": 1841
    },
    {
      "epoch": 17.216917182434425,
      "grad_norm": 0.34534889459609985,
      "learning_rate": 1.007271501161955e-06,
      "loss": 1.0903,
      "step": 1842
    },
    {
      "epoch": 17.2263483642794,
      "grad_norm": 0.3851577341556549,
      "learning_rate": 1.0006596649228329e-06,
      "loss": 1.0429,
      "step": 1843
    },
    {
      "epoch": 17.235779546124373,
      "grad_norm": 0.35973042249679565,
      "learning_rate": 9.940684574918402e-07,
      "loss": 1.1436,
      "step": 1844
    },
    {
      "epoch": 17.245210727969347,
      "grad_norm": 0.34284982085227966,
      "learning_rate": 9.874978939777258e-07,
      "loss": 1.1079,
      "step": 1845
    },
    {
      "epoch": 17.254641909814325,
      "grad_norm": 0.36148369312286377,
      "learning_rate": 9.80947989441915e-07,
      "loss": 1.0749,
      "step": 1846
    },
    {
      "epoch": 17.2640730916593,
      "grad_norm": 0.3657117187976837,
      "learning_rate": 9.744187588984798e-07,
      "loss": 1.094,
      "step": 1847
    },
    {
      "epoch": 17.273504273504273,
      "grad_norm": 0.37318217754364014,
      "learning_rate": 9.679102173140953e-07,
      "loss": 1.0805,
      "step": 1848
    },
    {
      "epoch": 17.28293545534925,
      "grad_norm": 0.37210139632225037,
      "learning_rate": 9.614223796080201e-07,
      "loss": 1.1021,
      "step": 1849
    },
    {
      "epoch": 17.292366637194224,
      "grad_norm": 0.3415978252887726,
      "learning_rate": 9.549552606520496e-07,
      "loss": 1.1106,
      "step": 1850
    },
    {
      "epoch": 17.301797819039198,
      "grad_norm": 0.3326810300350189,
      "learning_rate": 9.485088752704885e-07,
      "loss": 1.1647,
      "step": 1851
    },
    {
      "epoch": 17.311229000884172,
      "grad_norm": 0.3328113555908203,
      "learning_rate": 9.420832382401158e-07,
      "loss": 1.1513,
      "step": 1852
    },
    {
      "epoch": 17.32066018272915,
      "grad_norm": 0.35919007658958435,
      "learning_rate": 9.356783642901413e-07,
      "loss": 1.0768,
      "step": 1853
    },
    {
      "epoch": 17.330091364574123,
      "grad_norm": 0.41229695081710815,
      "learning_rate": 9.292942681021955e-07,
      "loss": 1.1296,
      "step": 1854
    },
    {
      "epoch": 17.339522546419097,
      "grad_norm": 0.3518863022327423,
      "learning_rate": 9.229309643102679e-07,
      "loss": 1.0759,
      "step": 1855
    },
    {
      "epoch": 17.348953728264075,
      "grad_norm": 0.3404510021209717,
      "learning_rate": 9.165884675006931e-07,
      "loss": 1.1437,
      "step": 1856
    },
    {
      "epoch": 17.35838491010905,
      "grad_norm": 0.3324368894100189,
      "learning_rate": 9.102667922121078e-07,
      "loss": 1.1171,
      "step": 1857
    },
    {
      "epoch": 17.367816091954023,
      "grad_norm": 0.36940711736679077,
      "learning_rate": 9.039659529354227e-07,
      "loss": 1.1533,
      "step": 1858
    },
    {
      "epoch": 17.377247273798996,
      "grad_norm": 0.38266098499298096,
      "learning_rate": 8.976859641137847e-07,
      "loss": 1.1229,
      "step": 1859
    },
    {
      "epoch": 17.386678455643974,
      "grad_norm": 0.3391178250312805,
      "learning_rate": 8.914268401425496e-07,
      "loss": 1.1054,
      "step": 1860
    },
    {
      "epoch": 17.396109637488948,
      "grad_norm": 0.3732472360134125,
      "learning_rate": 8.851885953692374e-07,
      "loss": 1.0959,
      "step": 1861
    },
    {
      "epoch": 17.405540819333922,
      "grad_norm": 0.3220933973789215,
      "learning_rate": 8.789712440935216e-07,
      "loss": 1.1072,
      "step": 1862
    },
    {
      "epoch": 17.4149720011789,
      "grad_norm": 0.35580775141716003,
      "learning_rate": 8.727748005671676e-07,
      "loss": 1.1061,
      "step": 1863
    },
    {
      "epoch": 17.424403183023873,
      "grad_norm": 0.36041074991226196,
      "learning_rate": 8.665992789940247e-07,
      "loss": 1.1113,
      "step": 1864
    },
    {
      "epoch": 17.433834364868847,
      "grad_norm": 0.3479507565498352,
      "learning_rate": 8.604446935299804e-07,
      "loss": 1.151,
      "step": 1865
    },
    {
      "epoch": 17.44326554671382,
      "grad_norm": 0.36747369170188904,
      "learning_rate": 8.543110582829272e-07,
      "loss": 1.1009,
      "step": 1866
    },
    {
      "epoch": 17.4526967285588,
      "grad_norm": 0.3599088788032532,
      "learning_rate": 8.481983873127442e-07,
      "loss": 1.0998,
      "step": 1867
    },
    {
      "epoch": 17.462127910403773,
      "grad_norm": 0.34373706579208374,
      "learning_rate": 8.421066946312462e-07,
      "loss": 1.096,
      "step": 1868
    },
    {
      "epoch": 17.471559092248746,
      "grad_norm": 0.3265552818775177,
      "learning_rate": 8.360359942021623e-07,
      "loss": 1.1391,
      "step": 1869
    },
    {
      "epoch": 17.480990274093724,
      "grad_norm": 0.33739030361175537,
      "learning_rate": 8.299862999411046e-07,
      "loss": 1.1588,
      "step": 1870
    },
    {
      "epoch": 17.490421455938698,
      "grad_norm": 0.3176223039627075,
      "learning_rate": 8.239576257155335e-07,
      "loss": 1.1321,
      "step": 1871
    },
    {
      "epoch": 17.499852637783672,
      "grad_norm": 0.3764789402484894,
      "learning_rate": 8.179499853447192e-07,
      "loss": 1.0795,
      "step": 1872
    },
    {
      "epoch": 17.509283819628646,
      "grad_norm": 0.3730662763118744,
      "learning_rate": 8.119633925997283e-07,
      "loss": 1.0693,
      "step": 1873
    },
    {
      "epoch": 17.518715001473623,
      "grad_norm": 0.36018553376197815,
      "learning_rate": 8.059978612033714e-07,
      "loss": 1.1159,
      "step": 1874
    },
    {
      "epoch": 17.528146183318597,
      "grad_norm": 0.33417245745658875,
      "learning_rate": 8.000534048301856e-07,
      "loss": 1.1325,
      "step": 1875
    },
    {
      "epoch": 17.53757736516357,
      "grad_norm": 0.3654640018939972,
      "learning_rate": 7.941300371063953e-07,
      "loss": 1.0899,
      "step": 1876
    },
    {
      "epoch": 17.54700854700855,
      "grad_norm": 0.3536992371082306,
      "learning_rate": 7.882277716098896e-07,
      "loss": 1.0834,
      "step": 1877
    },
    {
      "epoch": 17.556439728853523,
      "grad_norm": 0.34292587637901306,
      "learning_rate": 7.823466218701825e-07,
      "loss": 1.15,
      "step": 1878
    },
    {
      "epoch": 17.565870910698496,
      "grad_norm": 0.3480336666107178,
      "learning_rate": 7.764866013683825e-07,
      "loss": 1.0927,
      "step": 1879
    },
    {
      "epoch": 17.57530209254347,
      "grad_norm": 0.34363463521003723,
      "learning_rate": 7.706477235371678e-07,
      "loss": 1.1249,
      "step": 1880
    },
    {
      "epoch": 17.584733274388448,
      "grad_norm": 0.32646456360816956,
      "learning_rate": 7.648300017607535e-07,
      "loss": 1.123,
      "step": 1881
    },
    {
      "epoch": 17.594164456233422,
      "grad_norm": 0.38127532601356506,
      "learning_rate": 7.590334493748575e-07,
      "loss": 1.0792,
      "step": 1882
    },
    {
      "epoch": 17.603595638078396,
      "grad_norm": 0.3493293225765228,
      "learning_rate": 7.532580796666711e-07,
      "loss": 1.1162,
      "step": 1883
    },
    {
      "epoch": 17.613026819923373,
      "grad_norm": 0.34010186791419983,
      "learning_rate": 7.475039058748334e-07,
      "loss": 1.1191,
      "step": 1884
    },
    {
      "epoch": 17.622458001768347,
      "grad_norm": 0.37203073501586914,
      "learning_rate": 7.417709411893881e-07,
      "loss": 1.1121,
      "step": 1885
    },
    {
      "epoch": 17.63188918361332,
      "grad_norm": 0.36105799674987793,
      "learning_rate": 7.360591987517762e-07,
      "loss": 1.1221,
      "step": 1886
    },
    {
      "epoch": 17.641320365458295,
      "grad_norm": 0.38499781489372253,
      "learning_rate": 7.303686916547781e-07,
      "loss": 1.1227,
      "step": 1887
    },
    {
      "epoch": 17.650751547303273,
      "grad_norm": 0.3706691563129425,
      "learning_rate": 7.24699432942505e-07,
      "loss": 1.1635,
      "step": 1888
    },
    {
      "epoch": 17.660182729148246,
      "grad_norm": 0.39753901958465576,
      "learning_rate": 7.190514356103584e-07,
      "loss": 1.1315,
      "step": 1889
    },
    {
      "epoch": 17.66961391099322,
      "grad_norm": 0.34874609112739563,
      "learning_rate": 7.134247126050065e-07,
      "loss": 1.1226,
      "step": 1890
    },
    {
      "epoch": 17.679045092838198,
      "grad_norm": 0.3395390212535858,
      "learning_rate": 7.078192768243486e-07,
      "loss": 1.1201,
      "step": 1891
    },
    {
      "epoch": 17.688476274683172,
      "grad_norm": 0.34694305062294006,
      "learning_rate": 7.022351411174866e-07,
      "loss": 1.1182,
      "step": 1892
    },
    {
      "epoch": 17.697907456528146,
      "grad_norm": 0.37385135889053345,
      "learning_rate": 6.966723182847002e-07,
      "loss": 1.1279,
      "step": 1893
    },
    {
      "epoch": 17.70733863837312,
      "grad_norm": 0.3702867329120636,
      "learning_rate": 6.911308210774137e-07,
      "loss": 1.1338,
      "step": 1894
    },
    {
      "epoch": 17.716769820218097,
      "grad_norm": 0.3514758050441742,
      "learning_rate": 6.856106621981684e-07,
      "loss": 1.1432,
      "step": 1895
    },
    {
      "epoch": 17.72620100206307,
      "grad_norm": 0.3768755793571472,
      "learning_rate": 6.80111854300588e-07,
      "loss": 1.1308,
      "step": 1896
    },
    {
      "epoch": 17.735632183908045,
      "grad_norm": 0.3500189781188965,
      "learning_rate": 6.746344099893631e-07,
      "loss": 1.1095,
      "step": 1897
    },
    {
      "epoch": 17.745063365753023,
      "grad_norm": 0.3799448609352112,
      "learning_rate": 6.691783418202036e-07,
      "loss": 1.1391,
      "step": 1898
    },
    {
      "epoch": 17.754494547597997,
      "grad_norm": 0.3324774503707886,
      "learning_rate": 6.637436622998261e-07,
      "loss": 1.1045,
      "step": 1899
    },
    {
      "epoch": 17.76392572944297,
      "grad_norm": 0.3424401879310608,
      "learning_rate": 6.583303838859167e-07,
      "loss": 1.1101,
      "step": 1900
    },
    {
      "epoch": 17.773356911287944,
      "grad_norm": 0.3639782965183258,
      "learning_rate": 6.529385189871062e-07,
      "loss": 1.066,
      "step": 1901
    },
    {
      "epoch": 17.782788093132922,
      "grad_norm": 0.3160644471645355,
      "learning_rate": 6.475680799629381e-07,
      "loss": 1.1458,
      "step": 1902
    },
    {
      "epoch": 17.792219274977896,
      "grad_norm": 0.34876593947410583,
      "learning_rate": 6.422190791238447e-07,
      "loss": 1.0672,
      "step": 1903
    },
    {
      "epoch": 17.80165045682287,
      "grad_norm": 0.34946298599243164,
      "learning_rate": 6.368915287311106e-07,
      "loss": 1.102,
      "step": 1904
    },
    {
      "epoch": 17.811081638667847,
      "grad_norm": 0.36392420530319214,
      "learning_rate": 6.315854409968614e-07,
      "loss": 1.1427,
      "step": 1905
    },
    {
      "epoch": 17.82051282051282,
      "grad_norm": 0.32863858342170715,
      "learning_rate": 6.263008280840133e-07,
      "loss": 1.0898,
      "step": 1906
    },
    {
      "epoch": 17.829944002357795,
      "grad_norm": 0.37085703015327454,
      "learning_rate": 6.210377021062619e-07,
      "loss": 1.1307,
      "step": 1907
    },
    {
      "epoch": 17.83937518420277,
      "grad_norm": 0.3588517904281616,
      "learning_rate": 6.157960751280512e-07,
      "loss": 1.1184,
      "step": 1908
    },
    {
      "epoch": 17.848806366047747,
      "grad_norm": 0.38976702094078064,
      "learning_rate": 6.105759591645366e-07,
      "loss": 1.1112,
      "step": 1909
    },
    {
      "epoch": 17.85823754789272,
      "grad_norm": 0.34900641441345215,
      "learning_rate": 6.053773661815776e-07,
      "loss": 1.1203,
      "step": 1910
    },
    {
      "epoch": 17.867668729737694,
      "grad_norm": 0.3728037178516388,
      "learning_rate": 6.002003080956819e-07,
      "loss": 1.0788,
      "step": 1911
    },
    {
      "epoch": 17.877099911582672,
      "grad_norm": 0.33726581931114197,
      "learning_rate": 5.950447967740047e-07,
      "loss": 1.092,
      "step": 1912
    },
    {
      "epoch": 17.886531093427646,
      "grad_norm": 0.3627847731113434,
      "learning_rate": 5.899108440343082e-07,
      "loss": 1.1064,
      "step": 1913
    },
    {
      "epoch": 17.89596227527262,
      "grad_norm": 0.35644492506980896,
      "learning_rate": 5.847984616449343e-07,
      "loss": 1.1082,
      "step": 1914
    },
    {
      "epoch": 17.905393457117594,
      "grad_norm": 0.3868429362773895,
      "learning_rate": 5.797076613247788e-07,
      "loss": 1.0794,
      "step": 1915
    },
    {
      "epoch": 17.91482463896257,
      "grad_norm": 0.35225382447242737,
      "learning_rate": 5.746384547432738e-07,
      "loss": 1.0867,
      "step": 1916
    },
    {
      "epoch": 17.924255820807545,
      "grad_norm": 0.3679068684577942,
      "learning_rate": 5.695908535203431e-07,
      "loss": 1.1302,
      "step": 1917
    },
    {
      "epoch": 17.93368700265252,
      "grad_norm": 0.36490029096603394,
      "learning_rate": 5.645648692263927e-07,
      "loss": 1.151,
      "step": 1918
    },
    {
      "epoch": 17.943118184497497,
      "grad_norm": 0.3330237865447998,
      "learning_rate": 5.595605133822735e-07,
      "loss": 1.0953,
      "step": 1919
    },
    {
      "epoch": 17.95254936634247,
      "grad_norm": 0.37723982334136963,
      "learning_rate": 5.545777974592581e-07,
      "loss": 1.0674,
      "step": 1920
    },
    {
      "epoch": 17.961980548187444,
      "grad_norm": 0.3757132589817047,
      "learning_rate": 5.496167328790192e-07,
      "loss": 1.095,
      "step": 1921
    },
    {
      "epoch": 17.97141173003242,
      "grad_norm": 0.32355713844299316,
      "learning_rate": 5.446773310135922e-07,
      "loss": 1.0895,
      "step": 1922
    },
    {
      "epoch": 17.980842911877396,
      "grad_norm": 0.33823758363723755,
      "learning_rate": 5.397596031853614e-07,
      "loss": 1.1372,
      "step": 1923
    },
    {
      "epoch": 17.99027409372237,
      "grad_norm": 0.37743955850601196,
      "learning_rate": 5.348635606670261e-07,
      "loss": 1.0649,
      "step": 1924
    },
    {
      "epoch": 17.999705275567344,
      "grad_norm": 0.3305041491985321,
      "learning_rate": 5.299892146815777e-07,
      "loss": 1.1074,
      "step": 1925
    },
    {
      "epoch": 18.0,
      "grad_norm": 4.092054843902588,
      "learning_rate": 5.251365764022753e-07,
      "loss": 0.2808,
      "step": 1926
    },
    {
      "epoch": 18.009431181844974,
      "grad_norm": 0.3355821371078491,
      "learning_rate": 5.203056569526177e-07,
      "loss": 1.1235,
      "step": 1927
    },
    {
      "epoch": 18.01886236368995,
      "grad_norm": 0.333711177110672,
      "learning_rate": 5.154964674063124e-07,
      "loss": 1.1214,
      "step": 1928
    },
    {
      "epoch": 18.028293545534925,
      "grad_norm": 0.3608863353729248,
      "learning_rate": 5.107090187872688e-07,
      "loss": 1.1553,
      "step": 1929
    },
    {
      "epoch": 18.0377247273799,
      "grad_norm": 0.35556331276893616,
      "learning_rate": 5.059433220695476e-07,
      "loss": 1.1053,
      "step": 1930
    },
    {
      "epoch": 18.047155909224873,
      "grad_norm": 0.3626624047756195,
      "learning_rate": 5.011993881773569e-07,
      "loss": 1.1301,
      "step": 1931
    },
    {
      "epoch": 18.05658709106985,
      "grad_norm": 0.3657814860343933,
      "learning_rate": 4.964772279850172e-07,
      "loss": 1.1614,
      "step": 1932
    },
    {
      "epoch": 18.066018272914825,
      "grad_norm": 0.3771332800388336,
      "learning_rate": 4.917768523169342e-07,
      "loss": 1.0912,
      "step": 1933
    },
    {
      "epoch": 18.0754494547598,
      "grad_norm": 0.35259658098220825,
      "learning_rate": 4.870982719475826e-07,
      "loss": 1.1375,
      "step": 1934
    },
    {
      "epoch": 18.084880636604776,
      "grad_norm": 0.338179349899292,
      "learning_rate": 4.824414976014768e-07,
      "loss": 1.0868,
      "step": 1935
    },
    {
      "epoch": 18.09431181844975,
      "grad_norm": 0.3475888967514038,
      "learning_rate": 4.778065399531395e-07,
      "loss": 1.1061,
      "step": 1936
    },
    {
      "epoch": 18.103743000294724,
      "grad_norm": 0.34295448660850525,
      "learning_rate": 4.7319340962709073e-07,
      "loss": 1.0894,
      "step": 1937
    },
    {
      "epoch": 18.113174182139698,
      "grad_norm": 0.375753790140152,
      "learning_rate": 4.6860211719781613e-07,
      "loss": 1.1028,
      "step": 1938
    },
    {
      "epoch": 18.122605363984675,
      "grad_norm": 0.34445059299468994,
      "learning_rate": 4.640326731897382e-07,
      "loss": 1.1653,
      "step": 1939
    },
    {
      "epoch": 18.13203654582965,
      "grad_norm": 0.35579654574394226,
      "learning_rate": 4.5948508807720395e-07,
      "loss": 1.1337,
      "step": 1940
    },
    {
      "epoch": 18.141467727674623,
      "grad_norm": 0.33001941442489624,
      "learning_rate": 4.549593722844492e-07,
      "loss": 1.1606,
      "step": 1941
    },
    {
      "epoch": 18.1508989095196,
      "grad_norm": 0.3527394235134125,
      "learning_rate": 4.5045553618558003e-07,
      "loss": 1.1281,
      "step": 1942
    },
    {
      "epoch": 18.160330091364575,
      "grad_norm": 0.35668039321899414,
      "learning_rate": 4.459735901045525e-07,
      "loss": 1.0921,
      "step": 1943
    },
    {
      "epoch": 18.16976127320955,
      "grad_norm": 0.3525156080722809,
      "learning_rate": 4.415135443151419e-07,
      "loss": 1.1185,
      "step": 1944
    },
    {
      "epoch": 18.179192455054523,
      "grad_norm": 0.35818174481391907,
      "learning_rate": 4.370754090409213e-07,
      "loss": 1.1461,
      "step": 1945
    },
    {
      "epoch": 18.1886236368995,
      "grad_norm": 0.37193185091018677,
      "learning_rate": 4.3265919445524384e-07,
      "loss": 1.107,
      "step": 1946
    },
    {
      "epoch": 18.198054818744474,
      "grad_norm": 0.33225804567337036,
      "learning_rate": 4.282649106812076e-07,
      "loss": 1.0929,
      "step": 1947
    },
    {
      "epoch": 18.207486000589448,
      "grad_norm": 0.3467314541339874,
      "learning_rate": 4.2389256779164945e-07,
      "loss": 1.0949,
      "step": 1948
    },
    {
      "epoch": 18.216917182434425,
      "grad_norm": 0.34683260321617126,
      "learning_rate": 4.195421758091023e-07,
      "loss": 1.1311,
      "step": 1949
    },
    {
      "epoch": 18.2263483642794,
      "grad_norm": 0.38286638259887695,
      "learning_rate": 4.152137447057869e-07,
      "loss": 1.1036,
      "step": 1950
    },
    {
      "epoch": 18.235779546124373,
      "grad_norm": 0.3538690507411957,
      "learning_rate": 4.109072844035844e-07,
      "loss": 1.0973,
      "step": 1951
    },
    {
      "epoch": 18.245210727969347,
      "grad_norm": 0.32641279697418213,
      "learning_rate": 4.0662280477400953e-07,
      "loss": 1.1594,
      "step": 1952
    },
    {
      "epoch": 18.254641909814325,
      "grad_norm": 0.3561016917228699,
      "learning_rate": 4.0236031563819834e-07,
      "loss": 1.1244,
      "step": 1953
    },
    {
      "epoch": 18.2640730916593,
      "grad_norm": 0.3844098746776581,
      "learning_rate": 3.981198267668707e-07,
      "loss": 1.0877,
      "step": 1954
    },
    {
      "epoch": 18.273504273504273,
      "grad_norm": 0.37201160192489624,
      "learning_rate": 3.939013478803222e-07,
      "loss": 1.1352,
      "step": 1955
    },
    {
      "epoch": 18.28293545534925,
      "grad_norm": 0.34975212812423706,
      "learning_rate": 3.897048886483934e-07,
      "loss": 1.0671,
      "step": 1956
    },
    {
      "epoch": 18.292366637194224,
      "grad_norm": 0.3543373942375183,
      "learning_rate": 3.855304586904518e-07,
      "loss": 1.0981,
      "step": 1957
    },
    {
      "epoch": 18.301797819039198,
      "grad_norm": 0.3400764465332031,
      "learning_rate": 3.813780675753664e-07,
      "loss": 1.0904,
      "step": 1958
    },
    {
      "epoch": 18.311229000884172,
      "grad_norm": 0.3605409264564514,
      "learning_rate": 3.7724772482149006e-07,
      "loss": 1.0647,
      "step": 1959
    },
    {
      "epoch": 18.32066018272915,
      "grad_norm": 0.3667979836463928,
      "learning_rate": 3.7313943989662926e-07,
      "loss": 1.1171,
      "step": 1960
    },
    {
      "epoch": 18.330091364574123,
      "grad_norm": 0.36142683029174805,
      "learning_rate": 3.690532222180343e-07,
      "loss": 1.1008,
      "step": 1961
    },
    {
      "epoch": 18.339522546419097,
      "grad_norm": 0.34942516684532166,
      "learning_rate": 3.649890811523693e-07,
      "loss": 1.0953,
      "step": 1962
    },
    {
      "epoch": 18.348953728264075,
      "grad_norm": 0.33546969294548035,
      "learning_rate": 3.609470260156933e-07,
      "loss": 1.1249,
      "step": 1963
    },
    {
      "epoch": 18.35838491010905,
      "grad_norm": 0.346598356962204,
      "learning_rate": 3.569270660734403e-07,
      "loss": 1.1057,
      "step": 1964
    },
    {
      "epoch": 18.367816091954023,
      "grad_norm": 0.32841527462005615,
      "learning_rate": 3.5292921054039364e-07,
      "loss": 1.1233,
      "step": 1965
    },
    {
      "epoch": 18.377247273798996,
      "grad_norm": 0.3602093756198883,
      "learning_rate": 3.4895346858066723e-07,
      "loss": 1.1056,
      "step": 1966
    },
    {
      "epoch": 18.386678455643974,
      "grad_norm": 0.3584384024143219,
      "learning_rate": 3.449998493076889e-07,
      "loss": 1.1131,
      "step": 1967
    },
    {
      "epoch": 18.396109637488948,
      "grad_norm": 0.3494839072227478,
      "learning_rate": 3.410683617841726e-07,
      "loss": 1.1611,
      "step": 1968
    },
    {
      "epoch": 18.405540819333922,
      "grad_norm": 0.38179728388786316,
      "learning_rate": 3.371590150221005e-07,
      "loss": 1.1255,
      "step": 1969
    },
    {
      "epoch": 18.4149720011789,
      "grad_norm": 0.3754485845565796,
      "learning_rate": 3.332718179827055e-07,
      "loss": 1.1156,
      "step": 1970
    },
    {
      "epoch": 18.424403183023873,
      "grad_norm": 0.34828802943229675,
      "learning_rate": 3.294067795764422e-07,
      "loss": 1.1162,
      "step": 1971
    },
    {
      "epoch": 18.433834364868847,
      "grad_norm": 0.34820252656936646,
      "learning_rate": 3.2556390866297803e-07,
      "loss": 1.0907,
      "step": 1972
    },
    {
      "epoch": 18.44326554671382,
      "grad_norm": 0.3574383556842804,
      "learning_rate": 3.2174321405116114e-07,
      "loss": 1.1372,
      "step": 1973
    },
    {
      "epoch": 18.4526967285588,
      "grad_norm": 0.37333592772483826,
      "learning_rate": 3.1794470449901026e-07,
      "loss": 1.1213,
      "step": 1974
    },
    {
      "epoch": 18.462127910403773,
      "grad_norm": 0.40119728446006775,
      "learning_rate": 3.1416838871368925e-07,
      "loss": 1.0653,
      "step": 1975
    },
    {
      "epoch": 18.471559092248746,
      "grad_norm": 0.35031282901763916,
      "learning_rate": 3.1041427535148493e-07,
      "loss": 1.1207,
      "step": 1976
    },
    {
      "epoch": 18.480990274093724,
      "grad_norm": 0.34398144483566284,
      "learning_rate": 3.066823730177948e-07,
      "loss": 1.0699,
      "step": 1977
    },
    {
      "epoch": 18.490421455938698,
      "grad_norm": 0.361163854598999,
      "learning_rate": 3.029726902671026e-07,
      "loss": 1.0999,
      "step": 1978
    },
    {
      "epoch": 18.499852637783672,
      "grad_norm": 0.34523776173591614,
      "learning_rate": 2.99285235602953e-07,
      "loss": 1.0767,
      "step": 1979
    },
    {
      "epoch": 18.509283819628646,
      "grad_norm": 0.40337690711021423,
      "learning_rate": 2.9562001747794777e-07,
      "loss": 1.0846,
      "step": 1980
    },
    {
      "epoch": 18.518715001473623,
      "grad_norm": 0.3787711262702942,
      "learning_rate": 2.9197704429370976e-07,
      "loss": 1.1164,
      "step": 1981
    },
    {
      "epoch": 18.528146183318597,
      "grad_norm": 0.33337047696113586,
      "learning_rate": 2.883563244008691e-07,
      "loss": 1.1081,
      "step": 1982
    },
    {
      "epoch": 18.53757736516357,
      "grad_norm": 0.4048815369606018,
      "learning_rate": 2.8475786609905574e-07,
      "loss": 1.1111,
      "step": 1983
    },
    {
      "epoch": 18.54700854700855,
      "grad_norm": 0.4066120386123657,
      "learning_rate": 2.811816776368592e-07,
      "loss": 1.0772,
      "step": 1984
    },
    {
      "epoch": 18.556439728853523,
      "grad_norm": 0.3580658435821533,
      "learning_rate": 2.776277672118266e-07,
      "loss": 1.0947,
      "step": 1985
    },
    {
      "epoch": 18.565870910698496,
      "grad_norm": 0.34323322772979736,
      "learning_rate": 2.7409614297043806e-07,
      "loss": 1.1053,
      "step": 1986
    },
    {
      "epoch": 18.57530209254347,
      "grad_norm": 0.3275004029273987,
      "learning_rate": 2.7058681300808463e-07,
      "loss": 1.1241,
      "step": 1987
    },
    {
      "epoch": 18.584733274388448,
      "grad_norm": 0.33105409145355225,
      "learning_rate": 2.6709978536905714e-07,
      "loss": 1.1242,
      "step": 1988
    },
    {
      "epoch": 18.594164456233422,
      "grad_norm": 0.3507016897201538,
      "learning_rate": 2.636350680465216e-07,
      "loss": 1.1315,
      "step": 1989
    },
    {
      "epoch": 18.603595638078396,
      "grad_norm": 0.3313649892807007,
      "learning_rate": 2.6019266898250183e-07,
      "loss": 1.1365,
      "step": 1990
    },
    {
      "epoch": 18.613026819923373,
      "grad_norm": 0.35817885398864746,
      "learning_rate": 2.5677259606786686e-07,
      "loss": 1.133,
      "step": 1991
    },
    {
      "epoch": 18.622458001768347,
      "grad_norm": 0.3310462534427643,
      "learning_rate": 2.5337485714230226e-07,
      "loss": 1.1018,
      "step": 1992
    },
    {
      "epoch": 18.63188918361332,
      "grad_norm": 0.3629944324493408,
      "learning_rate": 2.4999945999430343e-07,
      "loss": 1.1335,
      "step": 1993
    },
    {
      "epoch": 18.641320365458295,
      "grad_norm": 0.3326142430305481,
      "learning_rate": 2.4664641236115225e-07,
      "loss": 1.1029,
      "step": 1994
    },
    {
      "epoch": 18.650751547303273,
      "grad_norm": 0.37040233612060547,
      "learning_rate": 2.4331572192889394e-07,
      "loss": 1.1242,
      "step": 1995
    },
    {
      "epoch": 18.660182729148246,
      "grad_norm": 0.3659326732158661,
      "learning_rate": 2.400073963323335e-07,
      "loss": 1.142,
      "step": 1996
    },
    {
      "epoch": 18.66961391099322,
      "grad_norm": 0.36431434750556946,
      "learning_rate": 2.3672144315500577e-07,
      "loss": 1.1426,
      "step": 1997
    },
    {
      "epoch": 18.679045092838198,
      "grad_norm": 0.346198171377182,
      "learning_rate": 2.334578699291601e-07,
      "loss": 1.0982,
      "step": 1998
    },
    {
      "epoch": 18.688476274683172,
      "grad_norm": 0.3402194082736969,
      "learning_rate": 2.3021668413575005e-07,
      "loss": 1.1052,
      "step": 1999
    },
    {
      "epoch": 18.697907456528146,
      "grad_norm": 0.32765135169029236,
      "learning_rate": 2.2699789320440812e-07,
      "loss": 1.1243,
      "step": 2000
    },
    {
      "epoch": 18.70733863837312,
      "grad_norm": 0.3759041726589203,
      "learning_rate": 2.238015045134334e-07,
      "loss": 1.0793,
      "step": 2001
    },
    {
      "epoch": 18.716769820218097,
      "grad_norm": 0.33318087458610535,
      "learning_rate": 2.2062752538977384e-07,
      "loss": 1.1087,
      "step": 2002
    },
    {
      "epoch": 18.72620100206307,
      "grad_norm": 0.342886358499527,
      "learning_rate": 2.17475963109004e-07,
      "loss": 1.088,
      "step": 2003
    },
    {
      "epoch": 18.735632183908045,
      "grad_norm": 0.38589614629745483,
      "learning_rate": 2.1434682489532177e-07,
      "loss": 1.1108,
      "step": 2004
    },
    {
      "epoch": 18.745063365753023,
      "grad_norm": 0.33829250931739807,
      "learning_rate": 2.112401179215151e-07,
      "loss": 1.1069,
      "step": 2005
    },
    {
      "epoch": 18.754494547597997,
      "grad_norm": 0.39504528045654297,
      "learning_rate": 2.0815584930895972e-07,
      "loss": 1.177,
      "step": 2006
    },
    {
      "epoch": 18.76392572944297,
      "grad_norm": 0.34557417035102844,
      "learning_rate": 2.0509402612759356e-07,
      "loss": 1.1128,
      "step": 2007
    },
    {
      "epoch": 18.773356911287944,
      "grad_norm": 0.3704267144203186,
      "learning_rate": 2.0205465539590464e-07,
      "loss": 1.0734,
      "step": 2008
    },
    {
      "epoch": 18.782788093132922,
      "grad_norm": 0.3694431185722351,
      "learning_rate": 1.9903774408091437e-07,
      "loss": 1.1027,
      "step": 2009
    },
    {
      "epoch": 18.792219274977896,
      "grad_norm": 0.37840917706489563,
      "learning_rate": 1.9604329909815977e-07,
      "loss": 1.0902,
      "step": 2010
    },
    {
      "epoch": 18.80165045682287,
      "grad_norm": 0.3378659188747406,
      "learning_rate": 1.9307132731168355e-07,
      "loss": 1.1309,
      "step": 2011
    },
    {
      "epoch": 18.811081638667847,
      "grad_norm": 0.33159059286117554,
      "learning_rate": 1.9012183553400842e-07,
      "loss": 1.1192,
      "step": 2012
    },
    {
      "epoch": 18.82051282051282,
      "grad_norm": 0.3388337194919586,
      "learning_rate": 1.8719483052613395e-07,
      "loss": 1.1405,
      "step": 2013
    },
    {
      "epoch": 18.829944002357795,
      "grad_norm": 0.3347862958908081,
      "learning_rate": 1.8429031899750428e-07,
      "loss": 1.1419,
      "step": 2014
    },
    {
      "epoch": 18.83937518420277,
      "grad_norm": 0.3601076006889343,
      "learning_rate": 1.8140830760601468e-07,
      "loss": 1.1174,
      "step": 2015
    },
    {
      "epoch": 18.848806366047747,
      "grad_norm": 0.3676997423171997,
      "learning_rate": 1.7854880295797406e-07,
      "loss": 1.1528,
      "step": 2016
    },
    {
      "epoch": 18.85823754789272,
      "grad_norm": 0.3664761781692505,
      "learning_rate": 1.757118116081058e-07,
      "loss": 1.1437,
      "step": 2017
    },
    {
      "epoch": 18.867668729737694,
      "grad_norm": 0.37050074338912964,
      "learning_rate": 1.7289734005952464e-07,
      "loss": 1.1206,
      "step": 2018
    },
    {
      "epoch": 18.877099911582672,
      "grad_norm": 0.3549294173717499,
      "learning_rate": 1.7010539476372657e-07,
      "loss": 1.1047,
      "step": 2019
    },
    {
      "epoch": 18.886531093427646,
      "grad_norm": 0.3508317768573761,
      "learning_rate": 1.673359821205689e-07,
      "loss": 1.1399,
      "step": 2020
    },
    {
      "epoch": 18.89596227527262,
      "grad_norm": 0.3636130690574646,
      "learning_rate": 1.6458910847826025e-07,
      "loss": 1.1019,
      "step": 2021
    },
    {
      "epoch": 18.905393457117594,
      "grad_norm": 0.34951987862586975,
      "learning_rate": 1.6186478013334172e-07,
      "loss": 1.0879,
      "step": 2022
    },
    {
      "epoch": 18.91482463896257,
      "grad_norm": 0.3578709363937378,
      "learning_rate": 1.5916300333067792e-07,
      "loss": 1.1675,
      "step": 2023
    },
    {
      "epoch": 18.924255820807545,
      "grad_norm": 0.3654489517211914,
      "learning_rate": 1.564837842634359e-07,
      "loss": 1.1208,
      "step": 2024
    },
    {
      "epoch": 18.93368700265252,
      "grad_norm": 0.3399783670902252,
      "learning_rate": 1.538271290730775e-07,
      "loss": 1.1222,
      "step": 2025
    },
    {
      "epoch": 18.943118184497497,
      "grad_norm": 0.38591912388801575,
      "learning_rate": 1.5119304384934253e-07,
      "loss": 1.1258,
      "step": 2026
    },
    {
      "epoch": 18.95254936634247,
      "grad_norm": 0.3527655303478241,
      "learning_rate": 1.4858153463022994e-07,
      "loss": 1.1181,
      "step": 2027
    },
    {
      "epoch": 18.961980548187444,
      "grad_norm": 0.3355504274368286,
      "learning_rate": 1.4599260740199684e-07,
      "loss": 1.112,
      "step": 2028
    },
    {
      "epoch": 18.97141173003242,
      "grad_norm": 0.35252508521080017,
      "learning_rate": 1.434262680991283e-07,
      "loss": 1.1011,
      "step": 2029
    },
    {
      "epoch": 18.980842911877396,
      "grad_norm": 0.3728266656398773,
      "learning_rate": 1.4088252260433643e-07,
      "loss": 1.1434,
      "step": 2030
    },
    {
      "epoch": 18.99027409372237,
      "grad_norm": 0.3787881135940552,
      "learning_rate": 1.3836137674854254e-07,
      "loss": 1.1114,
      "step": 2031
    },
    {
      "epoch": 18.999705275567344,
      "grad_norm": 0.3984695374965668,
      "learning_rate": 1.3586283631086272e-07,
      "loss": 1.1283,
      "step": 2032
    },
    {
      "epoch": 19.0,
      "grad_norm": 1.846036672592163,
      "learning_rate": 1.3338690701859336e-07,
      "loss": 0.9075,
      "step": 2033
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 2140,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.902959115460936e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
